{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "hf_llama_models = { 'tokenizer': LlamaTokenizer, 'model': LlamaForCausalLM}\n",
    "hf_auto_models = { 'tokenizer': AutoTokenizer, 'model': AutoModelForCausalLM}\n",
    "\n",
    "backends = {'hf_llama': hf_llama_models, 'hf_auto': hf_auto_models}\n",
    "\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of chat templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0957a8f292a442ff97053ddeb5778e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "backend_name = 'hf_auto'\n",
    "checkpoint = \"Khawn2u/Llama-3.1-8b-Chain-Of-Thought-GGUF\"\n",
    "\n",
    "backend = backends[backend_name]\n",
    "device='cuda:0'\n",
    "tokenizer = backend['tokenizer'].from_pretrained(checkpoint,device_map=\"auto\")\n",
    "model = backend['model'].from_pretrained(checkpoint,low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a researcher. You are also a bit of a comedian.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_tokens_in: 72\n"
     ]
    }
   ],
   "source": [
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=False,add_generation_prompt=True, )\n",
    "inputs = tokenizer(tokenized_chat, return_tensors=\"pt\", padding=False, truncation=True, max_length=2500).to(device)\n",
    "del tokenized_chat\n",
    "nb_tokens_in = len(inputs[0])\n",
    "print(f\"nb_tokens_in: {nb_tokens_in}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(inputs.input_ids, top_k=32, max_new_tokens=128, pad_token_id=tokenizer.eos_token_id, output_scores=True,return_dict_in_generate=True,\n",
    "                         output_hidden_states=True, output_attentions=True, attention_mask=inputs['attention_mask'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a friendly chatbot who always responds in the style of a researcher. You are also a bit of a comedian.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How many helicopters can a human eat in one sitting?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "A question that gets to the heart of the absurd. I must admit, I've never come across any research on this topic. In fact, I'm not even sure it's possible for a human to eat a helicopter in one sitting. Helicopters are large, complex machines made of metal, plastic, and other materials, not exactly something you'd find on the menu at your local diner.\n",
      "\n",
      "However, if we assume a hypothetical scenario where a human could somehow consume a helicopter, I'd estimate the number of helicopters a person could eat in one sitting would be... zero. That's right, folks, a big fat zero. Not\n",
      "nb of new tokens: 128 \n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs['sequences'][0],skip_special_tokens=False))\n",
    "outputs.__dict__.keys()\n",
    "nb_tokens_out = len(outputs.sequences[0])\n",
    "print(f\"nb of new tokens: { nb_tokens_out-nb_tokens_in} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0010, -0.0014, -0.0027,  ...,  0.0022, -0.0040,  0.0052],\n",
      "         [-0.0010, -0.0014, -0.0027,  ...,  0.0022, -0.0040,  0.0052],\n",
      "         [-0.0014, -0.0012, -0.0019,  ..., -0.0018, -0.0037,  0.0048],\n",
      "         ...,\n",
      "         [-0.0110, -0.0044, -0.0002,  ..., -0.0088, -0.0084,  0.0126],\n",
      "         [-0.0017, -0.0013, -0.0021,  ..., -0.0023, -0.0046,  0.0047],\n",
      "         [-0.0042,  0.0030, -0.0023,  ..., -0.0008, -0.0036,  0.0063]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.hidden_states[0][0][:5]-outputs.hidden_states[8][0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of elements : 32\n",
      "Hidden states shape for generated token [0] : torch.Size([1, 72, 4096])\n",
      "Attention weights shape for generated token [0] : torch.Size([1, 32, 72, 72])\n",
      "Last layer attentions for generated token [0] : tensor([0.2471, 0.2476, 0.0012, 0.0014, 0.0008, 0.0104, 0.0011, 0.0009, 0.0013,\n",
      "        0.0019, 0.0077, 0.0013, 0.0009, 0.0012, 0.0284, 0.0297, 0.0022, 0.0029,\n",
      "        0.0009, 0.0016, 0.0009, 0.0014, 0.0110, 0.0027, 0.0119, 0.0503, 0.0099,\n",
      "        0.0026, 0.0011, 0.0026, 0.0029, 0.0064, 0.0024, 0.0027, 0.0018, 0.0025,\n",
      "        0.0010, 0.0018, 0.0012, 0.0005, 0.0066, 0.0024, 0.0024, 0.0025, 0.0025,\n",
      "        0.0009, 0.0016, 0.0009, 0.0010, 0.0058, 0.0037, 0.0023, 0.0047, 0.0137,\n",
      "        0.0028, 0.0212, 0.0036, 0.0039, 0.0104, 0.0047, 0.0037, 0.0049, 0.0091,\n",
      "        0.0052, 0.0026, 0.0030, 0.0032, 0.0150, 0.0027, 0.0438, 0.0311, 0.0701],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [1] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [1] : torch.Size([1, 32, 1, 73])\n",
      "Last layer attentions for generated token [1] : tensor([0.2384, 0.2379, 0.0005, 0.0007, 0.0003, 0.0073, 0.0007, 0.0010, 0.0015,\n",
      "        0.0015, 0.0054, 0.0006, 0.0003, 0.0007, 0.0117, 0.0140, 0.0012, 0.0014,\n",
      "        0.0007, 0.0005, 0.0003, 0.0012, 0.0068, 0.0012, 0.0072, 0.0331, 0.0034,\n",
      "        0.0012, 0.0010, 0.0031, 0.0016, 0.0030, 0.0023, 0.0030, 0.0018, 0.0013,\n",
      "        0.0006, 0.0022, 0.0012, 0.0010, 0.0056, 0.0040, 0.0012, 0.0024, 0.0036,\n",
      "        0.0012, 0.0034, 0.0011, 0.0027, 0.0069, 0.0060, 0.0011, 0.0058, 0.0110,\n",
      "        0.0029, 0.0223, 0.0023, 0.0048, 0.0069, 0.0048, 0.0033, 0.0058, 0.0068,\n",
      "        0.0056, 0.0018, 0.0021, 0.0061, 0.0147, 0.0031, 0.0558, 0.0475, 0.0900,\n",
      "        0.0545], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [2] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [2] : torch.Size([1, 32, 1, 74])\n",
      "Last layer attentions for generated token [2] : tensor([2.9712e-01, 2.9712e-01, 3.1972e-04, 6.1417e-04, 2.0325e-04, 4.7607e-03,\n",
      "        8.0061e-04, 7.2193e-04, 1.1377e-03, 1.8473e-03, 6.8970e-03, 1.0881e-03,\n",
      "        1.1606e-03, 1.6308e-03, 2.0752e-02, 1.6724e-02, 1.2846e-03, 1.1520e-03,\n",
      "        9.1982e-04, 8.5402e-04, 2.3484e-04, 8.4734e-04, 1.2871e-02, 2.5253e-03,\n",
      "        8.6899e-03, 2.9846e-02, 3.3417e-03, 1.1673e-03, 1.2159e-03, 9.8896e-04,\n",
      "        5.2643e-04, 1.7729e-03, 2.3079e-03, 1.5650e-03, 9.0027e-04, 1.9207e-03,\n",
      "        1.3571e-03, 1.4791e-03, 1.4992e-03, 1.1473e-03, 3.6068e-03, 4.2343e-03,\n",
      "        2.0447e-03, 2.5806e-03, 2.1858e-03, 1.6308e-03, 8.6260e-04, 7.1096e-04,\n",
      "        1.3752e-03, 2.6665e-03, 3.0251e-03, 4.3368e-04, 4.9171e-03, 4.3068e-03,\n",
      "        2.9659e-03, 1.3916e-02, 2.5558e-03, 6.3477e-03, 1.0246e-02, 4.9362e-03,\n",
      "        4.7188e-03, 3.2578e-03, 6.0501e-03, 5.5542e-03, 3.4828e-03, 1.6918e-03,\n",
      "        9.5215e-03, 7.5645e-03, 4.3793e-03, 2.9083e-02, 2.0538e-02, 5.1056e-02,\n",
      "        3.1891e-02, 1.2512e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [3] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [3] : torch.Size([1, 32, 1, 75])\n",
      "Last layer attentions for generated token [3] : tensor([0.2407, 0.2407, 0.0006, 0.0009, 0.0006, 0.0064, 0.0007, 0.0009, 0.0014,\n",
      "        0.0023, 0.0097, 0.0011, 0.0004, 0.0014, 0.0264, 0.0181, 0.0010, 0.0017,\n",
      "        0.0006, 0.0007, 0.0004, 0.0010, 0.0111, 0.0023, 0.0112, 0.0332, 0.0019,\n",
      "        0.0010, 0.0010, 0.0018, 0.0013, 0.0026, 0.0021, 0.0022, 0.0015, 0.0016,\n",
      "        0.0008, 0.0015, 0.0010, 0.0008, 0.0033, 0.0032, 0.0015, 0.0015, 0.0023,\n",
      "        0.0012, 0.0021, 0.0009, 0.0019, 0.0042, 0.0032, 0.0012, 0.0072, 0.0080,\n",
      "        0.0030, 0.0193, 0.0024, 0.0061, 0.0115, 0.0075, 0.0048, 0.0054, 0.0089,\n",
      "        0.0063, 0.0035, 0.0021, 0.0061, 0.0208, 0.0045, 0.0451, 0.0334, 0.0712,\n",
      "        0.0351, 0.0118, 0.0128], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [4] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [4] : torch.Size([1, 32, 1, 76])\n",
      "Last layer attentions for generated token [4] : tensor([3.4570e-01, 3.4521e-01, 4.7326e-04, 7.1907e-04, 2.2674e-04, 6.1798e-03,\n",
      "        3.4690e-04, 5.0974e-04, 1.6327e-03, 2.1172e-03, 5.3635e-03, 7.5340e-04,\n",
      "        4.3607e-04, 1.0624e-03, 1.1520e-02, 1.8372e-02, 8.4066e-04, 6.2943e-04,\n",
      "        6.5994e-04, 9.0885e-04, 3.9554e-04, 1.6394e-03, 1.5350e-02, 2.3155e-03,\n",
      "        7.8354e-03, 1.8692e-02, 1.4982e-03, 1.2569e-03, 8.3256e-04, 1.3456e-03,\n",
      "        1.0061e-03, 2.2755e-03, 1.2569e-03, 9.9850e-04, 7.1049e-04, 1.2426e-03,\n",
      "        7.4482e-04, 1.5373e-03, 9.5415e-04, 6.7949e-04, 4.2839e-03, 1.7042e-03,\n",
      "        8.7595e-04, 1.4019e-03, 1.5955e-03, 7.0667e-04, 1.4610e-03, 1.1024e-03,\n",
      "        9.5415e-04, 2.7943e-03, 2.2106e-03, 5.1165e-04, 2.8515e-03, 3.9978e-03,\n",
      "        1.8902e-03, 9.0790e-03, 1.3723e-03, 3.3798e-03, 1.0475e-02, 5.0163e-03,\n",
      "        2.3251e-03, 3.9139e-03, 5.4474e-03, 4.8294e-03, 2.0485e-03, 1.6556e-03,\n",
      "        4.1046e-03, 3.9062e-03, 3.4428e-03, 2.0645e-02, 1.2596e-02, 3.0579e-02,\n",
      "        9.6054e-03, 6.6071e-03, 7.7553e-03, 1.6617e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [5] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [5] : torch.Size([1, 32, 1, 77])\n",
      "Last layer attentions for generated token [5] : tensor([4.2017e-01, 4.2017e-01, 3.2902e-04, 6.6996e-04, 1.3506e-04, 2.8648e-03,\n",
      "        4.0233e-05, 9.4295e-05, 3.4761e-04, 7.2002e-04, 7.3433e-04, 2.1160e-04,\n",
      "        1.5914e-04, 5.0068e-04, 2.5082e-03, 4.5776e-03, 2.9373e-04, 3.2401e-04,\n",
      "        2.4652e-04, 2.9850e-04, 7.5161e-05, 9.4080e-04, 1.5701e-02, 1.3857e-03,\n",
      "        1.5726e-03, 4.6349e-03, 2.8038e-04, 1.5247e-04, 2.1577e-04, 4.3678e-04,\n",
      "        3.1400e-04, 3.7718e-04, 3.1400e-04, 2.0504e-04, 1.8024e-04, 1.8096e-04,\n",
      "        3.2401e-04, 4.2486e-04, 2.1076e-04, 1.7822e-04, 1.2245e-03, 1.0157e-03,\n",
      "        6.2561e-04, 2.0432e-04, 3.1900e-04, 2.7394e-04, 3.7861e-04, 3.8910e-04,\n",
      "        2.1076e-04, 6.3276e-04, 1.2321e-03, 2.9492e-04, 3.4027e-03, 4.9896e-03,\n",
      "        1.9836e-03, 3.7079e-03, 3.7289e-04, 1.4658e-03, 3.2597e-03, 1.2131e-03,\n",
      "        1.6575e-03, 1.1549e-03, 2.4071e-03, 2.0313e-03, 1.3247e-03, 7.0190e-04,\n",
      "        2.7714e-03, 2.6855e-03, 3.5686e-03, 1.4114e-02, 7.2098e-03, 1.9485e-02,\n",
      "        6.2332e-03, 5.6152e-03, 4.9210e-03, 4.8409e-03, 8.5373e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [6] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [6] : torch.Size([1, 32, 1, 78])\n",
      "Last layer attentions for generated token [6] : tensor([3.5498e-01, 3.5498e-01, 6.7186e-04, 5.0926e-04, 3.1996e-04, 6.0883e-03,\n",
      "        1.7321e-04, 3.3665e-04, 7.6723e-04, 8.6594e-04, 2.1858e-03, 4.1723e-04,\n",
      "        1.8954e-04, 4.8876e-04, 6.5575e-03, 8.5754e-03, 6.8235e-04, 4.5109e-04,\n",
      "        2.8777e-04, 3.1996e-04, 1.0306e-04, 6.2990e-04, 1.9913e-02, 1.6375e-03,\n",
      "        6.2447e-03, 1.0101e-02, 4.6921e-04, 1.9860e-04, 3.1734e-04, 8.0109e-04,\n",
      "        3.4189e-04, 7.4816e-04, 5.4121e-04, 3.3402e-04, 4.3130e-04, 5.9509e-04,\n",
      "        3.1996e-04, 8.5926e-04, 6.6280e-04, 2.3496e-04, 2.3785e-03, 2.1152e-03,\n",
      "        5.6124e-04, 7.2384e-04, 9.2220e-04, 4.4489e-04, 1.1139e-03, 7.7057e-04,\n",
      "        7.1812e-04, 2.2907e-03, 3.1929e-03, 5.3167e-04, 4.8218e-03, 5.7220e-03,\n",
      "        1.7223e-03, 8.7051e-03, 6.6948e-04, 1.9989e-03, 3.8509e-03, 2.1858e-03,\n",
      "        2.1057e-03, 2.4338e-03, 4.2801e-03, 2.5368e-03, 1.4305e-03, 1.0738e-03,\n",
      "        3.7327e-03, 6.8207e-03, 3.3340e-03, 2.1744e-02, 1.6296e-02, 4.2877e-02,\n",
      "        8.1863e-03, 8.6441e-03, 7.1068e-03, 1.5259e-02, 6.1798e-03, 1.5266e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [7] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [7] : torch.Size([1, 32, 1, 79])\n",
      "Last layer attentions for generated token [7] : tensor([3.0493e-01, 3.0444e-01, 1.0071e-03, 9.2983e-04, 8.2064e-04, 4.4022e-03,\n",
      "        2.8753e-04, 3.3474e-04, 5.8556e-04, 1.3294e-03, 3.4847e-03, 3.9673e-04,\n",
      "        4.4966e-04, 6.4516e-04, 8.9111e-03, 1.2955e-02, 5.6934e-04, 1.3771e-03,\n",
      "        5.5647e-04, 7.2289e-04, 9.5546e-05, 4.4441e-04, 1.5251e-02, 1.8826e-03,\n",
      "        5.4779e-03, 1.6037e-02, 1.9159e-03, 4.4346e-04, 5.2786e-04, 7.6342e-04,\n",
      "        7.8440e-04, 1.4153e-03, 9.3889e-04, 5.4550e-04, 5.2261e-04, 8.6164e-04,\n",
      "        1.1616e-03, 8.9598e-04, 9.8419e-04, 6.4659e-04, 3.6068e-03, 2.8248e-03,\n",
      "        1.8673e-03, 1.1482e-03, 1.1959e-03, 1.1368e-03, 5.4359e-04, 7.3433e-04,\n",
      "        7.5579e-04, 2.6188e-03, 3.3188e-03, 7.7248e-04, 4.1466e-03, 1.4755e-02,\n",
      "        2.8172e-03, 1.0895e-02, 1.4544e-03, 3.6182e-03, 8.0643e-03, 2.5921e-03,\n",
      "        3.4523e-03, 2.4319e-03, 4.2191e-03, 3.0403e-03, 2.6550e-03, 1.2436e-03,\n",
      "        7.9575e-03, 9.8114e-03, 3.3169e-03, 5.4993e-02, 2.7618e-02, 4.4952e-02,\n",
      "        1.3985e-02, 9.9716e-03, 8.2779e-03, 1.0033e-02, 5.0850e-03, 1.3847e-02,\n",
      "        3.4752e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [8] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [8] : torch.Size([1, 32, 1, 80])\n",
      "Last layer attentions for generated token [8] : tensor([2.4988e-01, 2.4939e-01, 3.5977e-04, 3.4189e-04, 2.6417e-04, 4.9133e-03,\n",
      "        3.7265e-04, 8.9025e-04, 1.6670e-03, 1.2512e-03, 3.5362e-03, 6.7759e-04,\n",
      "        3.5834e-04, 6.2037e-04, 9.1095e-03, 1.2581e-02, 7.9203e-04, 6.4373e-04,\n",
      "        4.8518e-04, 6.3992e-04, 2.4152e-04, 5.3358e-04, 1.1147e-02, 2.2125e-03,\n",
      "        1.1551e-02, 2.1606e-02, 1.9188e-03, 5.6171e-04, 6.6185e-04, 1.6127e-03,\n",
      "        1.0366e-03, 3.0518e-03, 1.6994e-03, 1.0633e-03, 1.1663e-03, 1.1435e-03,\n",
      "        5.0831e-04, 1.4229e-03, 6.9761e-04, 6.5517e-04, 6.8703e-03, 2.7161e-03,\n",
      "        1.0252e-03, 1.3714e-03, 2.0351e-03, 1.1864e-03, 1.7405e-03, 5.4121e-04,\n",
      "        1.7128e-03, 5.7373e-03, 4.2992e-03, 1.6603e-03, 5.5199e-03, 8.3618e-03,\n",
      "        2.8267e-03, 8.4610e-03, 1.4286e-03, 2.7084e-03, 1.0078e-02, 4.8218e-03,\n",
      "        6.1836e-03, 6.1378e-03, 8.4305e-03, 5.2567e-03, 3.1567e-03, 3.1052e-03,\n",
      "        6.8817e-03, 1.7822e-02, 6.2790e-03, 6.3049e-02, 2.9510e-02, 5.8472e-02,\n",
      "        2.5345e-02, 1.6327e-02, 1.4587e-02, 2.2522e-02, 1.9035e-03, 9.1705e-03,\n",
      "        5.1193e-03, 1.2482e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [9] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [9] : torch.Size([1, 32, 1, 81])\n",
      "Last layer attentions for generated token [9] : tensor([2.4902e-01, 2.4902e-01, 5.4264e-04, 4.2439e-04, 2.2233e-04, 3.9825e-03,\n",
      "        4.0650e-04, 9.4700e-04, 1.4668e-03, 1.1559e-03, 3.3245e-03, 4.5609e-04,\n",
      "        1.8501e-04, 4.5776e-04, 9.6130e-03, 7.1716e-03, 7.0381e-04, 5.1498e-04,\n",
      "        3.4761e-04, 3.8266e-04, 1.8501e-04, 5.6410e-04, 8.3618e-03, 1.1311e-03,\n",
      "        7.8125e-03, 1.4000e-02, 1.2131e-03, 4.3011e-04, 3.6073e-04, 1.3914e-03,\n",
      "        6.2847e-04, 1.4753e-03, 9.1076e-04, 7.4339e-04, 8.1301e-04, 6.5708e-04,\n",
      "        3.7885e-04, 1.5097e-03, 9.1934e-04, 4.6229e-04, 6.0997e-03, 2.9888e-03,\n",
      "        9.3603e-04, 1.0939e-03, 1.8320e-03, 8.5354e-04, 1.6165e-03, 8.3256e-04,\n",
      "        1.9665e-03, 5.6572e-03, 5.2567e-03, 1.1396e-03, 4.7302e-03, 7.0152e-03,\n",
      "        2.7294e-03, 8.2474e-03, 1.5888e-03, 2.6417e-03, 7.4196e-03, 4.0207e-03,\n",
      "        2.7237e-03, 4.4327e-03, 5.7449e-03, 3.9291e-03, 1.5516e-03, 2.2354e-03,\n",
      "        6.7444e-03, 1.6647e-02, 5.1079e-03, 5.1331e-02, 2.9175e-02, 6.1676e-02,\n",
      "        2.9694e-02, 1.2886e-02, 1.5221e-02, 3.0457e-02, 3.3607e-03, 1.4412e-02,\n",
      "        8.7051e-03, 2.2903e-02, 2.8046e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [10] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [10] : torch.Size([1, 32, 1, 82])\n",
      "Last layer attentions for generated token [10] : tensor([2.9126e-01, 2.9077e-01, 7.8058e-04, 4.6158e-04, 3.7026e-04, 4.8676e-03,\n",
      "        2.5201e-04, 3.9554e-04, 8.7738e-04, 7.5483e-04, 2.3117e-03, 2.3854e-04,\n",
      "        3.1590e-04, 6.5708e-04, 1.0689e-02, 1.1467e-02, 3.8791e-04, 4.7994e-04,\n",
      "        2.7347e-04, 6.5613e-04, 1.9848e-04, 8.4066e-04, 9.8343e-03, 1.6336e-03,\n",
      "        8.4686e-03, 1.3008e-02, 1.3075e-03, 5.4598e-04, 3.4714e-04, 7.7581e-04,\n",
      "        6.3467e-04, 2.1420e-03, 7.4625e-04, 7.5197e-04, 8.4877e-04, 1.0118e-03,\n",
      "        4.6420e-04, 1.5316e-03, 5.7220e-04, 4.3702e-04, 6.2561e-03, 2.5196e-03,\n",
      "        1.0004e-03, 1.5163e-03, 1.6298e-03, 6.8474e-04, 9.6941e-04, 4.5609e-04,\n",
      "        9.7513e-04, 4.3526e-03, 3.8528e-03, 1.2236e-03, 2.7008e-03, 9.4452e-03,\n",
      "        2.0599e-03, 1.0826e-02, 1.9588e-03, 2.4414e-03, 6.1760e-03, 3.6125e-03,\n",
      "        2.4776e-03, 3.0689e-03, 5.2681e-03, 4.7073e-03, 2.4319e-03, 2.0733e-03,\n",
      "        5.1308e-03, 8.8043e-03, 2.7695e-03, 5.1544e-02, 2.1881e-02, 4.9530e-02,\n",
      "        2.0538e-02, 1.1169e-02, 8.0948e-03, 1.7380e-02, 2.5711e-03, 1.1002e-02,\n",
      "        5.6992e-03, 1.0658e-02, 1.6266e-02, 8.0414e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [11] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [11] : torch.Size([1, 32, 1, 83])\n",
      "Last layer attentions for generated token [11] : tensor([2.0996e-01, 2.0996e-01, 7.3814e-04, 6.2656e-04, 5.9795e-04, 5.0697e-03,\n",
      "        4.1914e-04, 4.9877e-04, 6.8951e-04, 1.0376e-03, 4.1618e-03, 5.4121e-04,\n",
      "        3.1996e-04, 5.6744e-04, 1.1772e-02, 1.7044e-02, 4.3750e-04, 7.0858e-04,\n",
      "        3.3402e-04, 5.8651e-04, 1.6999e-04, 3.4213e-04, 5.6915e-03, 1.7796e-03,\n",
      "        5.8403e-03, 2.3270e-02, 4.7226e-03, 6.3753e-04, 4.2820e-04, 1.0538e-03,\n",
      "        7.8440e-04, 2.4986e-03, 1.1129e-03, 9.6846e-04, 8.4019e-04, 1.4744e-03,\n",
      "        5.0259e-04, 1.0595e-03, 8.0156e-04, 4.4179e-04, 6.4316e-03, 2.7275e-03,\n",
      "        1.9054e-03, 2.4891e-03, 2.0580e-03, 8.1873e-04, 9.8991e-04, 5.2071e-04,\n",
      "        8.8739e-04, 3.3245e-03, 3.0422e-03, 1.4153e-03, 3.6945e-03, 1.2848e-02,\n",
      "        2.7599e-03, 1.4748e-02, 2.7523e-03, 3.3245e-03, 6.8092e-03, 3.6697e-03,\n",
      "        3.3913e-03, 3.4714e-03, 5.2528e-03, 3.9520e-03, 2.0065e-03, 1.6670e-03,\n",
      "        5.8174e-03, 1.8585e-02, 2.4242e-03, 3.8055e-02, 3.3752e-02, 8.3679e-02,\n",
      "        4.2419e-02, 2.3453e-02, 7.8430e-03, 1.4839e-02, 2.4185e-03, 6.2141e-03,\n",
      "        4.2229e-03, 1.7395e-02, 1.7105e-02, 6.4545e-03, 5.7800e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [12] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [12] : torch.Size([1, 32, 1, 84])\n",
      "Last layer attentions for generated token [12] : tensor([2.5635e-01, 2.5537e-01, 5.4216e-04, 7.8726e-04, 3.7861e-04, 4.9934e-03,\n",
      "        7.2956e-04, 1.1616e-03, 2.3956e-03, 1.3771e-03, 4.6196e-03, 7.4100e-04,\n",
      "        4.0388e-04, 1.8559e-03, 1.3374e-02, 1.3268e-02, 9.1887e-04, 5.2786e-04,\n",
      "        1.9956e-04, 3.3545e-04, 1.5414e-04, 2.2435e-04, 8.3542e-03, 9.7418e-04,\n",
      "        9.2468e-03, 1.9882e-02, 3.2139e-03, 8.0776e-04, 8.5831e-04, 1.5421e-03,\n",
      "        1.7958e-03, 2.0981e-03, 1.0929e-03, 1.1530e-03, 7.1573e-04, 8.3685e-04,\n",
      "        6.4754e-04, 8.8692e-04, 7.2527e-04, 3.7265e-04, 5.3444e-03, 3.3321e-03,\n",
      "        1.9798e-03, 8.3160e-04, 1.3132e-03, 6.8808e-04, 1.0700e-03, 7.5722e-04,\n",
      "        1.0872e-03, 4.1504e-03, 3.2234e-03, 7.1144e-04, 5.1193e-03, 5.8403e-03,\n",
      "        5.6419e-03, 8.1863e-03, 2.3708e-03, 2.0275e-03, 7.2327e-03, 3.2425e-03,\n",
      "        2.8572e-03, 5.8937e-03, 3.6812e-03, 2.3117e-03, 1.1959e-03, 1.9531e-03,\n",
      "        4.7684e-03, 1.0521e-02, 4.4823e-03, 4.5135e-02, 2.3834e-02, 3.0502e-02,\n",
      "        1.2993e-02, 1.6449e-02, 7.6103e-03, 2.9327e-02, 2.6207e-03, 6.8779e-03,\n",
      "        5.2261e-03, 1.4442e-02, 1.5381e-02, 8.6899e-03, 3.4485e-02, 2.8839e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [13] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [13] : torch.Size([1, 32, 1, 85])\n",
      "Last layer attentions for generated token [13] : tensor([2.7393e-01, 2.7393e-01, 2.5582e-04, 4.7040e-04, 1.6260e-04, 3.1071e-03,\n",
      "        6.1989e-04, 9.3365e-04, 1.3666e-03, 1.5669e-03, 3.8700e-03, 9.3365e-04,\n",
      "        4.9877e-04, 1.2131e-03, 9.8190e-03, 8.5449e-03, 7.9727e-04, 7.1049e-04,\n",
      "        2.6298e-04, 3.6860e-04, 1.7858e-04, 4.8351e-04, 6.8130e-03, 1.1644e-03,\n",
      "        6.9046e-03, 8.6899e-03, 2.3270e-03, 9.2840e-04, 5.5027e-04, 1.3990e-03,\n",
      "        1.8072e-03, 2.2659e-03, 1.6260e-03, 2.0523e-03, 8.9979e-04, 7.4148e-04,\n",
      "        2.9445e-04, 1.1444e-03, 6.3419e-04, 3.1161e-04, 3.6144e-03, 2.0046e-03,\n",
      "        8.3399e-04, 9.1743e-04, 1.6422e-03, 5.7411e-04, 1.2865e-03, 6.5184e-04,\n",
      "        9.9373e-04, 4.0932e-03, 2.4052e-03, 3.1948e-04, 3.3302e-03, 2.4853e-03,\n",
      "        2.9297e-03, 7.4158e-03, 1.8644e-03, 2.7637e-03, 4.5357e-03, 3.2463e-03,\n",
      "        1.6747e-03, 3.6621e-03, 4.9324e-03, 2.0885e-03, 1.8721e-03, 1.6584e-03,\n",
      "        3.5381e-03, 4.3831e-03, 2.0962e-03, 3.2684e-02, 1.5503e-02, 3.1219e-02,\n",
      "        1.0750e-02, 1.5282e-02, 8.5144e-03, 3.7018e-02, 3.5076e-03, 1.5320e-02,\n",
      "        1.3199e-02, 1.8555e-02, 2.0157e-02, 9.8419e-03, 2.2583e-02, 1.8814e-02,\n",
      "        2.8488e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [14] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [14] : torch.Size([1, 32, 1, 86])\n",
      "Last layer attentions for generated token [14] : tensor([0.1637, 0.1635, 0.0004, 0.0007, 0.0003, 0.0041, 0.0007, 0.0006, 0.0008,\n",
      "        0.0020, 0.0051, 0.0005, 0.0013, 0.0014, 0.0091, 0.0134, 0.0010, 0.0011,\n",
      "        0.0005, 0.0010, 0.0004, 0.0006, 0.0083, 0.0017, 0.0060, 0.0148, 0.0058,\n",
      "        0.0009, 0.0011, 0.0007, 0.0010, 0.0020, 0.0030, 0.0015, 0.0008, 0.0024,\n",
      "        0.0007, 0.0011, 0.0016, 0.0010, 0.0051, 0.0038, 0.0030, 0.0029, 0.0024,\n",
      "        0.0022, 0.0006, 0.0007, 0.0019, 0.0044, 0.0051, 0.0007, 0.0031, 0.0049,\n",
      "        0.0045, 0.0156, 0.0035, 0.0057, 0.0072, 0.0051, 0.0068, 0.0040, 0.0043,\n",
      "        0.0048, 0.0037, 0.0015, 0.0110, 0.0082, 0.0023, 0.0486, 0.0238, 0.0776,\n",
      "        0.0253, 0.0158, 0.0142, 0.0161, 0.0031, 0.0135, 0.0062, 0.0237, 0.0295,\n",
      "        0.0057, 0.0823, 0.0375, 0.0131, 0.0082], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [15] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [15] : torch.Size([1, 32, 1, 87])\n",
      "Last layer attentions for generated token [15] : tensor([0.2111, 0.2111, 0.0004, 0.0005, 0.0003, 0.0038, 0.0006, 0.0005, 0.0007,\n",
      "        0.0013, 0.0043, 0.0009, 0.0009, 0.0012, 0.0089, 0.0150, 0.0007, 0.0007,\n",
      "        0.0004, 0.0008, 0.0003, 0.0003, 0.0051, 0.0011, 0.0072, 0.0174, 0.0029,\n",
      "        0.0006, 0.0007, 0.0007, 0.0008, 0.0017, 0.0016, 0.0009, 0.0006, 0.0010,\n",
      "        0.0006, 0.0009, 0.0009, 0.0007, 0.0041, 0.0029, 0.0018, 0.0014, 0.0017,\n",
      "        0.0010, 0.0008, 0.0006, 0.0010, 0.0040, 0.0035, 0.0007, 0.0045, 0.0047,\n",
      "        0.0040, 0.0112, 0.0033, 0.0036, 0.0101, 0.0043, 0.0067, 0.0031, 0.0049,\n",
      "        0.0054, 0.0045, 0.0019, 0.0087, 0.0119, 0.0062, 0.0333, 0.0215, 0.0505,\n",
      "        0.0266, 0.0143, 0.0126, 0.0174, 0.0028, 0.0140, 0.0059, 0.0184, 0.0240,\n",
      "        0.0102, 0.0556, 0.0301, 0.0112, 0.0096, 0.0099], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [16] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [16] : torch.Size([1, 32, 1, 88])\n",
      "Last layer attentions for generated token [16] : tensor([2.5854e-01, 2.5806e-01, 2.7609e-04, 7.0763e-04, 1.5736e-04, 4.4937e-03,\n",
      "        5.0402e-04, 5.4789e-04, 1.9312e-03, 1.1692e-03, 4.0741e-03, 6.9284e-04,\n",
      "        5.9462e-04, 1.5707e-03, 2.0508e-02, 1.3878e-02, 9.1219e-04, 4.6229e-04,\n",
      "        3.9935e-04, 8.5020e-04, 4.3440e-04, 9.1076e-04, 1.0162e-02, 2.2430e-03,\n",
      "        1.4816e-02, 1.7395e-02, 2.6016e-03, 7.8058e-04, 6.7139e-04, 8.5878e-04,\n",
      "        1.3199e-03, 2.5024e-03, 9.7656e-04, 7.2050e-04, 3.3045e-04, 4.5609e-04,\n",
      "        2.6751e-04, 6.3181e-04, 3.8028e-04, 2.2364e-04, 4.9324e-03, 2.8648e-03,\n",
      "        1.3695e-03, 9.0361e-04, 9.8801e-04, 3.5167e-04, 7.3004e-04, 4.3273e-04,\n",
      "        6.4421e-04, 2.9144e-03, 2.3384e-03, 3.8099e-04, 3.1681e-03, 3.1109e-03,\n",
      "        3.8300e-03, 7.8506e-03, 1.6651e-03, 1.8291e-03, 7.0343e-03, 3.8757e-03,\n",
      "        2.5520e-03, 5.0697e-03, 4.2114e-03, 3.5114e-03, 1.7309e-03, 1.5917e-03,\n",
      "        5.7564e-03, 6.3095e-03, 4.9362e-03, 3.9276e-02, 1.9608e-02, 3.2135e-02,\n",
      "        1.3763e-02, 9.0256e-03, 6.9885e-03, 1.4381e-02, 1.7080e-03, 4.8828e-03,\n",
      "        4.2343e-03, 1.2215e-02, 1.3786e-02, 6.7139e-03, 2.8839e-02, 1.9272e-02,\n",
      "        1.7502e-02, 2.0889e-02, 8.5983e-03, 1.0460e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [17] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [17] : torch.Size([1, 32, 1, 89])\n",
      "Last layer attentions for generated token [17] : tensor([3.1714e-01, 3.1714e-01, 3.0971e-04, 6.0272e-04, 2.2650e-04, 4.7531e-03,\n",
      "        5.9891e-04, 7.2145e-04, 1.5812e-03, 1.7538e-03, 3.8719e-03, 7.5579e-04,\n",
      "        4.0936e-04, 1.1911e-03, 1.4458e-02, 1.5144e-02, 6.8808e-04, 7.0333e-04,\n",
      "        2.8634e-04, 4.3416e-04, 2.3639e-04, 6.1798e-04, 7.7934e-03, 1.5297e-03,\n",
      "        9.2392e-03, 1.2924e-02, 1.5812e-03, 6.2799e-04, 5.0068e-04, 8.6975e-04,\n",
      "        1.0996e-03, 3.1605e-03, 1.3161e-03, 1.1501e-03, 7.7248e-04, 7.5006e-04,\n",
      "        2.0230e-04, 9.5177e-04, 4.9591e-04, 2.3735e-04, 3.8605e-03, 1.9569e-03,\n",
      "        6.1083e-04, 1.1911e-03, 1.5268e-03, 4.5753e-04, 9.6464e-04, 3.8004e-04,\n",
      "        5.9652e-04, 3.0041e-03, 1.9684e-03, 3.9840e-04, 2.0828e-03, 3.2730e-03,\n",
      "        2.0714e-03, 6.3515e-03, 1.0309e-03, 2.0714e-03, 6.4240e-03, 3.1090e-03,\n",
      "        1.7004e-03, 3.3512e-03, 4.5547e-03, 2.8553e-03, 1.6031e-03, 1.0662e-03,\n",
      "        3.4809e-03, 4.3106e-03, 3.4771e-03, 2.0523e-02, 1.0338e-02, 2.0828e-02,\n",
      "        8.8882e-03, 9.3384e-03, 5.4932e-03, 1.6159e-02, 1.7405e-03, 5.7182e-03,\n",
      "        4.2305e-03, 8.1558e-03, 8.1406e-03, 7.9346e-03, 1.9104e-02, 8.1482e-03,\n",
      "        1.2413e-02, 1.2306e-02, 5.4398e-03, 6.4316e-03, 1.0231e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [18] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [18] : torch.Size([1, 32, 1, 90])\n",
      "Last layer attentions for generated token [18] : tensor([2.7295e-01, 2.7295e-01, 4.7684e-04, 5.5981e-04, 3.4213e-04, 3.7766e-03,\n",
      "        4.4632e-04, 5.8460e-04, 1.1625e-03, 1.5249e-03, 4.7379e-03, 6.6090e-04,\n",
      "        3.8767e-04, 8.4877e-04, 2.1011e-02, 1.9196e-02, 6.4039e-04, 9.8419e-04,\n",
      "        4.7326e-04, 3.7217e-04, 1.8537e-04, 8.9455e-04, 9.1934e-03, 3.0403e-03,\n",
      "        1.0109e-02, 1.5015e-02, 1.9608e-03, 8.1778e-04, 6.4564e-04, 8.2588e-04,\n",
      "        1.0767e-03, 3.5553e-03, 1.3638e-03, 9.9754e-04, 1.0414e-03, 6.9666e-04,\n",
      "        1.7476e-04, 1.1158e-03, 4.8637e-04, 2.7061e-04, 3.5210e-03, 2.3518e-03,\n",
      "        6.1464e-04, 1.6356e-03, 1.5669e-03, 4.3249e-04, 8.0824e-04, 6.8569e-04,\n",
      "        7.2145e-04, 3.0556e-03, 2.8210e-03, 6.3562e-04, 2.0103e-03, 4.1618e-03,\n",
      "        2.2984e-03, 1.0605e-02, 1.3275e-03, 2.7924e-03, 5.1918e-03, 2.7657e-03,\n",
      "        1.9274e-03, 2.8954e-03, 4.3030e-03, 3.1013e-03, 1.8501e-03, 1.2836e-03,\n",
      "        3.6392e-03, 6.6147e-03, 2.6569e-03, 2.0294e-02, 1.4915e-02, 2.9221e-02,\n",
      "        1.1963e-02, 8.7280e-03, 6.8283e-03, 2.0233e-02, 2.4529e-03, 8.0872e-03,\n",
      "        5.4054e-03, 1.4961e-02, 1.3779e-02, 1.0490e-02, 2.7100e-02, 8.3389e-03,\n",
      "        1.0818e-02, 1.6403e-02, 8.6746e-03, 5.5199e-03, 9.6664e-03, 1.0330e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [19] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [19] : torch.Size([1, 32, 1, 91])\n",
      "Last layer attentions for generated token [19] : tensor([3.2715e-01, 3.2715e-01, 1.0538e-03, 8.5974e-04, 4.1342e-04, 6.6986e-03,\n",
      "        2.3687e-04, 3.5906e-04, 8.1110e-04, 1.7576e-03, 3.6030e-03, 3.1376e-04,\n",
      "        4.7588e-04, 7.8440e-04, 7.4959e-03, 1.4359e-02, 5.8746e-04, 3.9434e-04,\n",
      "        2.9135e-04, 2.8920e-04, 1.1456e-04, 7.4005e-04, 3.3691e-02, 3.4008e-03,\n",
      "        4.2419e-03, 1.3275e-02, 8.3685e-04, 4.8137e-04, 4.9591e-04, 4.3416e-04,\n",
      "        6.0749e-04, 9.5749e-04, 9.2602e-04, 6.5565e-04, 6.1703e-04, 7.5293e-04,\n",
      "        2.3508e-04, 8.4305e-04, 3.4618e-04, 1.9801e-04, 1.9007e-03, 1.8024e-03,\n",
      "        4.8137e-04, 7.3671e-04, 8.3160e-04, 2.7275e-04, 5.5408e-04, 7.1430e-04,\n",
      "        3.7408e-04, 9.4080e-04, 1.2589e-03, 2.5511e-04, 2.6646e-03, 4.6501e-03,\n",
      "        1.4124e-03, 9.0027e-03, 6.2561e-04, 2.9697e-03, 4.9286e-03, 2.6817e-03,\n",
      "        1.1959e-03, 1.6441e-03, 4.3373e-03, 1.7033e-03, 2.0676e-03, 9.2793e-04,\n",
      "        2.8858e-03, 2.9430e-03, 1.6804e-03, 1.3069e-02, 1.0277e-02, 2.5406e-02,\n",
      "        5.0201e-03, 7.4730e-03, 7.0953e-03, 8.0795e-03, 4.5280e-03, 8.6060e-03,\n",
      "        7.8812e-03, 1.1238e-02, 6.2180e-03, 2.5177e-03, 1.8188e-02, 7.6294e-03,\n",
      "        8.0185e-03, 8.8959e-03, 5.4436e-03, 5.6343e-03, 2.6302e-03, 5.6686e-03,\n",
      "        3.0880e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [20] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [20] : torch.Size([1, 32, 1, 92])\n",
      "Last layer attentions for generated token [20] : tensor([0.2299, 0.2294, 0.0003, 0.0005, 0.0002, 0.0040, 0.0003, 0.0003, 0.0011,\n",
      "        0.0010, 0.0031, 0.0007, 0.0004, 0.0008, 0.0132, 0.0169, 0.0007, 0.0006,\n",
      "        0.0004, 0.0005, 0.0002, 0.0006, 0.0076, 0.0019, 0.0089, 0.0144, 0.0020,\n",
      "        0.0008, 0.0008, 0.0012, 0.0017, 0.0039, 0.0016, 0.0012, 0.0008, 0.0007,\n",
      "        0.0002, 0.0015, 0.0005, 0.0004, 0.0058, 0.0026, 0.0009, 0.0015, 0.0018,\n",
      "        0.0007, 0.0012, 0.0006, 0.0010, 0.0052, 0.0037, 0.0009, 0.0023, 0.0038,\n",
      "        0.0029, 0.0068, 0.0016, 0.0027, 0.0051, 0.0030, 0.0031, 0.0034, 0.0034,\n",
      "        0.0028, 0.0021, 0.0014, 0.0045, 0.0070, 0.0030, 0.0381, 0.0198, 0.0364,\n",
      "        0.0174, 0.0156, 0.0086, 0.0153, 0.0013, 0.0071, 0.0048, 0.0115, 0.0147,\n",
      "        0.0098, 0.0404, 0.0256, 0.0149, 0.0141, 0.0081, 0.0149, 0.0144, 0.0142,\n",
      "        0.0024, 0.0105], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [21] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [21] : torch.Size([1, 32, 1, 93])\n",
      "Last layer attentions for generated token [21] : tensor([1.8823e-01, 1.8823e-01, 3.3998e-04, 5.9938e-04, 2.2650e-04, 3.9043e-03,\n",
      "        4.0936e-04, 7.0333e-04, 1.4830e-03, 9.4080e-04, 2.8114e-03, 4.9019e-04,\n",
      "        2.6178e-04, 4.4274e-04, 9.9030e-03, 1.3939e-02, 9.9945e-04, 5.7840e-04,\n",
      "        3.3736e-04, 3.3021e-04, 1.8823e-04, 7.1859e-04, 7.1144e-03, 1.3294e-03,\n",
      "        5.3406e-03, 1.5762e-02, 1.9932e-03, 3.8767e-04, 5.5313e-04, 2.2202e-03,\n",
      "        1.6785e-03, 3.1605e-03, 1.5240e-03, 1.1501e-03, 1.4544e-03, 5.6601e-04,\n",
      "        1.6415e-04, 1.7443e-03, 5.9557e-04, 3.6001e-04, 7.8430e-03, 3.2368e-03,\n",
      "        6.2799e-04, 8.8215e-04, 1.4544e-03, 5.6839e-04, 1.2560e-03, 6.9380e-04,\n",
      "        1.0595e-03, 7.1297e-03, 4.0741e-03, 7.6485e-04, 2.7866e-03, 4.3869e-03,\n",
      "        3.1986e-03, 6.8817e-03, 1.8911e-03, 2.3956e-03, 7.4730e-03, 2.9850e-03,\n",
      "        2.5368e-03, 3.8967e-03, 4.6196e-03, 2.2106e-03, 1.5364e-03, 1.6260e-03,\n",
      "        4.6043e-03, 1.1108e-02, 3.6240e-03, 4.6722e-02, 2.5345e-02, 4.1443e-02,\n",
      "        1.7807e-02, 1.6235e-02, 9.6207e-03, 1.9211e-02, 1.2388e-03, 9.6054e-03,\n",
      "        9.3307e-03, 2.5864e-02, 2.1820e-02, 1.8250e-02, 5.0629e-02, 1.5137e-02,\n",
      "        1.3184e-02, 2.0004e-02, 9.6130e-03, 9.2545e-03, 1.2398e-02, 1.1948e-02,\n",
      "        1.6127e-03, 1.4488e-02, 1.8646e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [22] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [22] : torch.Size([1, 32, 1, 94])\n",
      "Last layer attentions for generated token [22] : tensor([2.2974e-01, 2.3022e-01, 4.3321e-04, 5.0735e-04, 3.4475e-04, 3.1395e-03,\n",
      "        4.1270e-04, 4.9305e-04, 9.5177e-04, 1.1644e-03, 2.1362e-03, 3.9220e-04,\n",
      "        4.1819e-04, 7.3862e-04, 8.9417e-03, 1.0849e-02, 6.6328e-04, 7.4291e-04,\n",
      "        2.6751e-04, 3.1137e-04, 1.0031e-04, 5.0068e-04, 8.7128e-03, 1.7338e-03,\n",
      "        5.1346e-03, 1.1520e-02, 2.1954e-03, 4.1580e-04, 6.8140e-04, 8.9598e-04,\n",
      "        1.0853e-03, 2.0390e-03, 2.5120e-03, 1.4400e-03, 1.2321e-03, 8.5974e-04,\n",
      "        3.8767e-04, 1.2732e-03, 5.4359e-04, 3.7193e-04, 4.1161e-03, 3.4008e-03,\n",
      "        9.1553e-04, 1.1415e-03, 1.6804e-03, 6.9666e-04, 7.3290e-04, 5.1975e-04,\n",
      "        8.3685e-04, 3.7804e-03, 3.3035e-03, 6.5565e-04, 3.7460e-03, 5.9357e-03,\n",
      "        4.1809e-03, 7.5302e-03, 1.7099e-03, 2.3289e-03, 5.2872e-03, 2.7866e-03,\n",
      "        2.4071e-03, 2.3937e-03, 3.5706e-03, 2.6112e-03, 1.5268e-03, 1.2932e-03,\n",
      "        7.2403e-03, 6.2790e-03, 3.3913e-03, 3.2959e-02, 2.1912e-02, 2.7618e-02,\n",
      "        1.4069e-02, 1.2260e-02, 7.7972e-03, 2.0096e-02, 8.6689e-04, 1.0277e-02,\n",
      "        8.6670e-03, 1.9104e-02, 1.9760e-02, 1.0437e-02, 4.4708e-02, 1.5594e-02,\n",
      "        1.0353e-02, 1.6769e-02, 8.5907e-03, 1.1337e-02, 9.4376e-03, 8.8654e-03,\n",
      "        1.9646e-03, 1.5266e-02, 1.3481e-02, 1.5305e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [23] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [23] : torch.Size([1, 32, 1, 95])\n",
      "Last layer attentions for generated token [23] : tensor([2.1497e-01, 2.1497e-01, 3.0780e-04, 3.3808e-04, 2.3544e-04, 4.3793e-03,\n",
      "        3.6120e-04, 4.9829e-04, 9.0790e-04, 1.1253e-03, 2.7275e-03, 5.6028e-04,\n",
      "        5.6028e-04, 9.2936e-04, 5.5389e-03, 1.3718e-02, 5.8270e-04, 5.2977e-04,\n",
      "        2.7418e-04, 4.0531e-04, 1.5199e-04, 3.5429e-04, 8.9951e-03, 2.1305e-03,\n",
      "        7.2899e-03, 1.7685e-02, 1.6317e-03, 5.8842e-04, 6.9904e-04, 1.1187e-03,\n",
      "        1.2627e-03, 2.5940e-03, 1.9073e-03, 1.1864e-03, 7.4100e-04, 7.1812e-04,\n",
      "        3.5906e-04, 6.6805e-04, 3.7622e-04, 3.3593e-04, 3.6583e-03, 2.4738e-03,\n",
      "        8.4782e-04, 9.4795e-04, 1.2197e-03, 5.0735e-04, 8.4162e-04, 5.2547e-04,\n",
      "        9.2411e-04, 2.9030e-03, 2.6817e-03, 7.7677e-04, 3.0937e-03, 4.3144e-03,\n",
      "        3.3417e-03, 7.8087e-03, 1.2684e-03, 3.0041e-03, 6.6719e-03, 3.3150e-03,\n",
      "        4.9286e-03, 3.8948e-03, 3.3474e-03, 2.8572e-03, 3.9177e-03, 1.4315e-03,\n",
      "        5.6725e-03, 8.3694e-03, 3.5915e-03, 2.2568e-02, 1.6220e-02, 3.2166e-02,\n",
      "        1.6724e-02, 1.2505e-02, 1.2901e-02, 9.4833e-03, 1.0185e-03, 7.2861e-03,\n",
      "        4.5052e-03, 1.1360e-02, 1.3153e-02, 5.9509e-03, 3.6346e-02, 1.8768e-02,\n",
      "        1.3237e-02, 1.1833e-02, 1.0353e-02, 1.5442e-02, 1.6022e-02, 1.6510e-02,\n",
      "        3.4637e-03, 1.4572e-02, 2.5681e-02, 1.6586e-02, 3.1616e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [24] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [24] : torch.Size([1, 32, 1, 96])\n",
      "Last layer attentions for generated token [24] : tensor([1.5112e-01, 1.5112e-01, 2.9707e-04, 2.9993e-04, 2.2995e-04, 3.0556e-03,\n",
      "        4.1723e-04, 5.6696e-04, 5.8508e-04, 6.6948e-04, 2.0657e-03, 1.6952e-04,\n",
      "        1.2362e-04, 3.4118e-04, 6.2256e-03, 9.5749e-03, 6.3229e-04, 4.0913e-04,\n",
      "        1.5140e-04, 1.1033e-04, 4.0412e-05, 2.8443e-04, 8.4763e-03, 1.4467e-03,\n",
      "        3.1738e-03, 1.3039e-02, 1.1930e-03, 3.1734e-04, 3.6669e-04, 1.1473e-03,\n",
      "        7.9823e-04, 1.2627e-03, 1.2188e-03, 1.1911e-03, 1.3084e-03, 5.8746e-04,\n",
      "        2.6512e-04, 1.1368e-03, 4.1890e-04, 3.0470e-04, 2.6226e-03, 1.5097e-03,\n",
      "        4.8971e-04, 8.2636e-04, 1.3065e-03, 3.8671e-04, 1.1454e-03, 3.8671e-04,\n",
      "        9.3460e-04, 3.5934e-03, 2.2984e-03, 4.7374e-04, 2.0561e-03, 6.5956e-03,\n",
      "        2.1343e-03, 8.1863e-03, 9.1314e-04, 1.1187e-03, 2.8133e-03, 1.3895e-03,\n",
      "        1.5488e-03, 2.6779e-03, 3.2558e-03, 1.4439e-03, 1.1721e-03, 9.7942e-04,\n",
      "        2.5940e-03, 6.8779e-03, 1.4677e-03, 3.9093e-02, 3.8971e-02, 4.7394e-02,\n",
      "        1.8570e-02, 1.2848e-02, 1.2245e-02, 1.7410e-02, 1.2894e-03, 1.2436e-02,\n",
      "        1.1986e-02, 2.2552e-02, 2.5635e-02, 1.1993e-02, 5.1849e-02, 1.7609e-02,\n",
      "        1.8524e-02, 2.4673e-02, 1.1108e-02, 1.6418e-02, 1.2779e-02, 1.4694e-02,\n",
      "        2.7294e-03, 1.4824e-02, 1.7807e-02, 1.1154e-02, 3.0090e-02, 5.3864e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [25] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [25] : torch.Size([1, 32, 1, 97])\n",
      "Last layer attentions for generated token [25] : tensor([0.1830, 0.1826, 0.0004, 0.0005, 0.0003, 0.0031, 0.0003, 0.0004, 0.0007,\n",
      "        0.0012, 0.0025, 0.0005, 0.0004, 0.0005, 0.0115, 0.0114, 0.0010, 0.0010,\n",
      "        0.0005, 0.0006, 0.0002, 0.0012, 0.0093, 0.0032, 0.0109, 0.0170, 0.0040,\n",
      "        0.0008, 0.0009, 0.0013, 0.0016, 0.0046, 0.0011, 0.0008, 0.0008, 0.0011,\n",
      "        0.0004, 0.0011, 0.0005, 0.0003, 0.0044, 0.0028, 0.0012, 0.0012, 0.0010,\n",
      "        0.0004, 0.0005, 0.0004, 0.0006, 0.0043, 0.0037, 0.0006, 0.0019, 0.0071,\n",
      "        0.0020, 0.0103, 0.0023, 0.0033, 0.0058, 0.0024, 0.0029, 0.0028, 0.0036,\n",
      "        0.0035, 0.0030, 0.0014, 0.0068, 0.0081, 0.0030, 0.0542, 0.0247, 0.0386,\n",
      "        0.0209, 0.0123, 0.0082, 0.0070, 0.0010, 0.0063, 0.0035, 0.0101, 0.0147,\n",
      "        0.0155, 0.0519, 0.0208, 0.0116, 0.0102, 0.0084, 0.0130, 0.0129, 0.0127,\n",
      "        0.0023, 0.0091, 0.0096, 0.0112, 0.0292, 0.0165, 0.0087],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [26] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [26] : torch.Size([1, 32, 1, 98])\n",
      "Last layer attentions for generated token [26] : tensor([1.8201e-01, 1.8164e-01, 2.3770e-04, 3.6597e-04, 1.6272e-04, 3.0003e-03,\n",
      "        2.9254e-04, 3.6597e-04, 6.0225e-04, 7.5102e-04, 1.7281e-03, 5.5504e-04,\n",
      "        3.4738e-04, 5.1117e-04, 9.2087e-03, 1.0002e-02, 4.6921e-04, 5.2547e-04,\n",
      "        3.0756e-04, 4.4942e-04, 1.8227e-04, 4.2892e-04, 4.2458e-03, 1.2360e-03,\n",
      "        6.2408e-03, 1.9547e-02, 2.9068e-03, 4.4775e-04, 4.0770e-04, 6.3133e-04,\n",
      "        5.3263e-04, 2.3479e-03, 1.0576e-03, 7.1239e-04, 6.4898e-04, 5.0831e-04,\n",
      "        3.0756e-04, 6.8521e-04, 3.1018e-04, 2.0492e-04, 3.9520e-03, 2.1305e-03,\n",
      "        1.0967e-03, 1.0452e-03, 9.9659e-04, 4.1246e-04, 5.5075e-04, 3.3522e-04,\n",
      "        5.6601e-04, 3.4332e-03, 3.0346e-03, 7.1812e-04, 3.7384e-03, 6.7291e-03,\n",
      "        2.1019e-03, 9.1629e-03, 1.5774e-03, 2.3212e-03, 6.3248e-03, 2.8343e-03,\n",
      "        2.3365e-03, 2.6169e-03, 3.8681e-03, 3.0231e-03, 1.7433e-03, 1.0529e-03,\n",
      "        4.9400e-03, 8.5144e-03, 2.9449e-03, 2.8183e-02, 2.3209e-02, 4.9194e-02,\n",
      "        3.3264e-02, 1.8051e-02, 7.0381e-03, 7.2327e-03, 1.1816e-03, 5.0850e-03,\n",
      "        3.1528e-03, 1.0460e-02, 1.4214e-02, 1.2215e-02, 5.9906e-02, 2.6474e-02,\n",
      "        1.3657e-02, 1.1154e-02, 8.6441e-03, 1.2032e-02, 1.3527e-02, 9.2926e-03,\n",
      "        1.2436e-03, 7.7286e-03, 7.8087e-03, 1.8829e-02, 2.9343e-02, 1.0567e-02,\n",
      "        1.0056e-02, 2.6031e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [27] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [27] : torch.Size([1, 32, 1, 99])\n",
      "Last layer attentions for generated token [27] : tensor([2.7197e-01, 2.7197e-01, 3.8409e-04, 3.2091e-04, 1.5914e-04, 3.2692e-03,\n",
      "        3.2520e-04, 6.5851e-04, 1.1158e-03, 7.3576e-04, 2.3518e-03, 2.8825e-04,\n",
      "        3.7575e-04, 4.6134e-04, 6.0158e-03, 9.3231e-03, 5.4598e-04, 3.0375e-04,\n",
      "        1.7822e-04, 2.6870e-04, 8.7559e-05, 2.8038e-04, 1.0452e-02, 7.9870e-04,\n",
      "        3.2215e-03, 1.4221e-02, 8.4877e-04, 3.5238e-04, 4.1366e-04, 5.8794e-04,\n",
      "        4.6492e-04, 1.1377e-03, 7.8630e-04, 8.0013e-04, 7.4911e-04, 4.9019e-04,\n",
      "        2.3985e-04, 7.9250e-04, 3.3355e-04, 2.3150e-04, 2.1496e-03, 2.0123e-03,\n",
      "        3.9625e-04, 4.0960e-04, 7.9393e-04, 3.2711e-04, 6.9809e-04, 4.0960e-04,\n",
      "        5.0783e-04, 2.3441e-03, 2.6894e-03, 3.7575e-04, 4.2381e-03, 3.1433e-03,\n",
      "        2.8629e-03, 7.2861e-03, 1.4668e-03, 1.5192e-03, 4.8218e-03, 1.5068e-03,\n",
      "        1.3504e-03, 1.6394e-03, 2.7447e-03, 1.5125e-03, 1.2207e-03, 9.6941e-04,\n",
      "        4.1046e-03, 5.4436e-03, 2.9526e-03, 2.3041e-02, 1.5717e-02, 2.9205e-02,\n",
      "        9.8724e-03, 8.8730e-03, 5.7526e-03, 9.3842e-03, 1.7824e-03, 7.5035e-03,\n",
      "        4.3716e-03, 6.9427e-03, 7.0686e-03, 4.5738e-03, 2.9373e-02, 7.5493e-03,\n",
      "        1.4076e-02, 8.7204e-03, 3.8528e-03, 4.2229e-03, 7.1602e-03, 9.6588e-03,\n",
      "        2.5845e-03, 1.5717e-02, 9.7504e-03, 1.1887e-02, 2.0966e-02, 1.7456e-02,\n",
      "        9.8038e-03, 1.8280e-02, 4.7836e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [28] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [28] : torch.Size([1, 32, 1, 100])\n",
      "Last layer attentions for generated token [28] : tensor([0.1984, 0.1987, 0.0004, 0.0005, 0.0002, 0.0028, 0.0003, 0.0003, 0.0004,\n",
      "        0.0017, 0.0024, 0.0003, 0.0010, 0.0011, 0.0074, 0.0091, 0.0007, 0.0010,\n",
      "        0.0005, 0.0007, 0.0002, 0.0009, 0.0093, 0.0021, 0.0059, 0.0136, 0.0031,\n",
      "        0.0008, 0.0006, 0.0005, 0.0008, 0.0019, 0.0010, 0.0007, 0.0008, 0.0015,\n",
      "        0.0005, 0.0008, 0.0010, 0.0005, 0.0040, 0.0034, 0.0010, 0.0011, 0.0007,\n",
      "        0.0005, 0.0002, 0.0004, 0.0005, 0.0030, 0.0039, 0.0006, 0.0026, 0.0072,\n",
      "        0.0022, 0.0102, 0.0018, 0.0036, 0.0057, 0.0020, 0.0021, 0.0016, 0.0031,\n",
      "        0.0024, 0.0032, 0.0011, 0.0114, 0.0075, 0.0026, 0.0298, 0.0212, 0.0482,\n",
      "        0.0153, 0.0113, 0.0053, 0.0038, 0.0018, 0.0084, 0.0032, 0.0122, 0.0109,\n",
      "        0.0054, 0.0703, 0.0125, 0.0075, 0.0059, 0.0090, 0.0073, 0.0071, 0.0116,\n",
      "        0.0023, 0.0099, 0.0061, 0.0057, 0.0231, 0.0078, 0.0045, 0.0459, 0.0195,\n",
      "        0.0034], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [29] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [29] : torch.Size([1, 32, 1, 101])\n",
      "Last layer attentions for generated token [29] : tensor([1.8506e-01, 1.8506e-01, 2.6703e-04, 4.8351e-04, 1.4186e-04, 2.9202e-03,\n",
      "        3.7217e-04, 4.1938e-04, 4.9305e-04, 9.0170e-04, 2.5234e-03, 7.3433e-04,\n",
      "        6.1512e-04, 6.8331e-04, 8.2703e-03, 1.3184e-02, 9.1410e-04, 6.8712e-04,\n",
      "        4.7064e-04, 4.9496e-04, 1.6320e-04, 4.7064e-04, 3.6182e-03, 1.1244e-03,\n",
      "        7.6218e-03, 2.4353e-02, 2.8934e-03, 6.5470e-04, 6.2466e-04, 6.7282e-04,\n",
      "        4.8637e-04, 1.8024e-03, 1.0958e-03, 7.2289e-04, 6.8855e-04, 7.5769e-04,\n",
      "        2.8253e-04, 6.5184e-04, 4.7326e-04, 3.9005e-04, 3.8719e-03, 3.3627e-03,\n",
      "        1.2741e-03, 8.5211e-04, 9.5797e-04, 3.8552e-04, 4.7064e-04, 3.2163e-04,\n",
      "        3.5453e-04, 2.6779e-03, 2.8191e-03, 6.1274e-04, 4.6272e-03, 5.6610e-03,\n",
      "        2.7313e-03, 9.0332e-03, 2.2297e-03, 3.1528e-03, 1.0124e-02, 3.0746e-03,\n",
      "        3.2215e-03, 2.7161e-03, 4.4746e-03, 3.6545e-03, 3.5152e-03, 1.6613e-03,\n",
      "        7.9498e-03, 1.3687e-02, 7.2823e-03, 2.6993e-02, 2.1225e-02, 4.0802e-02,\n",
      "        2.7512e-02, 1.0048e-02, 7.0000e-03, 7.0343e-03, 9.6369e-04, 4.6654e-03,\n",
      "        2.8553e-03, 8.1863e-03, 8.3389e-03, 9.7580e-03, 4.6753e-02, 1.8997e-02,\n",
      "        1.0269e-02, 1.0269e-02, 7.6103e-03, 1.0017e-02, 9.6970e-03, 8.2245e-03,\n",
      "        1.2417e-03, 9.5673e-03, 1.2260e-02, 1.2756e-02, 2.6062e-02, 1.1139e-02,\n",
      "        1.1040e-02, 3.2227e-02, 1.3855e-02, 5.7831e-03, 5.8174e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [30] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [30] : torch.Size([1, 32, 1, 102])\n",
      "Last layer attentions for generated token [30] : tensor([2.3560e-01, 2.3560e-01, 1.5330e-04, 2.2650e-04, 9.8169e-05, 2.8496e-03,\n",
      "        3.1257e-04, 4.4084e-04, 1.0214e-03, 6.0272e-04, 2.5616e-03, 5.4455e-04,\n",
      "        3.0470e-04, 1.1988e-03, 1.3374e-02, 1.2848e-02, 7.8297e-04, 4.8327e-04,\n",
      "        2.7752e-04, 4.3488e-04, 1.7369e-04, 3.7932e-04, 9.3307e-03, 1.5001e-03,\n",
      "        8.7967e-03, 1.9043e-02, 1.3790e-03, 6.6042e-04, 4.8327e-04, 7.8726e-04,\n",
      "        1.1702e-03, 1.8311e-03, 6.3515e-04, 8.5163e-04, 6.5136e-04, 6.9618e-04,\n",
      "        5.2357e-04, 8.8692e-04, 3.1567e-04, 1.7428e-04, 3.5458e-03, 2.0390e-03,\n",
      "        9.7847e-04, 5.9891e-04, 7.2670e-04, 2.2209e-04, 6.1941e-04, 3.2568e-04,\n",
      "        4.2319e-04, 2.5196e-03, 1.9417e-03, 3.3474e-04, 3.9253e-03, 3.1395e-03,\n",
      "        3.0136e-03, 6.1340e-03, 1.1797e-03, 1.2636e-03, 3.7575e-03, 1.3037e-03,\n",
      "        1.0824e-03, 2.7695e-03, 2.2202e-03, 1.6317e-03, 1.3742e-03, 1.1101e-03,\n",
      "        2.5597e-03, 4.3030e-03, 2.9316e-03, 1.9806e-02, 1.1673e-02, 2.1149e-02,\n",
      "        9.9182e-03, 1.1398e-02, 5.0888e-03, 1.4442e-02, 1.9379e-03, 7.4730e-03,\n",
      "        5.2719e-03, 1.1414e-02, 1.4450e-02, 9.6359e-03, 2.2873e-02, 1.5778e-02,\n",
      "        1.7212e-02, 2.8183e-02, 8.2321e-03, 1.0101e-02, 1.3229e-02, 8.4076e-03,\n",
      "        2.4185e-03, 1.0788e-02, 6.6986e-03, 1.3741e-02, 1.4992e-02, 6.3210e-03,\n",
      "        9.5139e-03, 1.7654e-02, 9.8953e-03, 9.4833e-03, 4.1275e-03, 8.6899e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [31] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [31] : torch.Size([1, 32, 1, 103])\n",
      "Last layer attentions for generated token [31] : tensor([2.9272e-01, 2.9272e-01, 1.4687e-04, 2.4509e-04, 7.7724e-05, 3.9749e-03,\n",
      "        3.1209e-04, 5.2595e-04, 1.0605e-03, 7.1478e-04, 1.6012e-03, 5.4264e-04,\n",
      "        4.5156e-04, 9.3031e-04, 8.5678e-03, 9.3155e-03, 8.1158e-04, 4.4465e-04,\n",
      "        3.2711e-04, 5.6219e-04, 3.2663e-04, 7.8487e-04, 9.1400e-03, 1.2159e-03,\n",
      "        7.4806e-03, 1.2978e-02, 1.2522e-03, 7.1764e-04, 4.0722e-04, 8.6689e-04,\n",
      "        1.0767e-03, 1.8244e-03, 8.4352e-04, 9.3031e-04, 5.5742e-04, 3.9387e-04,\n",
      "        1.6510e-04, 9.5987e-04, 3.5858e-04, 1.6260e-04, 3.5591e-03, 1.6260e-03,\n",
      "        5.3644e-04, 6.8188e-04, 9.4652e-04, 2.0397e-04, 7.2432e-04, 1.9014e-04,\n",
      "        3.0136e-04, 1.9608e-03, 1.3885e-03, 2.8324e-04, 2.4452e-03, 2.4261e-03,\n",
      "        1.9350e-03, 5.5275e-03, 7.5340e-04, 1.1396e-03, 3.2444e-03, 1.3857e-03,\n",
      "        6.4707e-04, 1.5154e-03, 1.9569e-03, 1.1377e-03, 9.0170e-04, 5.3930e-04,\n",
      "        2.0676e-03, 3.4771e-03, 1.9054e-03, 2.0798e-02, 9.4910e-03, 2.5131e-02,\n",
      "        7.8125e-03, 5.4626e-03, 3.5973e-03, 1.4084e-02, 1.6651e-03, 4.2610e-03,\n",
      "        3.3894e-03, 5.9128e-03, 6.3934e-03, 7.6714e-03, 1.7334e-02, 1.2367e-02,\n",
      "        1.3725e-02, 1.7426e-02, 4.7073e-03, 7.5378e-03, 1.1230e-02, 7.4501e-03,\n",
      "        1.6584e-03, 6.9084e-03, 4.6654e-03, 7.8506e-03, 9.1095e-03, 6.2065e-03,\n",
      "        7.3509e-03, 1.4519e-02, 4.4327e-03, 5.9700e-03, 2.3842e-03, 6.1035e-03,\n",
      "        1.1261e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [32] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [32] : torch.Size([1, 32, 1, 104])\n",
      "Last layer attentions for generated token [32] : tensor([2.4487e-01, 2.4536e-01, 1.2094e-04, 2.2066e-04, 6.8367e-05, 2.1706e-03,\n",
      "        4.8685e-04, 8.0109e-04, 7.0286e-04, 5.7220e-04, 1.5869e-03, 5.2404e-04,\n",
      "        2.9755e-04, 6.5517e-04, 7.5531e-03, 1.0468e-02, 7.2765e-04, 5.2214e-04,\n",
      "        4.5204e-04, 3.3784e-04, 8.9884e-05, 5.5027e-04, 1.0010e-02, 2.0885e-03,\n",
      "        3.1242e-03, 1.3168e-02, 1.0614e-03, 7.0667e-04, 3.8815e-04, 9.0408e-04,\n",
      "        6.4373e-04, 2.1114e-03, 1.3437e-03, 1.1539e-03, 8.1348e-04, 3.9101e-04,\n",
      "        2.5702e-04, 8.4448e-04, 3.1996e-04, 2.9922e-04, 2.4891e-03, 1.9989e-03,\n",
      "        7.3195e-04, 1.0529e-03, 1.1101e-03, 2.0647e-04, 6.1655e-04, 3.1376e-04,\n",
      "        3.0947e-04, 1.3523e-03, 1.6470e-03, 1.8585e-04, 2.4052e-03, 2.3060e-03,\n",
      "        2.1000e-03, 7.6904e-03, 1.0796e-03, 1.5678e-03, 2.1782e-03, 1.4277e-03,\n",
      "        8.5115e-04, 2.1152e-03, 2.8534e-03, 1.9083e-03, 1.3809e-03, 7.7629e-04,\n",
      "        3.2463e-03, 4.2686e-03, 2.1935e-03, 2.4963e-02, 1.1246e-02, 3.0899e-02,\n",
      "        1.4015e-02, 9.3613e-03, 6.2523e-03, 1.2444e-02, 1.3647e-03, 5.6305e-03,\n",
      "        4.3678e-03, 5.9128e-03, 8.5907e-03, 5.6419e-03, 2.0569e-02, 7.0076e-03,\n",
      "        1.1299e-02, 1.3039e-02, 6.4278e-03, 6.3667e-03, 1.6296e-02, 1.2886e-02,\n",
      "        3.0499e-03, 1.7792e-02, 9.0942e-03, 1.2856e-02, 1.6922e-02, 1.1063e-02,\n",
      "        1.5617e-02, 2.5253e-02, 6.6757e-03, 8.1406e-03, 4.8828e-03, 4.7569e-03,\n",
      "        1.0704e-02, 7.5188e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [33] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [33] : torch.Size([1, 32, 1, 105])\n",
      "Last layer attentions for generated token [33] : tensor([3.0078e-01, 3.0127e-01, 1.2434e-04, 2.2078e-04, 5.7399e-05, 1.7033e-03,\n",
      "        2.5034e-04, 4.3321e-04, 7.4577e-04, 7.1716e-04, 1.0538e-03, 3.2568e-04,\n",
      "        2.2697e-04, 5.1546e-04, 5.9547e-03, 7.1449e-03, 6.8808e-04, 5.0831e-04,\n",
      "        2.9707e-04, 3.5024e-04, 1.2732e-04, 7.0190e-04, 1.2268e-02, 2.1305e-03,\n",
      "        4.9210e-03, 8.7509e-03, 9.5367e-04, 6.4039e-04, 3.5977e-04, 6.0129e-04,\n",
      "        5.3692e-04, 1.4009e-03, 1.0414e-03, 7.5436e-04, 6.4898e-04, 4.8614e-04,\n",
      "        1.7738e-04, 6.1464e-04, 3.7479e-04, 2.1315e-04, 2.2850e-03, 1.9417e-03,\n",
      "        5.4741e-04, 8.5831e-04, 9.4271e-04, 2.1482e-04, 4.6468e-04, 3.5977e-04,\n",
      "        2.9659e-04, 1.4887e-03, 1.6098e-03, 1.8382e-04, 2.4738e-03, 2.5005e-03,\n",
      "        1.9417e-03, 4.7455e-03, 6.6042e-04, 1.4124e-03, 1.7471e-03, 1.2703e-03,\n",
      "        6.7759e-04, 1.3876e-03, 2.4509e-03, 1.6127e-03, 1.0872e-03, 8.3017e-04,\n",
      "        3.4180e-03, 2.6875e-03, 1.9226e-03, 1.9943e-02, 8.5373e-03, 2.2110e-02,\n",
      "        6.7024e-03, 5.4283e-03, 3.8357e-03, 9.2468e-03, 1.5087e-03, 5.0278e-03,\n",
      "        3.8376e-03, 7.1449e-03, 5.6000e-03, 3.4504e-03, 1.5366e-02, 3.5706e-03,\n",
      "        6.2599e-03, 8.5602e-03, 4.8714e-03, 3.3875e-03, 9.1782e-03, 8.2779e-03,\n",
      "        2.4490e-03, 2.1332e-02, 5.4970e-03, 6.4125e-03, 1.3046e-02, 7.2517e-03,\n",
      "        9.8724e-03, 1.8875e-02, 5.7678e-03, 5.5580e-03, 4.4823e-03, 4.1656e-03,\n",
      "        1.0727e-02, 7.0000e-03, 1.0521e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [34] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [34] : torch.Size([1, 32, 1, 106])\n",
      "Last layer attentions for generated token [34] : tensor([2.2681e-01, 2.2681e-01, 1.3804e-04, 2.0719e-04, 5.6207e-05, 1.6851e-03,\n",
      "        3.0375e-04, 3.3116e-04, 6.6090e-04, 6.0415e-04, 1.2255e-03, 3.3832e-04,\n",
      "        4.3702e-04, 4.9114e-04, 9.0179e-03, 8.1863e-03, 7.0238e-04, 4.3774e-04,\n",
      "        3.6502e-04, 5.7888e-04, 1.4985e-04, 4.3869e-04, 6.6528e-03, 1.3971e-03,\n",
      "        6.3438e-03, 1.3374e-02, 1.3914e-03, 5.8699e-04, 4.1771e-04, 4.3511e-04,\n",
      "        5.6648e-04, 1.3990e-03, 8.7738e-04, 6.0177e-04, 5.7983e-04, 7.3147e-04,\n",
      "        3.1781e-04, 7.2050e-04, 4.2677e-04, 3.0327e-04, 3.5381e-03, 2.7313e-03,\n",
      "        8.2111e-04, 9.2840e-04, 9.7132e-04, 3.5858e-04, 4.2677e-04, 3.3760e-04,\n",
      "        3.8624e-04, 2.1400e-03, 2.6741e-03, 2.4033e-04, 3.8834e-03, 3.0308e-03,\n",
      "        2.0161e-03, 5.8022e-03, 1.1005e-03, 2.2888e-03, 4.5891e-03, 1.9588e-03,\n",
      "        1.4191e-03, 1.4133e-03, 3.0441e-03, 2.3155e-03, 1.9779e-03, 1.1263e-03,\n",
      "        7.5836e-03, 6.3362e-03, 5.9929e-03, 2.8351e-02, 1.5160e-02, 3.9398e-02,\n",
      "        1.4854e-02, 6.2599e-03, 5.3902e-03, 7.3357e-03, 7.9107e-04, 3.7270e-03,\n",
      "        2.4071e-03, 6.0272e-03, 6.9885e-03, 5.3444e-03, 3.8452e-02, 1.1864e-02,\n",
      "        7.1144e-03, 7.5951e-03, 6.6910e-03, 8.0719e-03, 8.6517e-03, 8.8577e-03,\n",
      "        1.5764e-03, 8.8577e-03, 8.6212e-03, 1.1505e-02, 2.0569e-02, 7.4387e-03,\n",
      "        9.0942e-03, 3.3905e-02, 1.1826e-02, 7.2365e-03, 8.7662e-03, 1.0170e-02,\n",
      "        1.0506e-02, 7.4043e-03, 7.8506e-03, 6.8703e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [35] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [35] : torch.Size([1, 32, 1, 107])\n",
      "Last layer attentions for generated token [35] : tensor([2.1240e-01, 2.1240e-01, 2.8515e-04, 3.2759e-04, 1.6892e-04, 2.2888e-03,\n",
      "        2.7537e-04, 3.3259e-04, 5.9080e-04, 1.3180e-03, 2.8706e-03, 4.7660e-04,\n",
      "        3.7766e-04, 6.9094e-04, 1.6876e-02, 1.3367e-02, 1.0757e-03, 1.1930e-03,\n",
      "        3.4332e-04, 5.0259e-04, 1.3161e-04, 6.2895e-04, 1.1314e-02, 2.1057e-03,\n",
      "        7.5150e-03, 1.0887e-02, 1.2293e-03, 7.3385e-04, 7.2956e-04, 7.4959e-04,\n",
      "        7.1955e-04, 1.7567e-03, 7.4387e-04, 7.8440e-04, 7.1383e-04, 5.0259e-04,\n",
      "        3.5548e-04, 8.5974e-04, 4.1890e-04, 2.5105e-04, 2.8934e-03, 2.6970e-03,\n",
      "        7.9823e-04, 7.6580e-04, 7.9966e-04, 2.7537e-04, 3.3927e-04, 4.1986e-04,\n",
      "        2.5654e-04, 1.9150e-03, 2.2354e-03, 2.9659e-04, 3.4199e-03, 3.1853e-03,\n",
      "        2.2602e-03, 6.1035e-03, 5.3692e-04, 1.5326e-03, 2.9564e-03, 1.0977e-03,\n",
      "        1.1520e-03, 1.0557e-03, 2.2297e-03, 1.6155e-03, 1.3313e-03, 1.1635e-03,\n",
      "        4.2801e-03, 5.5885e-03, 2.0561e-03, 2.4933e-02, 1.5434e-02, 3.3386e-02,\n",
      "        8.1635e-03, 4.3411e-03, 3.1528e-03, 7.3662e-03, 1.2655e-03, 5.6953e-03,\n",
      "        2.5787e-03, 6.6032e-03, 8.7891e-03, 5.0125e-03, 3.8483e-02, 8.4076e-03,\n",
      "        9.0179e-03, 1.4580e-02, 8.8501e-03, 6.3705e-03, 1.0460e-02, 9.4528e-03,\n",
      "        2.6169e-03, 1.6357e-02, 7.0648e-03, 7.5645e-03, 1.9806e-02, 1.2115e-02,\n",
      "        8.8501e-03, 4.0466e-02, 1.5388e-02, 9.6207e-03, 8.9111e-03, 6.7635e-03,\n",
      "        1.2985e-02, 6.6071e-03, 9.0790e-03, 8.0185e-03, 8.9035e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [36] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [36] : torch.Size([1, 32, 1, 108])\n",
      "Last layer attentions for generated token [36] : tensor([2.5415e-01, 2.5464e-01, 1.6856e-04, 2.5010e-04, 1.3077e-04, 2.1286e-03,\n",
      "        3.0065e-04, 4.9162e-04, 9.8896e-04, 8.8835e-04, 2.8400e-03, 8.2159e-04,\n",
      "        2.4819e-04, 3.9816e-04, 1.0826e-02, 1.1940e-02, 9.9277e-04, 6.9761e-04,\n",
      "        3.3593e-04, 2.4724e-04, 1.5116e-04, 4.5824e-04, 8.6136e-03, 1.1120e-03,\n",
      "        5.2948e-03, 1.1482e-02, 1.2264e-03, 4.9829e-04, 4.6015e-04, 1.0166e-03,\n",
      "        6.8140e-04, 1.5993e-03, 1.0090e-03, 8.7309e-04, 1.2074e-03, 5.4312e-04,\n",
      "        2.5511e-04, 7.7629e-04, 3.0231e-04, 1.7321e-04, 2.6340e-03, 1.5411e-03,\n",
      "        3.6192e-04, 4.9543e-04, 6.5279e-04, 1.9181e-04, 5.2929e-04, 4.0054e-04,\n",
      "        3.3665e-04, 2.0256e-03, 1.7023e-03, 3.1257e-04, 2.9106e-03, 2.7332e-03,\n",
      "        1.3895e-03, 4.0321e-03, 4.9925e-04, 1.4791e-03, 3.0479e-03, 1.3447e-03,\n",
      "        7.6580e-04, 1.0166e-03, 1.8768e-03, 1.5030e-03, 1.1606e-03, 6.6042e-04,\n",
      "        2.1152e-03, 4.1733e-03, 1.8663e-03, 2.2034e-02, 9.9411e-03, 2.5146e-02,\n",
      "        7.6714e-03, 6.5765e-03, 4.6463e-03, 1.4702e-02, 1.2550e-03, 6.4583e-03,\n",
      "        3.3569e-03, 8.1329e-03, 1.0567e-02, 5.5161e-03, 2.0691e-02, 6.0997e-03,\n",
      "        6.4774e-03, 1.2215e-02, 5.4665e-03, 5.4512e-03, 6.8550e-03, 7.3433e-03,\n",
      "        2.2583e-03, 1.5320e-02, 9.4070e-03, 1.4824e-02, 2.5101e-02, 1.3924e-02,\n",
      "        1.2962e-02, 2.2919e-02, 6.6833e-03, 7.7286e-03, 4.8027e-03, 3.6259e-03,\n",
      "        6.7177e-03, 5.8975e-03, 7.1068e-03, 8.5449e-03, 6.9389e-03, 3.6087e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [37] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [37] : torch.Size([1, 32, 1, 109])\n",
      "Last layer attentions for generated token [37] : tensor([2.1008e-01, 2.1008e-01, 2.5415e-04, 5.6314e-04, 1.3721e-04, 3.0861e-03,\n",
      "        3.4142e-04, 3.0136e-04, 4.8733e-04, 6.6710e-04, 1.5726e-03, 3.6073e-04,\n",
      "        4.0150e-04, 7.3004e-04, 1.4320e-02, 1.4969e-02, 1.2913e-03, 1.2150e-03,\n",
      "        5.1260e-04, 7.4577e-04, 2.4259e-04, 5.7650e-04, 9.6970e-03, 2.5387e-03,\n",
      "        1.0796e-02, 1.5144e-02, 4.3182e-03, 5.6076e-04, 6.3658e-04, 7.6199e-04,\n",
      "        1.0433e-03, 2.8152e-03, 1.3371e-03, 6.4564e-04, 7.4291e-04, 1.0662e-03,\n",
      "        3.1638e-04, 8.7023e-04, 6.4421e-04, 3.1519e-04, 4.1924e-03, 2.9793e-03,\n",
      "        1.2369e-03, 8.2064e-04, 7.0333e-04, 5.0974e-04, 2.2089e-04, 3.1948e-04,\n",
      "        2.6655e-04, 2.4147e-03, 2.9182e-03, 2.6131e-04, 3.3550e-03, 6.8321e-03,\n",
      "        2.0828e-03, 5.6953e-03, 1.1873e-03, 3.0861e-03, 6.6795e-03, 2.3594e-03,\n",
      "        1.9894e-03, 1.7204e-03, 4.4975e-03, 2.6684e-03, 2.5139e-03, 1.1482e-03,\n",
      "        1.1887e-02, 6.4774e-03, 4.8752e-03, 3.0258e-02, 1.5007e-02, 2.4994e-02,\n",
      "        9.4833e-03, 6.5613e-03, 4.2725e-03, 4.3869e-03, 7.9870e-04, 4.4289e-03,\n",
      "        1.4658e-03, 5.6648e-03, 5.5275e-03, 4.3106e-03, 3.2410e-02, 1.2314e-02,\n",
      "        6.7940e-03, 4.3335e-03, 5.8556e-03, 8.4381e-03, 6.6986e-03, 8.9493e-03,\n",
      "        2.0771e-03, 8.3389e-03, 1.0033e-02, 1.0338e-02, 2.3331e-02, 8.9493e-03,\n",
      "        8.0566e-03, 2.6917e-02, 1.6495e-02, 5.8861e-03, 8.9951e-03, 1.0139e-02,\n",
      "        9.6893e-03, 5.6190e-03, 6.9313e-03, 6.7024e-03, 7.8430e-03, 3.8929e-03,\n",
      "        1.3725e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [38] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [38] : torch.Size([1, 32, 1, 110])\n",
      "Last layer attentions for generated token [38] : tensor([3.0078e-01, 3.0005e-01, 1.9634e-04, 4.2963e-04, 1.0592e-04, 2.6226e-03,\n",
      "        2.2423e-04, 3.8218e-04, 4.7565e-04, 3.7479e-04, 1.6346e-03, 2.3866e-04,\n",
      "        2.5105e-04, 3.6693e-04, 6.6147e-03, 9.0637e-03, 3.9983e-04, 3.3784e-04,\n",
      "        2.4629e-04, 2.2781e-04, 7.2241e-05, 3.0589e-04, 1.1665e-02, 1.5783e-03,\n",
      "        5.2414e-03, 1.1955e-02, 2.0313e-03, 2.9993e-04, 3.3474e-04, 4.9257e-04,\n",
      "        5.5075e-04, 1.4763e-03, 8.6308e-04, 4.5753e-04, 4.6015e-04, 2.8467e-04,\n",
      "        1.7393e-04, 5.8842e-04, 3.2949e-04, 2.3687e-04, 3.4466e-03, 2.1019e-03,\n",
      "        5.5075e-04, 5.4646e-04, 6.7329e-04, 4.2224e-04, 3.8218e-04, 2.6226e-04,\n",
      "        3.2067e-04, 1.9150e-03, 1.8959e-03, 2.3866e-04, 2.6150e-03, 5.9242e-03,\n",
      "        1.7948e-03, 3.9558e-03, 8.9407e-04, 2.2545e-03, 6.8855e-03, 1.6899e-03,\n",
      "        1.5936e-03, 2.0943e-03, 4.0054e-03, 1.6184e-03, 2.1763e-03, 9.3317e-04,\n",
      "        5.4665e-03, 4.0207e-03, 3.9139e-03, 3.1250e-02, 1.3115e-02, 1.6342e-02,\n",
      "        6.0806e-03, 6.1035e-03, 2.6226e-03, 2.2869e-03, 5.3167e-04, 2.7027e-03,\n",
      "        1.0633e-03, 3.1605e-03, 3.6659e-03, 1.9894e-03, 1.6876e-02, 8.1940e-03,\n",
      "        5.5199e-03, 2.7390e-03, 2.7828e-03, 5.5771e-03, 5.6229e-03, 7.2861e-03,\n",
      "        9.2745e-04, 5.5237e-03, 7.3700e-03, 7.9346e-03, 1.4275e-02, 7.0572e-03,\n",
      "        5.9738e-03, 1.5518e-02, 6.2447e-03, 5.1498e-03, 3.9558e-03, 4.4518e-03,\n",
      "        6.6109e-03, 3.8757e-03, 4.3831e-03, 5.4474e-03, 3.3436e-03, 2.9831e-03,\n",
      "        9.1171e-03, 5.3291e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [39] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [39] : torch.Size([1, 32, 1, 111])\n",
      "Last layer attentions for generated token [39] : tensor([2.8564e-01, 2.8564e-01, 4.9067e-04, 8.3590e-04, 1.8084e-04, 5.6076e-03,\n",
      "        3.4165e-04, 6.4230e-04, 7.5245e-04, 5.6553e-04, 2.2678e-03, 2.1303e-04,\n",
      "        2.2328e-04, 4.4060e-04, 9.4833e-03, 1.1017e-02, 5.9652e-04, 3.6454e-04,\n",
      "        3.1543e-04, 2.2769e-04, 1.6081e-04, 9.2554e-04, 1.1993e-02, 2.0962e-03,\n",
      "        7.5531e-03, 1.9012e-02, 2.6798e-03, 3.8052e-04, 3.6597e-04, 6.1512e-04,\n",
      "        5.4312e-04, 1.7462e-03, 8.6927e-04, 5.7125e-04, 5.0306e-04, 2.2769e-04,\n",
      "        1.4246e-04, 8.4114e-04, 2.7466e-04, 1.9622e-04, 3.6144e-03, 1.6851e-03,\n",
      "        2.8229e-04, 4.5729e-04, 6.2513e-04, 2.7347e-04, 4.4918e-04, 1.9932e-04,\n",
      "        3.3116e-04, 2.2240e-03, 2.1820e-03, 3.9649e-04, 2.2812e-03, 1.2215e-02,\n",
      "        1.5316e-03, 5.7983e-03, 1.0319e-03, 2.6913e-03, 8.2855e-03, 2.0046e-03,\n",
      "        1.4677e-03, 2.9354e-03, 5.9090e-03, 2.4147e-03, 1.5869e-03, 1.0691e-03,\n",
      "        4.0321e-03, 4.8637e-03, 4.3068e-03, 3.3691e-02, 1.3565e-02, 1.6891e-02,\n",
      "        6.2675e-03, 4.8485e-03, 2.4815e-03, 2.4185e-03, 4.5991e-04, 3.2959e-03,\n",
      "        1.8587e-03, 3.4046e-03, 4.7531e-03, 3.4828e-03, 1.8509e-02, 6.1035e-03,\n",
      "        5.1270e-03, 2.7046e-03, 2.0733e-03, 3.4046e-03, 4.0741e-03, 5.4855e-03,\n",
      "        5.5265e-04, 4.0398e-03, 4.7607e-03, 6.1989e-03, 1.2062e-02, 8.8425e-03,\n",
      "        7.1449e-03, 1.4015e-02, 3.8586e-03, 5.4321e-03, 4.2305e-03, 2.6646e-03,\n",
      "        5.0850e-03, 3.3417e-03, 3.7460e-03, 5.2338e-03, 4.8485e-03, 2.7447e-03,\n",
      "        1.0452e-02, 7.1144e-03, 3.9368e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [40] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [40] : torch.Size([1, 32, 1, 112])\n",
      "Last layer attentions for generated token [40] : tensor([2.2168e-01, 2.2168e-01, 3.7980e-04, 6.1417e-04, 1.4472e-04, 4.9362e-03,\n",
      "        3.8052e-04, 5.3024e-04, 7.4959e-04, 8.8453e-04, 2.7065e-03, 2.5606e-04,\n",
      "        4.9829e-04, 8.2302e-04, 9.2926e-03, 1.5961e-02, 5.0020e-04, 5.1308e-04,\n",
      "        3.8433e-04, 5.1594e-04, 2.5749e-04, 7.3051e-04, 1.2405e-02, 2.4643e-03,\n",
      "        7.4577e-03, 2.1179e-02, 2.5406e-03, 6.6137e-04, 7.0524e-04, 5.1689e-04,\n",
      "        6.3372e-04, 1.5373e-03, 1.3733e-03, 5.6696e-04, 4.8876e-04, 6.9189e-04,\n",
      "        2.7728e-04, 7.2479e-04, 5.5790e-04, 4.3368e-04, 3.3417e-03, 2.7790e-03,\n",
      "        6.4707e-04, 6.5756e-04, 7.0143e-04, 5.3453e-04, 2.9635e-04, 3.3188e-04,\n",
      "        5.0688e-04, 1.9007e-03, 3.5305e-03, 4.1461e-04, 3.8166e-03, 7.9880e-03,\n",
      "        2.6913e-03, 8.7357e-03, 7.6437e-04, 3.3875e-03, 6.0692e-03, 2.1305e-03,\n",
      "        2.4662e-03, 2.7218e-03, 4.4022e-03, 3.0022e-03, 3.0022e-03, 1.1139e-03,\n",
      "        8.6136e-03, 7.2060e-03, 4.9934e-03, 3.4027e-02, 2.3895e-02, 3.4119e-02,\n",
      "        1.0887e-02, 5.7716e-03, 4.2763e-03, 3.5725e-03, 8.5068e-04, 6.2752e-03,\n",
      "        2.2755e-03, 7.4883e-03, 1.0765e-02, 3.2997e-03, 3.3386e-02, 7.0763e-03,\n",
      "        5.3635e-03, 2.7008e-03, 3.9043e-03, 4.0436e-03, 6.2408e-03, 7.7324e-03,\n",
      "        1.3151e-03, 5.5351e-03, 6.2752e-03, 7.2136e-03, 1.6022e-02, 8.4000e-03,\n",
      "        5.9280e-03, 2.1515e-02, 4.8790e-03, 4.3221e-03, 6.7596e-03, 4.0283e-03,\n",
      "        6.8398e-03, 4.4289e-03, 4.3602e-03, 5.3024e-03, 6.6833e-03, 3.6392e-03,\n",
      "        1.2253e-02, 8.0261e-03, 5.0507e-03, 5.0430e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [41] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [41] : torch.Size([1, 32, 1, 113])\n",
      "Last layer attentions for generated token [41] : tensor([2.7515e-01, 2.7417e-01, 3.6359e-04, 5.2500e-04, 1.6105e-04, 3.7117e-03,\n",
      "        3.2711e-04, 5.5313e-04, 7.2861e-04, 7.0906e-04, 2.9259e-03, 7.9441e-04,\n",
      "        4.3607e-04, 4.2510e-04, 1.3084e-02, 1.2383e-02, 7.5483e-04, 5.8556e-04,\n",
      "        4.3011e-04, 4.2939e-04, 2.3341e-04, 1.0796e-03, 9.7656e-03, 1.6041e-03,\n",
      "        6.8779e-03, 1.2253e-02, 1.8396e-03, 3.7956e-04, 4.1127e-04, 6.7663e-04,\n",
      "        1.2789e-03, 2.3346e-03, 1.4496e-03, 7.0763e-04, 6.9141e-04, 5.1880e-04,\n",
      "        7.5161e-05, 7.8964e-04, 3.7074e-04, 1.9884e-04, 4.4212e-03, 1.4324e-03,\n",
      "        4.1699e-04, 5.7650e-04, 8.0681e-04, 1.9884e-04, 7.4911e-04, 3.4499e-04,\n",
      "        3.4237e-04, 2.4204e-03, 1.9016e-03, 2.6751e-04, 2.0943e-03, 4.9286e-03,\n",
      "        1.0958e-03, 6.5651e-03, 4.3607e-04, 1.9817e-03, 5.8556e-03, 2.5940e-03,\n",
      "        1.2035e-03, 2.2278e-03, 5.5084e-03, 2.2850e-03, 2.1839e-03, 1.2970e-03,\n",
      "        3.2310e-03, 4.7836e-03, 3.3207e-03, 2.6688e-02, 1.2695e-02, 2.2781e-02,\n",
      "        5.4016e-03, 6.2370e-03, 2.5597e-03, 6.9351e-03, 7.0524e-04, 2.3689e-03,\n",
      "        2.2011e-03, 4.4212e-03, 4.5662e-03, 4.4899e-03, 1.4664e-02, 3.7594e-03,\n",
      "        5.7678e-03, 4.8141e-03, 3.1567e-03, 2.6035e-03, 5.5695e-03, 5.2795e-03,\n",
      "        7.9250e-04, 5.5847e-03, 5.1003e-03, 6.3515e-03, 1.4099e-02, 6.8588e-03,\n",
      "        8.4534e-03, 1.6418e-02, 3.6831e-03, 5.6152e-03, 4.8027e-03, 2.0504e-03,\n",
      "        6.1226e-03, 4.7493e-03, 5.4436e-03, 8.9722e-03, 3.7918e-03, 4.0512e-03,\n",
      "        1.3641e-02, 6.0120e-03, 2.3155e-03, 7.1182e-03, 1.2772e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [42] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [42] : torch.Size([1, 32, 1, 114])\n",
      "Last layer attentions for generated token [42] : tensor([2.8247e-01, 2.8247e-01, 1.9717e-04, 4.6468e-04, 1.0186e-04, 1.8520e-03,\n",
      "        1.8513e-04, 3.9577e-04, 4.7016e-04, 3.5357e-04, 1.5841e-03, 1.3757e-04,\n",
      "        2.3961e-04, 2.9087e-04, 6.4125e-03, 7.9041e-03, 2.4533e-04, 2.9182e-04,\n",
      "        1.9181e-04, 2.1815e-04, 5.7817e-05, 2.6107e-04, 1.0124e-02, 1.1301e-03,\n",
      "        4.1580e-03, 8.7051e-03, 1.0843e-03, 3.3998e-04, 4.4417e-04, 4.8041e-04,\n",
      "        6.2418e-04, 1.6413e-03, 5.1212e-04, 4.4346e-04, 4.3488e-04, 3.6764e-04,\n",
      "        1.1772e-04, 4.8041e-04, 1.9562e-04, 1.6153e-04, 3.8033e-03, 1.7776e-03,\n",
      "        2.6631e-04, 5.2357e-04, 6.7997e-04, 2.8181e-04, 4.5562e-04, 2.8563e-04,\n",
      "        2.4533e-04, 1.7633e-03, 1.8702e-03, 2.1648e-04, 2.2850e-03, 3.5419e-03,\n",
      "        1.6508e-03, 3.8757e-03, 1.0014e-03, 1.6441e-03, 8.7509e-03, 2.3251e-03,\n",
      "        1.5564e-03, 1.5001e-03, 3.6716e-03, 1.7433e-03, 1.3657e-03, 1.0948e-03,\n",
      "        4.0207e-03, 2.5120e-03, 4.9744e-03, 2.7512e-02, 1.4351e-02, 1.7136e-02,\n",
      "        5.8289e-03, 6.3972e-03, 1.8997e-03, 2.7447e-03, 5.2738e-04, 1.3847e-03,\n",
      "        1.1454e-03, 2.1458e-03, 2.2106e-03, 2.2736e-03, 1.6312e-02, 7.5645e-03,\n",
      "        5.7030e-03, 3.1910e-03, 2.1172e-03, 4.3907e-03, 4.8943e-03, 6.7215e-03,\n",
      "        1.2026e-03, 6.9313e-03, 6.2904e-03, 9.9792e-03, 1.1017e-02, 5.3711e-03,\n",
      "        6.6910e-03, 1.8387e-02, 7.7133e-03, 5.7259e-03, 4.7913e-03, 5.1193e-03,\n",
      "        7.2632e-03, 5.4665e-03, 6.7596e-03, 5.6114e-03, 6.3133e-03, 5.3635e-03,\n",
      "        1.4999e-02, 1.1223e-02, 4.2229e-03, 8.2016e-03, 1.2115e-02, 8.8348e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [43] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [43] : torch.Size([1, 32, 1, 115])\n",
      "Last layer attentions for generated token [43] : tensor([2.8516e-01, 2.8516e-01, 3.8719e-04, 5.8842e-04, 1.9205e-04, 4.0894e-03,\n",
      "        2.8062e-04, 4.8184e-04, 5.8937e-04, 5.6887e-04, 2.3327e-03, 3.2544e-04,\n",
      "        3.0637e-04, 3.9649e-04, 9.8114e-03, 1.1002e-02, 5.6458e-04, 5.0116e-04,\n",
      "        3.6097e-04, 2.1422e-04, 8.2314e-05, 6.0940e-04, 1.1208e-02, 1.4505e-03,\n",
      "        5.3673e-03, 1.2177e-02, 1.3151e-03, 3.4451e-04, 4.9257e-04, 6.7186e-04,\n",
      "        5.6028e-04, 1.2894e-03, 9.3985e-04, 5.1212e-04, 5.2929e-04, 2.9469e-04,\n",
      "        1.4222e-04, 9.6798e-04, 2.4092e-04, 1.7560e-04, 2.6913e-03, 1.2522e-03,\n",
      "        2.1684e-04, 4.4131e-04, 5.7888e-04, 1.8907e-04, 4.5371e-04, 2.9063e-04,\n",
      "        2.5153e-04, 1.9665e-03, 1.8616e-03, 2.2900e-04, 2.1782e-03, 6.3133e-03,\n",
      "        1.1492e-03, 7.3891e-03, 8.5068e-04, 2.1133e-03, 7.2556e-03, 2.5406e-03,\n",
      "        1.6308e-03, 1.8806e-03, 5.6114e-03, 3.1357e-03, 1.6727e-03, 1.4029e-03,\n",
      "        2.9812e-03, 3.6964e-03, 3.6449e-03, 2.2293e-02, 1.1467e-02, 1.8158e-02,\n",
      "        5.2948e-03, 4.3259e-03, 1.9855e-03, 4.0436e-03, 6.0940e-04, 2.4166e-03,\n",
      "        2.2392e-03, 2.4700e-03, 3.2139e-03, 3.8738e-03, 1.6220e-02, 5.0812e-03,\n",
      "        4.5967e-03, 3.6201e-03, 2.3556e-03, 3.2864e-03, 4.1847e-03, 4.6806e-03,\n",
      "        6.7568e-04, 4.0169e-03, 3.8471e-03, 5.1880e-03, 1.1497e-02, 1.1391e-02,\n",
      "        9.5520e-03, 1.6022e-02, 4.7455e-03, 5.4131e-03, 3.7060e-03, 1.9207e-03,\n",
      "        3.6354e-03, 3.0041e-03, 3.6850e-03, 4.5891e-03, 4.6768e-03, 2.8095e-03,\n",
      "        1.2604e-02, 9.5825e-03, 3.9711e-03, 7.2670e-03, 1.3351e-02, 1.1421e-02,\n",
      "        6.5651e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [44] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [44] : torch.Size([1, 32, 1, 116])\n",
      "Last layer attentions for generated token [44] : tensor([1.7786e-01, 1.7786e-01, 2.3830e-04, 3.9840e-04, 1.2660e-04, 3.7689e-03,\n",
      "        3.1567e-04, 3.8528e-04, 5.1928e-04, 4.9591e-04, 2.2049e-03, 4.7302e-04,\n",
      "        4.9114e-04, 8.2064e-04, 1.9852e-02, 1.9119e-02, 4.7016e-04, 6.7091e-04,\n",
      "        4.5228e-04, 7.7534e-04, 5.6601e-04, 1.0490e-03, 9.0256e-03, 2.5806e-03,\n",
      "        2.1545e-02, 2.1378e-02, 2.7275e-03, 6.5184e-04, 6.0129e-04, 5.9080e-04,\n",
      "        5.7507e-04, 2.1572e-03, 9.8991e-04, 3.5501e-04, 4.3654e-04, 5.4550e-04,\n",
      "        3.0541e-04, 1.1110e-03, 4.7302e-04, 2.8801e-04, 6.0005e-03, 3.0308e-03,\n",
      "        5.7077e-04, 7.2575e-04, 7.7248e-04, 4.0460e-04, 3.0255e-04, 3.3355e-04,\n",
      "        2.8515e-04, 5.4932e-03, 5.2071e-03, 3.6263e-04, 2.8610e-03, 6.7139e-03,\n",
      "        2.1267e-03, 8.2550e-03, 1.5240e-03, 1.9150e-03, 9.3460e-03, 2.1038e-03,\n",
      "        2.0523e-03, 1.7452e-03, 4.5547e-03, 4.2877e-03, 2.5749e-03, 1.8415e-03,\n",
      "        9.3231e-03, 4.8828e-03, 5.9586e-03, 3.5858e-02, 2.2415e-02, 2.9358e-02,\n",
      "        7.3891e-03, 6.0005e-03, 3.1338e-03, 3.6488e-03, 4.7946e-04, 2.8896e-03,\n",
      "        1.3790e-03, 2.6989e-03, 4.7951e-03, 8.3466e-03, 3.6621e-02, 1.0544e-02,\n",
      "        6.5536e-03, 3.0479e-03, 4.5242e-03, 6.3972e-03, 8.2092e-03, 8.8043e-03,\n",
      "        1.2512e-03, 4.6577e-03, 7.4348e-03, 1.1948e-02, 1.9272e-02, 1.1864e-02,\n",
      "        9.4604e-03, 2.4582e-02, 8.4229e-03, 5.6076e-03, 6.7253e-03, 5.9891e-03,\n",
      "        7.5989e-03, 4.4785e-03, 4.2610e-03, 4.6959e-03, 7.9041e-03, 3.6087e-03,\n",
      "        1.7044e-02, 8.0032e-03, 2.7809e-03, 5.6877e-03, 8.5983e-03, 6.2790e-03,\n",
      "        4.4479e-03, 1.3474e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [45] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [45] : torch.Size([1, 32, 1, 117])\n",
      "Last layer attentions for generated token [45] : tensor([3.2373e-01, 3.2300e-01, 9.0897e-05, 1.7798e-04, 4.6790e-05, 1.7977e-03,\n",
      "        2.3031e-04, 4.1151e-04, 4.1628e-04, 2.6512e-04, 1.9512e-03, 2.8181e-04,\n",
      "        1.8656e-04, 4.1962e-04, 8.4991e-03, 7.2174e-03, 2.0099e-04, 1.5831e-04,\n",
      "        1.1230e-04, 1.1951e-04, 7.5102e-05, 3.0279e-04, 9.9640e-03, 1.0061e-03,\n",
      "        4.3526e-03, 7.6103e-03, 5.7125e-04, 2.7418e-04, 2.8944e-04, 2.9230e-04,\n",
      "        2.9302e-04, 5.7030e-04, 4.0126e-04, 3.1066e-04, 3.4308e-04, 2.7728e-04,\n",
      "        2.2149e-04, 7.9489e-04, 2.0492e-04, 1.5163e-04, 1.4935e-03, 8.8310e-04,\n",
      "        1.3030e-04, 2.7728e-04, 4.9543e-04, 2.3031e-04, 3.6669e-04, 2.7251e-04,\n",
      "        2.2852e-04, 1.5564e-03, 1.5259e-03, 8.2791e-05, 1.6756e-03, 1.2646e-03,\n",
      "        1.3943e-03, 3.4828e-03, 6.9332e-04, 1.4334e-03, 4.1122e-03, 1.1139e-03,\n",
      "        1.0824e-03, 9.1648e-04, 2.7905e-03, 2.1210e-03, 2.0142e-03, 9.9087e-04,\n",
      "        2.5177e-03, 1.1139e-03, 3.0193e-03, 1.2672e-02, 6.0158e-03, 1.0429e-02,\n",
      "        2.2869e-03, 3.6564e-03, 1.2674e-03, 3.7994e-03, 6.9571e-04, 4.2343e-03,\n",
      "        1.1749e-03, 1.9627e-03, 4.0169e-03, 2.9106e-03, 1.2619e-02, 3.4752e-03,\n",
      "        3.9711e-03, 1.9970e-03, 2.4338e-03, 2.6627e-03, 3.0937e-03, 5.1079e-03,\n",
      "        1.1768e-03, 4.9438e-03, 6.1951e-03, 5.4779e-03, 1.2093e-02, 1.0414e-02,\n",
      "        4.8523e-03, 9.1553e-03, 3.7251e-03, 4.4899e-03, 4.1237e-03, 2.4166e-03,\n",
      "        3.9711e-03, 3.6316e-03, 5.2376e-03, 5.3635e-03, 4.6425e-03, 3.5782e-03,\n",
      "        1.4397e-02, 7.8812e-03, 3.7498e-03, 4.5052e-03, 1.1505e-02, 6.9504e-03,\n",
      "        6.7444e-03, 9.4452e-03, 1.1810e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [46] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [46] : torch.Size([1, 32, 1, 118])\n",
      "Last layer attentions for generated token [46] : tensor([2.7686e-01, 2.7686e-01, 2.6608e-04, 4.2200e-04, 1.2815e-04, 3.0823e-03,\n",
      "        2.1219e-04, 4.5729e-04, 2.3675e-04, 5.3358e-04, 1.6174e-03, 3.9268e-04,\n",
      "        4.1056e-04, 6.3086e-04, 1.1818e-02, 1.0712e-02, 5.4836e-04, 3.8266e-04,\n",
      "        2.5392e-04, 1.4138e-04, 7.2181e-05, 5.6553e-04, 1.3016e-02, 1.2207e-03,\n",
      "        3.0041e-03, 6.0692e-03, 5.0783e-04, 2.7514e-04, 3.5262e-04, 3.9792e-04,\n",
      "        5.9748e-04, 5.0592e-04, 6.6376e-04, 3.6454e-04, 4.5800e-04, 2.0170e-04,\n",
      "        1.2922e-04, 7.1907e-04, 2.4140e-04, 1.5640e-04, 1.6756e-03, 6.8092e-04,\n",
      "        1.6785e-04, 2.7037e-04, 3.4380e-04, 1.4186e-04, 1.4412e-04, 1.9395e-04,\n",
      "        1.0926e-04, 8.3923e-04, 8.0061e-04, 1.1009e-04, 1.0443e-03, 3.9101e-03,\n",
      "        8.9169e-04, 6.7825e-03, 5.5456e-04, 2.5444e-03, 5.2414e-03, 1.7319e-03,\n",
      "        1.0023e-03, 1.0509e-03, 4.9438e-03, 1.8988e-03, 3.4103e-03, 1.0548e-03,\n",
      "        2.2564e-03, 1.8473e-03, 1.8654e-03, 2.0767e-02, 8.2703e-03, 1.0727e-02,\n",
      "        2.0981e-03, 2.7084e-03, 1.4935e-03, 3.5095e-03, 1.0509e-03, 3.7136e-03,\n",
      "        1.6918e-03, 1.9932e-03, 2.2526e-03, 2.4147e-03, 1.5266e-02, 4.2381e-03,\n",
      "        3.5820e-03, 3.7136e-03, 2.5692e-03, 3.5133e-03, 2.9755e-03, 5.4970e-03,\n",
      "        1.4877e-03, 4.3602e-03, 4.7913e-03, 3.5858e-03, 1.1833e-02, 1.0773e-02,\n",
      "        5.9738e-03, 1.3672e-02, 5.4817e-03, 6.8893e-03, 5.7030e-03, 2.2125e-03,\n",
      "        3.3321e-03, 2.9659e-03, 4.2000e-03, 6.3782e-03, 5.6915e-03, 2.2411e-03,\n",
      "        1.6418e-02, 1.4763e-02, 5.3024e-03, 7.6141e-03, 2.3499e-02, 1.8082e-02,\n",
      "        1.0284e-02, 1.2688e-02, 1.4236e-02, 8.4534e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [47] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [47] : torch.Size([1, 32, 1, 119])\n",
      "Last layer attentions for generated token [47] : tensor([2.0728e-01, 2.0691e-01, 2.9445e-04, 4.4465e-04, 1.7142e-04, 2.9488e-03,\n",
      "        2.2185e-04, 3.1233e-04, 3.6001e-04, 5.9843e-04, 1.6298e-03, 3.0923e-04,\n",
      "        3.0804e-04, 6.3467e-04, 1.7838e-02, 1.3763e-02, 6.3467e-04, 9.9087e-04,\n",
      "        3.1042e-04, 3.9935e-04, 3.4571e-04, 1.4105e-03, 6.4430e-03, 1.7757e-03,\n",
      "        1.1551e-02, 1.3954e-02, 2.9793e-03, 9.1219e-04, 6.6996e-04, 6.9284e-04,\n",
      "        8.0824e-04, 2.4872e-03, 7.7868e-04, 5.0879e-04, 7.2336e-04, 6.8188e-04,\n",
      "        2.0754e-04, 1.0118e-03, 4.8447e-04, 3.0923e-04, 6.7863e-03, 2.2202e-03,\n",
      "        6.8760e-04, 1.0481e-03, 8.3256e-04, 4.3344e-04, 4.1604e-04, 3.0255e-04,\n",
      "        3.8409e-04, 6.0883e-03, 4.6730e-03, 4.7803e-04, 1.6460e-03, 1.4664e-02,\n",
      "        1.4296e-03, 7.8583e-03, 1.1950e-03, 1.7300e-03, 6.1111e-03, 1.4811e-03,\n",
      "        1.4954e-03, 1.1787e-03, 3.5744e-03, 2.0161e-03, 1.5736e-03, 1.2989e-03,\n",
      "        5.8136e-03, 3.5038e-03, 2.2354e-03, 2.8183e-02, 1.8936e-02, 2.7039e-02,\n",
      "        7.7324e-03, 4.9362e-03, 1.6232e-03, 2.7428e-03, 5.0879e-04, 2.2602e-03,\n",
      "        1.2255e-03, 2.7943e-03, 3.7079e-03, 7.7477e-03, 3.6163e-02, 1.3138e-02,\n",
      "        8.3694e-03, 4.6501e-03, 3.6297e-03, 5.7411e-03, 5.8327e-03, 6.0081e-03,\n",
      "        1.3752e-03, 4.4594e-03, 3.9635e-03, 9.0866e-03, 1.1436e-02, 9.3689e-03,\n",
      "        9.0332e-03, 2.9388e-02, 1.0742e-02, 5.7373e-03, 5.8441e-03, 7.4463e-03,\n",
      "        8.4305e-03, 6.0005e-03, 5.4436e-03, 4.5433e-03, 9.0637e-03, 2.5787e-03,\n",
      "        1.5961e-02, 6.3553e-03, 2.0218e-03, 6.0539e-03, 6.8359e-03, 5.5084e-03,\n",
      "        2.3804e-03, 1.0651e-02, 1.1086e-02, 2.1515e-03, 5.8479e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [48] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [48] : torch.Size([1, 32, 1, 120])\n",
      "Last layer attentions for generated token [48] : tensor([1.9275e-01, 1.9275e-01, 2.2483e-04, 4.1437e-04, 1.5879e-04, 2.7142e-03,\n",
      "        1.8561e-04, 3.1328e-04, 3.4881e-04, 4.1580e-04, 2.0924e-03, 4.2820e-04,\n",
      "        2.9373e-04, 3.8838e-04, 1.0597e-02, 1.0376e-02, 5.5981e-04, 4.7684e-04,\n",
      "        3.9601e-04, 3.5787e-04, 9.5904e-05, 4.6587e-04, 4.2038e-03, 1.4687e-03,\n",
      "        4.3030e-03, 1.5656e-02, 2.7122e-03, 3.6836e-04, 3.1948e-04, 4.6396e-04,\n",
      "        4.1175e-04, 1.2369e-03, 8.8549e-04, 5.7983e-04, 6.7902e-04, 4.0388e-04,\n",
      "        2.0552e-04, 8.7214e-04, 2.3746e-04, 2.3282e-04, 4.8218e-03, 1.3771e-03,\n",
      "        7.4577e-04, 9.9945e-04, 1.0748e-03, 3.2830e-04, 6.4898e-04, 2.3699e-04,\n",
      "        3.4809e-04, 3.3321e-03, 1.8101e-03, 3.3283e-04, 3.0556e-03, 6.4659e-03,\n",
      "        2.1706e-03, 5.6992e-03, 5.9462e-04, 8.5163e-04, 3.9444e-03, 1.0977e-03,\n",
      "        9.5367e-04, 8.9788e-04, 2.8496e-03, 1.3962e-03, 7.3576e-04, 6.6090e-04,\n",
      "        2.4967e-03, 5.0697e-03, 2.2278e-03, 2.8732e-02, 2.0660e-02, 4.0863e-02,\n",
      "        2.1149e-02, 1.4015e-02, 4.1466e-03, 5.8861e-03, 9.9945e-04, 3.2558e-03,\n",
      "        3.0251e-03, 6.7596e-03, 8.0261e-03, 6.8474e-03, 3.2410e-02, 1.3603e-02,\n",
      "        1.2474e-02, 9.4604e-03, 4.9171e-03, 6.5765e-03, 7.0419e-03, 5.0163e-03,\n",
      "        6.3896e-04, 5.6763e-03, 7.0190e-03, 1.1299e-02, 1.4221e-02, 1.0468e-02,\n",
      "        1.2077e-02, 3.0380e-02, 9.2926e-03, 8.7204e-03, 5.3139e-03, 5.4321e-03,\n",
      "        1.2703e-02, 6.2256e-03, 5.8670e-03, 7.6485e-03, 6.3248e-03, 2.7084e-03,\n",
      "        1.2688e-02, 4.5700e-03, 1.2197e-03, 3.9825e-03, 5.0240e-03, 4.4594e-03,\n",
      "        1.2712e-03, 1.2421e-02, 7.8888e-03, 8.6021e-04, 6.6299e-03, 1.1841e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [49] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [49] : torch.Size([1, 32, 1, 121])\n",
      "Last layer attentions for generated token [49] : tensor([3.3569e-01, 3.3569e-01, 1.6320e-04, 2.8634e-04, 1.0252e-04, 2.0351e-03,\n",
      "        1.1843e-04, 2.4116e-04, 2.8849e-04, 3.0303e-04, 1.0748e-03, 1.8203e-04,\n",
      "        2.6488e-04, 5.7840e-04, 1.1063e-02, 8.6517e-03, 1.4973e-04, 3.2330e-04,\n",
      "        3.3545e-04, 5.2691e-04, 2.0146e-04, 5.7745e-04, 1.7944e-02, 9.8419e-04,\n",
      "        4.8561e-03, 6.5079e-03, 2.3282e-04, 1.1212e-04, 1.9681e-04, 3.1257e-04,\n",
      "        6.0749e-04, 1.3609e-03, 2.2924e-04, 3.1948e-04, 3.4809e-04, 3.7265e-04,\n",
      "        3.3212e-04, 4.6396e-04, 2.2745e-04, 1.5450e-04, 3.4008e-03, 9.5367e-04,\n",
      "        3.4809e-04, 3.9291e-04, 3.7575e-04, 2.3282e-04, 2.1780e-04, 2.7871e-04,\n",
      "        1.9300e-04, 3.2635e-03, 1.7405e-03, 1.2314e-04, 2.6302e-03, 2.5082e-03,\n",
      "        1.6346e-03, 5.3596e-03, 1.0490e-03, 1.5574e-03, 1.0239e-02, 2.3441e-03,\n",
      "        2.0828e-03, 1.4830e-03, 7.3509e-03, 5.2719e-03, 1.1988e-03, 1.4601e-03,\n",
      "        3.8643e-03, 1.5697e-03, 4.6768e-03, 9.3460e-03, 4.1466e-03, 1.0399e-02,\n",
      "        6.0349e-03, 2.8439e-03, 7.5006e-04, 2.6913e-03, 9.4795e-04, 9.8419e-04,\n",
      "        9.1553e-04, 1.9150e-03, 1.8272e-03, 4.3602e-03, 7.0267e-03, 2.1591e-03,\n",
      "        1.2388e-03, 1.7748e-03, 1.3771e-03, 1.9836e-03, 2.7466e-03, 2.0428e-03,\n",
      "        3.8838e-04, 2.9545e-03, 1.7614e-03, 4.0588e-03, 8.1863e-03, 3.1109e-03,\n",
      "        2.8648e-03, 8.0795e-03, 3.6774e-03, 1.2760e-03, 1.9760e-03, 1.2436e-03,\n",
      "        2.6569e-03, 1.4515e-03, 1.5030e-03, 1.7710e-03, 1.5125e-03, 1.5097e-03,\n",
      "        3.8280e-03, 3.0251e-03, 1.2197e-03, 3.6449e-03, 2.6722e-03, 5.4016e-03,\n",
      "        3.8395e-03, 1.2466e-02, 7.3509e-03, 1.5907e-03, 7.0648e-03, 1.6357e-02,\n",
      "        1.2039e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [50] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [50] : torch.Size([1, 32, 1, 122])\n",
      "Last layer attentions for generated token [50] : tensor([3.8110e-01, 3.8110e-01, 3.8624e-04, 3.6287e-04, 2.6798e-04, 1.1711e-03,\n",
      "        1.7047e-05, 1.0830e-04, 1.7512e-04, 2.4796e-04, 8.4019e-04, 2.1124e-04,\n",
      "        1.6642e-04, 7.6485e-04, 1.9653e-02, 5.3139e-03, 9.5904e-05, 2.8634e-04,\n",
      "        1.3793e-04, 1.2410e-04, 1.2958e-04, 4.6396e-04, 2.0889e-02, 7.6056e-04,\n",
      "        2.2602e-03, 2.9621e-03, 6.5446e-05, 5.6624e-05, 9.8586e-05, 1.3375e-04,\n",
      "        2.0552e-04, 4.5586e-04, 6.3896e-05, 8.8036e-05, 1.5819e-04, 1.0055e-04,\n",
      "        3.4213e-04, 4.1509e-04, 1.2958e-04, 7.1585e-05, 2.3193e-03, 6.0749e-04,\n",
      "        2.1875e-04, 1.0997e-04, 1.0622e-04, 6.9380e-05, 1.3113e-04, 1.5152e-04,\n",
      "        5.8889e-05, 1.0252e-03, 6.9380e-04, 1.7715e-04, 3.6640e-03, 6.7291e-03,\n",
      "        1.3933e-03, 2.9087e-03, 2.3472e-04, 7.0620e-04, 7.3357e-03, 8.6498e-04,\n",
      "        5.8317e-04, 6.6328e-04, 3.7689e-03, 1.2054e-03, 6.7663e-04, 1.0376e-03,\n",
      "        1.7786e-03, 1.2636e-03, 3.7498e-03, 8.3084e-03, 5.1575e-03, 3.5095e-03,\n",
      "        8.9455e-04, 1.2589e-03, 2.3103e-04, 1.0996e-03, 6.8951e-04, 5.4884e-04,\n",
      "        6.7091e-04, 4.7040e-04, 4.1580e-04, 1.6193e-03, 2.7199e-03, 4.2987e-04,\n",
      "        4.0078e-04, 1.6317e-03, 4.4703e-04, 3.9387e-04, 6.3658e-04, 7.9060e-04,\n",
      "        3.1090e-04, 1.0920e-03, 7.7534e-04, 3.4790e-03, 2.2736e-03, 2.8210e-03,\n",
      "        2.3022e-03, 5.7564e-03, 1.4458e-03, 8.3208e-04, 7.9393e-04, 3.7003e-04,\n",
      "        1.3876e-03, 1.0252e-03, 5.6076e-04, 1.1988e-03, 4.8161e-04, 3.1948e-04,\n",
      "        2.2964e-03, 2.0752e-03, 4.5490e-04, 2.3041e-03, 1.4458e-03, 4.7874e-03,\n",
      "        1.5755e-03, 1.0086e-02, 3.9139e-03, 1.9798e-03, 3.4046e-03, 1.5213e-02,\n",
      "        1.3664e-02, 1.0666e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [51] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [51] : torch.Size([1, 32, 1, 123])\n",
      "Last layer attentions for generated token [51] : tensor([1.9165e-01, 1.9238e-01, 2.5272e-04, 3.3474e-04, 1.6093e-04, 2.1515e-03,\n",
      "        3.3164e-04, 5.0354e-04, 5.0163e-04, 6.1846e-04, 3.2101e-03, 3.6693e-04,\n",
      "        6.5184e-04, 6.0606e-04, 1.8021e-02, 1.2581e-02, 4.0555e-04, 6.3658e-04,\n",
      "        3.3212e-04, 3.5357e-04, 8.6486e-05, 3.3951e-04, 8.4686e-03, 1.4124e-03,\n",
      "        7.2441e-03, 1.7563e-02, 1.5821e-03, 5.8651e-04, 9.1743e-04, 4.0078e-04,\n",
      "        5.7745e-04, 2.4643e-03, 1.0681e-03, 6.7234e-04, 4.2582e-04, 5.5647e-04,\n",
      "        3.0732e-04, 4.3321e-04, 4.3154e-04, 3.0661e-04, 3.6640e-03, 2.5482e-03,\n",
      "        6.3181e-04, 8.5497e-04, 8.2541e-04, 5.8174e-04, 3.2520e-04, 3.5286e-04,\n",
      "        5.9462e-04, 4.4060e-03, 3.6602e-03, 2.7966e-04, 3.6392e-03, 4.6692e-03,\n",
      "        2.0084e-03, 7.3395e-03, 9.6321e-04, 3.4485e-03, 8.3923e-03, 1.8549e-03,\n",
      "        2.8610e-03, 2.8439e-03, 5.2872e-03, 2.2469e-03, 1.2932e-03, 7.0858e-04,\n",
      "        7.8125e-03, 2.6722e-03, 4.7836e-03, 2.0569e-02, 1.4435e-02, 2.4780e-02,\n",
      "        6.5804e-03, 4.7302e-03, 2.1229e-03, 6.0921e-03, 5.0640e-04, 2.4166e-03,\n",
      "        1.7357e-03, 4.5738e-03, 6.7863e-03, 5.5962e-03, 3.2623e-02, 7.9575e-03,\n",
      "        6.8092e-03, 5.0163e-03, 4.9973e-03, 5.0926e-03, 9.3765e-03, 8.5449e-03,\n",
      "        1.0958e-03, 6.2332e-03, 7.1831e-03, 8.1100e-03, 1.9485e-02, 6.5002e-03,\n",
      "        7.4577e-03, 2.7039e-02, 7.0229e-03, 6.8550e-03, 6.3896e-03, 3.5267e-03,\n",
      "        8.8043e-03, 4.6539e-03, 5.2795e-03, 6.8054e-03, 4.0474e-03, 4.3678e-03,\n",
      "        1.0742e-02, 6.9962e-03, 2.9087e-03, 6.1951e-03, 8.4915e-03, 6.2828e-03,\n",
      "        3.5515e-03, 1.3138e-02, 1.0063e-02, 9.8038e-04, 5.5275e-03, 2.5009e-02,\n",
      "        6.5804e-03, 1.9188e-03, 1.6037e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [52] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [52] : torch.Size([1, 32, 1, 124])\n",
      "Last layer attentions for generated token [52] : tensor([1.5918e-01, 1.5918e-01, 3.5071e-04, 4.7588e-04, 2.5821e-04, 5.3902e-03,\n",
      "        3.6764e-04, 7.2956e-04, 8.4305e-04, 4.9162e-04, 2.7065e-03, 4.3654e-04,\n",
      "        4.3917e-04, 5.6267e-04, 1.0994e-02, 1.0315e-02, 3.6192e-04, 4.0627e-04,\n",
      "        1.7166e-04, 2.1195e-04, 8.3327e-05, 3.8004e-04, 8.1406e-03, 1.3847e-03,\n",
      "        4.8218e-03, 2.7618e-02, 1.1501e-03, 3.1328e-04, 5.7268e-04, 6.4135e-04,\n",
      "        8.4305e-04, 2.0390e-03, 1.9588e-03, 1.1568e-03, 7.2145e-04, 3.8671e-04,\n",
      "        2.6274e-04, 7.5006e-04, 2.9778e-04, 2.6059e-04, 3.5400e-03, 1.7576e-03,\n",
      "        3.9220e-04, 5.9795e-04, 8.5831e-04, 3.2139e-04, 6.3515e-04, 2.7704e-04,\n",
      "        6.1464e-04, 3.5019e-03, 2.3766e-03, 4.9162e-04, 3.7460e-03, 7.5645e-03,\n",
      "        1.7834e-03, 7.5340e-03, 3.3927e-04, 1.1845e-03, 4.2381e-03, 1.0872e-03,\n",
      "        1.1797e-03, 1.9188e-03, 3.5877e-03, 1.0738e-03, 6.8140e-04, 4.1914e-04,\n",
      "        2.4853e-03, 4.8370e-03, 2.3766e-03, 3.3051e-02, 2.3773e-02, 3.6194e-02,\n",
      "        8.2321e-03, 7.7934e-03, 3.1757e-03, 1.1681e-02, 5.2643e-04, 2.9259e-03,\n",
      "        4.4365e-03, 5.6686e-03, 1.1185e-02, 9.5367e-03, 3.8727e-02, 7.5493e-03,\n",
      "        9.6436e-03, 8.4381e-03, 3.8948e-03, 4.8294e-03, 1.1230e-02, 8.2397e-03,\n",
      "        8.9741e-04, 4.6272e-03, 5.7793e-03, 1.0925e-02, 1.1131e-02, 1.0246e-02,\n",
      "        1.0887e-02, 3.0396e-02, 5.0850e-03, 1.2093e-02, 3.7460e-03, 3.8052e-03,\n",
      "        9.1019e-03, 6.2752e-03, 6.3934e-03, 1.1177e-02, 5.7793e-03, 5.0621e-03,\n",
      "        1.5091e-02, 4.2763e-03, 2.2717e-03, 5.9471e-03, 1.1002e-02, 6.7062e-03,\n",
      "        2.6608e-03, 1.3947e-02, 1.0384e-02, 7.1144e-04, 6.5880e-03, 1.8433e-02,\n",
      "        4.0665e-03, 1.7643e-03, 1.3908e-02, 9.1248e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [53] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [53] : torch.Size([1, 32, 1, 125])\n",
      "Last layer attentions for generated token [53] : tensor([0.1732, 0.1732, 0.0007, 0.0006, 0.0004, 0.0048, 0.0002, 0.0005, 0.0010,\n",
      "        0.0009, 0.0024, 0.0003, 0.0006, 0.0015, 0.0122, 0.0133, 0.0003, 0.0006,\n",
      "        0.0002, 0.0003, 0.0002, 0.0005, 0.0145, 0.0021, 0.0060, 0.0197, 0.0007,\n",
      "        0.0004, 0.0005, 0.0006, 0.0011, 0.0016, 0.0011, 0.0006, 0.0005, 0.0004,\n",
      "        0.0004, 0.0008, 0.0003, 0.0003, 0.0049, 0.0022, 0.0004, 0.0004, 0.0005,\n",
      "        0.0003, 0.0002, 0.0004, 0.0004, 0.0029, 0.0023, 0.0005, 0.0054, 0.0065,\n",
      "        0.0028, 0.0064, 0.0004, 0.0013, 0.0072, 0.0012, 0.0014, 0.0022, 0.0042,\n",
      "        0.0015, 0.0014, 0.0009, 0.0033, 0.0048, 0.0038, 0.0191, 0.0150, 0.0220,\n",
      "        0.0051, 0.0058, 0.0024, 0.0073, 0.0009, 0.0027, 0.0048, 0.0046, 0.0061,\n",
      "        0.0078, 0.0257, 0.0039, 0.0046, 0.0066, 0.0034, 0.0028, 0.0062, 0.0056,\n",
      "        0.0017, 0.0056, 0.0060, 0.0147, 0.0137, 0.0119, 0.0127, 0.0305, 0.0063,\n",
      "        0.0077, 0.0053, 0.0028, 0.0058, 0.0042, 0.0039, 0.0072, 0.0054, 0.0036,\n",
      "        0.0146, 0.0057, 0.0030, 0.0086, 0.0114, 0.0080, 0.0043, 0.0194, 0.0146,\n",
      "        0.0026, 0.0100, 0.0262, 0.0085, 0.0047, 0.0153, 0.0121, 0.0104],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [54] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [54] : torch.Size([1, 32, 1, 126])\n",
      "Last layer attentions for generated token [54] : tensor([1.6833e-01, 1.6833e-01, 4.3726e-04, 5.3883e-04, 2.6894e-04, 3.5191e-03,\n",
      "        4.6730e-04, 1.2388e-03, 1.5001e-03, 9.1171e-04, 1.7452e-03, 4.4346e-04,\n",
      "        3.3736e-04, 7.4673e-04, 6.5308e-03, 7.1297e-03, 5.1451e-04, 3.8218e-04,\n",
      "        1.4174e-04, 1.2708e-04, 8.4639e-05, 4.0126e-04, 1.1147e-02, 1.5411e-03,\n",
      "        4.6844e-03, 1.8478e-02, 9.3699e-04, 2.3639e-04, 3.3998e-04, 7.3528e-04,\n",
      "        6.9475e-04, 1.3132e-03, 1.3113e-03, 6.3276e-04, 7.1430e-04, 3.9506e-04,\n",
      "        4.0603e-04, 9.3126e-04, 4.3988e-04, 2.5105e-04, 5.2986e-03, 2.7828e-03,\n",
      "        3.3998e-04, 5.4550e-04, 8.5115e-04, 2.8014e-04, 5.5742e-04, 3.9911e-04,\n",
      "        5.0259e-04, 5.4588e-03, 2.9736e-03, 4.8971e-04, 8.7357e-03, 1.1589e-02,\n",
      "        2.8267e-03, 5.2757e-03, 3.1495e-04, 1.1730e-03, 6.9733e-03, 1.4095e-03,\n",
      "        1.2054e-03, 2.9144e-03, 6.0272e-03, 1.6508e-03, 8.2016e-04, 1.0996e-03,\n",
      "        3.8719e-03, 6.1913e-03, 6.6795e-03, 2.7985e-02, 2.2064e-02, 2.2598e-02,\n",
      "        3.8834e-03, 5.6419e-03, 2.1477e-03, 8.0109e-03, 5.2738e-04, 3.4199e-03,\n",
      "        5.6000e-03, 7.4692e-03, 1.0811e-02, 1.5762e-02, 2.6443e-02, 2.4395e-03,\n",
      "        2.8057e-03, 5.6801e-03, 3.4733e-03, 1.9503e-03, 4.5013e-03, 3.5839e-03,\n",
      "        6.1798e-04, 3.2730e-03, 5.2185e-03, 1.2642e-02, 1.1093e-02, 1.3893e-02,\n",
      "        1.6006e-02, 2.6794e-02, 3.0937e-03, 7.7057e-03, 3.9558e-03, 1.0977e-03,\n",
      "        5.0659e-03, 4.0703e-03, 3.1967e-03, 1.0086e-02, 3.0174e-03, 3.0861e-03,\n",
      "        1.7303e-02, 6.2332e-03, 1.7014e-03, 6.9656e-03, 8.4915e-03, 8.6365e-03,\n",
      "        2.2812e-03, 2.0462e-02, 1.3290e-02, 8.0919e-04, 9.1858e-03, 2.4841e-02,\n",
      "        7.2174e-03, 5.2834e-03, 2.0172e-02, 1.0246e-02, 7.8354e-03, 9.6817e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [55] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [55] : torch.Size([1, 32, 1, 127])\n",
      "Last layer attentions for generated token [55] : tensor([2.0569e-01, 2.0569e-01, 9.8991e-04, 7.5054e-04, 7.1764e-04, 5.4970e-03,\n",
      "        2.8038e-04, 6.4802e-04, 8.6880e-04, 9.5987e-04, 2.4700e-03, 4.2915e-04,\n",
      "        8.4877e-04, 2.1381e-03, 1.0979e-02, 1.3351e-02, 3.5381e-04, 5.1355e-04,\n",
      "        1.9956e-04, 2.7823e-04, 1.7071e-04, 4.7135e-04, 1.6632e-02, 2.0199e-03,\n",
      "        4.9171e-03, 1.6541e-02, 7.0238e-04, 3.5787e-04, 6.0511e-04, 6.6853e-04,\n",
      "        9.1600e-04, 1.1091e-03, 1.3218e-03, 6.7377e-04, 7.2145e-04, 4.0960e-04,\n",
      "        4.2582e-04, 7.2002e-04, 3.2473e-04, 3.2401e-04, 3.9978e-03, 2.3022e-03,\n",
      "        4.5156e-04, 3.4618e-04, 3.8338e-04, 2.9278e-04, 1.7345e-04, 2.7943e-04,\n",
      "        3.0923e-04, 2.0847e-03, 1.7796e-03, 6.0177e-04, 6.4163e-03, 7.0496e-03,\n",
      "        2.8877e-03, 6.2561e-03, 3.2330e-04, 1.2398e-03, 6.7520e-03, 1.2989e-03,\n",
      "        1.5612e-03, 1.9140e-03, 3.7155e-03, 1.4610e-03, 1.6041e-03, 8.5354e-04,\n",
      "        2.8458e-03, 6.7787e-03, 3.1834e-03, 2.4124e-02, 1.7120e-02, 2.1606e-02,\n",
      "        4.8256e-03, 5.2452e-03, 2.5444e-03, 5.9052e-03, 7.5197e-04, 2.2106e-03,\n",
      "        3.6087e-03, 4.3526e-03, 4.7607e-03, 5.1727e-03, 2.3926e-02, 3.4084e-03,\n",
      "        3.3627e-03, 6.3667e-03, 3.3035e-03, 2.7103e-03, 4.5700e-03, 4.3564e-03,\n",
      "        1.4181e-03, 5.6801e-03, 5.8212e-03, 1.1993e-02, 1.1742e-02, 9.3689e-03,\n",
      "        1.0735e-02, 2.7786e-02, 5.6610e-03, 5.4893e-03, 3.8338e-03, 1.7757e-03,\n",
      "        3.9864e-03, 3.1376e-03, 2.5578e-03, 5.2109e-03, 3.5801e-03, 2.4052e-03,\n",
      "        9.9411e-03, 4.8447e-03, 2.9469e-03, 5.3749e-03, 8.2092e-03, 5.3940e-03,\n",
      "        3.9482e-03, 1.1948e-02, 1.0719e-02, 2.5024e-03, 7.5340e-03, 2.2675e-02,\n",
      "        7.5264e-03, 4.3068e-03, 1.0216e-02, 1.0513e-02, 8.8882e-03, 9.4452e-03,\n",
      "        3.9444e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [56] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [56] : torch.Size([1, 32, 1, 128])\n",
      "Last layer attentions for generated token [56] : tensor([1.7517e-01, 1.7480e-01, 1.6415e-04, 3.6430e-04, 1.8454e-04, 3.7956e-03,\n",
      "        6.0415e-04, 8.7547e-04, 5.6982e-04, 9.6512e-04, 2.9221e-03, 4.8542e-04,\n",
      "        3.9911e-04, 1.1644e-03, 1.2276e-02, 1.4542e-02, 3.9768e-04, 9.9945e-04,\n",
      "        2.6798e-04, 2.6226e-04, 6.6042e-05, 3.1400e-04, 7.1945e-03, 1.3218e-03,\n",
      "        5.4741e-03, 1.5587e-02, 1.2035e-03, 2.8801e-04, 3.6001e-04, 5.8746e-04,\n",
      "        6.0415e-04, 8.1730e-04, 1.4544e-03, 5.9366e-04, 5.9128e-04, 4.4274e-04,\n",
      "        4.2820e-04, 8.8215e-04, 3.8099e-04, 2.9969e-04, 4.1313e-03, 2.4433e-03,\n",
      "        6.6471e-04, 8.0490e-04, 6.8712e-04, 3.7074e-04, 2.2089e-04, 2.3472e-04,\n",
      "        3.2902e-04, 2.7580e-03, 2.5005e-03, 2.5630e-04, 3.4485e-03, 4.4365e-03,\n",
      "        2.6722e-03, 8.2321e-03, 5.5408e-04, 1.3113e-03, 5.8670e-03, 1.4210e-03,\n",
      "        1.4458e-03, 2.0084e-03, 3.3855e-03, 1.9493e-03, 8.5688e-04, 6.5041e-04,\n",
      "        4.5166e-03, 4.2839e-03, 2.7180e-03, 1.8433e-02, 1.2680e-02, 2.4033e-02,\n",
      "        9.0637e-03, 8.5068e-03, 3.8242e-03, 1.2650e-02, 5.1785e-04, 3.1090e-03,\n",
      "        3.3226e-03, 4.2725e-03, 8.1940e-03, 6.6719e-03, 3.0502e-02, 5.0545e-03,\n",
      "        3.2425e-03, 4.3182e-03, 4.1580e-03, 5.0926e-03, 7.4310e-03, 6.3820e-03,\n",
      "        1.3037e-03, 8.4839e-03, 7.9498e-03, 1.4824e-02, 1.5068e-02, 1.0422e-02,\n",
      "        1.1543e-02, 3.5004e-02, 7.0610e-03, 5.5656e-03, 4.5280e-03, 3.5648e-03,\n",
      "        5.2414e-03, 3.8204e-03, 3.5763e-03, 5.8937e-03, 6.0120e-03, 3.0289e-03,\n",
      "        1.4076e-02, 6.7444e-03, 2.0618e-03, 5.2643e-03, 7.1106e-03, 5.1956e-03,\n",
      "        2.4433e-03, 1.0040e-02, 9.2926e-03, 8.2874e-04, 6.3019e-03, 2.3117e-02,\n",
      "        1.2909e-02, 3.2978e-03, 1.7395e-02, 1.3321e-02, 9.1705e-03, 1.2016e-02,\n",
      "        2.6855e-03, 7.1449e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [57] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [57] : torch.Size([1, 32, 1, 129])\n",
      "Last layer attentions for generated token [57] : tensor([2.1191e-01, 2.1191e-01, 3.3450e-04, 4.6992e-04, 2.2328e-04, 3.4618e-03,\n",
      "        3.7241e-04, 5.7459e-04, 8.0729e-04, 6.7329e-04, 1.6174e-03, 2.5201e-04,\n",
      "        6.5517e-04, 1.3390e-03, 7.0610e-03, 7.6752e-03, 1.9395e-04, 3.9577e-04,\n",
      "        1.6081e-04, 2.8157e-04, 6.3241e-05, 2.3031e-04, 1.8768e-02, 1.9588e-03,\n",
      "        3.3131e-03, 1.1490e-02, 3.3593e-04, 1.9932e-04, 4.8470e-04, 3.9792e-04,\n",
      "        5.2023e-04, 4.8971e-04, 6.1750e-04, 3.2496e-04, 2.3866e-04, 4.6253e-04,\n",
      "        7.6580e-04, 4.3726e-04, 2.9230e-04, 3.2115e-04, 1.9588e-03, 1.8082e-03,\n",
      "        3.7980e-04, 5.4836e-04, 5.3978e-04, 4.8470e-04, 2.2948e-04, 3.9482e-04,\n",
      "        5.0497e-04, 1.2827e-03, 1.5135e-03, 1.4472e-04, 5.3558e-03, 2.1896e-03,\n",
      "        2.1000e-03, 4.6768e-03, 1.8299e-04, 1.2426e-03, 4.4441e-03, 1.0586e-03,\n",
      "        1.0490e-03, 1.1606e-03, 2.5444e-03, 1.3733e-03, 9.6989e-04, 4.4322e-04,\n",
      "        3.4409e-03, 2.2144e-03, 2.4910e-03, 1.1871e-02, 7.2708e-03, 1.5503e-02,\n",
      "        5.1384e-03, 3.8509e-03, 3.0785e-03, 5.9662e-03, 1.1787e-03, 2.6340e-03,\n",
      "        2.1210e-03, 3.5000e-03, 6.3362e-03, 2.6760e-03, 1.8906e-02, 3.4409e-03,\n",
      "        2.7962e-03, 2.8248e-03, 3.0022e-03, 3.3550e-03, 3.9864e-03, 4.8866e-03,\n",
      "        2.4643e-03, 8.3694e-03, 7.3586e-03, 1.6388e-02, 1.4198e-02, 8.4763e-03,\n",
      "        7.0801e-03, 3.2349e-02, 5.8327e-03, 5.3253e-03, 5.0812e-03, 4.8065e-03,\n",
      "        3.7842e-03, 3.0231e-03, 3.5934e-03, 4.3259e-03, 5.9700e-03, 3.2806e-03,\n",
      "        1.0399e-02, 3.9177e-03, 2.8057e-03, 3.3131e-03, 8.6212e-03, 4.4708e-03,\n",
      "        4.0131e-03, 9.2773e-03, 9.5749e-03, 1.9321e-03, 5.4817e-03, 3.1372e-02,\n",
      "        8.6212e-03, 4.0398e-03, 1.4420e-02, 1.7242e-02, 1.5793e-02, 2.2919e-02,\n",
      "        6.0921e-03, 9.5215e-03, 1.1345e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [58] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [58] : torch.Size([1, 32, 1, 130])\n",
      "Last layer attentions for generated token [58] : tensor([1.9519e-01, 1.9519e-01, 5.3978e-04, 5.8222e-04, 3.0756e-04, 4.9095e-03,\n",
      "        2.7299e-04, 7.8201e-04, 1.1091e-03, 8.1825e-04, 2.0084e-03, 4.6897e-04,\n",
      "        3.7384e-04, 1.0605e-03, 1.4740e-02, 1.1299e-02, 5.6648e-04, 5.5027e-04,\n",
      "        2.5153e-04, 2.6608e-04, 1.3149e-04, 4.8470e-04, 1.3977e-02, 1.5345e-03,\n",
      "        4.9210e-03, 2.4368e-02, 7.3051e-04, 2.7037e-04, 4.4656e-04, 8.0872e-04,\n",
      "        8.1158e-04, 1.1654e-03, 7.7915e-04, 4.7445e-04, 4.2439e-04, 3.5334e-04,\n",
      "        4.5276e-04, 1.0653e-03, 4.2439e-04, 3.3975e-04, 4.6005e-03, 1.6623e-03,\n",
      "        4.1699e-04, 6.1035e-04, 9.2363e-04, 3.8123e-04, 5.7125e-04, 4.5276e-04,\n",
      "        4.9353e-04, 2.4738e-03, 1.8511e-03, 3.9864e-04, 4.3411e-03, 5.6953e-03,\n",
      "        1.6718e-03, 6.0730e-03, 2.3997e-04, 1.4610e-03, 7.9880e-03, 1.0757e-03,\n",
      "        9.8324e-04, 1.4439e-03, 3.3417e-03, 1.6289e-03, 1.3361e-03, 5.9986e-04,\n",
      "        2.4567e-03, 4.4441e-03, 3.7613e-03, 1.8860e-02, 1.3611e-02, 1.7410e-02,\n",
      "        4.3831e-03, 3.0460e-03, 2.1343e-03, 6.1646e-03, 7.3195e-04, 1.7891e-03,\n",
      "        2.6741e-03, 3.0365e-03, 4.1199e-03, 6.9084e-03, 1.9073e-02, 1.7424e-03,\n",
      "        2.7523e-03, 3.5686e-03, 2.2640e-03, 1.5469e-03, 4.4975e-03, 5.6496e-03,\n",
      "        1.1005e-03, 3.8624e-03, 6.0234e-03, 1.5030e-02, 1.1765e-02, 1.6388e-02,\n",
      "        9.0714e-03, 2.2766e-02, 3.8948e-03, 5.0201e-03, 3.5706e-03, 1.7424e-03,\n",
      "        3.9177e-03, 3.7384e-03, 3.1090e-03, 5.1003e-03, 4.2152e-03, 2.3422e-03,\n",
      "        1.2123e-02, 3.4142e-03, 1.5259e-03, 3.8357e-03, 8.1711e-03, 7.5760e-03,\n",
      "        3.4237e-03, 1.5343e-02, 6.9733e-03, 1.8349e-03, 4.2915e-03, 2.2293e-02,\n",
      "        7.5874e-03, 4.2610e-03, 1.5793e-02, 1.4206e-02, 1.9424e-02, 2.0508e-02,\n",
      "        5.2605e-03, 8.4686e-03, 7.6637e-03, 8.8043e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [59] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [59] : torch.Size([1, 32, 1, 131])\n",
      "Last layer attentions for generated token [59] : tensor([2.1558e-01, 2.1558e-01, 3.6168e-04, 7.3624e-04, 2.4962e-04, 2.5291e-03,\n",
      "        2.2197e-04, 3.2616e-04, 4.7731e-04, 7.2336e-04, 1.4410e-03, 4.0174e-04,\n",
      "        5.2309e-04, 1.2922e-03, 1.1612e-02, 1.0483e-02, 4.8280e-04, 5.9509e-04,\n",
      "        2.4843e-04, 3.8648e-04, 2.0850e-04, 4.4751e-04, 9.1934e-03, 1.7529e-03,\n",
      "        5.5733e-03, 1.0750e-02, 1.3256e-03, 2.3353e-04, 3.8505e-04, 4.2605e-04,\n",
      "        4.3106e-04, 8.2445e-04, 8.4925e-04, 3.0804e-04, 2.8396e-04, 5.7459e-04,\n",
      "        4.4656e-04, 7.5054e-04, 3.2926e-04, 2.4188e-04, 3.6526e-03, 1.8396e-03,\n",
      "        5.6458e-04, 6.7949e-04, 6.5613e-04, 3.8505e-04, 2.3437e-04, 2.1672e-04,\n",
      "        2.6989e-04, 3.2711e-03, 2.3060e-03, 3.4237e-04, 4.2572e-03, 6.9962e-03,\n",
      "        1.7595e-03, 5.1422e-03, 4.6706e-04, 1.5831e-03, 6.1913e-03, 1.0729e-03,\n",
      "        1.2627e-03, 1.1539e-03, 2.6512e-03, 1.8730e-03, 1.1091e-03, 6.1417e-04,\n",
      "        3.4618e-03, 4.7493e-03, 3.5496e-03, 2.5345e-02, 1.3023e-02, 1.7212e-02,\n",
      "        6.4087e-03, 5.1498e-03, 2.1553e-03, 3.7212e-03, 5.1117e-04, 1.5621e-03,\n",
      "        2.4948e-03, 2.1896e-03, 2.7676e-03, 5.8403e-03, 1.9638e-02, 4.0970e-03,\n",
      "        2.3212e-03, 2.7218e-03, 2.3670e-03, 3.0460e-03, 3.6659e-03, 4.7951e-03,\n",
      "        1.3809e-03, 3.9597e-03, 6.8016e-03, 1.4000e-02, 9.9411e-03, 1.1200e-02,\n",
      "        9.6130e-03, 2.4353e-02, 7.1144e-03, 4.9591e-03, 5.0812e-03, 4.5586e-03,\n",
      "        4.9210e-03, 3.4809e-03, 2.9659e-03, 4.5586e-03, 6.8283e-03, 2.6569e-03,\n",
      "        1.6663e-02, 4.6043e-03, 1.5049e-03, 4.3869e-03, 5.8289e-03, 4.6043e-03,\n",
      "        2.8896e-03, 1.2207e-02, 8.4839e-03, 1.6050e-03, 5.1155e-03, 1.9608e-02,\n",
      "        1.6541e-02, 4.5128e-03, 1.5686e-02, 1.2177e-02, 8.0414e-03, 9.0332e-03,\n",
      "        4.5242e-03, 6.3820e-03, 6.8970e-03, 5.3139e-03, 2.0447e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [60] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [60] : torch.Size([1, 32, 1, 132])\n",
      "Last layer attentions for generated token [60] : tensor([0.2194, 0.2194, 0.0004, 0.0009, 0.0003, 0.0036, 0.0004, 0.0007, 0.0010,\n",
      "        0.0009, 0.0016, 0.0007, 0.0003, 0.0009, 0.0138, 0.0091, 0.0006, 0.0005,\n",
      "        0.0003, 0.0003, 0.0002, 0.0005, 0.0107, 0.0016, 0.0079, 0.0209, 0.0010,\n",
      "        0.0003, 0.0004, 0.0005, 0.0005, 0.0008, 0.0008, 0.0004, 0.0003, 0.0003,\n",
      "        0.0004, 0.0006, 0.0004, 0.0003, 0.0031, 0.0014, 0.0006, 0.0010, 0.0012,\n",
      "        0.0005, 0.0006, 0.0004, 0.0005, 0.0032, 0.0018, 0.0005, 0.0050, 0.0077,\n",
      "        0.0016, 0.0048, 0.0004, 0.0012, 0.0067, 0.0014, 0.0012, 0.0013, 0.0042,\n",
      "        0.0022, 0.0010, 0.0008, 0.0028, 0.0054, 0.0045, 0.0242, 0.0176, 0.0209,\n",
      "        0.0052, 0.0025, 0.0019, 0.0059, 0.0006, 0.0015, 0.0017, 0.0029, 0.0039,\n",
      "        0.0060, 0.0164, 0.0023, 0.0017, 0.0028, 0.0021, 0.0017, 0.0033, 0.0030,\n",
      "        0.0007, 0.0027, 0.0045, 0.0090, 0.0072, 0.0101, 0.0076, 0.0194, 0.0024,\n",
      "        0.0038, 0.0033, 0.0026, 0.0038, 0.0031, 0.0024, 0.0036, 0.0062, 0.0026,\n",
      "        0.0121, 0.0030, 0.0011, 0.0034, 0.0047, 0.0062, 0.0024, 0.0128, 0.0059,\n",
      "        0.0011, 0.0060, 0.0206, 0.0073, 0.0037, 0.0165, 0.0139, 0.0124, 0.0126,\n",
      "        0.0036, 0.0087, 0.0084, 0.0087, 0.0036, 0.0072], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [61] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [61] : torch.Size([1, 32, 1, 133])\n",
      "Last layer attentions for generated token [61] : tensor([2.6099e-01, 2.6050e-01, 7.8344e-04, 8.2111e-04, 5.4502e-04, 2.5940e-03,\n",
      "        1.6165e-04, 2.7180e-04, 2.8706e-04, 6.0797e-04, 1.0853e-03, 1.3828e-04,\n",
      "        5.1498e-04, 1.1921e-03, 5.5122e-03, 1.1688e-02, 2.5344e-04, 4.8375e-04,\n",
      "        2.6560e-04, 3.8791e-04, 7.6652e-05, 3.9721e-04, 1.2932e-02, 2.3479e-03,\n",
      "        2.7599e-03, 9.1553e-03, 1.0920e-03, 2.6870e-04, 3.3188e-04, 4.0650e-04,\n",
      "        4.7708e-04, 5.4932e-04, 5.3453e-04, 2.0838e-04, 2.1088e-04, 5.0688e-04,\n",
      "        3.1042e-04, 4.3178e-04, 3.4761e-04, 3.2163e-04, 2.5349e-03, 1.6232e-03,\n",
      "        4.4465e-04, 7.0906e-04, 6.7663e-04, 4.9591e-04, 1.7214e-04, 2.4462e-04,\n",
      "        3.1042e-04, 2.0962e-03, 1.6050e-03, 4.4990e-04, 3.0937e-03, 6.8169e-03,\n",
      "        1.5888e-03, 4.6120e-03, 3.1281e-04, 1.2741e-03, 4.4861e-03, 8.2111e-04,\n",
      "        1.0099e-03, 1.0443e-03, 2.4509e-03, 2.0332e-03, 1.0624e-03, 6.9427e-04,\n",
      "        3.4466e-03, 4.0855e-03, 2.3575e-03, 1.8478e-02, 1.2260e-02, 1.9592e-02,\n",
      "        3.7098e-03, 3.2902e-03, 1.6422e-03, 1.9550e-03, 8.8978e-04, 1.3456e-03,\n",
      "        1.3275e-03, 2.7771e-03, 2.7752e-03, 3.5629e-03, 1.8875e-02, 2.0199e-03,\n",
      "        1.9016e-03, 2.0485e-03, 2.7409e-03, 1.1511e-03, 1.9360e-03, 2.6150e-03,\n",
      "        1.1759e-03, 4.0932e-03, 5.1460e-03, 9.7122e-03, 1.0536e-02, 8.4305e-03,\n",
      "        6.8741e-03, 2.3331e-02, 4.4136e-03, 3.3932e-03, 4.3526e-03, 1.3170e-03,\n",
      "        3.1967e-03, 2.4090e-03, 2.3174e-03, 3.0899e-03, 3.4637e-03, 1.8110e-03,\n",
      "        8.7967e-03, 5.3444e-03, 1.7939e-03, 4.3297e-03, 5.1956e-03, 5.0163e-03,\n",
      "        2.6226e-03, 6.7825e-03, 1.2718e-02, 1.7014e-03, 4.4746e-03, 1.5434e-02,\n",
      "        1.1307e-02, 4.6844e-03, 8.0872e-03, 9.5901e-03, 5.7106e-03, 9.0256e-03,\n",
      "        2.9373e-03, 5.6725e-03, 9.4833e-03, 6.5918e-03, 2.3346e-03, 4.8523e-03,\n",
      "        4.1580e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [62] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [62] : torch.Size([1, 32, 1, 134])\n",
      "Last layer attentions for generated token [62] : tensor([2.7881e-01, 2.7832e-01, 1.1463e-03, 1.5450e-03, 6.1846e-04, 2.8706e-03,\n",
      "        3.4809e-04, 5.5313e-04, 6.7616e-04, 6.3658e-04, 1.1435e-03, 3.0851e-04,\n",
      "        5.2547e-04, 8.5831e-04, 1.1177e-02, 1.0292e-02, 5.1451e-04, 5.9557e-04,\n",
      "        5.2547e-04, 6.1083e-04, 2.5868e-04, 1.0996e-03, 1.7975e-02, 3.5095e-03,\n",
      "        6.1417e-03, 2.4826e-02, 1.2035e-03, 5.9223e-04, 6.4039e-04, 5.1451e-04,\n",
      "        4.0627e-04, 5.7602e-04, 5.6505e-04, 2.5582e-04, 2.4676e-04, 5.8079e-04,\n",
      "        6.8188e-04, 4.4441e-04, 4.8041e-04, 4.5848e-04, 2.0962e-03, 1.9455e-03,\n",
      "        9.1171e-04, 9.0837e-04, 8.4162e-04, 6.8855e-04, 5.1355e-04, 6.3658e-04,\n",
      "        6.8045e-04, 1.5726e-03, 2.0065e-03, 6.1226e-04, 6.2256e-03, 1.2260e-02,\n",
      "        1.7138e-03, 5.4741e-03, 3.6049e-04, 1.3962e-03, 5.6190e-03, 1.0700e-03,\n",
      "        1.5335e-03, 9.8228e-04, 3.1509e-03, 2.3842e-03, 1.2369e-03, 6.7234e-04,\n",
      "        3.1548e-03, 3.9062e-03, 5.7793e-03, 1.2680e-02, 1.3443e-02, 1.2718e-02,\n",
      "        4.5319e-03, 1.2541e-03, 1.2102e-03, 1.5783e-03, 4.3321e-04, 9.8228e-04,\n",
      "        6.4039e-04, 2.0962e-03, 2.2793e-03, 2.1534e-03, 1.0216e-02, 2.0065e-03,\n",
      "        1.2512e-03, 1.0700e-03, 1.0719e-03, 9.2602e-04, 1.8635e-03, 1.6575e-03,\n",
      "        7.5006e-04, 1.8892e-03, 2.2659e-03, 5.4474e-03, 4.9667e-03, 4.0627e-03,\n",
      "        2.9697e-03, 1.2871e-02, 1.8492e-03, 1.8349e-03, 2.2221e-03, 1.7271e-03,\n",
      "        2.3937e-03, 1.3142e-03, 1.0681e-03, 1.5726e-03, 2.9869e-03, 1.1301e-03,\n",
      "        5.8022e-03, 1.9760e-03, 1.0872e-03, 1.7271e-03, 2.6646e-03, 3.2959e-03,\n",
      "        1.5450e-03, 6.7520e-03, 5.2795e-03, 1.1597e-03, 2.3193e-03, 1.4252e-02,\n",
      "        9.0408e-03, 5.5389e-03, 9.1248e-03, 1.1154e-02, 8.7051e-03, 1.0048e-02,\n",
      "        3.2845e-03, 5.5923e-03, 6.6261e-03, 8.1558e-03, 4.3716e-03, 1.0094e-02,\n",
      "        5.2338e-03, 2.0351e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [63] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [63] : torch.Size([1, 32, 1, 135])\n",
      "Last layer attentions for generated token [63] : tensor([2.2192e-01, 2.2144e-01, 5.5313e-04, 9.4318e-04, 3.7146e-04, 2.8019e-03,\n",
      "        2.8753e-04, 5.8079e-04, 1.2083e-03, 7.8964e-04, 1.9932e-03, 4.9686e-04,\n",
      "        3.2091e-04, 7.0906e-04, 1.7120e-02, 1.1070e-02, 8.0967e-04, 6.7139e-04,\n",
      "        4.8256e-04, 4.7135e-04, 1.8287e-04, 5.8556e-04, 1.5083e-02, 2.9907e-03,\n",
      "        7.8125e-03, 3.1311e-02, 1.0691e-03, 5.0592e-04, 4.8733e-04, 6.2466e-04,\n",
      "        5.5122e-04, 9.0885e-04, 5.1785e-04, 4.2748e-04, 3.6716e-04, 5.7316e-04,\n",
      "        4.8447e-04, 7.6962e-04, 3.7813e-04, 3.3617e-04, 3.2654e-03, 1.5192e-03,\n",
      "        7.5626e-04, 1.2760e-03, 1.4181e-03, 5.9462e-04, 8.8263e-04, 5.1451e-04,\n",
      "        8.6689e-04, 3.5686e-03, 1.5335e-03, 4.8161e-04, 5.9586e-03, 9.1019e-03,\n",
      "        1.7414e-03, 5.6381e-03, 3.4976e-04, 1.3123e-03, 6.3629e-03, 1.0195e-03,\n",
      "        1.0462e-03, 1.4925e-03, 4.2572e-03, 2.4414e-03, 1.1349e-03, 9.6560e-04,\n",
      "        2.1896e-03, 5.6114e-03, 3.7918e-03, 1.8967e-02, 1.5869e-02, 2.2644e-02,\n",
      "        6.2981e-03, 2.1381e-03, 1.4782e-03, 5.2223e-03, 5.6648e-04, 1.5545e-03,\n",
      "        1.8425e-03, 3.3894e-03, 4.7722e-03, 4.0855e-03, 1.3313e-02, 1.5821e-03,\n",
      "        1.7862e-03, 2.6970e-03, 1.3065e-03, 8.1158e-04, 2.6894e-03, 2.0561e-03,\n",
      "        5.2595e-04, 2.5368e-03, 2.4948e-03, 9.1553e-03, 5.2452e-03, 6.8626e-03,\n",
      "        7.2403e-03, 1.6617e-02, 1.6069e-03, 4.9400e-03, 2.1095e-03, 2.0542e-03,\n",
      "        5.5771e-03, 3.6354e-03, 3.0956e-03, 3.2406e-03, 5.8479e-03, 2.5120e-03,\n",
      "        1.0918e-02, 1.7080e-03, 1.0080e-03, 3.3665e-03, 4.0436e-03, 3.7193e-03,\n",
      "        1.1625e-03, 9.4452e-03, 4.3831e-03, 6.8712e-04, 4.6921e-03, 1.5526e-02,\n",
      "        4.3945e-03, 2.9945e-03, 1.4847e-02, 1.1566e-02, 1.1482e-02, 1.2527e-02,\n",
      "        3.0346e-03, 7.7286e-03, 8.9188e-03, 1.0750e-02, 5.8212e-03, 1.0307e-02,\n",
      "        6.5460e-03, 7.8487e-04, 5.1422e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [64] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [64] : torch.Size([1, 32, 1, 136])\n",
      "Last layer attentions for generated token [64] : tensor([1.9348e-01, 1.9348e-01, 6.9666e-04, 5.9700e-04, 3.5381e-04, 3.6430e-03,\n",
      "        1.6260e-04, 3.7289e-04, 6.0654e-04, 4.9210e-04, 1.0176e-03, 1.5879e-04,\n",
      "        2.2399e-04, 6.7902e-04, 6.2675e-03, 1.1070e-02, 3.8099e-04, 4.7970e-04,\n",
      "        2.1374e-04, 2.1374e-04, 7.3552e-05, 2.5988e-04, 9.6207e-03, 1.1806e-03,\n",
      "        2.7027e-03, 1.6846e-02, 6.5565e-04, 2.4700e-04, 3.4833e-04, 6.1369e-04,\n",
      "        8.1778e-04, 8.7023e-04, 6.4421e-04, 4.6492e-04, 3.4094e-04, 3.3879e-04,\n",
      "        2.5678e-04, 5.7507e-04, 2.5320e-04, 2.1040e-04, 3.6831e-03, 1.7586e-03,\n",
      "        5.1785e-04, 5.6648e-04, 8.0013e-04, 4.0388e-04, 5.1260e-04, 3.1948e-04,\n",
      "        6.6090e-04, 2.7847e-03, 1.9293e-03, 4.8065e-04, 5.2528e-03, 5.3215e-03,\n",
      "        1.9426e-03, 4.7798e-03, 2.2840e-04, 1.0958e-03, 4.0245e-03, 6.5184e-04,\n",
      "        7.6199e-04, 1.6165e-03, 2.4166e-03, 8.5878e-04, 7.7581e-04, 5.2357e-04,\n",
      "        2.3766e-03, 4.3297e-03, 2.4586e-03, 1.4511e-02, 1.2749e-02, 2.1027e-02,\n",
      "        4.8752e-03, 4.4861e-03, 2.3232e-03, 4.6577e-03, 4.7779e-04, 2.0599e-03,\n",
      "        3.3398e-03, 3.7727e-03, 6.0425e-03, 4.5395e-03, 1.8448e-02, 2.9068e-03,\n",
      "        3.2597e-03, 3.3360e-03, 2.0752e-03, 1.4572e-03, 3.2616e-03, 3.2082e-03,\n",
      "        8.7881e-04, 3.1185e-03, 4.8561e-03, 1.6388e-02, 8.1940e-03, 7.5836e-03,\n",
      "        1.0361e-02, 2.2141e-02, 2.8400e-03, 6.5193e-03, 3.0346e-03, 2.8763e-03,\n",
      "        4.1237e-03, 3.4466e-03, 3.6068e-03, 4.3221e-03, 4.4861e-03, 3.3169e-03,\n",
      "        1.1284e-02, 2.4605e-03, 1.6289e-03, 5.3406e-03, 6.1493e-03, 4.3488e-03,\n",
      "        2.1191e-03, 9.4223e-03, 6.8779e-03, 1.2131e-03, 4.9629e-03, 2.2873e-02,\n",
      "        6.9656e-03, 3.5591e-03, 1.5213e-02, 1.4084e-02, 1.9287e-02, 2.2400e-02,\n",
      "        8.7662e-03, 1.6846e-02, 1.4915e-02, 1.6479e-02, 7.0076e-03, 1.3641e-02,\n",
      "        6.5727e-03, 1.0433e-03, 7.1754e-03, 6.6261e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [65] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [65] : torch.Size([1, 32, 1, 137])\n",
      "Last layer attentions for generated token [65] : tensor([0.1675, 0.1675, 0.0004, 0.0010, 0.0003, 0.0041, 0.0003, 0.0003, 0.0004,\n",
      "        0.0007, 0.0018, 0.0003, 0.0004, 0.0012, 0.0190, 0.0191, 0.0004, 0.0011,\n",
      "        0.0003, 0.0004, 0.0002, 0.0005, 0.0080, 0.0020, 0.0076, 0.0262, 0.0021,\n",
      "        0.0006, 0.0005, 0.0004, 0.0007, 0.0015, 0.0008, 0.0004, 0.0003, 0.0007,\n",
      "        0.0005, 0.0005, 0.0003, 0.0002, 0.0047, 0.0025, 0.0010, 0.0009, 0.0006,\n",
      "        0.0004, 0.0002, 0.0002, 0.0004, 0.0028, 0.0022, 0.0004, 0.0027, 0.0082,\n",
      "        0.0022, 0.0063, 0.0006, 0.0019, 0.0064, 0.0011, 0.0012, 0.0015, 0.0019,\n",
      "        0.0012, 0.0008, 0.0005, 0.0045, 0.0039, 0.0021, 0.0205, 0.0124, 0.0285,\n",
      "        0.0093, 0.0070, 0.0042, 0.0038, 0.0005, 0.0021, 0.0026, 0.0026, 0.0043,\n",
      "        0.0041, 0.0309, 0.0081, 0.0048, 0.0029, 0.0025, 0.0041, 0.0052, 0.0053,\n",
      "        0.0013, 0.0035, 0.0037, 0.0107, 0.0058, 0.0051, 0.0056, 0.0270, 0.0069,\n",
      "        0.0049, 0.0038, 0.0088, 0.0064, 0.0037, 0.0036, 0.0030, 0.0107, 0.0038,\n",
      "        0.0143, 0.0035, 0.0019, 0.0046, 0.0040, 0.0023, 0.0016, 0.0111, 0.0081,\n",
      "        0.0010, 0.0047, 0.0249, 0.0167, 0.0051, 0.0271, 0.0146, 0.0093, 0.0094,\n",
      "        0.0036, 0.0073, 0.0071, 0.0057, 0.0035, 0.0051, 0.0029, 0.0005, 0.0026,\n",
      "        0.0026, 0.0072], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [66] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [66] : torch.Size([1, 32, 1, 138])\n",
      "Last layer attentions for generated token [66] : tensor([1.9226e-01, 1.9226e-01, 1.7500e-04, 4.3559e-04, 1.3268e-04, 2.3556e-03,\n",
      "        2.4962e-04, 3.2496e-04, 4.2558e-04, 6.6185e-04, 2.2221e-03, 4.2152e-04,\n",
      "        2.2125e-04, 5.4550e-04, 1.2810e-02, 1.1971e-02, 3.7336e-04, 7.3242e-04,\n",
      "        3.0303e-04, 2.1946e-04, 7.9513e-05, 3.8385e-04, 3.7575e-03, 1.0185e-03,\n",
      "        4.8943e-03, 1.8158e-02, 1.7691e-03, 4.0364e-04, 2.7800e-04, 3.8290e-04,\n",
      "        4.7207e-04, 1.2407e-03, 9.6846e-04, 5.1022e-04, 2.9898e-04, 2.8133e-04,\n",
      "        3.7336e-04, 4.2057e-04, 1.9908e-04, 1.4567e-04, 3.3245e-03, 1.6413e-03,\n",
      "        6.0368e-04, 7.7534e-04, 6.9761e-04, 2.2388e-04, 3.3021e-04, 1.6057e-04,\n",
      "        3.5763e-04, 2.6855e-03, 1.6317e-03, 2.5225e-04, 2.8362e-03, 5.7793e-03,\n",
      "        2.0351e-03, 5.2299e-03, 7.6008e-04, 1.4038e-03, 4.8523e-03, 1.3905e-03,\n",
      "        1.0681e-03, 2.3708e-03, 2.0580e-03, 1.1196e-03, 7.2813e-04, 6.6948e-04,\n",
      "        3.4714e-03, 3.4447e-03, 2.5368e-03, 1.5945e-02, 1.0475e-02, 2.1805e-02,\n",
      "        8.2321e-03, 7.8430e-03, 3.4199e-03, 7.1373e-03, 3.1638e-04, 2.7885e-03,\n",
      "        2.7008e-03, 2.5978e-03, 7.2861e-03, 5.8365e-03, 2.1820e-02, 6.2523e-03,\n",
      "        5.5237e-03, 6.6147e-03, 3.2692e-03, 4.7455e-03, 7.4768e-03, 5.6190e-03,\n",
      "        7.9679e-04, 3.6125e-03, 4.5280e-03, 1.1780e-02, 7.3318e-03, 5.9700e-03,\n",
      "        1.0033e-02, 2.1683e-02, 5.4779e-03, 6.0654e-03, 3.4313e-03, 6.0577e-03,\n",
      "        8.4305e-03, 6.3019e-03, 5.4893e-03, 6.3400e-03, 1.1040e-02, 4.1389e-03,\n",
      "        1.4488e-02, 3.0994e-03, 1.2760e-03, 6.0883e-03, 4.9515e-03, 2.5024e-03,\n",
      "        9.4414e-04, 1.1253e-02, 4.9438e-03, 5.9414e-04, 5.2757e-03, 1.9455e-02,\n",
      "        6.8207e-03, 2.8114e-03, 2.6520e-02, 1.4572e-02, 8.5297e-03, 9.8343e-03,\n",
      "        2.1496e-03, 6.3934e-03, 4.4594e-03, 5.0354e-03, 3.1700e-03, 4.6806e-03,\n",
      "        2.4395e-03, 2.7490e-04, 2.0771e-03, 1.9360e-03, 9.8877e-03, 1.7670e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [67] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [67] : torch.Size([1, 32, 1, 139])\n",
      "Last layer attentions for generated token [67] : tensor([1.5002e-01, 1.5002e-01, 2.6798e-04, 2.8300e-04, 2.0707e-04, 1.7166e-03,\n",
      "        2.1827e-04, 3.5000e-04, 3.8385e-04, 3.1066e-04, 1.3084e-03, 2.8300e-04,\n",
      "        1.5152e-04, 4.4179e-04, 5.3558e-03, 5.6877e-03, 1.4853e-04, 4.6754e-04,\n",
      "        1.3578e-04, 1.2505e-04, 1.4281e-04, 3.4404e-04, 6.9847e-03, 1.6851e-03,\n",
      "        4.7112e-03, 1.7517e-02, 5.9795e-04, 3.2115e-04, 4.3559e-04, 7.3671e-04,\n",
      "        4.4417e-04, 1.0538e-03, 7.1287e-04, 5.1928e-04, 7.6151e-04, 3.8743e-04,\n",
      "        3.8457e-04, 5.7936e-04, 3.4523e-04, 4.0364e-04, 2.7237e-03, 1.4124e-03,\n",
      "        4.2558e-04, 8.0442e-04, 1.1501e-03, 3.2115e-04, 9.5367e-04, 4.2391e-04,\n",
      "        8.3160e-04, 2.8191e-03, 1.8339e-03, 4.9257e-04, 3.6049e-03, 6.3972e-03,\n",
      "        1.6880e-03, 6.0158e-03, 2.3973e-04, 8.6308e-04, 2.1534e-03, 1.0662e-03,\n",
      "        1.0872e-03, 1.2903e-03, 2.5864e-03, 1.0166e-03, 6.8521e-04, 1.2579e-03,\n",
      "        1.9760e-03, 7.1182e-03, 1.9226e-03, 2.4338e-02, 2.1378e-02, 3.0807e-02,\n",
      "        5.9242e-03, 4.2191e-03, 2.1286e-03, 9.9716e-03, 4.9877e-04, 3.3550e-03,\n",
      "        3.4504e-03, 3.8738e-03, 9.0103e-03, 8.0261e-03, 2.6352e-02, 5.2567e-03,\n",
      "        4.7569e-03, 1.1948e-02, 5.1079e-03, 3.7861e-03, 5.8556e-03, 3.8433e-03,\n",
      "        6.0606e-04, 5.3978e-03, 3.5973e-03, 1.1665e-02, 8.5144e-03, 1.0330e-02,\n",
      "        1.4664e-02, 3.3966e-02, 4.6387e-03, 7.5989e-03, 3.2730e-03, 4.7798e-03,\n",
      "        8.8654e-03, 7.8011e-03, 9.3155e-03, 9.6130e-03, 7.3738e-03, 4.2000e-03,\n",
      "        1.6220e-02, 2.5387e-03, 1.8492e-03, 4.8904e-03, 5.6229e-03, 3.6182e-03,\n",
      "        1.6136e-03, 1.4076e-02, 6.5498e-03, 9.3126e-04, 9.7809e-03, 2.1591e-02,\n",
      "        2.8839e-03, 1.1415e-03, 1.7960e-02, 1.3832e-02, 1.7792e-02, 1.3412e-02,\n",
      "        3.1261e-03, 8.2397e-03, 3.8605e-03, 5.3749e-03, 2.3098e-03, 3.9101e-03,\n",
      "        2.7504e-03, 3.3021e-04, 4.2534e-03, 3.4008e-03, 1.1040e-02, 2.2171e-02,\n",
      "        1.0651e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [68] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [68] : torch.Size([1, 32, 1, 140])\n",
      "Last layer attentions for generated token [68] : tensor([1.5686e-01, 1.5710e-01, 1.7941e-04, 2.3580e-04, 1.4186e-04, 1.5230e-03,\n",
      "        3.2234e-04, 4.6897e-04, 4.2129e-04, 4.7636e-04, 1.2579e-03, 2.9993e-04,\n",
      "        2.3258e-04, 5.5361e-04, 4.1924e-03, 5.1918e-03, 1.4877e-04, 6.1893e-04,\n",
      "        2.3174e-04, 1.8549e-04, 1.6987e-04, 3.1853e-04, 6.7863e-03, 1.7223e-03,\n",
      "        5.0621e-03, 1.1467e-02, 7.0286e-04, 3.8362e-04, 3.7694e-04, 7.8821e-04,\n",
      "        4.8876e-04, 7.4339e-04, 9.5654e-04, 5.0926e-04, 6.9571e-04, 4.5896e-04,\n",
      "        3.2115e-04, 7.3910e-04, 4.6086e-04, 4.8757e-04, 2.3251e-03, 1.4677e-03,\n",
      "        4.1962e-04, 9.4366e-04, 1.2770e-03, 3.8719e-04, 5.3358e-04, 3.7098e-04,\n",
      "        6.4707e-04, 2.3746e-03, 1.7977e-03, 2.8229e-04, 2.8954e-03, 3.2749e-03,\n",
      "        1.6794e-03, 5.3291e-03, 2.1219e-04, 1.0653e-03, 1.8454e-03, 1.0843e-03,\n",
      "        1.0166e-03, 1.1425e-03, 2.8172e-03, 1.5831e-03, 8.1348e-04, 1.3828e-03,\n",
      "        3.0289e-03, 4.5853e-03, 1.6127e-03, 1.5396e-02, 1.3725e-02, 2.7817e-02,\n",
      "        4.9706e-03, 3.8776e-03, 2.4910e-03, 1.1391e-02, 6.5994e-04, 3.4142e-03,\n",
      "        4.1313e-03, 5.4321e-03, 8.4000e-03, 6.4659e-03, 3.0380e-02, 3.1376e-03,\n",
      "        3.2864e-03, 1.1780e-02, 8.3160e-03, 3.4790e-03, 6.5193e-03, 4.2381e-03,\n",
      "        9.1457e-04, 9.7351e-03, 4.4556e-03, 1.5656e-02, 1.9958e-02, 1.0735e-02,\n",
      "        1.6251e-02, 3.8391e-02, 4.6844e-03, 6.3744e-03, 5.2872e-03, 3.5934e-03,\n",
      "        6.4926e-03, 5.6915e-03, 7.2556e-03, 1.0071e-02, 8.7357e-03, 4.6921e-03,\n",
      "        1.6052e-02, 3.1033e-03, 1.6174e-03, 4.4708e-03, 6.1989e-03, 3.5858e-03,\n",
      "        1.5364e-03, 8.0490e-03, 6.1111e-03, 1.1835e-03, 1.0223e-02, 2.3010e-02,\n",
      "        2.3708e-03, 8.0395e-04, 1.2703e-02, 1.0956e-02, 1.3084e-02, 1.3618e-02,\n",
      "        3.3588e-03, 7.9956e-03, 4.7035e-03, 6.5231e-03, 2.3327e-03, 4.6921e-03,\n",
      "        2.6627e-03, 3.5954e-04, 4.0894e-03, 2.4376e-03, 1.0681e-02, 2.3102e-02,\n",
      "        9.9792e-03, 7.6065e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [69] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [69] : torch.Size([1, 32, 1, 141])\n",
      "Last layer attentions for generated token [69] : tensor([1.3916e-01, 1.3940e-01, 1.2994e-04, 2.9850e-04, 1.1557e-04, 1.2426e-03,\n",
      "        6.4707e-04, 6.2943e-04, 6.4325e-04, 7.4196e-04, 1.4467e-03, 4.6706e-04,\n",
      "        3.0494e-04, 9.0694e-04, 8.2169e-03, 5.3482e-03, 4.7994e-04, 1.1444e-03,\n",
      "        3.7813e-04, 3.3975e-04, 2.7180e-04, 4.5443e-04, 6.3400e-03, 2.3575e-03,\n",
      "        6.5689e-03, 1.4290e-02, 1.9331e-03, 8.2445e-04, 5.3024e-04, 6.8617e-04,\n",
      "        5.1594e-04, 8.8787e-04, 1.1692e-03, 6.2323e-04, 8.0347e-04, 7.5626e-04,\n",
      "        6.7949e-04, 9.5797e-04, 7.8344e-04, 6.9141e-04, 2.9869e-03, 2.3556e-03,\n",
      "        1.0004e-03, 1.1854e-03, 1.1721e-03, 6.3944e-04, 3.7956e-04, 3.7670e-04,\n",
      "        6.3467e-04, 2.9488e-03, 2.4033e-03, 2.4366e-04, 3.7804e-03, 4.0817e-03,\n",
      "        2.2106e-03, 5.5923e-03, 3.5048e-04, 2.0962e-03, 3.5019e-03, 1.2093e-03,\n",
      "        1.5831e-03, 1.5202e-03, 3.9597e-03, 2.4757e-03, 1.6880e-03, 1.8196e-03,\n",
      "        5.0926e-03, 4.4899e-03, 2.7180e-03, 1.9257e-02, 1.5053e-02, 2.6855e-02,\n",
      "        8.6823e-03, 5.0087e-03, 3.7498e-03, 1.0727e-02, 7.2336e-04, 4.5128e-03,\n",
      "        3.6831e-03, 5.0468e-03, 7.8735e-03, 5.7144e-03, 3.0655e-02, 5.2986e-03,\n",
      "        2.9678e-03, 6.4240e-03, 6.3744e-03, 4.3716e-03, 5.4398e-03, 4.3983e-03,\n",
      "        1.1053e-03, 7.0915e-03, 4.3297e-03, 1.1559e-02, 1.5190e-02, 8.7814e-03,\n",
      "        1.1902e-02, 3.6163e-02, 6.0158e-03, 6.0081e-03, 5.5313e-03, 7.6675e-03,\n",
      "        6.9389e-03, 4.3068e-03, 5.2109e-03, 5.8136e-03, 1.2032e-02, 4.2534e-03,\n",
      "        1.5289e-02, 3.1967e-03, 2.2697e-03, 3.3245e-03, 5.6267e-03, 3.0937e-03,\n",
      "        2.2621e-03, 8.9569e-03, 6.0768e-03, 1.5163e-03, 1.0506e-02, 2.7695e-02,\n",
      "        7.2250e-03, 1.6947e-03, 1.9165e-02, 1.3069e-02, 1.1940e-02, 1.2466e-02,\n",
      "        3.7231e-03, 8.8577e-03, 4.4212e-03, 4.9782e-03, 1.7653e-03, 4.2191e-03,\n",
      "        1.3914e-03, 4.5443e-04, 3.4771e-03, 1.8940e-03, 8.8043e-03, 2.2018e-02,\n",
      "        8.4839e-03, 6.2714e-03, 1.2413e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [70] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [70] : torch.Size([1, 32, 1, 142])\n",
      "Last layer attentions for generated token [70] : tensor([1.8201e-01, 1.8201e-01, 4.8327e-04, 5.3883e-04, 4.1175e-04, 2.3651e-03,\n",
      "        2.8634e-04, 5.0068e-04, 4.9353e-04, 6.3753e-04, 1.3714e-03, 4.1008e-04,\n",
      "        3.5429e-04, 6.7616e-04, 8.4000e-03, 7.8659e-03, 4.6921e-04, 6.8951e-04,\n",
      "        2.5916e-04, 3.9434e-04, 1.7023e-04, 9.2602e-04, 9.3079e-03, 2.9488e-03,\n",
      "        4.5395e-03, 1.5587e-02, 2.1877e-03, 5.9557e-04, 5.5933e-04, 6.5899e-04,\n",
      "        8.7166e-04, 1.8435e-03, 1.4400e-03, 7.7677e-04, 5.8651e-04, 5.5933e-04,\n",
      "        2.5511e-04, 5.8842e-04, 4.1986e-04, 3.2258e-04, 2.1458e-03, 1.3447e-03,\n",
      "        9.1171e-04, 1.0109e-03, 1.0309e-03, 2.9373e-04, 6.0368e-04, 2.5654e-04,\n",
      "        3.6907e-04, 1.9817e-03, 1.6317e-03, 6.5804e-04, 3.3283e-03, 5.9280e-03,\n",
      "        1.5297e-03, 7.2670e-03, 3.6049e-04, 1.4677e-03, 2.4796e-03, 1.1015e-03,\n",
      "        9.5367e-04, 1.6155e-03, 2.6245e-03, 1.5116e-03, 9.5558e-04, 9.5177e-04,\n",
      "        2.3994e-03, 3.2158e-03, 1.6832e-03, 2.3895e-02, 1.9897e-02, 2.3727e-02,\n",
      "        4.1733e-03, 5.0545e-03, 2.1725e-03, 9.2316e-03, 6.3753e-04, 2.5291e-03,\n",
      "        4.3449e-03, 3.2158e-03, 3.3340e-03, 4.1618e-03, 2.1225e-02, 4.5433e-03,\n",
      "        3.8605e-03, 7.0381e-03, 4.2458e-03, 3.2578e-03, 5.2986e-03, 3.3665e-03,\n",
      "        9.1314e-04, 5.9280e-03, 3.2597e-03, 7.7438e-03, 6.2637e-03, 5.5771e-03,\n",
      "        1.1971e-02, 2.9724e-02, 3.9902e-03, 5.1384e-03, 4.2458e-03, 4.2114e-03,\n",
      "        1.0391e-02, 4.6387e-03, 5.8899e-03, 5.4092e-03, 4.9171e-03, 4.2114e-03,\n",
      "        1.0239e-02, 3.7861e-03, 1.5326e-03, 4.3907e-03, 4.6730e-03, 2.8019e-03,\n",
      "        1.4200e-03, 7.8049e-03, 6.5536e-03, 7.9679e-04, 5.6725e-03, 2.0477e-02,\n",
      "        3.8528e-03, 1.4915e-03, 1.3809e-02, 8.1177e-03, 5.4665e-03, 1.2360e-02,\n",
      "        2.9316e-03, 8.9874e-03, 5.3406e-03, 4.4975e-03, 1.6699e-03, 4.1771e-03,\n",
      "        1.6088e-03, 5.4216e-04, 4.5967e-03, 1.9569e-03, 9.4452e-03, 2.8931e-02,\n",
      "        1.1070e-02, 9.3079e-03, 1.2115e-02, 6.4583e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [71] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [71] : torch.Size([1, 32, 1, 143])\n",
      "Last layer attentions for generated token [71] : tensor([0.1730, 0.1733, 0.0002, 0.0003, 0.0002, 0.0014, 0.0005, 0.0008, 0.0008,\n",
      "        0.0007, 0.0011, 0.0005, 0.0002, 0.0005, 0.0069, 0.0053, 0.0003, 0.0009,\n",
      "        0.0003, 0.0003, 0.0002, 0.0009, 0.0069, 0.0026, 0.0059, 0.0118, 0.0019,\n",
      "        0.0006, 0.0005, 0.0011, 0.0013, 0.0019, 0.0015, 0.0010, 0.0011, 0.0006,\n",
      "        0.0002, 0.0007, 0.0004, 0.0003, 0.0021, 0.0011, 0.0006, 0.0011, 0.0012,\n",
      "        0.0003, 0.0009, 0.0003, 0.0006, 0.0027, 0.0018, 0.0005, 0.0027, 0.0041,\n",
      "        0.0012, 0.0067, 0.0003, 0.0015, 0.0022, 0.0012, 0.0008, 0.0018, 0.0039,\n",
      "        0.0019, 0.0012, 0.0014, 0.0026, 0.0029, 0.0019, 0.0148, 0.0114, 0.0204,\n",
      "        0.0047, 0.0049, 0.0016, 0.0166, 0.0005, 0.0028, 0.0052, 0.0041, 0.0038,\n",
      "        0.0066, 0.0140, 0.0030, 0.0032, 0.0093, 0.0047, 0.0027, 0.0056, 0.0035,\n",
      "        0.0010, 0.0089, 0.0032, 0.0096, 0.0076, 0.0044, 0.0146, 0.0202, 0.0034,\n",
      "        0.0058, 0.0038, 0.0028, 0.0091, 0.0063, 0.0091, 0.0094, 0.0044, 0.0036,\n",
      "        0.0138, 0.0028, 0.0011, 0.0044, 0.0051, 0.0039, 0.0013, 0.0093, 0.0051,\n",
      "        0.0008, 0.0089, 0.0141, 0.0030, 0.0009, 0.0127, 0.0071, 0.0091, 0.0138,\n",
      "        0.0035, 0.0101, 0.0057, 0.0052, 0.0025, 0.0048, 0.0023, 0.0005, 0.0051,\n",
      "        0.0027, 0.0149, 0.0304, 0.0124, 0.0122, 0.0136, 0.0078, 0.0178],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [72] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [72] : torch.Size([1, 32, 1, 144])\n",
      "Last layer attentions for generated token [72] : tensor([2.2644e-01, 2.2644e-01, 2.7084e-04, 3.6311e-04, 2.1136e-04, 1.3762e-03,\n",
      "        3.5262e-04, 4.2367e-04, 7.3195e-04, 7.7772e-04, 1.8368e-03, 5.7936e-04,\n",
      "        5.8270e-04, 1.0185e-03, 2.1667e-02, 1.0345e-02, 3.9339e-04, 1.1492e-03,\n",
      "        4.7922e-04, 4.7445e-04, 4.4918e-04, 1.1454e-03, 1.0880e-02, 4.9629e-03,\n",
      "        1.1559e-02, 1.2062e-02, 1.8587e-03, 6.5756e-04, 8.8120e-04, 5.9748e-04,\n",
      "        1.4563e-03, 2.2202e-03, 1.2627e-03, 4.6349e-04, 7.5674e-04, 1.2646e-03,\n",
      "        2.5201e-04, 7.7772e-04, 6.2752e-04, 5.2118e-04, 2.7905e-03, 1.2951e-03,\n",
      "        5.7936e-04, 1.0405e-03, 8.9884e-04, 6.3610e-04, 3.1376e-04, 2.7466e-04,\n",
      "        4.8304e-04, 3.0556e-03, 1.9417e-03, 3.5882e-04, 2.9259e-03, 7.5531e-03,\n",
      "        1.2903e-03, 5.9357e-03, 4.9257e-04, 1.5774e-03, 3.8471e-03, 1.2026e-03,\n",
      "        1.3838e-03, 1.2140e-03, 5.0659e-03, 2.4662e-03, 1.6689e-03, 2.5158e-03,\n",
      "        4.6387e-03, 2.9430e-03, 2.3022e-03, 3.2440e-02, 1.6968e-02, 1.9333e-02,\n",
      "        2.3632e-03, 2.2030e-03, 1.1454e-03, 4.0588e-03, 3.5334e-04, 1.8806e-03,\n",
      "        2.0084e-03, 3.1242e-03, 3.4885e-03, 6.0806e-03, 2.1301e-02, 2.6035e-03,\n",
      "        1.3895e-03, 2.9144e-03, 2.1095e-03, 1.9474e-03, 2.4834e-03, 2.1229e-03,\n",
      "        7.3910e-04, 3.3531e-03, 1.9932e-03, 6.6681e-03, 5.4016e-03, 3.3131e-03,\n",
      "        8.3389e-03, 1.5030e-02, 1.3494e-03, 3.4428e-03, 2.5673e-03, 2.5711e-03,\n",
      "        3.8204e-03, 2.8419e-03, 3.4828e-03, 3.0041e-03, 3.9825e-03, 1.9255e-03,\n",
      "        1.1757e-02, 2.0218e-03, 1.6890e-03, 2.7351e-03, 4.7073e-03, 2.3479e-03,\n",
      "        2.1305e-03, 8.4915e-03, 6.8359e-03, 1.5593e-03, 1.1093e-02, 1.2650e-02,\n",
      "        3.4580e-03, 9.5654e-04, 8.0032e-03, 4.8790e-03, 6.0463e-03, 9.1248e-03,\n",
      "        3.0880e-03, 5.2910e-03, 2.9430e-03, 3.4828e-03, 1.7490e-03, 2.6474e-03,\n",
      "        1.8654e-03, 3.6097e-04, 3.7918e-03, 1.4791e-03, 7.3547e-03, 1.7975e-02,\n",
      "        7.7744e-03, 4.6082e-03, 5.1308e-03, 3.7174e-03, 4.8637e-03, 1.9398e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [73] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [73] : torch.Size([1, 32, 1, 145])\n",
      "Last layer attentions for generated token [73] : tensor([2.2302e-01, 2.2339e-01, 5.8270e-04, 8.2159e-04, 4.3893e-04, 2.0790e-03,\n",
      "        1.5628e-04, 2.6631e-04, 7.4387e-04, 6.0129e-04, 8.4305e-04, 3.5691e-04,\n",
      "        7.4244e-04, 1.9073e-03, 1.1520e-02, 7.8430e-03, 2.9063e-04, 8.1205e-04,\n",
      "        1.9288e-04, 4.1723e-04, 3.7193e-04, 5.5408e-04, 1.3054e-02, 4.1885e-03,\n",
      "        9.2087e-03, 1.0689e-02, 1.3237e-03, 7.1096e-04, 1.2703e-03, 6.9618e-04,\n",
      "        1.9054e-03, 2.4872e-03, 9.3842e-04, 4.3559e-04, 4.2295e-04, 1.2293e-03,\n",
      "        2.1601e-04, 4.9448e-04, 5.9175e-04, 5.1117e-04, 4.6043e-03, 1.2121e-03,\n",
      "        6.5660e-04, 8.7118e-04, 6.8665e-04, 6.6948e-04, 2.8729e-04, 2.0218e-04,\n",
      "        3.9434e-04, 2.4033e-03, 1.3447e-03, 4.4584e-04, 2.7390e-03, 6.4011e-03,\n",
      "        1.6861e-03, 5.6114e-03, 5.3978e-04, 2.3174e-03, 6.2256e-03, 1.3313e-03,\n",
      "        2.2202e-03, 1.7910e-03, 7.1030e-03, 1.8587e-03, 2.9774e-03, 1.9989e-03,\n",
      "        3.5763e-03, 2.2182e-03, 2.7103e-03, 2.0599e-02, 1.3931e-02, 1.4153e-02,\n",
      "        2.4052e-03, 2.4490e-03, 7.9489e-04, 2.8439e-03, 7.0572e-04, 1.5621e-03,\n",
      "        1.6899e-03, 1.9178e-03, 2.0237e-03, 3.6583e-03, 1.1673e-02, 4.4518e-03,\n",
      "        2.1553e-03, 2.5291e-03, 1.7529e-03, 2.8801e-03, 2.5463e-03, 2.2297e-03,\n",
      "        1.2531e-03, 2.6073e-03, 1.9894e-03, 8.4381e-03, 3.4943e-03, 2.7695e-03,\n",
      "        5.7373e-03, 9.0942e-03, 1.7843e-03, 2.9030e-03, 1.6403e-03, 3.2063e-03,\n",
      "        2.3689e-03, 1.6088e-03, 2.5444e-03, 1.6184e-03, 2.8744e-03, 1.9426e-03,\n",
      "        1.0902e-02, 2.3994e-03, 2.0657e-03, 4.7760e-03, 6.2294e-03, 4.1122e-03,\n",
      "        4.2496e-03, 1.6479e-02, 8.8425e-03, 5.6038e-03, 1.3412e-02, 1.3046e-02,\n",
      "        4.3068e-03, 2.1839e-03, 9.1095e-03, 4.2610e-03, 6.2447e-03, 7.2517e-03,\n",
      "        6.0806e-03, 1.1528e-02, 3.6793e-03, 3.7270e-03, 2.9469e-03, 3.2063e-03,\n",
      "        3.2406e-03, 7.1096e-04, 3.7079e-03, 3.4122e-03, 8.5220e-03, 1.5900e-02,\n",
      "        1.2657e-02, 7.8812e-03, 7.8888e-03, 7.9880e-03, 9.1400e-03, 4.2610e-03,\n",
      "        5.7640e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [74] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [74] : torch.Size([1, 32, 1, 146])\n",
      "Last layer attentions for generated token [74] : tensor([2.7832e-01, 2.7832e-01, 7.1859e-04, 7.0047e-04, 4.3750e-04, 3.0861e-03,\n",
      "        1.5247e-04, 2.9850e-04, 6.5041e-04, 8.2254e-04, 8.6880e-04, 4.5586e-04,\n",
      "        3.8171e-04, 1.3638e-03, 7.9346e-03, 6.9122e-03, 3.7956e-04, 7.3290e-04,\n",
      "        2.1660e-04, 2.0838e-04, 3.3092e-04, 6.0272e-04, 9.1858e-03, 2.7790e-03,\n",
      "        7.2441e-03, 1.0384e-02, 1.0376e-03, 5.1880e-04, 5.8079e-04, 7.5626e-04,\n",
      "        1.1692e-03, 1.9493e-03, 8.9073e-04, 4.4966e-04, 6.8188e-04, 9.7466e-04,\n",
      "        1.2982e-04, 7.2861e-04, 5.4979e-04, 2.5415e-04, 4.4022e-03, 1.1549e-03,\n",
      "        3.4881e-04, 7.2432e-04, 6.0034e-04, 2.9373e-04, 2.6035e-04, 1.9193e-04,\n",
      "        2.5535e-04, 2.8267e-03, 1.3905e-03, 5.2071e-04, 3.2978e-03, 8.1024e-03,\n",
      "        1.6546e-03, 5.1460e-03, 3.1757e-04, 1.5821e-03, 4.0627e-03, 1.0138e-03,\n",
      "        9.7847e-04, 1.8282e-03, 5.1155e-03, 1.5011e-03, 1.3113e-03, 1.3609e-03,\n",
      "        1.8711e-03, 2.8877e-03, 1.9388e-03, 1.1749e-02, 9.9945e-03, 1.3680e-02,\n",
      "        2.4147e-03, 3.4084e-03, 7.4577e-04, 3.9139e-03, 6.4659e-04, 1.8206e-03,\n",
      "        3.8662e-03, 3.2749e-03, 2.0370e-03, 5.2109e-03, 1.1551e-02, 1.1311e-03,\n",
      "        1.5488e-03, 3.3302e-03, 1.7376e-03, 9.8228e-04, 2.1858e-03, 1.7891e-03,\n",
      "        6.6710e-04, 3.2616e-03, 1.3475e-03, 6.6605e-03, 4.3755e-03, 3.9749e-03,\n",
      "        5.9242e-03, 7.8812e-03, 1.4400e-03, 2.3537e-03, 1.3876e-03, 6.1607e-04,\n",
      "        2.2182e-03, 1.8387e-03, 2.4433e-03, 2.2182e-03, 1.9989e-03, 1.2054e-03,\n",
      "        6.7444e-03, 1.7004e-03, 6.1131e-04, 3.9291e-03, 3.7365e-03, 3.2520e-03,\n",
      "        1.1511e-03, 8.3466e-03, 5.2071e-03, 1.7929e-03, 5.8556e-03, 8.9722e-03,\n",
      "        2.7981e-03, 1.4353e-03, 5.7793e-03, 3.4752e-03, 4.9210e-03, 4.8676e-03,\n",
      "        3.1166e-03, 6.2103e-03, 3.7079e-03, 3.9825e-03, 2.5787e-03, 2.7103e-03,\n",
      "        3.5038e-03, 5.0449e-04, 3.6030e-03, 2.8839e-03, 6.5384e-03, 1.2497e-02,\n",
      "        1.1192e-02, 9.0103e-03, 9.8877e-03, 5.0659e-03, 7.5836e-03, 3.0289e-03,\n",
      "        4.4594e-03, 3.4866e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [75] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [75] : torch.Size([1, 32, 1, 147])\n",
      "Last layer attentions for generated token [75] : tensor([2.0532e-01, 2.0496e-01, 5.4407e-04, 8.3447e-04, 4.6349e-04, 2.0008e-03,\n",
      "        3.4046e-04, 3.1543e-04, 5.3549e-04, 8.9502e-04, 1.6947e-03, 9.6941e-04,\n",
      "        7.4053e-04, 1.8005e-03, 2.0676e-02, 1.1047e-02, 3.7456e-04, 1.2741e-03,\n",
      "        3.0994e-04, 4.6897e-04, 7.6866e-04, 9.8705e-04, 9.9564e-03, 3.2177e-03,\n",
      "        1.3840e-02, 1.4229e-02, 2.1992e-03, 4.1461e-04, 8.2302e-04, 4.9829e-04,\n",
      "        8.1158e-04, 1.3752e-03, 9.1600e-04, 3.6883e-04, 4.0030e-04, 9.9468e-04,\n",
      "        3.0446e-04, 6.3848e-04, 5.6124e-04, 4.0102e-04, 3.6526e-03, 1.3752e-03,\n",
      "        6.2132e-04, 7.3481e-04, 6.0892e-04, 7.4911e-04, 1.6594e-04, 1.9324e-04,\n",
      "        4.8661e-04, 4.0665e-03, 2.1057e-03, 4.1866e-04, 3.4313e-03, 1.0498e-02,\n",
      "        2.1076e-03, 6.1989e-03, 1.3227e-03, 1.8082e-03, 4.7455e-03, 1.4553e-03,\n",
      "        2.4281e-03, 1.6146e-03, 4.4479e-03, 2.4681e-03, 2.1763e-03, 1.5526e-03,\n",
      "        3.8471e-03, 4.0436e-03, 2.1572e-03, 2.7008e-02, 1.6678e-02, 1.8005e-02,\n",
      "        4.4327e-03, 4.2191e-03, 1.0099e-03, 4.1847e-03, 3.3569e-04, 1.6785e-03,\n",
      "        1.5860e-03, 2.3746e-03, 3.5915e-03, 7.4005e-03, 2.0020e-02, 6.3705e-03,\n",
      "        2.3403e-03, 2.4357e-03, 1.5469e-03, 3.0994e-03, 2.3575e-03, 2.2678e-03,\n",
      "        6.3324e-04, 2.1992e-03, 2.2717e-03, 6.5842e-03, 4.5815e-03, 2.9297e-03,\n",
      "        7.1106e-03, 1.1475e-02, 1.9779e-03, 2.4395e-03, 1.7052e-03, 3.4847e-03,\n",
      "        3.2082e-03, 2.2659e-03, 2.6226e-03, 2.7657e-03, 4.6196e-03, 2.0485e-03,\n",
      "        1.1482e-02, 2.3270e-03, 1.4362e-03, 3.7022e-03, 4.1428e-03, 2.3193e-03,\n",
      "        2.5616e-03, 8.2397e-03, 8.4686e-03, 2.0828e-03, 7.4806e-03, 1.3664e-02,\n",
      "        6.3286e-03, 1.2207e-03, 7.5684e-03, 5.1117e-03, 5.5008e-03, 5.8136e-03,\n",
      "        3.0251e-03, 6.1531e-03, 3.9215e-03, 3.7174e-03, 3.0785e-03, 3.7575e-03,\n",
      "        3.5095e-03, 4.4489e-04, 2.3975e-03, 3.6163e-03, 9.0103e-03, 1.5884e-02,\n",
      "        1.7670e-02, 7.5455e-03, 8.8959e-03, 5.9471e-03, 7.4158e-03, 2.9087e-03,\n",
      "        1.2503e-03, 2.5997e-03, 9.2087e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [76] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [76] : torch.Size([1, 32, 1, 148])\n",
      "Last layer attentions for generated token [76] : tensor([2.1179e-01, 2.1179e-01, 1.5247e-04, 3.3236e-04, 1.4222e-04, 1.4029e-03,\n",
      "        1.4782e-04, 2.1005e-04, 6.1131e-04, 4.7159e-04, 8.3113e-04, 4.6706e-04,\n",
      "        3.4714e-04, 1.6766e-03, 1.0666e-02, 8.3771e-03, 1.7965e-04, 7.4625e-04,\n",
      "        1.4496e-04, 2.0361e-04, 4.5276e-04, 7.4482e-04, 8.2932e-03, 4.4785e-03,\n",
      "        1.3268e-02, 1.6235e-02, 1.0834e-03, 6.9714e-04, 8.8787e-04, 8.6403e-04,\n",
      "        1.1072e-03, 2.4586e-03, 8.8644e-04, 4.5896e-04, 8.0395e-04, 1.3256e-03,\n",
      "        2.6393e-04, 6.0654e-04, 4.3201e-04, 3.8791e-04, 4.3106e-03, 1.4496e-03,\n",
      "        6.1512e-04, 8.8644e-04, 7.1096e-04, 6.5756e-04, 3.4714e-04, 3.0088e-04,\n",
      "        5.5361e-04, 5.1994e-03, 2.4719e-03, 2.3162e-04, 4.4250e-03, 7.8201e-03,\n",
      "        2.8667e-03, 6.3820e-03, 3.5596e-04, 1.2159e-03, 3.2864e-03, 7.9775e-04,\n",
      "        1.8778e-03, 1.5802e-03, 4.8332e-03, 1.8730e-03, 1.2970e-03, 2.0828e-03,\n",
      "        2.6646e-03, 2.3956e-03, 2.2964e-03, 1.8555e-02, 1.4473e-02, 2.1469e-02,\n",
      "        3.9406e-03, 3.4580e-03, 8.0395e-04, 4.5891e-03, 2.6035e-04, 1.6870e-03,\n",
      "        2.1267e-03, 3.0575e-03, 3.6621e-03, 8.1482e-03, 2.2827e-02, 4.4670e-03,\n",
      "        2.2793e-03, 4.9667e-03, 2.5711e-03, 2.9926e-03, 3.8280e-03, 1.9722e-03,\n",
      "        6.7568e-04, 3.8013e-03, 1.9855e-03, 9.9945e-03, 5.6152e-03, 3.5458e-03,\n",
      "        7.8011e-03, 1.4267e-02, 1.3781e-03, 2.8496e-03, 1.5736e-03, 2.9087e-03,\n",
      "        4.8141e-03, 2.8687e-03, 2.6894e-03, 2.2392e-03, 2.0714e-03, 1.1072e-03,\n",
      "        9.8724e-03, 1.5430e-03, 8.8453e-04, 3.5954e-03, 4.0703e-03, 1.9341e-03,\n",
      "        1.1787e-03, 1.2474e-02, 6.1531e-03, 1.4133e-03, 9.9716e-03, 1.4778e-02,\n",
      "        3.8757e-03, 1.2569e-03, 8.3466e-03, 3.9711e-03, 3.6526e-03, 5.2605e-03,\n",
      "        1.7099e-03, 6.0158e-03, 3.3417e-03, 3.0403e-03, 3.8528e-03, 3.6488e-03,\n",
      "        2.9430e-03, 4.5800e-04, 2.6646e-03, 2.5692e-03, 8.0795e-03, 1.8494e-02,\n",
      "        9.4910e-03, 6.8817e-03, 7.2479e-03, 5.8670e-03, 9.5367e-03, 3.3512e-03,\n",
      "        2.0771e-03, 3.2806e-03, 1.8723e-02, 5.8823e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [77] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [77] : torch.Size([1, 32, 1, 149])\n",
      "Last layer attentions for generated token [77] : tensor([2.1240e-01, 2.1240e-01, 4.3917e-04, 4.8590e-04, 2.8896e-04, 2.6817e-03,\n",
      "        2.2829e-04, 4.4775e-04, 5.1546e-04, 6.9475e-04, 1.0595e-03, 3.8743e-04,\n",
      "        3.2496e-04, 1.2484e-03, 1.1177e-02, 9.6741e-03, 3.8671e-04, 7.0858e-04,\n",
      "        1.9979e-04, 2.3913e-04, 2.2829e-04, 9.0599e-04, 1.1841e-02, 4.8103e-03,\n",
      "        7.4615e-03, 1.6586e-02, 1.1253e-03, 7.3671e-04, 8.5115e-04, 1.1845e-03,\n",
      "        1.0862e-03, 1.7586e-03, 7.5865e-04, 4.1652e-04, 4.8399e-04, 1.2608e-03,\n",
      "        1.9145e-04, 7.3814e-04, 6.7616e-04, 3.8671e-04, 3.3436e-03, 1.2808e-03,\n",
      "        3.3784e-04, 7.5293e-04, 5.4646e-04, 3.2377e-04, 2.1696e-04, 2.2864e-04,\n",
      "        2.8896e-04, 1.7109e-03, 1.2264e-03, 1.4675e-04, 2.1648e-03, 5.9776e-03,\n",
      "        1.9836e-03, 8.2397e-03, 3.9506e-04, 2.7676e-03, 3.0041e-03, 9.4414e-04,\n",
      "        1.1911e-03, 1.4286e-03, 4.3068e-03, 1.5297e-03, 1.0929e-03, 1.4315e-03,\n",
      "        2.4338e-03, 2.2430e-03, 1.5869e-03, 1.2764e-02, 1.1299e-02, 1.8311e-02,\n",
      "        3.4199e-03, 2.9850e-03, 8.8882e-04, 8.6288e-03, 5.2881e-04, 2.9049e-03,\n",
      "        4.0245e-03, 3.1624e-03, 3.2673e-03, 5.0240e-03, 2.1164e-02, 2.9621e-03,\n",
      "        2.9144e-03, 7.0686e-03, 3.0956e-03, 2.0790e-03, 3.9825e-03, 2.9373e-03,\n",
      "        9.5177e-04, 4.0131e-03, 2.5024e-03, 6.3362e-03, 7.4768e-03, 4.9477e-03,\n",
      "        6.9275e-03, 1.3054e-02, 2.3975e-03, 3.0479e-03, 2.0123e-03, 8.4972e-04,\n",
      "        3.3054e-03, 2.2850e-03, 3.4866e-03, 3.3073e-03, 2.4776e-03, 1.1797e-03,\n",
      "        8.3771e-03, 2.5368e-03, 1.1930e-03, 3.5954e-03, 5.8937e-03, 3.5076e-03,\n",
      "        1.5087e-03, 7.7362e-03, 5.8861e-03, 1.7967e-03, 6.4735e-03, 1.2756e-02,\n",
      "        3.6259e-03, 2.3155e-03, 6.6185e-03, 4.3793e-03, 5.7869e-03, 6.4507e-03,\n",
      "        2.6665e-03, 4.1428e-03, 2.9678e-03, 4.1580e-03, 3.3894e-03, 4.0283e-03,\n",
      "        3.4904e-03, 6.5804e-04, 4.1618e-03, 2.3708e-03, 6.0539e-03, 1.5778e-02,\n",
      "        1.2283e-02, 1.1711e-02, 9.5901e-03, 7.6294e-03, 1.4412e-02, 5.2261e-03,\n",
      "        3.9825e-03, 4.5967e-03, 1.8188e-02, 4.9553e-03, 6.5231e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [78] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [78] : torch.Size([1, 32, 1, 150])\n",
      "Last layer attentions for generated token [78] : tensor([2.5732e-01, 2.5684e-01, 5.7173e-04, 5.2261e-04, 3.6335e-04, 2.8744e-03,\n",
      "        4.6015e-04, 5.8985e-04, 7.0190e-04, 9.8801e-04, 9.8038e-04, 9.7847e-04,\n",
      "        4.8876e-04, 1.4153e-03, 8.3618e-03, 7.2289e-03, 4.6015e-04, 8.7023e-04,\n",
      "        3.1829e-04, 2.1911e-04, 4.8232e-04, 1.2484e-03, 1.0452e-02, 4.2992e-03,\n",
      "        8.6899e-03, 1.5152e-02, 1.2197e-03, 6.8665e-04, 8.6164e-04, 1.1940e-03,\n",
      "        1.6413e-03, 1.8225e-03, 1.1988e-03, 5.4121e-04, 9.1362e-04, 1.0233e-03,\n",
      "        2.5320e-04, 6.6185e-04, 7.1716e-04, 6.3038e-04, 3.6316e-03, 1.0891e-03,\n",
      "        4.2486e-04, 8.1873e-04, 6.0272e-04, 4.7755e-04, 2.7275e-04, 3.7336e-04,\n",
      "        7.0333e-04, 3.6125e-03, 2.0905e-03, 4.1652e-04, 3.3684e-03, 9.5520e-03,\n",
      "        2.0142e-03, 7.0114e-03, 3.4332e-04, 1.5602e-03, 2.9030e-03, 8.9407e-04,\n",
      "        1.2636e-03, 1.3475e-03, 4.4250e-03, 1.4153e-03, 1.4944e-03, 1.1683e-03,\n",
      "        1.7452e-03, 4.1656e-03, 1.9398e-03, 1.6312e-02, 1.2329e-02, 1.7258e-02,\n",
      "        3.6316e-03, 2.0561e-03, 6.5660e-04, 2.8591e-03, 3.3665e-04, 1.7138e-03,\n",
      "        2.0065e-03, 2.3975e-03, 3.2520e-03, 4.8485e-03, 1.4702e-02, 1.5211e-03,\n",
      "        1.2913e-03, 3.1700e-03, 1.8044e-03, 9.5940e-04, 2.1229e-03, 1.3714e-03,\n",
      "        3.8528e-04, 1.6737e-03, 1.8024e-03, 4.9553e-03, 5.1270e-03, 4.3297e-03,\n",
      "        5.0545e-03, 1.0513e-02, 1.3475e-03, 2.5444e-03, 1.0929e-03, 5.0735e-04,\n",
      "        1.8044e-03, 1.9817e-03, 2.0428e-03, 2.3251e-03, 1.1711e-03, 5.7077e-04,\n",
      "        4.1542e-03, 1.2245e-03, 7.9060e-04, 2.0523e-03, 3.0537e-03, 2.5082e-03,\n",
      "        1.2779e-03, 6.9160e-03, 4.2953e-03, 1.6289e-03, 4.3259e-03, 1.0292e-02,\n",
      "        3.6488e-03, 1.3819e-03, 5.1384e-03, 3.5992e-03, 6.2828e-03, 6.1836e-03,\n",
      "        1.6003e-03, 3.5362e-03, 1.9073e-03, 2.1744e-03, 2.7657e-03, 3.2673e-03,\n",
      "        3.8166e-03, 4.8232e-04, 3.4885e-03, 2.6722e-03, 6.7215e-03, 1.1726e-02,\n",
      "        7.5989e-03, 7.1640e-03, 6.8970e-03, 6.4278e-03, 9.1934e-03, 3.6526e-03,\n",
      "        2.6493e-03, 4.2381e-03, 1.9043e-02, 6.0616e-03, 6.6795e-03, 4.3831e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [79] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [79] : torch.Size([1, 32, 1, 151])\n",
      "Last layer attentions for generated token [79] : tensor([1.9568e-01, 1.9568e-01, 1.6661e-03, 1.8549e-03, 1.6537e-03, 4.5891e-03,\n",
      "        4.8971e-04, 3.9816e-04, 5.1022e-04, 1.0071e-03, 1.1911e-03, 8.1348e-04,\n",
      "        9.2554e-04, 1.3437e-03, 2.3544e-02, 1.5350e-02, 6.4516e-04, 1.7614e-03,\n",
      "        3.4595e-04, 4.7183e-04, 6.0940e-04, 2.0370e-03, 7.8506e-03, 4.3144e-03,\n",
      "        1.6373e-02, 1.8555e-02, 4.1466e-03, 9.3126e-04, 1.0166e-03, 1.1606e-03,\n",
      "        1.6241e-03, 3.3894e-03, 7.7486e-04, 4.6086e-04, 4.8113e-04, 1.6336e-03,\n",
      "        2.1517e-04, 7.8249e-04, 7.0715e-04, 4.9734e-04, 7.5417e-03, 2.3594e-03,\n",
      "        6.3610e-04, 1.8177e-03, 8.2636e-04, 5.5361e-04, 2.5797e-04, 1.8477e-04,\n",
      "        4.5300e-04, 5.6114e-03, 2.9240e-03, 7.5531e-04, 2.4719e-03, 2.1637e-02,\n",
      "        1.9369e-03, 1.1627e-02, 1.6880e-03, 2.3594e-03, 5.5923e-03, 1.2579e-03,\n",
      "        2.4300e-03, 1.6956e-03, 4.0474e-03, 2.6760e-03, 2.4223e-03, 1.3390e-03,\n",
      "        3.5305e-03, 4.9438e-03, 1.9951e-03, 2.9709e-02, 2.7435e-02, 3.2532e-02,\n",
      "        7.2556e-03, 3.2825e-03, 5.0402e-04, 1.8177e-03, 1.9217e-04, 1.0805e-03,\n",
      "        1.2140e-03, 2.0638e-03, 2.4433e-03, 8.2474e-03, 2.6932e-02, 4.1695e-03,\n",
      "        2.4433e-03, 2.5311e-03, 2.0142e-03, 1.7395e-03, 2.3937e-03, 1.7281e-03,\n",
      "        5.9509e-04, 1.9217e-03, 1.3523e-03, 4.0855e-03, 4.2038e-03, 1.8358e-03,\n",
      "        3.8433e-03, 1.4122e-02, 4.5433e-03, 2.0370e-03, 1.7862e-03, 9.1314e-04,\n",
      "        2.1210e-03, 1.3895e-03, 1.2312e-03, 1.6947e-03, 9.8515e-04, 5.8603e-04,\n",
      "        5.6267e-03, 2.9945e-03, 1.1702e-03, 3.4294e-03, 3.1071e-03, 2.7657e-03,\n",
      "        1.4877e-03, 7.2517e-03, 4.6997e-03, 1.4849e-03, 3.3493e-03, 1.2100e-02,\n",
      "        8.4610e-03, 1.9350e-03, 6.1493e-03, 3.2883e-03, 4.5433e-03, 4.7150e-03,\n",
      "        1.6775e-03, 2.6836e-03, 1.8845e-03, 1.4114e-03, 2.3041e-03, 2.2430e-03,\n",
      "        2.8324e-03, 4.0197e-04, 1.5993e-03, 2.0771e-03, 6.6681e-03, 1.2978e-02,\n",
      "        7.6866e-03, 4.1924e-03, 3.3760e-03, 3.9597e-03, 4.1656e-03, 1.8997e-03,\n",
      "        1.0929e-03, 2.6951e-03, 6.4926e-03, 2.5940e-03, 4.7874e-03, 1.5936e-03,\n",
      "        1.2245e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [80] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [80] : torch.Size([1, 32, 1, 152])\n",
      "Last layer attentions for generated token [80] : tensor([1.4050e-01, 1.4050e-01, 2.5678e-04, 4.8375e-04, 2.0206e-04, 3.6182e-03,\n",
      "        4.5085e-04, 3.6144e-04, 5.0116e-04, 3.8338e-04, 1.9722e-03, 4.1461e-04,\n",
      "        4.8089e-04, 1.0204e-03, 1.9592e-02, 1.3016e-02, 4.3535e-04, 5.1403e-04,\n",
      "        1.5581e-04, 2.6965e-04, 1.2517e-04, 2.5153e-04, 3.7251e-03, 7.7152e-04,\n",
      "        5.2147e-03, 2.3605e-02, 2.4834e-03, 4.7421e-04, 3.1233e-04, 3.0208e-04,\n",
      "        5.4932e-04, 1.1053e-03, 6.6519e-04, 6.1369e-04, 3.4761e-04, 3.6359e-04,\n",
      "        2.0051e-04, 3.5882e-04, 2.1207e-04, 1.0216e-04, 2.6817e-03, 1.0986e-03,\n",
      "        6.7568e-04, 7.3051e-04, 5.6219e-04, 2.9731e-04, 2.2757e-04, 1.3590e-04,\n",
      "        3.0041e-04, 2.0828e-03, 1.3628e-03, 4.5967e-04, 2.6131e-03, 5.1422e-03,\n",
      "        1.3943e-03, 5.7411e-03, 9.2697e-04, 1.7662e-03, 5.5313e-03, 1.1797e-03,\n",
      "        7.8678e-04, 1.9112e-03, 2.4643e-03, 8.7070e-04, 9.4891e-04, 5.2929e-04,\n",
      "        1.5316e-03, 3.2673e-03, 1.9321e-03, 1.8173e-02, 1.1864e-02, 2.6825e-02,\n",
      "        1.1971e-02, 1.2733e-02, 2.9621e-03, 8.8882e-03, 6.7139e-04, 2.3785e-03,\n",
      "        3.0632e-03, 3.8357e-03, 6.2065e-03, 8.0566e-03, 1.7075e-02, 1.5312e-02,\n",
      "        8.5907e-03, 9.0942e-03, 2.7447e-03, 7.8506e-03, 6.9656e-03, 4.9286e-03,\n",
      "        1.5879e-03, 4.1466e-03, 4.3297e-03, 1.4267e-02, 6.4888e-03, 3.6469e-03,\n",
      "        7.6408e-03, 1.3374e-02, 6.9771e-03, 7.0343e-03, 2.9774e-03, 1.1353e-02,\n",
      "        8.2779e-03, 6.4659e-03, 5.6763e-03, 4.8676e-03, 7.4844e-03, 3.0117e-03,\n",
      "        1.1909e-02, 3.0937e-03, 1.0920e-03, 4.0359e-03, 5.6877e-03, 2.6684e-03,\n",
      "        1.4572e-03, 7.7057e-03, 4.1122e-03, 7.5626e-04, 7.4043e-03, 1.2543e-02,\n",
      "        5.2071e-03, 1.0796e-03, 9.7656e-03, 7.3204e-03, 6.1722e-03, 1.0551e-02,\n",
      "        1.5678e-03, 4.1122e-03, 3.9368e-03, 3.2711e-03, 2.5463e-03, 4.1771e-03,\n",
      "        2.4376e-03, 3.2210e-04, 2.1763e-03, 2.4872e-03, 9.9106e-03, 2.1561e-02,\n",
      "        8.1100e-03, 7.2823e-03, 9.9487e-03, 2.8133e-03, 9.7656e-03, 2.5768e-03,\n",
      "        1.6413e-03, 1.9455e-03, 9.4528e-03, 2.3003e-03, 1.9503e-03, 1.4038e-03,\n",
      "        1.3824e-02, 4.0009e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [81] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [81] : torch.Size([1, 32, 1, 153])\n",
      "Last layer attentions for generated token [81] : tensor([2.1277e-01, 2.1277e-01, 8.2159e-04, 1.4935e-03, 4.7827e-04, 3.5038e-03,\n",
      "        2.5892e-04, 2.2376e-04, 3.0112e-04, 9.8133e-04, 1.7958e-03, 4.2367e-04,\n",
      "        1.3676e-03, 1.2379e-03, 1.0124e-02, 7.8201e-03, 3.5620e-04, 9.8133e-04,\n",
      "        2.3913e-04, 4.9639e-04, 2.2721e-04, 7.4673e-04, 6.7444e-03, 1.3390e-03,\n",
      "        4.3106e-03, 8.5907e-03, 2.1114e-03, 7.7772e-04, 8.6927e-04, 6.1417e-04,\n",
      "        7.0429e-04, 1.4906e-03, 8.9884e-04, 4.6182e-04, 3.2926e-04, 1.0109e-03,\n",
      "        5.2834e-04, 6.0701e-04, 9.1648e-04, 4.2129e-04, 4.1351e-03, 2.9964e-03,\n",
      "        9.3269e-04, 1.1787e-03, 5.6362e-04, 8.4448e-04, 1.8263e-04, 4.2701e-04,\n",
      "        6.1512e-04, 3.4790e-03, 2.9278e-03, 2.9016e-04, 2.9411e-03, 7.0038e-03,\n",
      "        1.5802e-03, 6.3744e-03, 6.5374e-04, 2.2659e-03, 3.4752e-03, 9.2030e-04,\n",
      "        1.2798e-03, 1.1883e-03, 2.3174e-03, 1.1721e-03, 1.0128e-03, 6.4850e-04,\n",
      "        4.4289e-03, 4.0970e-03, 1.6403e-03, 2.7252e-02, 1.6159e-02, 3.1921e-02,\n",
      "        6.9008e-03, 7.6103e-03, 2.0027e-03, 2.7599e-03, 5.5361e-04, 2.6836e-03,\n",
      "        3.3226e-03, 3.4218e-03, 4.7493e-03, 5.3558e-03, 2.5757e-02, 7.6447e-03,\n",
      "        3.8586e-03, 3.7518e-03, 4.1695e-03, 4.5662e-03, 3.5343e-03, 3.9291e-03,\n",
      "        1.5926e-03, 2.7142e-03, 2.6131e-03, 4.2076e-03, 7.2441e-03, 3.4180e-03,\n",
      "        2.7046e-03, 2.1698e-02, 7.8354e-03, 2.8706e-03, 4.4899e-03, 4.1695e-03,\n",
      "        3.5515e-03, 2.0027e-03, 1.6861e-03, 2.2259e-03, 2.2812e-03, 1.1606e-03,\n",
      "        6.8893e-03, 3.9043e-03, 1.4963e-03, 3.3417e-03, 4.9973e-03, 3.2558e-03,\n",
      "        2.4471e-03, 6.1417e-03, 7.9193e-03, 2.0752e-03, 3.0251e-03, 1.7258e-02,\n",
      "        6.3248e-03, 1.5230e-03, 6.4354e-03, 6.4697e-03, 3.9368e-03, 7.5607e-03,\n",
      "        2.0599e-03, 2.6264e-03, 2.4128e-03, 2.3251e-03, 1.0967e-03, 2.9068e-03,\n",
      "        1.3227e-03, 6.5374e-04, 2.0504e-03, 1.2217e-03, 2.8610e-03, 9.5215e-03,\n",
      "        3.7327e-03, 1.8320e-03, 1.6375e-03, 2.5806e-03, 2.7523e-03, 1.0614e-03,\n",
      "        1.1883e-03, 2.0161e-03, 3.0918e-03, 2.1267e-03, 2.6512e-03, 1.3227e-03,\n",
      "        3.7766e-03, 3.0380e-02, 9.7427e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [82] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [82] : torch.Size([1, 32, 1, 154])\n",
      "Last layer attentions for generated token [82] : tensor([0.1403, 0.1403, 0.0002, 0.0004, 0.0002, 0.0042, 0.0003, 0.0003, 0.0004,\n",
      "        0.0006, 0.0022, 0.0004, 0.0012, 0.0016, 0.0172, 0.0170, 0.0005, 0.0007,\n",
      "        0.0003, 0.0006, 0.0001, 0.0002, 0.0053, 0.0010, 0.0052, 0.0216, 0.0031,\n",
      "        0.0007, 0.0005, 0.0003, 0.0003, 0.0010, 0.0017, 0.0012, 0.0005, 0.0006,\n",
      "        0.0003, 0.0005, 0.0003, 0.0002, 0.0021, 0.0013, 0.0010, 0.0011, 0.0011,\n",
      "        0.0006, 0.0003, 0.0001, 0.0004, 0.0016, 0.0013, 0.0004, 0.0024, 0.0044,\n",
      "        0.0014, 0.0122, 0.0018, 0.0023, 0.0037, 0.0015, 0.0017, 0.0011, 0.0026,\n",
      "        0.0013, 0.0013, 0.0004, 0.0029, 0.0032, 0.0015, 0.0133, 0.0112, 0.0358,\n",
      "        0.0137, 0.0111, 0.0061, 0.0139, 0.0009, 0.0040, 0.0038, 0.0031, 0.0056,\n",
      "        0.0050, 0.0246, 0.0156, 0.0133, 0.0086, 0.0029, 0.0072, 0.0064, 0.0072,\n",
      "        0.0025, 0.0036, 0.0052, 0.0072, 0.0053, 0.0052, 0.0079, 0.0149, 0.0058,\n",
      "        0.0059, 0.0024, 0.0079, 0.0073, 0.0048, 0.0046, 0.0032, 0.0068, 0.0027,\n",
      "        0.0089, 0.0026, 0.0010, 0.0025, 0.0055, 0.0022, 0.0016, 0.0047, 0.0039,\n",
      "        0.0009, 0.0044, 0.0147, 0.0042, 0.0008, 0.0080, 0.0066, 0.0029, 0.0082,\n",
      "        0.0012, 0.0028, 0.0028, 0.0019, 0.0007, 0.0019, 0.0013, 0.0002, 0.0015,\n",
      "        0.0022, 0.0061, 0.0171, 0.0065, 0.0055, 0.0097, 0.0027, 0.0069, 0.0022,\n",
      "        0.0011, 0.0014, 0.0057, 0.0014, 0.0019, 0.0009, 0.0130, 0.0414, 0.0079,\n",
      "        0.0218], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [83] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [83] : torch.Size([1, 32, 1, 155])\n",
      "Last layer attentions for generated token [83] : tensor([1.8433e-01, 1.8433e-01, 1.0031e-04, 2.3198e-04, 6.3539e-05, 1.9855e-03,\n",
      "        4.0007e-04, 3.5095e-04, 3.9840e-04, 6.1846e-04, 1.8692e-03, 3.8171e-04,\n",
      "        7.9060e-04, 1.8873e-03, 1.1574e-02, 9.0790e-03, 4.9210e-04, 6.8188e-04,\n",
      "        2.4486e-04, 3.3474e-04, 1.2684e-04, 3.2330e-04, 8.6975e-03, 1.4315e-03,\n",
      "        4.9171e-03, 1.1909e-02, 2.3251e-03, 7.4148e-04, 7.2432e-04, 3.4618e-04,\n",
      "        4.5586e-04, 1.0538e-03, 1.1549e-03, 7.5579e-04, 4.3678e-04, 7.6485e-04,\n",
      "        3.8528e-04, 6.2656e-04, 5.8651e-04, 4.7135e-04, 1.8635e-03, 1.5850e-03,\n",
      "        1.0939e-03, 1.0681e-03, 7.5912e-04, 7.7248e-04, 2.2125e-04, 2.4581e-04,\n",
      "        4.3321e-04, 1.2369e-03, 1.3876e-03, 1.1456e-04, 3.1185e-03, 2.3174e-03,\n",
      "        2.1458e-03, 6.9199e-03, 1.0471e-03, 3.1528e-03, 3.1490e-03, 1.3371e-03,\n",
      "        2.4166e-03, 1.4019e-03, 2.7523e-03, 1.8139e-03, 2.0847e-03, 5.2357e-04,\n",
      "        4.3068e-03, 1.6623e-03, 2.0466e-03, 1.0193e-02, 5.7526e-03, 2.2858e-02,\n",
      "        7.0877e-03, 4.5471e-03, 2.4529e-03, 9.2392e-03, 1.1263e-03, 4.9362e-03,\n",
      "        3.3417e-03, 5.7030e-03, 5.1727e-03, 4.4708e-03, 2.2446e-02, 9.1858e-03,\n",
      "        5.1537e-03, 5.7220e-03, 4.1504e-03, 6.7863e-03, 4.7913e-03, 4.4861e-03,\n",
      "        1.7891e-03, 4.6234e-03, 5.2910e-03, 4.3297e-03, 6.8359e-03, 5.1727e-03,\n",
      "        4.7684e-03, 1.5930e-02, 4.7722e-03, 3.2845e-03, 3.0766e-03, 4.8447e-03,\n",
      "        5.1880e-03, 2.9736e-03, 3.5934e-03, 3.2120e-03, 4.2839e-03, 2.3346e-03,\n",
      "        7.7324e-03, 3.7689e-03, 2.2316e-03, 2.8362e-03, 6.2447e-03, 3.2463e-03,\n",
      "        2.9297e-03, 5.1270e-03, 6.0997e-03, 1.9102e-03, 4.2305e-03, 2.0462e-02,\n",
      "        4.5471e-03, 1.1892e-03, 8.4152e-03, 6.5041e-03, 3.4618e-03, 8.2626e-03,\n",
      "        1.8635e-03, 2.8057e-03, 2.4471e-03, 2.7275e-03, 7.7868e-04, 1.9283e-03,\n",
      "        9.1028e-04, 2.7752e-04, 1.8969e-03, 2.9125e-03, 5.9357e-03, 1.7838e-02,\n",
      "        7.7820e-03, 6.7482e-03, 5.5962e-03, 5.4321e-03, 7.4158e-03, 3.0308e-03,\n",
      "        2.2392e-03, 3.1853e-03, 8.5220e-03, 2.7447e-03, 3.1300e-03, 1.4830e-03,\n",
      "        1.2108e-02, 3.6652e-02, 5.2872e-03, 1.5686e-02, 4.2801e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [84] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [84] : torch.Size([1, 32, 1, 156])\n",
      "Last layer attentions for generated token [84] : tensor([1.1578e-01, 1.1578e-01, 1.0389e-04, 2.9778e-04, 6.9797e-05, 1.5783e-03,\n",
      "        3.1638e-04, 3.2830e-04, 4.4966e-04, 5.7745e-04, 2.2373e-03, 5.8317e-04,\n",
      "        3.7289e-04, 6.6853e-04, 1.3016e-02, 7.8430e-03, 4.1509e-04, 5.8317e-04,\n",
      "        2.1327e-04, 4.1175e-04, 1.5748e-04, 4.5943e-04, 3.8986e-03, 1.0500e-03,\n",
      "        4.3983e-03, 8.7204e-03, 1.1005e-03, 6.6042e-04, 3.8528e-04, 6.0987e-04,\n",
      "        6.9237e-04, 9.4652e-04, 6.8712e-04, 9.1028e-04, 5.7268e-04, 6.3276e-04,\n",
      "        2.1160e-04, 5.8317e-04, 2.5320e-04, 1.8489e-04, 2.2659e-03, 6.2895e-04,\n",
      "        3.4213e-04, 7.0858e-04, 7.9203e-04, 2.4986e-04, 4.4608e-04, 2.3377e-04,\n",
      "        4.0388e-04, 2.1019e-03, 6.9523e-04, 1.4424e-04, 1.2951e-03, 1.8349e-03,\n",
      "        9.2268e-04, 5.3558e-03, 7.4720e-04, 2.2392e-03, 2.5597e-03, 1.4124e-03,\n",
      "        1.0052e-03, 1.6823e-03, 2.7847e-03, 1.3294e-03, 1.3647e-03, 8.2684e-04,\n",
      "        9.0837e-04, 1.5907e-03, 8.5640e-04, 1.2161e-02, 5.5351e-03, 2.5375e-02,\n",
      "        9.1019e-03, 7.3471e-03, 3.0670e-03, 2.2858e-02, 1.5583e-03, 4.2229e-03,\n",
      "        9.7733e-03, 5.4512e-03, 7.4539e-03, 6.9885e-03, 1.4221e-02, 4.4632e-03,\n",
      "        7.3929e-03, 2.4048e-02, 4.2763e-03, 3.4103e-03, 6.5346e-03, 4.7684e-03,\n",
      "        1.9436e-03, 6.5804e-03, 4.4212e-03, 8.5449e-03, 7.5951e-03, 7.4196e-03,\n",
      "        1.2695e-02, 1.3268e-02, 4.9515e-03, 9.4452e-03, 1.9646e-03, 1.8854e-03,\n",
      "        6.5460e-03, 6.0043e-03, 8.1253e-03, 5.6229e-03, 6.6261e-03, 2.7199e-03,\n",
      "        1.1772e-02, 2.3155e-03, 7.8154e-04, 6.3934e-03, 3.4599e-03, 3.9673e-03,\n",
      "        7.1716e-04, 6.3095e-03, 4.5853e-03, 1.1215e-03, 8.1329e-03, 1.0216e-02,\n",
      "        3.0575e-03, 7.9679e-04, 9.1934e-03, 6.8893e-03, 6.9313e-03, 1.2703e-02,\n",
      "        2.3842e-03, 4.0703e-03, 5.7106e-03, 3.7842e-03, 2.3670e-03, 2.2907e-03,\n",
      "        3.8910e-03, 2.1327e-04, 2.1324e-03, 2.9774e-03, 1.1253e-02, 1.8845e-02,\n",
      "        7.0114e-03, 1.0803e-02, 1.8738e-02, 4.3755e-03, 1.4351e-02, 5.4054e-03,\n",
      "        2.9831e-03, 2.8648e-03, 1.5244e-02, 3.3951e-03, 3.2291e-03, 2.7790e-03,\n",
      "        3.0426e-02, 4.4586e-02, 1.0406e-02, 2.1317e-02, 5.4550e-03, 4.5624e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [85] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [85] : torch.Size([1, 32, 1, 157])\n",
      "Last layer attentions for generated token [85] : tensor([1.4832e-01, 1.4795e-01, 3.9220e-05, 1.1796e-04, 3.3498e-05, 1.5097e-03,\n",
      "        2.0254e-04, 1.7703e-04, 3.2759e-04, 3.4404e-04, 2.0542e-03, 4.2224e-04,\n",
      "        3.1137e-04, 1.3828e-03, 1.0597e-02, 8.9798e-03, 2.3043e-04, 5.5075e-04,\n",
      "        1.7297e-04, 2.5654e-04, 1.4567e-04, 3.2115e-04, 3.9787e-03, 1.2360e-03,\n",
      "        6.1302e-03, 1.2146e-02, 8.8167e-04, 5.9414e-04, 4.8876e-04, 4.7660e-04,\n",
      "        4.8304e-04, 1.2875e-03, 5.6696e-04, 4.1485e-04, 3.5214e-04, 1.0090e-03,\n",
      "        5.5504e-04, 5.6362e-04, 4.0603e-04, 3.4523e-04, 3.1567e-03, 1.9016e-03,\n",
      "        9.5129e-04, 7.9203e-04, 5.7697e-04, 5.5265e-04, 4.2057e-04, 4.3488e-04,\n",
      "        5.8842e-04, 2.5291e-03, 1.8702e-03, 7.7963e-05, 3.0708e-03, 1.8253e-03,\n",
      "        2.1954e-03, 3.9368e-03, 3.6407e-04, 1.3895e-03, 2.0885e-03, 1.0252e-03,\n",
      "        1.7710e-03, 1.6088e-03, 1.8196e-03, 1.4133e-03, 1.0386e-03, 7.8726e-04,\n",
      "        2.4471e-03, 1.4210e-03, 2.1439e-03, 1.1063e-02, 6.9237e-03, 2.1667e-02,\n",
      "        7.7477e-03, 7.3128e-03, 2.8858e-03, 1.1337e-02, 6.8665e-04, 4.2953e-03,\n",
      "        3.7384e-03, 6.6757e-03, 8.1024e-03, 6.3744e-03, 2.0248e-02, 9.6817e-03,\n",
      "        7.3662e-03, 9.0561e-03, 5.0774e-03, 6.6338e-03, 5.9509e-03, 3.8071e-03,\n",
      "        1.4734e-03, 3.5763e-03, 4.5738e-03, 7.4272e-03, 5.6381e-03, 4.3564e-03,\n",
      "        5.4932e-03, 1.3313e-02, 3.6430e-03, 3.0231e-03, 1.8740e-03, 5.0430e-03,\n",
      "        6.0768e-03, 3.8128e-03, 4.0855e-03, 3.1013e-03, 3.3531e-03, 1.4791e-03,\n",
      "        7.8888e-03, 2.1229e-03, 1.1930e-03, 3.0842e-03, 3.2291e-03, 2.0409e-03,\n",
      "        1.3666e-03, 6.1417e-03, 5.3482e-03, 9.2745e-04, 5.5313e-03, 1.5732e-02,\n",
      "        3.6907e-03, 1.1168e-03, 8.3923e-03, 8.0338e-03, 6.0501e-03, 8.3389e-03,\n",
      "        2.1439e-03, 4.3182e-03, 3.5744e-03, 2.6245e-03, 2.3365e-03, 2.2583e-03,\n",
      "        1.8091e-03, 2.0695e-04, 1.9913e-03, 3.0060e-03, 1.1940e-02, 1.6647e-02,\n",
      "        6.3477e-03, 6.0349e-03, 9.9487e-03, 5.9509e-03, 1.0376e-02, 3.7231e-03,\n",
      "        2.1591e-03, 2.8458e-03, 1.1147e-02, 4.8027e-03, 3.7918e-03, 2.7714e-03,\n",
      "        1.6205e-02, 5.9723e-02, 1.3016e-02, 1.9089e-02, 6.6872e-03, 7.8888e-03,\n",
      "        1.5839e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [86] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [86] : torch.Size([1, 32, 1, 158])\n",
      "Last layer attentions for generated token [86] : tensor([1.6357e-01, 1.6296e-01, 6.0737e-05, 1.3852e-04, 3.7134e-05, 1.8282e-03,\n",
      "        3.4070e-04, 4.0317e-04, 5.3930e-04, 3.0422e-04, 1.3409e-03, 3.8838e-04,\n",
      "        1.3268e-04, 3.9458e-04, 6.6757e-03, 5.9357e-03, 2.1744e-04, 4.8423e-04,\n",
      "        1.0830e-04, 1.3852e-04, 1.1438e-04, 3.3998e-04, 2.8152e-03, 5.4359e-04,\n",
      "        4.7417e-03, 1.1848e-02, 4.5586e-04, 3.6621e-04, 3.9673e-04, 9.2793e-04,\n",
      "        6.5279e-04, 1.6823e-03, 3.9363e-04, 4.4441e-04, 3.9983e-04, 3.5357e-04,\n",
      "        2.5082e-04, 7.0620e-04, 3.5214e-04, 2.1195e-04, 2.8362e-03, 1.5182e-03,\n",
      "        3.0017e-04, 6.0511e-04, 7.4291e-04, 3.4070e-04, 8.8024e-04, 3.6907e-04,\n",
      "        5.6505e-04, 3.3245e-03, 2.2488e-03, 1.7476e-04, 3.2806e-03, 3.2654e-03,\n",
      "        1.7614e-03, 4.2648e-03, 3.6621e-04, 1.0414e-03, 2.4338e-03, 8.7166e-04,\n",
      "        8.6355e-04, 1.6479e-03, 2.1629e-03, 8.9073e-04, 3.1567e-04, 8.5020e-04,\n",
      "        1.4820e-03, 2.2793e-03, 2.6302e-03, 1.9424e-02, 1.0216e-02, 1.9455e-02,\n",
      "        5.8746e-03, 4.1580e-03, 1.5230e-03, 1.3687e-02, 4.8923e-04, 4.7836e-03,\n",
      "        4.9515e-03, 4.3144e-03, 1.1299e-02, 1.4908e-02, 1.3901e-02, 4.6005e-03,\n",
      "        4.6883e-03, 1.0490e-02, 2.6054e-03, 2.6226e-03, 5.3368e-03, 2.7981e-03,\n",
      "        6.9380e-04, 3.2272e-03, 2.8915e-03, 8.1406e-03, 4.5052e-03, 4.5967e-03,\n",
      "        8.4381e-03, 1.0666e-02, 2.1477e-03, 5.2032e-03, 1.5373e-03, 2.5692e-03,\n",
      "        4.7493e-03, 4.5776e-03, 6.1951e-03, 3.5095e-03, 2.8362e-03, 1.6031e-03,\n",
      "        7.8812e-03, 1.3752e-03, 5.8746e-04, 3.2024e-03, 2.2354e-03, 3.0613e-03,\n",
      "        6.9237e-04, 5.5656e-03, 3.0231e-03, 2.8968e-04, 4.9362e-03, 8.7051e-03,\n",
      "        3.3855e-03, 9.0313e-04, 7.9803e-03, 6.7406e-03, 1.2589e-02, 1.0696e-02,\n",
      "        3.4714e-03, 4.5052e-03, 2.7256e-03, 2.2888e-03, 2.2469e-03, 1.9007e-03,\n",
      "        2.0161e-03, 1.5759e-04, 1.7977e-03, 1.8969e-03, 8.3008e-03, 1.3062e-02,\n",
      "        7.0000e-03, 9.9869e-03, 1.2756e-02, 2.9869e-03, 9.5749e-03, 3.3512e-03,\n",
      "        1.5745e-03, 2.8839e-03, 1.3306e-02, 3.9825e-03, 2.1992e-03, 4.6501e-03,\n",
      "        2.2339e-02, 4.3243e-02, 1.2367e-02, 1.4053e-02, 5.7297e-03, 4.5700e-03,\n",
      "        2.7466e-02, 9.2545e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [87] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [87] : torch.Size([1, 32, 1, 159])\n",
      "Last layer attentions for generated token [87] : tensor([2.1216e-01, 2.1167e-01, 1.0622e-04, 1.7715e-04, 6.0499e-05, 1.6289e-03,\n",
      "        2.9659e-04, 3.1710e-04, 3.6716e-04, 2.5678e-04, 1.1063e-03, 2.3568e-04,\n",
      "        2.6393e-04, 4.4370e-04, 6.6376e-03, 5.0125e-03, 1.8346e-04, 4.2343e-04,\n",
      "        1.4520e-04, 1.8930e-04, 7.0214e-05, 3.9840e-04, 4.6463e-03, 9.8610e-04,\n",
      "        3.3951e-03, 9.3307e-03, 5.6076e-04, 2.7752e-04, 5.2500e-04, 3.8838e-04,\n",
      "        4.0388e-04, 1.0710e-03, 4.8447e-04, 2.7275e-04, 1.9765e-04, 2.4700e-04,\n",
      "        1.7369e-04, 4.5776e-04, 2.9492e-04, 2.5177e-04, 2.2736e-03, 1.6136e-03,\n",
      "        3.7217e-04, 6.3562e-04, 5.2071e-04, 4.3178e-04, 4.0722e-04, 2.7227e-04,\n",
      "        6.0511e-04, 1.6823e-03, 1.7738e-03, 1.4234e-04, 2.2240e-03, 3.1490e-03,\n",
      "        1.8530e-03, 4.3869e-03, 2.6035e-04, 1.3609e-03, 1.6642e-03, 6.1941e-04,\n",
      "        8.7738e-04, 1.0643e-03, 1.7395e-03, 6.9809e-04, 5.1069e-04, 7.8344e-04,\n",
      "        1.6193e-03, 1.1873e-03, 1.3638e-03, 1.6052e-02, 9.0866e-03, 1.7792e-02,\n",
      "        3.8242e-03, 3.4657e-03, 1.0300e-03, 4.3144e-03, 2.3699e-04, 2.2793e-03,\n",
      "        3.3035e-03, 3.3207e-03, 8.7357e-03, 3.9825e-03, 1.3954e-02, 2.7771e-03,\n",
      "        2.9392e-03, 5.4245e-03, 2.2545e-03, 1.6327e-03, 4.3716e-03, 2.2259e-03,\n",
      "        5.5647e-04, 2.7905e-03, 3.3436e-03, 6.9313e-03, 5.1384e-03, 3.3722e-03,\n",
      "        5.4245e-03, 1.0941e-02, 1.3638e-03, 3.6182e-03, 1.5154e-03, 1.5097e-03,\n",
      "        2.5082e-03, 1.8101e-03, 2.8706e-03, 2.4204e-03, 2.0027e-03, 1.2913e-03,\n",
      "        5.7030e-03, 1.3723e-03, 8.2541e-04, 3.0174e-03, 2.5864e-03, 2.6913e-03,\n",
      "        8.9598e-04, 4.9896e-03, 3.7003e-03, 6.1941e-04, 6.1111e-03, 9.2773e-03,\n",
      "        2.3975e-03, 6.9237e-04, 5.4588e-03, 4.0398e-03, 6.9008e-03, 7.7438e-03,\n",
      "        2.3174e-03, 4.4823e-03, 3.5210e-03, 1.8644e-03, 1.6994e-03, 1.1616e-03,\n",
      "        1.7176e-03, 1.4806e-04, 1.4048e-03, 1.5726e-03, 6.5384e-03, 1.2947e-02,\n",
      "        5.3177e-03, 5.2834e-03, 1.0468e-02, 4.6692e-03, 7.4348e-03, 2.7390e-03,\n",
      "        1.7681e-03, 2.9850e-03, 1.0361e-02, 4.9248e-03, 2.7847e-03, 3.9062e-03,\n",
      "        1.6678e-02, 5.4535e-02, 1.1726e-02, 1.7548e-02, 6.8245e-03, 9.9030e-03,\n",
      "        1.7624e-02, 1.3512e-02, 5.9700e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [88] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [88] : torch.Size([1, 32, 1, 160])\n",
      "Last layer attentions for generated token [88] : tensor([1.4246e-01, 1.4221e-01, 8.6725e-05, 1.9622e-04, 6.0558e-05, 1.0853e-03,\n",
      "        1.9383e-04, 1.6975e-04, 2.0719e-04, 2.9278e-04, 1.1873e-03, 2.0361e-04,\n",
      "        4.5347e-04, 1.0395e-03, 1.0193e-02, 7.0763e-03, 1.6141e-04, 6.9427e-04,\n",
      "        2.1291e-04, 3.7599e-04, 1.0663e-04, 3.2020e-04, 5.3482e-03, 1.7986e-03,\n",
      "        4.1351e-03, 8.0032e-03, 5.4598e-04, 4.0650e-04, 4.0555e-04, 2.6250e-04,\n",
      "        3.6788e-04, 7.3004e-04, 6.2227e-04, 3.4022e-04, 2.1458e-04, 3.3903e-04,\n",
      "        2.2447e-04, 3.9935e-04, 2.1839e-04, 2.5845e-04, 2.5711e-03, 1.2445e-03,\n",
      "        5.1260e-04, 8.0347e-04, 6.0320e-04, 6.7139e-04, 2.1005e-04, 2.1708e-04,\n",
      "        6.7425e-04, 1.5736e-03, 1.1263e-03, 9.8646e-05, 1.5230e-03, 2.0676e-03,\n",
      "        1.6041e-03, 5.2986e-03, 6.1131e-04, 1.5383e-03, 2.2411e-03, 1.1244e-03,\n",
      "        2.2106e-03, 1.5516e-03, 2.5043e-03, 1.5688e-03, 1.3819e-03, 7.9441e-04,\n",
      "        2.0065e-03, 1.1177e-03, 1.2226e-03, 9.1782e-03, 5.0621e-03, 1.4732e-02,\n",
      "        7.9041e-03, 5.5466e-03, 2.3880e-03, 6.6605e-03, 3.8266e-04, 3.2406e-03,\n",
      "        3.1681e-03, 3.0098e-03, 9.7809e-03, 3.8605e-03, 1.3756e-02, 5.4474e-03,\n",
      "        4.2381e-03, 5.2376e-03, 2.3651e-03, 2.9888e-03, 5.2414e-03, 3.3474e-03,\n",
      "        1.4982e-03, 4.8141e-03, 3.5362e-03, 6.6338e-03, 7.0801e-03, 4.8676e-03,\n",
      "        6.5842e-03, 1.0849e-02, 2.8400e-03, 3.3207e-03, 1.6899e-03, 3.2310e-03,\n",
      "        2.8133e-03, 2.0676e-03, 2.6798e-03, 1.9426e-03, 2.8820e-03, 1.6060e-03,\n",
      "        8.0109e-03, 1.6899e-03, 1.3733e-03, 4.1122e-03, 3.3398e-03, 2.4986e-03,\n",
      "        1.9274e-03, 5.9128e-03, 6.5269e-03, 1.3819e-03, 7.1449e-03, 1.2726e-02,\n",
      "        5.4893e-03, 8.9645e-04, 1.0628e-02, 6.7253e-03, 7.3204e-03, 9.8419e-03,\n",
      "        3.5915e-03, 5.5122e-03, 6.6643e-03, 3.1872e-03, 1.9684e-03, 2.0218e-03,\n",
      "        3.1147e-03, 2.6870e-04, 1.6556e-03, 2.8915e-03, 9.8267e-03, 1.7242e-02,\n",
      "        7.9269e-03, 6.5346e-03, 1.3725e-02, 4.4441e-03, 9.8724e-03, 4.1428e-03,\n",
      "        2.7027e-03, 2.8248e-03, 1.4641e-02, 4.3983e-03, 4.0550e-03, 3.6583e-03,\n",
      "        3.1525e-02, 5.4657e-02, 1.3870e-02, 2.3468e-02, 8.2779e-03, 8.1787e-03,\n",
      "        2.8381e-02, 1.7502e-02, 1.1543e-02, 5.4474e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [89] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [89] : torch.Size([1, 32, 1, 161])\n",
      "Last layer attentions for generated token [89] : tensor([2.0654e-01, 2.0654e-01, 8.9884e-05, 2.1386e-04, 7.2777e-05, 8.5068e-04,\n",
      "        2.4772e-04, 2.4629e-04, 3.5262e-04, 2.4867e-04, 9.9850e-04, 3.9649e-04,\n",
      "        2.6941e-04, 7.3338e-04, 8.2550e-03, 4.1733e-03, 2.6250e-04, 5.0497e-04,\n",
      "        1.9622e-04, 2.2590e-04, 1.0628e-04, 3.5000e-04, 4.2381e-03, 1.2751e-03,\n",
      "        5.1575e-03, 6.3782e-03, 7.8249e-04, 2.5940e-04, 3.9482e-04, 3.6740e-04,\n",
      "        6.2990e-04, 1.3943e-03, 6.0558e-04, 3.2306e-04, 3.2353e-04, 4.5896e-04,\n",
      "        2.1219e-04, 3.4523e-04, 2.5940e-04, 2.7728e-04, 3.2978e-03, 1.6088e-03,\n",
      "        6.6662e-04, 6.4611e-04, 5.5027e-04, 4.1389e-04, 3.1805e-04, 2.4092e-04,\n",
      "        3.6740e-04, 2.0523e-03, 1.7443e-03, 1.6594e-04, 3.1853e-03, 3.6430e-03,\n",
      "        2.0142e-03, 2.9469e-03, 4.5013e-04, 1.1072e-03, 2.9240e-03, 1.2426e-03,\n",
      "        1.8349e-03, 1.8768e-03, 2.7390e-03, 1.7338e-03, 9.7752e-04, 9.3651e-04,\n",
      "        2.1477e-03, 2.1667e-03, 3.3169e-03, 2.4704e-02, 7.3318e-03, 1.5686e-02,\n",
      "        6.0844e-03, 4.3411e-03, 1.4706e-03, 4.4594e-03, 2.3818e-04, 8.3923e-04,\n",
      "        1.1654e-03, 1.7176e-03, 2.5005e-03, 4.8103e-03, 1.2657e-02, 4.4060e-03,\n",
      "        2.3708e-03, 3.2082e-03, 2.3193e-03, 3.4122e-03, 4.0855e-03, 2.2831e-03,\n",
      "        5.6553e-04, 2.7275e-03, 3.7918e-03, 6.2485e-03, 5.9052e-03, 3.5019e-03,\n",
      "        5.7564e-03, 1.1688e-02, 2.9030e-03, 3.2234e-03, 1.9217e-03, 3.2749e-03,\n",
      "        3.2368e-03, 2.2392e-03, 2.7256e-03, 2.4529e-03, 3.7365e-03, 1.6499e-03,\n",
      "        7.5150e-03, 2.9964e-03, 1.8034e-03, 3.6964e-03, 3.5343e-03, 2.3098e-03,\n",
      "        1.6470e-03, 9.4604e-03, 5.0735e-03, 1.0548e-03, 6.8359e-03, 1.0475e-02,\n",
      "        5.3558e-03, 1.2798e-03, 9.6893e-03, 4.8790e-03, 4.6349e-03, 6.9427e-03,\n",
      "        1.9121e-03, 3.8319e-03, 2.5749e-03, 2.5558e-03, 2.0580e-03, 2.2087e-03,\n",
      "        1.5135e-03, 2.1636e-04, 1.2674e-03, 2.3041e-03, 8.2474e-03, 1.0635e-02,\n",
      "        6.6795e-03, 4.2992e-03, 8.8196e-03, 4.2839e-03, 6.7711e-03, 3.0136e-03,\n",
      "        1.5135e-03, 1.7242e-03, 8.0109e-03, 2.8114e-03, 2.8152e-03, 2.1458e-03,\n",
      "        1.6830e-02, 3.9581e-02, 8.4381e-03, 1.5259e-02, 8.9493e-03, 7.8354e-03,\n",
      "        1.5419e-02, 1.1154e-02, 6.2561e-03, 3.4485e-03, 4.8332e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [90] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [90] : torch.Size([1, 32, 1, 162])\n",
      "Last layer attentions for generated token [90] : tensor([2.4548e-01, 2.4548e-01, 1.4591e-04, 2.0099e-04, 8.1539e-05, 1.4114e-03,\n",
      "        1.8954e-04, 3.9053e-04, 4.1819e-04, 2.2423e-04, 6.5279e-04, 1.9789e-04,\n",
      "        1.2875e-04, 4.1151e-04, 7.1754e-03, 5.5084e-03, 2.5368e-04, 4.9543e-04,\n",
      "        1.5473e-04, 1.0389e-04, 7.6890e-05, 3.6836e-04, 4.6387e-03, 1.0700e-03,\n",
      "        3.4103e-03, 1.0773e-02, 8.4782e-04, 3.4118e-04, 5.3358e-04, 8.1682e-04,\n",
      "        4.8399e-04, 1.8625e-03, 5.9414e-04, 4.6372e-04, 5.4407e-04, 3.3784e-04,\n",
      "        1.6928e-04, 4.6635e-04, 2.8014e-04, 2.6846e-04, 2.9964e-03, 1.5230e-03,\n",
      "        3.3021e-04, 6.6042e-04, 7.3242e-04, 3.9268e-04, 6.4898e-04, 2.8801e-04,\n",
      "        4.3130e-04, 2.1114e-03, 2.5482e-03, 2.2423e-04, 4.1237e-03, 8.0414e-03,\n",
      "        1.5812e-03, 5.0278e-03, 3.8600e-04, 1.1635e-03, 3.1281e-03, 1.1148e-03,\n",
      "        1.5869e-03, 1.7796e-03, 2.9831e-03, 1.3981e-03, 4.1986e-04, 1.1864e-03,\n",
      "        2.1801e-03, 1.9350e-03, 3.4771e-03, 2.0370e-02, 1.1101e-02, 1.7761e-02,\n",
      "        4.8828e-03, 2.8000e-03, 8.1396e-04, 3.3607e-03, 1.2672e-04, 1.2026e-03,\n",
      "        1.3762e-03, 1.3313e-03, 2.7447e-03, 5.4131e-03, 1.2451e-02, 2.8267e-03,\n",
      "        2.4109e-03, 3.3531e-03, 1.4114e-03, 1.4172e-03, 2.3518e-03, 1.5354e-03,\n",
      "        2.4962e-04, 1.7624e-03, 2.2392e-03, 6.1340e-03, 3.8567e-03, 4.1924e-03,\n",
      "        6.6643e-03, 1.1581e-02, 2.1553e-03, 3.4866e-03, 1.5659e-03, 1.4114e-03,\n",
      "        2.2335e-03, 2.0218e-03, 2.0771e-03, 2.2945e-03, 1.5745e-03, 9.7752e-04,\n",
      "        5.2948e-03, 1.9274e-03, 8.3303e-04, 3.4199e-03, 3.0479e-03, 2.4529e-03,\n",
      "        9.7609e-04, 8.0261e-03, 3.0918e-03, 3.8600e-04, 3.8147e-03, 9.2316e-03,\n",
      "        4.6959e-03, 1.4057e-03, 8.3237e-03, 4.4518e-03, 6.8779e-03, 5.5199e-03,\n",
      "        2.4376e-03, 4.5357e-03, 1.9741e-03, 2.6093e-03, 2.1439e-03, 2.1439e-03,\n",
      "        1.6203e-03, 2.2912e-04, 1.3552e-03, 1.7233e-03, 4.7646e-03, 6.6452e-03,\n",
      "        4.8370e-03, 4.8027e-03, 6.4125e-03, 2.5120e-03, 4.5357e-03, 1.9407e-03,\n",
      "        8.9216e-04, 2.0466e-03, 6.1188e-03, 2.2240e-03, 1.6031e-03, 2.4700e-03,\n",
      "        1.0864e-02, 2.6031e-02, 6.7482e-03, 9.5901e-03, 5.2071e-03, 5.3596e-03,\n",
      "        1.3573e-02, 7.8506e-03, 7.7629e-03, 2.2030e-03, 4.7836e-03, 4.3945e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [91] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [91] : torch.Size([1, 32, 1, 163])\n",
      "Last layer attentions for generated token [91] : tensor([1.6919e-01, 1.6919e-01, 1.6654e-04, 2.5249e-04, 1.2422e-04, 1.3752e-03,\n",
      "        2.3937e-04, 2.4319e-04, 3.9554e-04, 4.4203e-04, 1.2665e-03, 3.7217e-04,\n",
      "        2.6083e-04, 7.5054e-04, 1.1635e-02, 7.0877e-03, 2.0123e-04, 6.9284e-04,\n",
      "        1.2422e-04, 2.1589e-04, 1.3435e-04, 2.4462e-04, 3.8967e-03, 1.2665e-03,\n",
      "        6.3210e-03, 1.2001e-02, 1.2016e-03, 4.7064e-04, 6.9809e-04, 4.9877e-04,\n",
      "        5.4502e-04, 1.3800e-03, 1.1358e-03, 6.8617e-04, 4.0650e-04, 4.0555e-04,\n",
      "        1.8573e-04, 2.7871e-04, 2.6655e-04, 2.9445e-04, 2.0542e-03, 1.0138e-03,\n",
      "        6.8998e-04, 6.6757e-04, 6.1512e-04, 6.6996e-04, 2.2757e-04, 2.4271e-04,\n",
      "        6.0177e-04, 1.6623e-03, 1.5135e-03, 2.7347e-04, 2.6951e-03, 4.1656e-03,\n",
      "        1.6670e-03, 5.2414e-03, 4.3702e-04, 1.9894e-03, 3.1300e-03, 1.2083e-03,\n",
      "        2.3575e-03, 1.8520e-03, 2.2430e-03, 1.1196e-03, 8.1301e-04, 5.7888e-04,\n",
      "        1.8053e-03, 2.8324e-03, 1.5383e-03, 1.6129e-02, 9.6436e-03, 2.4384e-02,\n",
      "        1.0818e-02, 3.9787e-03, 3.0384e-03, 1.0666e-02, 4.0245e-04, 2.4624e-03,\n",
      "        2.4757e-03, 2.5368e-03, 5.7640e-03, 4.7607e-03, 1.0841e-02, 7.2403e-03,\n",
      "        4.7493e-03, 8.1177e-03, 3.5362e-03, 4.1809e-03, 5.3368e-03, 3.5973e-03,\n",
      "        8.7738e-04, 3.9558e-03, 5.3558e-03, 5.6725e-03, 6.8932e-03, 4.5090e-03,\n",
      "        5.3978e-03, 8.9951e-03, 2.7294e-03, 2.2392e-03, 1.6394e-03, 2.7542e-03,\n",
      "        2.7542e-03, 3.0689e-03, 2.9945e-03, 2.0103e-03, 2.6016e-03, 1.2035e-03,\n",
      "        4.9019e-03, 1.5087e-03, 1.1139e-03, 2.5845e-03, 3.7632e-03, 2.0065e-03,\n",
      "        1.4229e-03, 5.4245e-03, 4.0550e-03, 9.0170e-04, 4.2305e-03, 8.2016e-03,\n",
      "        3.9711e-03, 1.1787e-03, 6.8703e-03, 6.9313e-03, 4.9782e-03, 7.4005e-03,\n",
      "        1.3342e-03, 3.0842e-03, 3.0174e-03, 2.7447e-03, 1.7033e-03, 2.5444e-03,\n",
      "        1.5459e-03, 2.1839e-04, 1.3390e-03, 1.7147e-03, 5.5618e-03, 1.1497e-02,\n",
      "        6.0883e-03, 4.6158e-03, 9.2316e-03, 6.8893e-03, 1.3214e-02, 3.2730e-03,\n",
      "        1.3313e-03, 1.9274e-03, 6.6261e-03, 2.4090e-03, 3.4504e-03, 2.1439e-03,\n",
      "        1.3458e-02, 4.8889e-02, 1.0605e-02, 1.9012e-02, 1.0345e-02, 9.0103e-03,\n",
      "        2.2400e-02, 1.2642e-02, 8.7891e-03, 3.7117e-03, 6.5346e-03, 5.5923e-03,\n",
      "        7.3395e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [92] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [92] : torch.Size([1, 32, 1, 164])\n",
      "Last layer attentions for generated token [92] : tensor([1.8872e-01, 1.8909e-01, 9.4473e-05, 1.8358e-04, 8.4043e-05, 8.5545e-04,\n",
      "        3.0375e-04, 2.8205e-04, 3.0017e-04, 2.9516e-04, 9.9659e-04, 7.3576e-04,\n",
      "        2.6703e-04, 4.7421e-04, 7.7934e-03, 3.8834e-03, 1.9312e-04, 5.3835e-04,\n",
      "        1.7309e-04, 2.0552e-04, 1.9014e-04, 3.7074e-04, 4.0474e-03, 1.1158e-03,\n",
      "        4.8256e-03, 6.7291e-03, 8.5020e-04, 2.7609e-04, 4.4990e-04, 5.5981e-04,\n",
      "        8.9455e-04, 1.5097e-03, 1.2302e-03, 8.1635e-04, 5.2166e-04, 3.6645e-04,\n",
      "        1.1533e-04, 3.4285e-04, 2.3067e-04, 2.4319e-04, 1.6813e-03, 9.2125e-04,\n",
      "        3.9625e-04, 5.4693e-04, 7.1049e-04, 3.7360e-04, 4.8351e-04, 3.0446e-04,\n",
      "        4.8351e-04, 2.0123e-03, 1.3456e-03, 1.8787e-04, 2.8248e-03, 3.0098e-03,\n",
      "        1.1921e-03, 3.4637e-03, 3.0923e-04, 1.3409e-03, 2.7428e-03, 1.3065e-03,\n",
      "        1.4725e-03, 1.4324e-03, 2.8858e-03, 1.4582e-03, 9.1410e-04, 7.0906e-04,\n",
      "        1.6298e-03, 1.9388e-03, 1.9016e-03, 1.4435e-02, 6.2485e-03, 1.3557e-02,\n",
      "        5.6725e-03, 3.7193e-03, 2.0027e-03, 9.1553e-03, 2.2006e-04, 1.3781e-03,\n",
      "        2.2430e-03, 2.0103e-03, 3.1204e-03, 5.6648e-03, 9.9030e-03, 3.9215e-03,\n",
      "        3.7117e-03, 4.8447e-03, 2.8648e-03, 3.1776e-03, 5.0392e-03, 3.4637e-03,\n",
      "        9.2316e-04, 4.3564e-03, 5.1689e-03, 6.1226e-03, 7.9727e-03, 4.5319e-03,\n",
      "        7.0915e-03, 1.0361e-02, 2.8992e-03, 2.5654e-03, 1.7796e-03, 2.0790e-03,\n",
      "        3.2692e-03, 3.5324e-03, 3.5248e-03, 3.4847e-03, 2.0237e-03, 1.6699e-03,\n",
      "        5.6458e-03, 2.0294e-03, 9.3603e-04, 2.6417e-03, 5.3711e-03, 3.3245e-03,\n",
      "        1.3199e-03, 7.3967e-03, 6.2943e-03, 9.7656e-04, 4.8599e-03, 8.7662e-03,\n",
      "        3.5496e-03, 1.0061e-03, 8.4305e-03, 8.0185e-03, 8.2550e-03, 1.3550e-02,\n",
      "        2.1648e-03, 4.0398e-03, 3.2310e-03, 3.3989e-03, 1.8005e-03, 3.2959e-03,\n",
      "        1.6251e-03, 2.7823e-04, 2.2259e-03, 2.8820e-03, 6.5727e-03, 1.1276e-02,\n",
      "        1.0139e-02, 6.7863e-03, 1.2085e-02, 7.7095e-03, 1.2802e-02, 4.6921e-03,\n",
      "        1.7099e-03, 1.9255e-03, 9.4223e-03, 3.0231e-03, 2.7390e-03, 2.9259e-03,\n",
      "        1.4191e-02, 3.4424e-02, 5.3711e-03, 1.3084e-02, 6.3133e-03, 5.5733e-03,\n",
      "        1.9348e-02, 9.3384e-03, 6.9389e-03, 3.4561e-03, 6.2523e-03, 5.1498e-03,\n",
      "        4.9171e-03, 1.3649e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [93] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [93] : torch.Size([1, 32, 1, 165])\n",
      "Last layer attentions for generated token [93] : tensor([2.0190e-01, 2.0190e-01, 1.0449e-04, 1.9681e-04, 7.2956e-05, 1.0090e-03,\n",
      "        4.0364e-04, 4.7493e-04, 5.5933e-04, 4.1819e-04, 7.3957e-04, 8.0776e-04,\n",
      "        2.6536e-04, 4.8709e-04, 7.2479e-03, 3.0670e-03, 3.7050e-04, 5.6171e-04,\n",
      "        1.9598e-04, 1.9991e-04, 2.1446e-04, 6.5660e-04, 5.9204e-03, 1.2856e-03,\n",
      "        6.2256e-03, 6.1989e-03, 1.0433e-03, 3.0351e-04, 3.7050e-04, 5.1451e-04,\n",
      "        9.0122e-04, 1.5030e-03, 1.1892e-03, 7.0429e-04, 6.3515e-04, 3.1996e-04,\n",
      "        7.9811e-05, 4.4346e-04, 2.5177e-04, 2.0945e-04, 1.8435e-03, 9.7609e-04,\n",
      "        3.9911e-04, 5.4312e-04, 6.9094e-04, 2.8634e-04, 5.7936e-04, 3.2258e-04,\n",
      "        3.8838e-04, 2.4052e-03, 1.7033e-03, 2.4021e-04, 3.3321e-03, 3.3226e-03,\n",
      "        1.0910e-03, 3.3512e-03, 2.9540e-04, 1.5812e-03, 3.3379e-03, 1.6880e-03,\n",
      "        1.4515e-03, 1.7605e-03, 4.6921e-03, 1.6994e-03, 1.0490e-03, 1.1501e-03,\n",
      "        2.1915e-03, 2.0905e-03, 3.1757e-03, 1.4870e-02, 6.6566e-03, 1.2848e-02,\n",
      "        3.3836e-03, 3.1967e-03, 1.5144e-03, 9.1858e-03, 2.8419e-04, 1.2884e-03,\n",
      "        2.5387e-03, 2.5253e-03, 2.2278e-03, 5.4092e-03, 9.6741e-03, 2.3537e-03,\n",
      "        2.4452e-03, 4.0741e-03, 2.7409e-03, 1.9569e-03, 4.8485e-03, 2.9316e-03,\n",
      "        8.5306e-04, 3.6545e-03, 3.6125e-03, 4.8790e-03, 6.5231e-03, 4.2992e-03,\n",
      "        7.6103e-03, 8.8730e-03, 1.5965e-03, 2.2926e-03, 1.9569e-03, 1.1892e-03,\n",
      "        2.8839e-03, 2.7809e-03, 3.1319e-03, 3.9978e-03, 1.8539e-03, 1.7471e-03,\n",
      "        6.5079e-03, 2.4204e-03, 7.5436e-04, 2.2812e-03, 4.4556e-03, 3.6297e-03,\n",
      "        1.0490e-03, 8.1482e-03, 4.2114e-03, 7.7820e-04, 5.7678e-03, 8.2703e-03,\n",
      "        3.0041e-03, 1.1683e-03, 9.4223e-03, 5.7640e-03, 6.7940e-03, 1.0994e-02,\n",
      "        2.0237e-03, 4.8141e-03, 3.0937e-03, 2.9869e-03, 2.0962e-03, 2.7103e-03,\n",
      "        2.0580e-03, 3.2640e-04, 2.4738e-03, 2.6398e-03, 8.9493e-03, 1.1360e-02,\n",
      "        6.9923e-03, 5.3406e-03, 9.9869e-03, 7.3509e-03, 1.1040e-02, 4.1924e-03,\n",
      "        1.6603e-03, 1.3819e-03, 1.2138e-02, 3.2043e-03, 2.3594e-03, 2.2526e-03,\n",
      "        1.4320e-02, 3.2166e-02, 4.6654e-03, 1.0353e-02, 4.9133e-03, 5.9471e-03,\n",
      "        1.5198e-02, 6.8398e-03, 5.0125e-03, 3.5954e-03, 6.3210e-03, 3.8166e-03,\n",
      "        4.6310e-03, 1.3771e-02, 1.1261e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [94] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [94] : torch.Size([1, 32, 1, 166])\n",
      "Last layer attentions for generated token [94] : tensor([2.5293e-01, 2.5293e-01, 8.7678e-05, 2.3103e-04, 7.4685e-05, 1.1024e-03,\n",
      "        2.0957e-04, 2.4211e-04, 3.8242e-04, 2.3556e-04, 8.7166e-04, 3.1281e-04,\n",
      "        2.2566e-04, 6.4898e-04, 8.0948e-03, 5.2223e-03, 1.7440e-04, 4.1103e-04,\n",
      "        1.8203e-04, 1.4293e-04, 1.3530e-04, 3.5286e-04, 6.3362e-03, 1.4629e-03,\n",
      "        5.7220e-03, 9.3765e-03, 6.2943e-04, 3.0494e-04, 4.0150e-04, 3.3689e-04,\n",
      "        4.7040e-04, 1.2465e-03, 4.2915e-04, 2.4402e-04, 2.5177e-04, 3.5858e-04,\n",
      "        2.4402e-04, 2.8372e-04, 1.9765e-04, 2.1195e-04, 2.2373e-03, 1.6613e-03,\n",
      "        3.5501e-04, 4.9305e-04, 3.7503e-04, 4.2081e-04, 2.1613e-04, 3.0661e-04,\n",
      "        3.8242e-04, 1.6937e-03, 1.9302e-03, 1.2362e-04, 3.7422e-03, 2.7790e-03,\n",
      "        2.4166e-03, 2.5597e-03, 3.3879e-04, 1.2178e-03, 3.9482e-03, 1.0214e-03,\n",
      "        1.4372e-03, 1.0176e-03, 2.0847e-03, 1.4801e-03, 6.8283e-04, 6.6090e-04,\n",
      "        2.7409e-03, 1.1282e-03, 3.8967e-03, 1.0582e-02, 6.6223e-03, 9.9106e-03,\n",
      "        3.4180e-03, 2.1381e-03, 6.3038e-04, 2.1095e-03, 1.7238e-04, 8.2207e-04,\n",
      "        6.0844e-04, 1.2989e-03, 2.0390e-03, 2.1572e-03, 9.1171e-03, 2.8839e-03,\n",
      "        1.7471e-03, 1.4772e-03, 1.3638e-03, 1.5240e-03, 2.3289e-03, 1.9474e-03,\n",
      "        6.2180e-04, 2.4128e-03, 3.3741e-03, 7.1297e-03, 4.4670e-03, 2.1343e-03,\n",
      "        3.6774e-03, 8.0185e-03, 2.0332e-03, 1.8349e-03, 1.5335e-03, 2.0752e-03,\n",
      "        2.3060e-03, 2.0676e-03, 2.2030e-03, 1.7071e-03, 1.7338e-03, 1.0662e-03,\n",
      "        5.3291e-03, 2.0599e-03, 1.2589e-03, 1.7643e-03, 2.5120e-03, 2.1019e-03,\n",
      "        1.4830e-03, 6.5041e-03, 3.6392e-03, 8.5020e-04, 3.0613e-03, 6.2523e-03,\n",
      "        4.1809e-03, 1.7748e-03, 6.9008e-03, 4.6959e-03, 6.2828e-03, 4.8523e-03,\n",
      "        2.5597e-03, 3.8242e-03, 1.9531e-03, 1.7338e-03, 1.5211e-03, 1.5154e-03,\n",
      "        1.2684e-03, 2.0075e-04, 1.4067e-03, 2.6264e-03, 8.2626e-03, 7.9956e-03,\n",
      "        5.7640e-03, 4.0474e-03, 5.3787e-03, 3.0041e-03, 5.1422e-03, 2.3308e-03,\n",
      "        1.0805e-03, 1.6222e-03, 7.2098e-03, 2.2163e-03, 2.8534e-03, 2.2812e-03,\n",
      "        1.0780e-02, 3.3875e-02, 8.5373e-03, 1.0948e-02, 4.5624e-03, 8.0872e-03,\n",
      "        1.0979e-02, 1.2741e-02, 7.0915e-03, 3.0308e-03, 4.9629e-03, 5.9967e-03,\n",
      "        4.1351e-03, 8.7814e-03, 6.4125e-03, 3.5896e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [95] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [95] : torch.Size([1, 32, 1, 167])\n",
      "Last layer attentions for generated token [95] : tensor([2.7930e-01, 2.7930e-01, 1.5116e-04, 2.0576e-04, 7.7784e-05, 1.7128e-03,\n",
      "        2.7061e-04, 3.9196e-04, 2.9778e-04, 2.5225e-04, 6.7234e-04, 1.7607e-04,\n",
      "        2.0027e-04, 3.8600e-04, 5.3711e-03, 4.2610e-03, 1.5652e-04, 3.0708e-04,\n",
      "        1.4257e-04, 9.4950e-05, 7.8738e-05, 4.3321e-04, 6.5079e-03, 1.1940e-03,\n",
      "        3.1319e-03, 1.0780e-02, 6.7759e-04, 2.3413e-04, 4.1485e-04, 4.3559e-04,\n",
      "        4.4346e-04, 1.0757e-03, 7.2384e-04, 3.6907e-04, 4.3154e-04, 2.3878e-04,\n",
      "        1.7881e-04, 5.0163e-04, 2.3317e-04, 2.7704e-04, 1.4372e-03, 8.3160e-04,\n",
      "        1.8740e-04, 3.9911e-04, 3.7861e-04, 3.6192e-04, 2.5511e-04, 2.1231e-04,\n",
      "        3.5000e-04, 1.4620e-03, 1.4315e-03, 1.4818e-04, 2.6455e-03, 5.9395e-03,\n",
      "        1.2054e-03, 4.8065e-03, 3.6836e-04, 1.6155e-03, 4.0817e-03, 1.1635e-03,\n",
      "        1.3371e-03, 8.5115e-04, 3.7746e-03, 1.6127e-03, 6.0511e-04, 8.5640e-04,\n",
      "        1.8253e-03, 1.2341e-03, 2.5826e-03, 1.0910e-02, 6.7406e-03, 1.1322e-02,\n",
      "        3.3417e-03, 1.5535e-03, 6.7759e-04, 2.4586e-03, 1.4424e-04, 1.2751e-03,\n",
      "        1.2684e-03, 1.2264e-03, 2.4357e-03, 3.2825e-03, 8.8043e-03, 2.5234e-03,\n",
      "        1.5268e-03, 1.4534e-03, 8.8882e-04, 1.3008e-03, 1.7500e-03, 1.5688e-03,\n",
      "        3.5691e-04, 1.5659e-03, 2.1362e-03, 4.3144e-03, 4.0741e-03, 4.3907e-03,\n",
      "        6.8283e-03, 7.8278e-03, 1.2579e-03, 2.6016e-03, 1.2627e-03, 9.3317e-04,\n",
      "        1.4172e-03, 1.3742e-03, 1.5478e-03, 1.8072e-03, 1.6184e-03, 7.7677e-04,\n",
      "        5.5161e-03, 1.7653e-03, 8.9025e-04, 1.7471e-03, 2.9144e-03, 3.2005e-03,\n",
      "        1.1940e-03, 6.9199e-03, 4.3869e-03, 7.4816e-04, 3.5591e-03, 6.0425e-03,\n",
      "        5.2719e-03, 1.4734e-03, 6.2599e-03, 4.5166e-03, 9.1476e-03, 5.9929e-03,\n",
      "        2.7866e-03, 3.6793e-03, 2.2449e-03, 1.7366e-03, 1.7166e-03, 1.4887e-03,\n",
      "        1.6670e-03, 2.3878e-04, 1.6537e-03, 2.2449e-03, 4.7073e-03, 6.7711e-03,\n",
      "        4.5090e-03, 3.4065e-03, 3.0518e-03, 1.9341e-03, 2.5311e-03, 1.7262e-03,\n",
      "        7.5579e-04, 1.5116e-03, 5.8556e-03, 1.7195e-03, 1.5631e-03, 2.4643e-03,\n",
      "        1.4832e-02, 1.6220e-02, 6.0310e-03, 5.7373e-03, 2.2469e-03, 4.2610e-03,\n",
      "        7.3204e-03, 8.3847e-03, 5.4779e-03, 1.8272e-03, 5.0125e-03, 5.7259e-03,\n",
      "        3.1719e-03, 9.0714e-03, 4.7722e-03, 5.9776e-03, 1.9722e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [96] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [96] : torch.Size([1, 32, 1, 168])\n",
      "Last layer attentions for generated token [96] : tensor([1.5881e-01, 1.5918e-01, 1.0455e-04, 2.5129e-04, 7.3552e-05, 1.3475e-03,\n",
      "        1.8239e-04, 2.2745e-04, 5.1260e-04, 4.9973e-04, 9.5558e-04, 4.2987e-04,\n",
      "        4.7684e-04, 1.0891e-03, 1.3756e-02, 6.9466e-03, 2.7823e-04, 6.6090e-04,\n",
      "        1.6069e-04, 3.1161e-04, 3.2902e-04, 5.4884e-04, 5.2872e-03, 1.8187e-03,\n",
      "        6.5308e-03, 1.0216e-02, 8.1921e-04, 2.8038e-04, 3.6860e-04, 2.9373e-04,\n",
      "        4.1604e-04, 1.1759e-03, 7.1049e-04, 3.2401e-04, 3.5429e-04, 5.3930e-04,\n",
      "        2.2745e-04, 3.0541e-04, 2.1493e-04, 1.8311e-04, 3.1471e-03, 1.8492e-03,\n",
      "        4.5323e-04, 4.0865e-04, 3.0971e-04, 4.7135e-04, 1.0830e-04, 2.1827e-04,\n",
      "        3.5858e-04, 2.0447e-03, 2.3174e-03, 1.4460e-04, 4.7569e-03, 4.0932e-03,\n",
      "        2.4185e-03, 4.7188e-03, 5.7650e-04, 1.6499e-03, 4.4060e-03, 1.2836e-03,\n",
      "        2.2755e-03, 1.0834e-03, 2.5463e-03, 1.6232e-03, 1.1921e-03, 8.4686e-04,\n",
      "        4.2877e-03, 1.8406e-03, 3.4523e-03, 1.1383e-02, 9.7427e-03, 2.2919e-02,\n",
      "        7.3853e-03, 3.7785e-03, 1.8015e-03, 2.8343e-03, 2.0790e-04, 1.8711e-03,\n",
      "        1.4057e-03, 2.3251e-03, 4.8294e-03, 4.2038e-03, 1.6998e-02, 4.7112e-03,\n",
      "        2.2049e-03, 2.1305e-03, 1.8911e-03, 2.2373e-03, 2.2411e-03, 2.3518e-03,\n",
      "        8.0156e-04, 3.1338e-03, 3.8643e-03, 6.2828e-03, 6.6910e-03, 4.1199e-03,\n",
      "        6.7139e-03, 1.3412e-02, 2.7676e-03, 2.5215e-03, 2.6569e-03, 2.6569e-03,\n",
      "        2.0370e-03, 1.8177e-03, 1.3771e-03, 1.6117e-03, 2.1896e-03, 9.6321e-04,\n",
      "        7.3471e-03, 1.7653e-03, 1.2665e-03, 1.5020e-03, 2.8343e-03, 1.7405e-03,\n",
      "        2.1763e-03, 8.8501e-03, 7.1335e-03, 1.3294e-03, 5.3406e-03, 1.1559e-02,\n",
      "        5.4398e-03, 1.6775e-03, 9.2010e-03, 4.2839e-03, 4.3335e-03, 5.6953e-03,\n",
      "        2.6474e-03, 4.6883e-03, 2.7542e-03, 2.3594e-03, 2.4700e-03, 2.2736e-03,\n",
      "        2.0905e-03, 3.4285e-04, 2.1458e-03, 2.5253e-03, 7.1144e-03, 1.0849e-02,\n",
      "        6.5651e-03, 3.7098e-03, 8.5754e-03, 4.5242e-03, 4.5586e-03, 2.3422e-03,\n",
      "        1.4000e-03, 2.2755e-03, 1.2451e-02, 3.9825e-03, 3.5763e-03, 2.6035e-03,\n",
      "        1.4732e-02, 4.6906e-02, 1.0284e-02, 2.0172e-02, 8.5907e-03, 7.4654e-03,\n",
      "        1.8814e-02, 2.2537e-02, 9.9869e-03, 4.4250e-03, 8.5449e-03, 8.7280e-03,\n",
      "        3.8357e-03, 7.9575e-03, 5.4550e-03, 4.0245e-03, 2.2411e-03, 5.7907e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [97] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [97] : torch.Size([1, 32, 1, 169])\n",
      "Last layer attentions for generated token [97] : tensor([1.4197e-01, 1.4197e-01, 1.0687e-04, 3.0017e-04, 8.7559e-05, 1.7452e-03,\n",
      "        1.5426e-04, 1.5128e-04, 2.9397e-04, 4.5609e-04, 1.0443e-03, 4.3511e-04,\n",
      "        4.0174e-04, 8.7738e-04, 8.8882e-03, 9.4376e-03, 4.0960e-04, 5.6744e-04,\n",
      "        1.6487e-04, 3.0017e-04, 2.0194e-04, 3.3116e-04, 2.5616e-03, 8.6880e-04,\n",
      "        5.5046e-03, 1.1650e-02, 1.5535e-03, 5.6553e-04, 3.6359e-04, 3.5048e-04,\n",
      "        3.2854e-04, 1.0080e-03, 8.1635e-04, 7.1192e-04, 3.5667e-04, 5.9271e-04,\n",
      "        3.2973e-04, 3.6144e-04, 1.7965e-04, 1.6296e-04, 3.2883e-03, 1.4725e-03,\n",
      "        9.8991e-04, 6.6471e-04, 5.4693e-04, 3.3760e-04, 1.9050e-04, 1.5485e-04,\n",
      "        3.0375e-04, 2.5730e-03, 1.7395e-03, 2.4223e-04, 3.4561e-03, 3.6125e-03,\n",
      "        2.5139e-03, 4.9629e-03, 1.0748e-03, 1.7920e-03, 3.0651e-03, 1.2770e-03,\n",
      "        1.7080e-03, 1.3390e-03, 2.0084e-03, 1.6041e-03, 1.1072e-03, 6.3801e-04,\n",
      "        3.2768e-03, 3.0155e-03, 2.2030e-03, 9.1248e-03, 6.7787e-03, 2.1942e-02,\n",
      "        1.4473e-02, 3.9330e-03, 2.8553e-03, 5.9395e-03, 4.5085e-04, 2.6741e-03,\n",
      "        1.6813e-03, 2.8343e-03, 6.4392e-03, 4.5471e-03, 1.5030e-02, 1.5732e-02,\n",
      "        7.3395e-03, 5.8022e-03, 2.4796e-03, 5.5466e-03, 4.4708e-03, 2.6894e-03,\n",
      "        1.0567e-03, 3.3836e-03, 3.5553e-03, 5.8517e-03, 7.1564e-03, 4.2267e-03,\n",
      "        5.3635e-03, 1.1101e-02, 5.4321e-03, 3.2101e-03, 2.2831e-03, 7.9041e-03,\n",
      "        4.2191e-03, 2.7943e-03, 2.5101e-03, 1.6603e-03, 5.2872e-03, 1.6375e-03,\n",
      "        6.3248e-03, 1.2894e-03, 8.5545e-04, 1.9407e-03, 2.0485e-03, 1.4524e-03,\n",
      "        9.4891e-04, 4.6768e-03, 5.2528e-03, 9.6369e-04, 4.7035e-03, 1.0040e-02,\n",
      "        4.0894e-03, 1.0214e-03, 9.4604e-03, 7.1716e-03, 5.1575e-03, 7.0877e-03,\n",
      "        1.9016e-03, 3.6011e-03, 2.7618e-03, 2.3613e-03, 1.8024e-03, 2.4090e-03,\n",
      "        1.6060e-03, 2.4271e-04, 1.4811e-03, 3.1300e-03, 5.8746e-03, 1.3084e-02,\n",
      "        5.3787e-03, 3.5629e-03, 9.1934e-03, 3.1872e-03, 4.9171e-03, 1.3247e-03,\n",
      "        9.4128e-04, 1.3819e-03, 6.4278e-03, 2.1572e-03, 2.8076e-03, 1.2474e-03,\n",
      "        1.3039e-02, 6.3049e-02, 1.5793e-02, 2.7298e-02, 1.0620e-02, 7.2212e-03,\n",
      "        2.2186e-02, 1.5945e-02, 8.2169e-03, 3.9406e-03, 5.1270e-03, 4.9210e-03,\n",
      "        5.0659e-03, 7.2021e-03, 5.9624e-03, 2.6245e-03, 9.4700e-04, 3.7785e-03,\n",
      "        8.8272e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [98] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [98] : torch.Size([1, 32, 1, 170])\n",
      "Last layer attentions for generated token [98] : tensor([1.3477e-01, 1.3477e-01, 1.9491e-04, 3.8671e-04, 1.9717e-04, 2.0885e-03,\n",
      "        3.3355e-04, 3.7122e-04, 5.9319e-04, 5.5838e-04, 1.2836e-03, 2.6226e-04,\n",
      "        2.1958e-04, 8.5974e-04, 7.2975e-03, 1.0078e-02, 6.1798e-04, 5.8842e-04,\n",
      "        1.2171e-04, 2.5558e-04, 8.3625e-05, 3.1090e-04, 4.6883e-03, 1.1482e-03,\n",
      "        4.1428e-03, 1.4496e-02, 2.3041e-03, 8.3971e-04, 4.0865e-04, 6.7759e-04,\n",
      "        9.2411e-04, 1.5030e-03, 6.5422e-04, 8.3351e-04, 4.7493e-04, 3.5977e-04,\n",
      "        3.9434e-04, 4.0460e-04, 1.7917e-04, 1.4007e-04, 2.8057e-03, 1.3056e-03,\n",
      "        1.0090e-03, 6.0749e-04, 5.2643e-04, 2.1696e-04, 3.5501e-04, 2.2209e-04,\n",
      "        2.6798e-04, 1.8730e-03, 1.0490e-03, 3.3140e-04, 2.6531e-03, 5.1384e-03,\n",
      "        2.1534e-03, 4.5547e-03, 6.9904e-04, 1.2903e-03, 2.5291e-03, 1.1616e-03,\n",
      "        8.0633e-04, 2.5291e-03, 1.6079e-03, 9.4795e-04, 7.4720e-04, 9.0837e-04,\n",
      "        1.1911e-03, 2.2964e-03, 1.1568e-03, 1.2589e-02, 9.0637e-03, 1.1726e-02,\n",
      "        4.5624e-03, 4.7798e-03, 2.1248e-03, 8.6746e-03, 5.8746e-04, 2.2278e-03,\n",
      "        2.6798e-03, 2.2221e-03, 3.8319e-03, 4.5624e-03, 1.0628e-02, 9.2010e-03,\n",
      "        8.6365e-03, 1.3710e-02, 2.4548e-03, 5.0278e-03, 7.0381e-03, 2.9526e-03,\n",
      "        1.7166e-03, 3.5152e-03, 3.0079e-03, 8.5602e-03, 4.6310e-03, 4.6768e-03,\n",
      "        8.4763e-03, 1.0300e-02, 3.8662e-03, 4.1122e-03, 1.5497e-03, 4.9210e-03,\n",
      "        7.0724e-03, 3.7117e-03, 4.7760e-03, 3.1109e-03, 4.6997e-03, 1.9321e-03,\n",
      "        7.1449e-03, 1.2465e-03, 4.9782e-04, 3.8357e-03, 1.6880e-03, 1.3056e-03,\n",
      "        3.6407e-04, 3.0727e-03, 2.1133e-03, 3.7408e-04, 5.3596e-03, 7.1678e-03,\n",
      "        2.2202e-03, 1.1063e-03, 7.5531e-03, 4.9591e-03, 3.5152e-03, 5.1460e-03,\n",
      "        1.4887e-03, 4.0321e-03, 3.1090e-03, 1.3676e-03, 1.1196e-03, 1.3380e-03,\n",
      "        1.3952e-03, 1.5450e-04, 1.0052e-03, 1.5297e-03, 8.9951e-03, 1.3786e-02,\n",
      "        4.1580e-03, 5.2414e-03, 1.6708e-02, 5.1117e-03, 1.4420e-02, 2.0351e-03,\n",
      "        1.1892e-03, 1.6155e-03, 9.1858e-03, 3.9864e-03, 2.6321e-03, 1.9035e-03,\n",
      "        2.5772e-02, 5.9875e-02, 1.4923e-02, 2.1606e-02, 6.1035e-03, 5.5084e-03,\n",
      "        2.2842e-02, 6.3248e-03, 1.0620e-02, 6.0196e-03, 4.1466e-03, 2.7981e-03,\n",
      "        9.3613e-03, 7.3166e-03, 7.1411e-03, 1.8873e-03, 5.1165e-04, 3.3550e-03,\n",
      "        1.0193e-02, 1.9821e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [99] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [99] : torch.Size([1, 32, 1, 171])\n",
      "Last layer attentions for generated token [99] : tensor([1.7029e-01, 1.6956e-01, 1.7452e-04, 3.1924e-04, 1.5044e-04, 2.2106e-03,\n",
      "        3.6740e-04, 3.3450e-04, 6.3372e-04, 6.3372e-04, 1.5755e-03, 5.1498e-04,\n",
      "        3.3903e-04, 1.1101e-03, 8.5373e-03, 1.1192e-02, 5.1212e-04, 6.0797e-04,\n",
      "        1.7524e-04, 2.7680e-04, 1.4138e-04, 4.9734e-04, 4.7379e-03, 1.4629e-03,\n",
      "        5.3177e-03, 1.1475e-02, 1.8053e-03, 7.6437e-04, 2.9302e-04, 6.8617e-04,\n",
      "        7.6723e-04, 1.3390e-03, 7.6866e-04, 7.8249e-04, 6.0320e-04, 4.6635e-04,\n",
      "        3.1853e-04, 6.0940e-04, 2.0134e-04, 1.3804e-04, 3.1796e-03, 1.4057e-03,\n",
      "        7.6437e-04, 6.2847e-04, 6.4087e-04, 2.8610e-04, 3.5000e-04, 2.2376e-04,\n",
      "        3.8648e-04, 3.2234e-03, 1.2093e-03, 3.5477e-04, 2.5139e-03, 4.5586e-03,\n",
      "        1.9531e-03, 3.9711e-03, 6.4468e-04, 1.0986e-03, 1.7042e-03, 9.1648e-04,\n",
      "        6.2847e-04, 1.8301e-03, 2.1305e-03, 9.2363e-04, 7.1526e-04, 7.9632e-04,\n",
      "        1.0347e-03, 3.0346e-03, 1.1072e-03, 1.3725e-02, 6.2752e-03, 1.2878e-02,\n",
      "        5.0735e-03, 6.2065e-03, 2.2278e-03, 9.1705e-03, 5.5695e-04, 2.9240e-03,\n",
      "        3.0117e-03, 2.6550e-03, 4.9820e-03, 7.2136e-03, 8.3771e-03, 8.9340e-03,\n",
      "        1.0323e-02, 1.4511e-02, 2.4281e-03, 4.3297e-03, 5.5847e-03, 2.8820e-03,\n",
      "        1.8330e-03, 4.9019e-03, 2.9564e-03, 6.8970e-03, 4.5586e-03, 4.3182e-03,\n",
      "        1.0117e-02, 6.4926e-03, 2.4624e-03, 4.9553e-03, 1.0586e-03, 3.1452e-03,\n",
      "        4.9706e-03, 3.4008e-03, 5.2490e-03, 3.8528e-03, 2.6398e-03, 1.2407e-03,\n",
      "        7.0457e-03, 9.1839e-04, 3.1495e-04, 2.1210e-03, 1.0166e-03, 1.0614e-03,\n",
      "        2.6107e-04, 2.1534e-03, 1.8530e-03, 4.1389e-04, 6.0158e-03, 4.4098e-03,\n",
      "        1.5526e-03, 4.7922e-04, 3.5229e-03, 2.5082e-03, 2.6913e-03, 3.7136e-03,\n",
      "        1.1587e-03, 2.2411e-03, 2.7256e-03, 1.0948e-03, 1.0824e-03, 8.1205e-04,\n",
      "        1.3676e-03, 6.9439e-05, 6.1512e-04, 1.5545e-03, 8.4305e-03, 9.2010e-03,\n",
      "        3.5095e-03, 5.1651e-03, 1.2825e-02, 2.2697e-03, 6.8703e-03, 2.1458e-03,\n",
      "        8.2302e-04, 1.3227e-03, 9.4681e-03, 2.1191e-03, 2.2449e-03, 2.0065e-03,\n",
      "        2.7771e-02, 3.9734e-02, 1.3573e-02, 1.9135e-02, 5.0850e-03, 3.2120e-03,\n",
      "        2.8152e-02, 6.0387e-03, 9.6970e-03, 6.1302e-03, 3.5133e-03, 1.6098e-03,\n",
      "        5.9471e-03, 6.0577e-03, 6.9885e-03, 2.7580e-03, 3.3641e-04, 3.7384e-03,\n",
      "        5.7602e-03, 1.1711e-02, 1.5640e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [100] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [100] : torch.Size([1, 32, 1, 172])\n",
      "Last layer attentions for generated token [100] : tensor([2.1509e-01, 2.1460e-01, 1.2267e-04, 3.8171e-04, 1.0252e-04, 2.6703e-03,\n",
      "        3.7503e-04, 2.4700e-04, 5.9366e-04, 5.3835e-04, 1.1711e-03, 4.8637e-04,\n",
      "        6.2799e-04, 1.1511e-03, 1.0338e-02, 7.3853e-03, 3.5787e-04, 5.4121e-04,\n",
      "        1.9002e-04, 2.8539e-04, 1.5450e-04, 5.4789e-04, 5.2261e-03, 1.9760e-03,\n",
      "        9.8572e-03, 1.5823e-02, 1.7042e-03, 7.7724e-04, 4.8256e-04, 3.5095e-04,\n",
      "        5.1069e-04, 1.6422e-03, 7.8773e-04, 4.2248e-04, 2.5988e-04, 7.6056e-04,\n",
      "        4.9210e-04, 4.9973e-04, 2.8706e-04, 2.5463e-04, 2.9697e-03, 1.9722e-03,\n",
      "        7.5150e-04, 8.0776e-04, 4.2081e-04, 3.5501e-04, 1.1480e-04, 1.6582e-04,\n",
      "        3.4690e-04, 1.8606e-03, 1.7338e-03, 1.7858e-04, 2.1496e-03, 3.1796e-03,\n",
      "        2.7714e-03, 3.5095e-03, 4.6945e-04, 1.9684e-03, 3.4695e-03, 1.2560e-03,\n",
      "        1.4744e-03, 1.3533e-03, 2.1667e-03, 1.8930e-03, 8.5163e-04, 6.6614e-04,\n",
      "        2.6722e-03, 1.2131e-03, 2.5597e-03, 8.1100e-03, 5.4054e-03, 1.0986e-02,\n",
      "        4.4441e-03, 2.1019e-03, 1.3695e-03, 2.6131e-03, 2.9492e-04, 1.5545e-03,\n",
      "        1.1024e-03, 1.7996e-03, 4.6349e-03, 2.4910e-03, 1.1414e-02, 7.2937e-03,\n",
      "        3.8624e-03, 2.4815e-03, 1.7309e-03, 3.1490e-03, 3.9062e-03, 2.4986e-03,\n",
      "        9.8038e-04, 2.3174e-03, 2.6207e-03, 6.3400e-03, 5.4054e-03, 2.8725e-03,\n",
      "        3.2654e-03, 5.8937e-03, 2.3251e-03, 1.9312e-03, 1.2665e-03, 3.4351e-03,\n",
      "        1.8930e-03, 1.2589e-03, 1.4887e-03, 1.0853e-03, 3.4924e-03, 1.2083e-03,\n",
      "        4.6196e-03, 1.3876e-03, 8.3542e-04, 1.3800e-03, 1.3638e-03, 8.9073e-04,\n",
      "        5.7983e-04, 3.2883e-03, 2.8553e-03, 5.2595e-04, 2.6035e-03, 4.6463e-03,\n",
      "        3.3627e-03, 1.5917e-03, 4.5929e-03, 3.1929e-03, 4.0627e-03, 3.5820e-03,\n",
      "        1.6165e-03, 2.0695e-03, 2.1706e-03, 1.4744e-03, 1.8387e-03, 1.4601e-03,\n",
      "        1.4744e-03, 1.9300e-04, 9.4652e-04, 1.8244e-03, 5.6686e-03, 8.3466e-03,\n",
      "        3.4924e-03, 2.6035e-03, 4.5586e-03, 2.8477e-03, 3.9062e-03, 1.2865e-03,\n",
      "        1.0452e-03, 1.3113e-03, 5.4131e-03, 2.1400e-03, 2.2678e-03, 1.0920e-03,\n",
      "        1.0078e-02, 4.8157e-02, 1.1322e-02, 1.5839e-02, 5.7678e-03, 8.6670e-03,\n",
      "        1.5282e-02, 1.1116e-02, 7.0953e-03, 4.1695e-03, 3.4657e-03, 3.8128e-03,\n",
      "        5.3787e-03, 6.4201e-03, 4.9629e-03, 3.2940e-03, 1.3714e-03, 5.5847e-03,\n",
      "        7.6981e-03, 1.6556e-02, 9.3613e-03, 1.0529e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [101] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [101] : torch.Size([1, 32, 1, 173])\n",
      "Last layer attentions for generated token [101] : tensor([2.3779e-01, 2.3730e-01, 2.5392e-04, 1.7381e-04, 1.4353e-04, 1.8082e-03,\n",
      "        1.7381e-04, 2.9063e-04, 8.0395e-04, 5.9175e-04, 9.5844e-04, 4.4394e-04,\n",
      "        2.2328e-04, 4.8184e-04, 5.8708e-03, 5.3177e-03, 2.7084e-04, 3.7837e-04,\n",
      "        1.1444e-04, 1.1492e-04, 6.6221e-05, 3.0994e-04, 4.3602e-03, 7.3767e-04,\n",
      "        4.6310e-03, 6.5804e-03, 6.5088e-04, 3.3116e-04, 2.5845e-04, 4.3368e-04,\n",
      "        5.0020e-04, 1.0920e-03, 6.0940e-04, 3.1042e-04, 4.1389e-04, 3.9172e-04,\n",
      "        1.9622e-04, 5.5456e-04, 2.7895e-04, 1.6022e-04, 3.6659e-03, 2.1381e-03,\n",
      "        4.2701e-04, 4.3869e-04, 3.6669e-04, 1.6987e-04, 1.6856e-04, 2.4509e-04,\n",
      "        2.3484e-04, 1.6594e-03, 1.7319e-03, 2.8443e-04, 2.8419e-03, 4.6005e-03,\n",
      "        2.2850e-03, 2.9812e-03, 3.0446e-04, 1.4992e-03, 2.2259e-03, 1.1692e-03,\n",
      "        9.4700e-04, 1.8272e-03, 2.6531e-03, 1.5020e-03, 6.3848e-04, 1.3380e-03,\n",
      "        3.0556e-03, 2.5673e-03, 3.1910e-03, 8.8272e-03, 5.9357e-03, 1.1589e-02,\n",
      "        3.7098e-03, 2.1534e-03, 1.1358e-03, 5.8060e-03, 3.0994e-04, 1.2159e-03,\n",
      "        1.5554e-03, 2.5520e-03, 3.3131e-03, 4.6425e-03, 1.0338e-02, 2.3613e-03,\n",
      "        2.1687e-03, 4.0054e-03, 2.0676e-03, 1.4362e-03, 2.7008e-03, 1.6241e-03,\n",
      "        5.1212e-04, 3.1757e-03, 2.7390e-03, 6.4697e-03, 8.4991e-03, 4.3793e-03,\n",
      "        7.9269e-03, 7.9117e-03, 1.7805e-03, 1.7939e-03, 1.5049e-03, 7.3195e-04,\n",
      "        1.3332e-03, 1.3485e-03, 1.7939e-03, 2.3632e-03, 1.9169e-03, 1.0281e-03,\n",
      "        5.8250e-03, 1.4105e-03, 8.0872e-04, 1.9932e-03, 2.0256e-03, 1.4219e-03,\n",
      "        6.1274e-04, 3.8471e-03, 2.3537e-03, 4.4751e-04, 4.1237e-03, 5.6038e-03,\n",
      "        1.5135e-03, 7.3910e-04, 3.3512e-03, 2.5845e-03, 4.0588e-03, 3.9253e-03,\n",
      "        1.3075e-03, 2.1973e-03, 2.6436e-03, 2.4567e-03, 1.8568e-03, 1.7624e-03,\n",
      "        1.8692e-03, 2.3580e-04, 1.0633e-03, 1.4334e-03, 6.3400e-03, 6.4926e-03,\n",
      "        3.3131e-03, 3.3836e-03, 8.0490e-03, 3.7613e-03, 5.4550e-03, 1.4448e-03,\n",
      "        9.2888e-04, 8.8453e-04, 6.3095e-03, 1.4610e-03, 1.1091e-03, 9.1076e-04,\n",
      "        7.5493e-03, 3.0029e-02, 4.8561e-03, 8.6517e-03, 4.8103e-03, 2.4624e-03,\n",
      "        1.4015e-02, 6.1493e-03, 6.0081e-03, 3.9291e-03, 4.8332e-03, 3.2291e-03,\n",
      "        6.9809e-03, 1.0788e-02, 7.9880e-03, 8.8120e-03, 2.0065e-03, 9.8877e-03,\n",
      "        1.0887e-02, 5.6038e-03, 6.2943e-03, 1.3535e-02, 7.2517e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [102] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [102] : torch.Size([1, 32, 1, 174])\n",
      "Last layer attentions for generated token [102] : tensor([1.9092e-01, 1.9092e-01, 1.5974e-04, 3.3307e-04, 1.1027e-04, 2.4586e-03,\n",
      "        3.5739e-04, 3.6359e-04, 9.0170e-04, 9.1791e-04, 1.5278e-03, 4.5514e-04,\n",
      "        5.5790e-04, 7.4911e-04, 7.1602e-03, 7.1564e-03, 3.3951e-04, 6.1512e-04,\n",
      "        1.7548e-04, 2.4080e-04, 8.5890e-05, 3.8028e-04, 5.1003e-03, 1.1091e-03,\n",
      "        5.7564e-03, 1.1726e-02, 1.5192e-03, 9.6178e-04, 8.0824e-04, 5.1689e-04,\n",
      "        6.2847e-04, 1.8339e-03, 1.1244e-03, 5.9605e-04, 5.1880e-04, 9.1600e-04,\n",
      "        6.5708e-04, 7.5483e-04, 4.6229e-04, 4.3178e-04, 4.2267e-03, 2.1954e-03,\n",
      "        5.7316e-04, 9.7132e-04, 6.3324e-04, 4.6873e-04, 1.8036e-04, 2.3067e-04,\n",
      "        4.5347e-04, 1.9932e-03, 1.5430e-03, 1.4210e-04, 2.3785e-03, 3.1319e-03,\n",
      "        2.5215e-03, 4.4670e-03, 5.0688e-04, 2.7637e-03, 4.2572e-03, 1.4753e-03,\n",
      "        2.1534e-03, 1.6556e-03, 2.5215e-03, 2.0237e-03, 1.3962e-03, 8.3065e-04,\n",
      "        2.5597e-03, 9.7132e-04, 2.3689e-03, 5.1956e-03, 4.7722e-03, 1.0300e-02,\n",
      "        4.0779e-03, 1.8702e-03, 1.4982e-03, 4.2877e-03, 5.5456e-04, 3.3398e-03,\n",
      "        1.7738e-03, 2.8934e-03, 8.2245e-03, 3.5706e-03, 9.5139e-03, 4.2648e-03,\n",
      "        2.8000e-03, 3.0651e-03, 2.0733e-03, 2.2583e-03, 3.1109e-03, 2.0504e-03,\n",
      "        6.3705e-04, 2.2602e-03, 2.8152e-03, 6.1951e-03, 5.6152e-03, 4.2572e-03,\n",
      "        4.1504e-03, 6.9504e-03, 1.8129e-03, 1.9217e-03, 1.4782e-03, 1.3275e-03,\n",
      "        1.9054e-03, 1.6680e-03, 2.1591e-03, 1.7920e-03, 3.1433e-03, 1.5554e-03,\n",
      "        7.0801e-03, 1.7128e-03, 1.1425e-03, 2.4586e-03, 2.4624e-03, 1.8997e-03,\n",
      "        9.2649e-04, 4.1847e-03, 2.7637e-03, 7.1478e-04, 2.0924e-03, 5.2452e-03,\n",
      "        2.3041e-03, 1.3914e-03, 4.5128e-03, 3.4389e-03, 4.6425e-03, 4.2648e-03,\n",
      "        1.6394e-03, 2.5558e-03, 2.9736e-03, 2.1591e-03, 2.2736e-03, 1.5736e-03,\n",
      "        2.5158e-03, 2.8658e-04, 1.2474e-03, 2.1629e-03, 6.8474e-03, 1.0941e-02,\n",
      "        4.4823e-03, 4.9210e-03, 9.1171e-03, 4.4937e-03, 6.5384e-03, 2.2297e-03,\n",
      "        1.6108e-03, 3.4428e-03, 7.7438e-03, 2.5692e-03, 2.8076e-03, 1.5669e-03,\n",
      "        9.6741e-03, 3.0853e-02, 7.2823e-03, 1.1765e-02, 5.1575e-03, 4.2877e-03,\n",
      "        1.7776e-02, 1.0025e-02, 8.0566e-03, 3.5000e-03, 5.3902e-03, 4.5853e-03,\n",
      "        5.8746e-03, 7.6599e-03, 6.5193e-03, 6.3171e-03, 2.4242e-03, 9.0485e-03,\n",
      "        1.1887e-02, 1.0117e-02, 7.5989e-03, 1.7410e-02, 1.6327e-02, 1.1436e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [103] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [103] : torch.Size([1, 32, 1, 175])\n",
      "Last layer attentions for generated token [103] : tensor([2.8760e-01, 2.8760e-01, 3.0947e-04, 4.1652e-04, 1.7917e-04, 1.3657e-03,\n",
      "        1.6057e-04, 3.6836e-04, 5.5170e-04, 4.7112e-04, 6.2799e-04, 1.9836e-04,\n",
      "        1.5569e-04, 1.8930e-04, 4.1122e-03, 3.7575e-03, 4.5133e-04, 4.5228e-04,\n",
      "        2.1112e-04, 1.5211e-04, 2.7597e-05, 5.7936e-04, 7.4501e-03, 9.2602e-04,\n",
      "        2.5768e-03, 6.3057e-03, 1.0538e-03, 3.1757e-04, 4.5586e-04, 6.4421e-04,\n",
      "        1.0824e-03, 2.0695e-03, 6.9237e-04, 2.8753e-04, 4.3654e-04, 3.3736e-04,\n",
      "        1.1253e-04, 5.5742e-04, 3.4404e-04, 2.8300e-04, 4.1466e-03, 1.3905e-03,\n",
      "        2.6488e-04, 5.4121e-04, 4.1175e-04, 2.5868e-04, 2.5654e-04, 2.3282e-04,\n",
      "        2.3282e-04, 1.8816e-03, 1.3084e-03, 2.1446e-04, 1.7958e-03, 7.4387e-03,\n",
      "        1.3933e-03, 3.6564e-03, 5.2261e-04, 1.9417e-03, 7.0686e-03, 1.7958e-03,\n",
      "        1.8063e-03, 1.9531e-03, 4.1618e-03, 2.1610e-03, 1.1549e-03, 1.6642e-03,\n",
      "        2.8744e-03, 2.0351e-03, 3.6297e-03, 1.9562e-02, 9.9945e-03, 1.1444e-02,\n",
      "        2.6531e-03, 1.6870e-03, 6.9904e-04, 1.8816e-03, 2.1529e-04, 7.4434e-04,\n",
      "        1.0052e-03, 1.7099e-03, 1.2484e-03, 2.5597e-03, 7.2403e-03, 1.0958e-03,\n",
      "        8.7690e-04, 1.2388e-03, 9.7847e-04, 6.8951e-04, 1.2655e-03, 9.7418e-04,\n",
      "        1.7846e-04, 1.3447e-03, 1.9035e-03, 3.3340e-03, 3.4447e-03, 3.6068e-03,\n",
      "        4.4060e-03, 5.6305e-03, 6.6948e-04, 1.1730e-03, 9.0981e-04, 2.4116e-04,\n",
      "        6.4898e-04, 5.5170e-04, 9.2793e-04, 1.5154e-03, 1.5354e-03, 7.2527e-04,\n",
      "        4.8752e-03, 2.4261e-03, 9.2793e-04, 3.4447e-03, 2.5539e-03, 3.5954e-03,\n",
      "        1.0395e-03, 8.3084e-03, 2.2030e-03, 6.3658e-04, 2.3556e-03, 3.2005e-03,\n",
      "        2.2469e-03, 1.3342e-03, 4.3564e-03, 1.6479e-03, 2.6569e-03, 2.7084e-03,\n",
      "        1.0786e-03, 2.5082e-03, 1.5478e-03, 1.3399e-03, 1.1711e-03, 8.8549e-04,\n",
      "        1.4887e-03, 1.7369e-04, 8.1730e-04, 1.2369e-03, 4.9477e-03, 4.2915e-03,\n",
      "        2.1191e-03, 1.8969e-03, 3.9940e-03, 2.4681e-03, 2.8114e-03, 1.0767e-03,\n",
      "        5.5742e-04, 1.0252e-03, 4.4174e-03, 9.8419e-04, 1.1044e-03, 9.2411e-04,\n",
      "        6.6109e-03, 1.3077e-02, 3.2997e-03, 4.2610e-03, 1.7071e-03, 2.9640e-03,\n",
      "        3.8490e-03, 3.0785e-03, 3.3569e-03, 1.3657e-03, 3.9597e-03, 2.3632e-03,\n",
      "        3.1719e-03, 5.8937e-03, 3.6869e-03, 5.3406e-03, 1.4973e-03, 9.4147e-03,\n",
      "        7.1526e-03, 2.4452e-03, 2.9449e-03, 7.5111e-03, 5.2071e-03, 8.0719e-03,\n",
      "        1.5869e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [104] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [104] : torch.Size([1, 32, 1, 176])\n",
      "Last layer attentions for generated token [104] : tensor([1.8396e-01, 1.8420e-01, 1.6248e-04, 2.9206e-04, 1.7238e-04, 1.2608e-03,\n",
      "        6.3419e-04, 4.8518e-04, 7.3719e-04, 6.8951e-04, 1.1148e-03, 4.7970e-04,\n",
      "        3.9148e-04, 6.8951e-04, 4.9477e-03, 4.8065e-03, 4.1914e-04, 7.5436e-04,\n",
      "        2.5082e-04, 3.5286e-04, 1.1802e-04, 7.1001e-04, 5.7144e-03, 2.0351e-03,\n",
      "        5.7869e-03, 6.8092e-03, 1.2560e-03, 5.9223e-04, 4.1819e-04, 5.8508e-04,\n",
      "        1.0109e-03, 1.6556e-03, 1.3504e-03, 5.9700e-04, 5.1975e-04, 5.3692e-04,\n",
      "        3.8171e-04, 4.9114e-04, 3.1519e-04, 2.6131e-04, 3.7136e-03, 1.6870e-03,\n",
      "        5.7745e-04, 8.6498e-04, 5.1165e-04, 4.1819e-04, 1.9372e-04, 2.5511e-04,\n",
      "        3.1638e-04, 1.4009e-03, 1.3371e-03, 1.8346e-04, 1.8454e-03, 3.4008e-03,\n",
      "        2.2182e-03, 3.6736e-03, 4.1032e-04, 2.1954e-03, 3.7613e-03, 1.7786e-03,\n",
      "        1.9283e-03, 2.3766e-03, 3.6697e-03, 2.4357e-03, 1.7338e-03, 1.5001e-03,\n",
      "        2.5253e-03, 1.4515e-03, 2.1820e-03, 1.0979e-02, 5.6725e-03, 1.2268e-02,\n",
      "        4.4823e-03, 3.1891e-03, 1.6127e-03, 5.9128e-03, 3.0780e-04, 1.8730e-03,\n",
      "        1.6727e-03, 1.5240e-03, 3.8090e-03, 2.8858e-03, 7.1907e-03, 3.5229e-03,\n",
      "        2.1400e-03, 2.3022e-03, 1.8587e-03, 2.2335e-03, 2.9774e-03, 1.7996e-03,\n",
      "        6.4898e-04, 2.2335e-03, 3.5305e-03, 5.9319e-03, 4.6654e-03, 3.1700e-03,\n",
      "        4.7836e-03, 7.0229e-03, 2.2469e-03, 1.9169e-03, 1.6708e-03, 1.6756e-03,\n",
      "        2.0142e-03, 1.5392e-03, 1.8225e-03, 1.9302e-03, 3.9558e-03, 1.4286e-03,\n",
      "        7.7744e-03, 2.2278e-03, 1.5821e-03, 3.4409e-03, 2.9488e-03, 2.6226e-03,\n",
      "        1.5659e-03, 6.9656e-03, 3.4180e-03, 1.1940e-03, 4.4975e-03, 4.4365e-03,\n",
      "        3.6278e-03, 1.2178e-03, 3.5057e-03, 2.3155e-03, 3.4313e-03, 3.6449e-03,\n",
      "        1.4544e-03, 3.0766e-03, 3.6106e-03, 1.7052e-03, 1.7576e-03, 1.5335e-03,\n",
      "        1.7090e-03, 2.6751e-04, 1.1873e-03, 1.5450e-03, 6.2637e-03, 7.8011e-03,\n",
      "        3.4981e-03, 2.8896e-03, 7.9803e-03, 4.6387e-03, 5.1155e-03, 2.5024e-03,\n",
      "        1.4944e-03, 2.8667e-03, 7.7400e-03, 2.7390e-03, 2.8477e-03, 1.5659e-03,\n",
      "        8.6594e-03, 2.4399e-02, 7.2327e-03, 9.1553e-03, 6.0844e-03, 4.6844e-03,\n",
      "        1.6632e-02, 1.0727e-02, 7.9193e-03, 5.2376e-03, 8.3466e-03, 5.9967e-03,\n",
      "        6.0234e-03, 1.0094e-02, 6.9885e-03, 6.7863e-03, 2.8133e-03, 1.0979e-02,\n",
      "        1.5251e-02, 8.2397e-03, 8.0795e-03, 1.8387e-02, 1.7273e-02, 1.3344e-02,\n",
      "        1.3550e-02, 8.0109e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [105] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [105] : torch.Size([1, 32, 1, 177])\n",
      "Last layer attentions for generated token [105] : tensor([2.1875e-01, 2.1924e-01, 1.2851e-04, 2.6679e-04, 1.4007e-04, 1.3580e-03,\n",
      "        2.2423e-04, 3.7122e-04, 6.4898e-04, 4.3988e-04, 6.4135e-04, 3.2187e-04,\n",
      "        1.2028e-04, 4.4966e-04, 5.0316e-03, 6.0501e-03, 3.3021e-04, 6.1941e-04,\n",
      "        1.0741e-04, 1.4234e-04, 7.8559e-05, 3.6049e-04, 6.0234e-03, 1.6785e-03,\n",
      "        5.1880e-03, 9.2392e-03, 1.1082e-03, 3.5071e-04, 3.9124e-04, 8.5497e-04,\n",
      "        5.7936e-04, 2.3403e-03, 9.0981e-04, 5.1355e-04, 5.2166e-04, 3.8314e-04,\n",
      "        2.1279e-04, 4.7207e-04, 3.0899e-04, 2.4629e-04, 3.3379e-03, 1.6384e-03,\n",
      "        3.6120e-04, 6.6328e-04, 4.7755e-04, 3.6764e-04, 2.6536e-04, 1.8561e-04,\n",
      "        2.8968e-04, 1.5240e-03, 1.5497e-03, 3.4881e-04, 2.2430e-03, 1.0880e-02,\n",
      "        1.8291e-03, 4.6463e-03, 4.7755e-04, 2.8114e-03, 3.6449e-03, 1.9379e-03,\n",
      "        2.7924e-03, 3.9635e-03, 4.4174e-03, 2.4204e-03, 1.0929e-03, 1.8044e-03,\n",
      "        3.1605e-03, 2.7142e-03, 2.5902e-03, 1.5823e-02, 8.5983e-03, 1.2665e-02,\n",
      "        4.5433e-03, 2.3117e-03, 9.4795e-04, 4.9248e-03, 2.6011e-04, 1.9150e-03,\n",
      "        1.7767e-03, 1.6003e-03, 2.8114e-03, 3.6774e-03, 7.8812e-03, 2.3956e-03,\n",
      "        1.8063e-03, 2.4185e-03, 1.3523e-03, 1.2960e-03, 1.8930e-03, 1.3790e-03,\n",
      "        3.1567e-04, 1.5383e-03, 2.8687e-03, 6.2904e-03, 2.9163e-03, 3.6030e-03,\n",
      "        5.4703e-03, 5.6267e-03, 1.2484e-03, 1.4744e-03, 8.8167e-04, 6.3133e-04,\n",
      "        1.1253e-03, 9.5940e-04, 1.5211e-03, 1.8291e-03, 1.3552e-03, 7.9346e-04,\n",
      "        4.9362e-03, 1.2903e-03, 1.0252e-03, 4.3678e-03, 2.2259e-03, 2.3708e-03,\n",
      "        1.1015e-03, 5.3825e-03, 2.6016e-03, 6.9904e-04, 3.2444e-03, 4.5166e-03,\n",
      "        2.5349e-03, 1.0557e-03, 3.2825e-03, 1.9608e-03, 3.0689e-03, 2.4509e-03,\n",
      "        1.0014e-03, 2.1286e-03, 1.7338e-03, 1.0233e-03, 1.0786e-03, 8.3494e-04,\n",
      "        1.2684e-03, 1.3316e-04, 7.4100e-04, 1.4887e-03, 4.7379e-03, 4.2000e-03,\n",
      "        2.7809e-03, 3.0327e-03, 9.3079e-03, 3.3913e-03, 4.5547e-03, 1.6670e-03,\n",
      "        8.5640e-04, 1.8129e-03, 5.8174e-03, 1.6737e-03, 1.8129e-03, 1.7796e-03,\n",
      "        8.1253e-03, 2.0416e-02, 5.1918e-03, 4.8027e-03, 2.8515e-03, 3.1261e-03,\n",
      "        9.5291e-03, 5.7945e-03, 6.8398e-03, 2.8114e-03, 5.4054e-03, 4.7150e-03,\n",
      "        9.5215e-03, 9.5596e-03, 6.4659e-03, 5.6534e-03, 1.9684e-03, 9.2316e-03,\n",
      "        7.9956e-03, 5.3520e-03, 6.3667e-03, 1.1948e-02, 8.1940e-03, 1.3191e-02,\n",
      "        1.7471e-02, 8.2703e-03, 1.7975e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [106] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [106] : torch.Size([1, 32, 1, 178])\n",
      "Last layer attentions for generated token [106] : tensor([2.4451e-01, 2.4500e-01, 1.6475e-04, 3.7408e-04, 1.7679e-04, 2.4834e-03,\n",
      "        2.6894e-04, 3.2187e-04, 3.0971e-04, 4.4250e-04, 6.7377e-04, 3.6407e-04,\n",
      "        3.5691e-04, 8.5831e-04, 5.9814e-03, 9.4376e-03, 3.6907e-04, 5.2071e-04,\n",
      "        1.4091e-04, 2.7585e-04, 9.4235e-05, 4.4966e-04, 1.1993e-02, 2.2774e-03,\n",
      "        5.4436e-03, 9.3079e-03, 1.2369e-03, 3.4547e-04, 4.0770e-04, 5.1260e-04,\n",
      "        5.6601e-04, 1.7900e-03, 1.1253e-03, 5.6267e-04, 4.8327e-04, 5.3692e-04,\n",
      "        2.1994e-04, 3.3665e-04, 3.0541e-04, 1.8311e-04, 2.6093e-03, 1.9283e-03,\n",
      "        8.4829e-04, 1.0805e-03, 6.0606e-04, 5.3692e-04, 1.5724e-04, 2.2876e-04,\n",
      "        2.6369e-04, 1.5755e-03, 1.6289e-03, 3.2568e-04, 2.1381e-03, 5.6458e-03,\n",
      "        2.0790e-03, 6.4201e-03, 5.3263e-04, 2.7428e-03, 3.6621e-03, 1.6289e-03,\n",
      "        2.3136e-03, 2.0409e-03, 4.2953e-03, 2.1286e-03, 1.1234e-03, 1.1320e-03,\n",
      "        2.9144e-03, 2.0256e-03, 1.8835e-03, 1.0185e-02, 5.7678e-03, 1.4168e-02,\n",
      "        3.1357e-03, 1.5297e-03, 8.0299e-04, 2.8629e-03, 4.0293e-04, 1.3237e-03,\n",
      "        8.1396e-04, 1.6441e-03, 2.1629e-03, 2.2602e-03, 6.5536e-03, 2.9984e-03,\n",
      "        1.6766e-03, 1.6766e-03, 1.4372e-03, 1.3714e-03, 1.4830e-03, 9.6655e-04,\n",
      "        4.7851e-04, 2.3994e-03, 2.4662e-03, 3.9368e-03, 2.9144e-03, 2.8381e-03,\n",
      "        2.5902e-03, 5.0201e-03, 1.4095e-03, 1.1711e-03, 1.4009e-03, 1.2035e-03,\n",
      "        1.3237e-03, 9.7418e-04, 1.3771e-03, 1.4229e-03, 1.7509e-03, 9.7847e-04,\n",
      "        4.6082e-03, 1.8101e-03, 1.0090e-03, 2.2240e-03, 2.1935e-03, 2.1667e-03,\n",
      "        1.0109e-03, 3.2768e-03, 2.6016e-03, 9.4986e-04, 2.6321e-03, 3.6983e-03,\n",
      "        2.4986e-03, 1.0662e-03, 1.7900e-03, 1.1988e-03, 1.5659e-03, 2.2030e-03,\n",
      "        8.8215e-04, 1.4801e-03, 1.6193e-03, 8.7357e-04, 7.4863e-04, 7.2670e-04,\n",
      "        8.7500e-04, 1.1635e-04, 4.4346e-04, 1.2608e-03, 3.3245e-03, 4.2496e-03,\n",
      "        2.2964e-03, 1.9989e-03, 3.4771e-03, 2.2430e-03, 2.2964e-03, 1.2150e-03,\n",
      "        8.0776e-04, 1.6766e-03, 4.6692e-03, 1.4544e-03, 1.8148e-03, 1.2388e-03,\n",
      "        8.6441e-03, 2.1698e-02, 5.3978e-03, 7.7209e-03, 3.1986e-03, 2.9221e-03,\n",
      "        9.1858e-03, 5.9929e-03, 4.9477e-03, 2.2392e-03, 4.7569e-03, 3.4771e-03,\n",
      "        4.2915e-03, 5.8289e-03, 4.5891e-03, 3.5839e-03, 1.2531e-03, 5.5885e-03,\n",
      "        1.0376e-02, 7.2174e-03, 7.0648e-03, 1.3138e-02, 1.1497e-02, 1.0933e-02,\n",
      "        1.6266e-02, 7.6447e-03, 1.6373e-02, 5.2719e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [107] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [107] : torch.Size([1, 32, 1, 179])\n",
      "Last layer attentions for generated token [107] : tensor([2.1667e-01, 2.1667e-01, 1.2982e-04, 2.0659e-04, 1.4770e-04, 1.3771e-03,\n",
      "        4.2725e-04, 5.3930e-04, 3.8528e-04, 4.3821e-04, 1.1415e-03, 5.8746e-04,\n",
      "        3.0017e-04, 3.8314e-04, 3.6907e-03, 4.4975e-03, 2.5558e-04, 6.2418e-04,\n",
      "        2.2256e-04, 3.0780e-04, 8.4162e-05, 4.2081e-04, 6.5155e-03, 1.1501e-03,\n",
      "        2.9030e-03, 5.0125e-03, 5.7268e-04, 2.4581e-04, 2.7704e-04, 4.5586e-04,\n",
      "        7.8773e-04, 9.7036e-04, 1.2732e-03, 6.3276e-04, 7.3957e-04, 4.5037e-04,\n",
      "        1.5295e-04, 4.2391e-04, 2.7633e-04, 2.3234e-04, 1.7977e-03, 8.1110e-04,\n",
      "        3.1996e-04, 6.3753e-04, 5.7268e-04, 4.3321e-04, 3.6335e-04, 3.1996e-04,\n",
      "        4.1819e-04, 1.6870e-03, 1.0557e-03, 1.7059e-04, 1.5907e-03, 3.7937e-03,\n",
      "        1.1282e-03, 3.6316e-03, 2.1827e-04, 1.1778e-03, 2.2240e-03, 1.1635e-03,\n",
      "        1.4124e-03, 1.3418e-03, 4.7035e-03, 1.7118e-03, 1.3189e-03, 1.3418e-03,\n",
      "        1.4181e-03, 1.4830e-03, 1.3189e-03, 8.9264e-03, 5.1689e-03, 1.1414e-02,\n",
      "        3.0403e-03, 2.7332e-03, 1.1826e-03, 8.0185e-03, 5.1737e-04, 2.1305e-03,\n",
      "        2.3670e-03, 2.2697e-03, 3.0212e-03, 3.2902e-03, 4.3144e-03, 2.2697e-03,\n",
      "        1.9188e-03, 2.7218e-03, 1.4343e-03, 1.6289e-03, 2.0943e-03, 1.3771e-03,\n",
      "        5.1451e-04, 2.2144e-03, 3.1662e-03, 3.8795e-03, 3.3836e-03, 3.6907e-03,\n",
      "        4.8141e-03, 5.1003e-03, 1.1387e-03, 2.2602e-03, 1.2932e-03, 8.4972e-04,\n",
      "        1.8024e-03, 1.5974e-03, 2.0447e-03, 3.1509e-03, 1.2293e-03, 1.1940e-03,\n",
      "        6.1417e-03, 1.9207e-03, 9.8419e-04, 2.8191e-03, 3.4542e-03, 4.4518e-03,\n",
      "        1.3552e-03, 5.3825e-03, 4.8676e-03, 1.2684e-03, 4.8676e-03, 3.6659e-03,\n",
      "        2.0237e-03, 6.8808e-04, 2.3212e-03, 2.1420e-03, 2.8057e-03, 3.9215e-03,\n",
      "        9.0837e-04, 2.2507e-03, 2.8534e-03, 1.4915e-03, 1.5535e-03, 1.2007e-03,\n",
      "        1.8797e-03, 2.4867e-04, 1.0738e-03, 1.4067e-03, 5.8441e-03, 6.4621e-03,\n",
      "        3.1300e-03, 3.8719e-03, 6.2256e-03, 3.5534e-03, 5.9090e-03, 2.7790e-03,\n",
      "        9.6655e-04, 2.1019e-03, 6.6223e-03, 1.9588e-03, 2.2392e-03, 1.7118e-03,\n",
      "        1.0506e-02, 1.5266e-02, 5.6038e-03, 6.5880e-03, 2.7332e-03, 3.9139e-03,\n",
      "        1.0574e-02, 4.9973e-03, 3.9825e-03, 3.9787e-03, 6.9962e-03, 3.8452e-03,\n",
      "        5.0888e-03, 1.0635e-02, 7.9269e-03, 8.2016e-03, 1.6832e-03, 9.9945e-03,\n",
      "        1.2589e-02, 5.2185e-03, 6.9847e-03, 2.5604e-02, 7.9422e-03, 1.6968e-02,\n",
      "        1.6663e-02, 8.9874e-03, 1.1139e-02, 6.6566e-03, 8.1406e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [108] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [108] : torch.Size([1, 32, 1, 180])\n",
      "Last layer attentions for generated token [108] : tensor([1.5979e-01, 1.5979e-01, 6.0260e-05, 1.5211e-04, 6.4433e-05, 1.0252e-03,\n",
      "        2.8825e-04, 4.1509e-04, 3.6860e-04, 2.8920e-04, 6.0749e-04, 2.2614e-04,\n",
      "        3.1352e-04, 5.7650e-04, 5.5084e-03, 5.9891e-03, 2.3520e-04, 4.3249e-04,\n",
      "        1.4186e-04, 2.5225e-04, 3.9995e-05, 2.4843e-04, 4.1771e-03, 1.1263e-03,\n",
      "        3.1929e-03, 6.0463e-03, 1.0376e-03, 3.7289e-04, 2.8372e-04, 3.2592e-04,\n",
      "        2.8419e-04, 1.0138e-03, 1.1091e-03, 5.8460e-04, 6.0511e-04, 5.2500e-04,\n",
      "        2.4033e-04, 3.8099e-04, 2.6703e-04, 2.8253e-04, 2.6398e-03, 1.7824e-03,\n",
      "        7.1192e-04, 9.9945e-04, 6.4564e-04, 7.8344e-04, 1.5950e-04, 1.9419e-04,\n",
      "        3.9077e-04, 2.1057e-03, 2.0523e-03, 1.2565e-04, 2.3804e-03, 3.9215e-03,\n",
      "        2.6703e-03, 4.4899e-03, 4.9591e-04, 1.4410e-03, 2.9907e-03, 1.1826e-03,\n",
      "        2.8324e-03, 1.5020e-03, 3.5591e-03, 2.8706e-03, 9.6512e-04, 1.4029e-03,\n",
      "        4.5013e-03, 1.1597e-03, 2.5787e-03, 6.2828e-03, 6.1531e-03, 1.4267e-02,\n",
      "        5.4932e-03, 2.0523e-03, 1.0233e-03, 3.3112e-03, 2.1160e-04, 1.7843e-03,\n",
      "        7.0763e-04, 2.0599e-03, 3.5076e-03, 3.1166e-03, 9.8038e-03, 4.4975e-03,\n",
      "        2.5120e-03, 1.8444e-03, 1.6909e-03, 2.1572e-03, 3.3073e-03, 1.8997e-03,\n",
      "        6.7806e-04, 2.7370e-03, 3.6049e-03, 4.1313e-03, 5.2528e-03, 3.8376e-03,\n",
      "        5.3482e-03, 7.0267e-03, 1.9836e-03, 1.9684e-03, 1.9779e-03, 1.6794e-03,\n",
      "        2.2221e-03, 1.4982e-03, 1.6308e-03, 2.0027e-03, 2.4948e-03, 1.2712e-03,\n",
      "        7.9880e-03, 2.2678e-03, 1.5059e-03, 2.2888e-03, 2.4700e-03, 2.1629e-03,\n",
      "        1.4992e-03, 5.0850e-03, 3.9291e-03, 7.3862e-04, 5.3825e-03, 4.6387e-03,\n",
      "        3.0670e-03, 1.1044e-03, 4.9286e-03, 2.3365e-03, 1.8444e-03, 2.8477e-03,\n",
      "        9.4843e-04, 1.8978e-03, 2.1477e-03, 1.0624e-03, 1.6022e-03, 8.4877e-04,\n",
      "        1.4105e-03, 1.7512e-04, 7.5340e-04, 1.4877e-03, 7.5226e-03, 8.9340e-03,\n",
      "        2.9278e-03, 3.1433e-03, 6.7749e-03, 3.2940e-03, 3.5686e-03, 1.4744e-03,\n",
      "        7.0333e-04, 1.8110e-03, 4.6883e-03, 1.7996e-03, 2.6016e-03, 9.5987e-04,\n",
      "        1.1017e-02, 2.9388e-02, 8.3237e-03, 1.2100e-02, 5.2567e-03, 4.5319e-03,\n",
      "        1.8478e-02, 1.2863e-02, 1.0201e-02, 5.3673e-03, 8.0795e-03, 5.8327e-03,\n",
      "        6.2180e-03, 1.0765e-02, 1.0262e-02, 7.4806e-03, 2.2736e-03, 1.0574e-02,\n",
      "        1.3138e-02, 1.3443e-02, 9.5444e-03, 2.8351e-02, 1.8127e-02, 1.3199e-02,\n",
      "        1.5213e-02, 8.6136e-03, 1.8860e-02, 6.9656e-03, 7.8011e-03, 7.8278e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [109] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [109] : torch.Size([1, 32, 1, 181])\n",
      "Last layer attentions for generated token [109] : tensor([2.4744e-01, 2.4707e-01, 4.0710e-05, 1.0353e-04, 3.3379e-05, 8.6832e-04,\n",
      "        1.4484e-04, 2.5034e-04, 2.9206e-04, 2.4068e-04, 7.5293e-04, 2.3794e-04,\n",
      "        1.9574e-04, 3.6860e-04, 4.1962e-03, 3.1128e-03, 3.2067e-04, 3.7575e-04,\n",
      "        1.4150e-04, 2.1493e-04, 3.8087e-05, 3.3879e-04, 8.1024e-03, 1.2245e-03,\n",
      "        1.7042e-03, 4.1161e-03, 2.8968e-04, 1.7822e-04, 1.9991e-04, 1.4949e-04,\n",
      "        1.7607e-04, 6.2943e-04, 4.1676e-04, 2.8968e-04, 3.8457e-04, 4.1842e-04,\n",
      "        1.4830e-04, 5.2452e-04, 1.8239e-04, 1.6677e-04, 1.2178e-03, 6.8712e-04,\n",
      "        2.0754e-04, 6.0511e-04, 4.4107e-04, 3.8910e-04, 2.1923e-04, 1.3554e-04,\n",
      "        2.8038e-04, 8.5020e-04, 7.7391e-04, 7.8142e-05, 1.3638e-03, 4.2915e-03,\n",
      "        1.6479e-03, 4.1847e-03, 3.9697e-04, 1.9283e-03, 2.0866e-03, 1.2293e-03,\n",
      "        2.2526e-03, 1.4858e-03, 4.2763e-03, 2.9583e-03, 2.4662e-03, 1.8644e-03,\n",
      "        2.5158e-03, 1.0138e-03, 2.2049e-03, 9.1629e-03, 4.5433e-03, 1.1879e-02,\n",
      "        3.2234e-03, 1.6365e-03, 5.8889e-04, 3.5725e-03, 4.0388e-04, 2.1801e-03,\n",
      "        1.0071e-03, 1.4963e-03, 2.3003e-03, 2.1458e-03, 5.0430e-03, 8.9979e-04,\n",
      "        9.6512e-04, 1.3714e-03, 7.4148e-04, 7.0190e-04, 1.1129e-03, 1.2274e-03,\n",
      "        5.6505e-04, 2.2545e-03, 2.9182e-03, 2.7142e-03, 2.9469e-03, 5.0011e-03,\n",
      "        3.8166e-03, 3.8433e-03, 7.9250e-04, 1.5259e-03, 9.6321e-04, 4.6301e-04,\n",
      "        9.1743e-04, 9.2840e-04, 1.5068e-03, 2.0180e-03, 1.0519e-03, 7.4863e-04,\n",
      "        6.1226e-03, 1.8625e-03, 1.2465e-03, 2.0123e-03, 2.5921e-03, 3.1872e-03,\n",
      "        1.7138e-03, 4.3755e-03, 4.4060e-03, 1.6642e-03, 4.9019e-03, 4.0741e-03,\n",
      "        1.5364e-03, 6.2561e-04, 2.4090e-03, 9.0313e-04, 1.1282e-03, 1.7309e-03,\n",
      "        5.4359e-04, 9.2459e-04, 1.1396e-03, 6.9237e-04, 4.8709e-04, 3.3283e-04,\n",
      "        6.5041e-04, 6.1572e-05, 3.9148e-04, 1.0357e-03, 4.2496e-03, 3.8128e-03,\n",
      "        1.6804e-03, 2.9202e-03, 6.6986e-03, 2.4014e-03, 2.8515e-03, 1.5173e-03,\n",
      "        8.0013e-04, 1.9512e-03, 5.3940e-03, 1.4181e-03, 2.5005e-03, 1.4038e-03,\n",
      "        8.5983e-03, 2.0935e-02, 5.1003e-03, 7.7667e-03, 1.7958e-03, 3.9406e-03,\n",
      "        7.3586e-03, 8.9417e-03, 4.9515e-03, 2.8667e-03, 5.3368e-03, 4.5357e-03,\n",
      "        3.9787e-03, 6.1684e-03, 5.8708e-03, 6.3782e-03, 2.9125e-03, 6.6299e-03,\n",
      "        9.4757e-03, 5.0964e-03, 4.5166e-03, 1.4267e-02, 1.3252e-02, 1.0796e-02,\n",
      "        2.1973e-02, 9.3384e-03, 1.5297e-02, 7.8201e-03, 7.1564e-03, 7.9727e-03,\n",
      "        5.3024e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [110] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [110] : torch.Size([1, 32, 1, 182])\n",
      "Last layer attentions for generated token [110] : tensor([2.4475e-01, 2.4426e-01, 8.5533e-05, 1.5616e-04, 4.5061e-05, 9.2363e-04,\n",
      "        1.3351e-04, 3.0041e-04, 1.7214e-04, 2.7728e-04, 5.1498e-04, 1.4722e-04,\n",
      "        1.5247e-04, 2.5582e-04, 4.0436e-03, 3.5820e-03, 3.3450e-04, 6.4230e-04,\n",
      "        2.3663e-04, 1.7560e-04, 4.1544e-05, 6.0081e-04, 9.4223e-03, 1.6823e-03,\n",
      "        1.6651e-03, 5.4703e-03, 3.8958e-04, 1.9658e-04, 1.7834e-04, 1.9658e-04,\n",
      "        3.0398e-04, 5.2309e-04, 6.3705e-04, 3.1304e-04, 4.7636e-04, 4.0507e-04,\n",
      "        1.3995e-04, 5.2214e-04, 2.4283e-04, 2.1553e-04, 1.1950e-03, 6.2132e-04,\n",
      "        2.9802e-04, 6.9714e-04, 4.6897e-04, 3.8505e-04, 2.2674e-04, 2.5988e-04,\n",
      "        3.8958e-04, 6.6519e-04, 8.0681e-04, 1.0198e-04, 1.2045e-03, 1.0910e-02,\n",
      "        1.1654e-03, 4.6158e-03, 2.7299e-04, 2.1992e-03, 1.8854e-03, 1.4000e-03,\n",
      "        1.6193e-03, 1.1005e-03, 4.9133e-03, 1.6556e-03, 1.9150e-03, 1.9836e-03,\n",
      "        2.0161e-03, 8.1015e-04, 1.5392e-03, 1.0513e-02, 5.3940e-03, 1.0834e-02,\n",
      "        3.1643e-03, 2.0370e-03, 1.0262e-03, 2.9087e-03, 3.6454e-04, 2.2430e-03,\n",
      "        1.1721e-03, 2.4529e-03, 2.5787e-03, 1.3885e-03, 5.3368e-03, 1.0281e-03,\n",
      "        9.9850e-04, 1.3437e-03, 7.6389e-04, 5.1117e-04, 9.8896e-04, 9.0361e-04,\n",
      "        3.7599e-04, 1.8797e-03, 1.4219e-03, 1.8225e-03, 3.4084e-03, 2.5997e-03,\n",
      "        2.8477e-03, 3.9101e-03, 8.7786e-04, 1.5421e-03, 9.5272e-04, 3.9172e-04,\n",
      "        7.9298e-04, 8.2922e-04, 1.2693e-03, 1.8930e-03, 8.8263e-04, 5.3549e-04,\n",
      "        4.4823e-03, 1.3838e-03, 6.3467e-04, 1.1377e-03, 2.2068e-03, 2.6417e-03,\n",
      "        7.8058e-04, 2.9202e-03, 3.4275e-03, 1.5974e-03, 3.6945e-03, 4.8103e-03,\n",
      "        1.7996e-03, 5.6648e-04, 2.5272e-03, 1.1311e-03, 1.5783e-03, 1.7939e-03,\n",
      "        4.7994e-04, 9.8705e-04, 1.3514e-03, 7.1764e-04, 7.5960e-04, 4.4298e-04,\n",
      "        9.0027e-04, 9.2506e-05, 5.2214e-04, 6.0463e-04, 2.8248e-03, 3.3417e-03,\n",
      "        1.3018e-03, 2.4757e-03, 3.6945e-03, 2.2659e-03, 3.0041e-03, 1.6022e-03,\n",
      "        1.1673e-03, 1.8654e-03, 7.1754e-03, 1.6174e-03, 2.1992e-03, 1.5526e-03,\n",
      "        8.9569e-03, 1.8066e-02, 7.1678e-03, 5.9586e-03, 1.7815e-03, 5.8403e-03,\n",
      "        6.7978e-03, 5.0240e-03, 3.4885e-03, 2.8248e-03, 6.6833e-03, 3.8757e-03,\n",
      "        3.1853e-03, 8.1100e-03, 6.0692e-03, 9.6436e-03, 1.9474e-03, 6.0730e-03,\n",
      "        6.3210e-03, 3.8986e-03, 3.6564e-03, 2.0493e-02, 1.2985e-02, 9.6664e-03,\n",
      "        3.0548e-02, 8.2092e-03, 1.3100e-02, 9.3918e-03, 1.0284e-02, 1.0231e-02,\n",
      "        4.3030e-03, 4.9782e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [111] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [111] : torch.Size([1, 32, 1, 183])\n",
      "Last layer attentions for generated token [111] : tensor([1.4771e-01, 1.4771e-01, 9.3997e-05, 2.1017e-04, 9.2149e-05, 1.8139e-03,\n",
      "        2.6107e-04, 2.4915e-04, 3.8815e-04, 5.8937e-04, 1.4372e-03, 2.7728e-04,\n",
      "        4.8018e-04, 6.1417e-04, 1.0956e-02, 8.0185e-03, 4.8876e-04, 1.1930e-03,\n",
      "        2.0850e-04, 4.0030e-04, 3.5286e-05, 3.3379e-04, 6.0692e-03, 1.8072e-03,\n",
      "        5.2681e-03, 1.0132e-02, 1.3494e-03, 5.0831e-04, 3.1233e-04, 2.3675e-04,\n",
      "        2.2721e-04, 1.2283e-03, 9.4938e-04, 5.9509e-04, 5.1928e-04, 8.6260e-04,\n",
      "        3.5477e-04, 5.7125e-04, 3.8433e-04, 2.1136e-04, 3.4733e-03, 1.7242e-03,\n",
      "        7.4053e-04, 1.8206e-03, 8.9884e-04, 6.2990e-04, 1.6236e-04, 1.6689e-04,\n",
      "        4.3964e-04, 1.8549e-03, 1.5621e-03, 1.1927e-04, 1.7443e-03, 4.2686e-03,\n",
      "        2.6932e-03, 5.2490e-03, 3.5954e-04, 1.6870e-03, 2.6646e-03, 1.0986e-03,\n",
      "        1.7090e-03, 1.3838e-03, 2.3651e-03, 1.7309e-03, 8.1062e-04, 7.9298e-04,\n",
      "        2.7790e-03, 1.2217e-03, 1.4544e-03, 6.6528e-03, 7.2212e-03, 1.9318e-02,\n",
      "        5.8937e-03, 2.5558e-03, 1.4362e-03, 6.0997e-03, 3.8815e-04, 2.5558e-03,\n",
      "        1.2121e-03, 3.0937e-03, 7.4310e-03, 2.4357e-03, 9.9258e-03, 4.4479e-03,\n",
      "        3.0308e-03, 3.3779e-03, 1.7242e-03, 2.4796e-03, 4.1504e-03, 2.5864e-03,\n",
      "        6.1893e-04, 3.5400e-03, 3.2501e-03, 4.7531e-03, 6.0539e-03, 4.1084e-03,\n",
      "        4.5357e-03, 7.2708e-03, 2.0237e-03, 1.8930e-03, 1.4000e-03, 1.9474e-03,\n",
      "        2.9297e-03, 1.7138e-03, 1.9588e-03, 1.9531e-03, 2.4242e-03, 1.4267e-03,\n",
      "        8.0414e-03, 1.9970e-03, 6.4230e-04, 2.4414e-03, 2.3823e-03, 1.6909e-03,\n",
      "        6.1655e-04, 3.8052e-03, 3.1223e-03, 4.0030e-04, 3.3989e-03, 4.8676e-03,\n",
      "        2.0657e-03, 8.0585e-04, 4.2152e-03, 2.3193e-03, 1.5306e-03, 2.5158e-03,\n",
      "        4.5991e-04, 1.2093e-03, 2.1496e-03, 1.2217e-03, 1.0691e-03, 8.2970e-04,\n",
      "        1.2140e-03, 1.2553e-04, 4.6897e-04, 7.2384e-04, 4.7493e-03, 1.0223e-02,\n",
      "        2.8362e-03, 3.1033e-03, 5.3711e-03, 2.9736e-03, 6.2447e-03, 1.7138e-03,\n",
      "        1.1978e-03, 1.9970e-03, 5.3329e-03, 1.9569e-03, 2.5177e-03, 8.8835e-04,\n",
      "        9.8038e-03, 3.6835e-02, 8.5373e-03, 1.3153e-02, 4.8180e-03, 4.2953e-03,\n",
      "        2.0309e-02, 1.0971e-02, 6.9923e-03, 3.9902e-03, 5.4321e-03, 2.9354e-03,\n",
      "        4.9438e-03, 8.1177e-03, 7.1182e-03, 6.2294e-03, 1.0262e-03, 7.1030e-03,\n",
      "        1.7487e-02, 1.3542e-02, 1.3901e-02, 2.9251e-02, 2.3712e-02, 1.3802e-02,\n",
      "        1.2871e-02, 6.8855e-03, 1.4351e-02, 5.8022e-03, 6.6757e-03, 6.6147e-03,\n",
      "        3.2024e-03, 8.1348e-04, 6.6032e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [112] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [112] : torch.Size([1, 32, 1, 184])\n",
      "Last layer attentions for generated token [112] : tensor([1.6736e-01, 1.6699e-01, 7.3075e-05, 1.3816e-04, 6.6519e-05, 1.5697e-03,\n",
      "        2.8729e-04, 2.4331e-04, 2.9182e-04, 4.9734e-04, 1.7548e-03, 2.3043e-04,\n",
      "        3.2997e-04, 5.0020e-04, 7.3013e-03, 7.8278e-03, 2.4676e-04, 8.2636e-04,\n",
      "        1.7524e-04, 2.8396e-04, 3.8981e-05, 2.0695e-04, 5.6572e-03, 1.0185e-03,\n",
      "        3.7613e-03, 1.2375e-02, 9.9277e-04, 4.0364e-04, 1.8728e-04, 2.1470e-04,\n",
      "        2.7943e-04, 5.9175e-04, 6.5374e-04, 4.5204e-04, 3.0947e-04, 4.2200e-04,\n",
      "        2.7728e-04, 4.7922e-04, 1.9395e-04, 1.2970e-04, 1.7443e-03, 9.9850e-04,\n",
      "        3.9339e-04, 7.7009e-04, 6.2275e-04, 3.3188e-04, 2.0051e-04, 1.7524e-04,\n",
      "        3.6240e-04, 1.5488e-03, 1.0548e-03, 1.1587e-04, 1.4830e-03, 3.2501e-03,\n",
      "        1.5697e-03, 3.7441e-03, 2.4009e-04, 1.0042e-03, 1.5278e-03, 6.6948e-04,\n",
      "        7.8726e-04, 9.1839e-04, 1.6584e-03, 1.1139e-03, 6.2513e-04, 6.4754e-04,\n",
      "        1.5011e-03, 1.0929e-03, 9.4032e-04, 6.1378e-03, 5.5199e-03, 1.5930e-02,\n",
      "        4.1542e-03, 2.5234e-03, 1.2026e-03, 7.6141e-03, 3.8743e-04, 2.0752e-03,\n",
      "        2.1095e-03, 3.1013e-03, 5.1308e-03, 3.0937e-03, 7.2021e-03, 3.8986e-03,\n",
      "        3.4695e-03, 4.7760e-03, 1.5154e-03, 2.0599e-03, 2.9678e-03, 2.0027e-03,\n",
      "        7.1812e-04, 2.7905e-03, 2.7199e-03, 3.9978e-03, 4.5166e-03, 4.2191e-03,\n",
      "        4.8981e-03, 6.5155e-03, 1.7967e-03, 2.8725e-03, 1.1749e-03, 1.7395e-03,\n",
      "        3.1624e-03, 2.4986e-03, 2.8076e-03, 3.3855e-03, 2.0828e-03, 1.4153e-03,\n",
      "        9.4452e-03, 1.8826e-03, 5.7125e-04, 2.6264e-03, 2.1095e-03, 1.7090e-03,\n",
      "        4.1556e-04, 2.9125e-03, 3.8719e-03, 4.1223e-04, 4.3869e-03, 5.1498e-03,\n",
      "        1.7109e-03, 6.2847e-04, 3.8986e-03, 3.2520e-03, 2.4910e-03, 3.3951e-03,\n",
      "        5.6791e-04, 1.4219e-03, 3.1338e-03, 1.3390e-03, 1.2188e-03, 9.7179e-04,\n",
      "        1.5850e-03, 1.1677e-04, 5.4932e-04, 1.0262e-03, 6.5308e-03, 9.7275e-03,\n",
      "        3.1872e-03, 3.7575e-03, 6.1417e-03, 2.9240e-03, 6.2675e-03, 1.7872e-03,\n",
      "        9.0599e-04, 1.1816e-03, 5.7487e-03, 1.6479e-03, 2.0123e-03, 9.9468e-04,\n",
      "        1.1597e-02, 3.7567e-02, 9.1019e-03, 1.4389e-02, 3.9673e-03, 3.8834e-03,\n",
      "        1.6800e-02, 8.6288e-03, 5.4970e-03, 4.3640e-03, 4.5395e-03, 2.6188e-03,\n",
      "        4.6196e-03, 7.3853e-03, 5.9738e-03, 4.9019e-03, 7.2527e-04, 6.2103e-03,\n",
      "        1.5884e-02, 1.0849e-02, 1.3580e-02, 3.7506e-02, 1.3329e-02, 1.5900e-02,\n",
      "        9.3689e-03, 5.3749e-03, 1.0818e-02, 6.3515e-03, 7.5073e-03, 7.7515e-03,\n",
      "        3.9368e-03, 1.0862e-03, 7.9269e-03, 7.5760e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [113] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [113] : torch.Size([1, 32, 1, 185])\n",
      "Last layer attentions for generated token [113] : tensor([1.2952e-01, 1.2915e-01, 8.8096e-05, 2.0802e-04, 7.3612e-05, 2.7027e-03,\n",
      "        2.1505e-04, 2.2984e-04, 4.5705e-04, 3.6860e-04, 1.5383e-03, 2.5129e-04,\n",
      "        2.3568e-04, 3.5667e-04, 9.2392e-03, 7.9193e-03, 3.9935e-04, 4.5085e-04,\n",
      "        1.6773e-04, 2.3198e-04, 6.1750e-05, 4.2605e-04, 6.8474e-03, 2.2755e-03,\n",
      "        5.6038e-03, 1.8661e-02, 1.3351e-03, 6.0415e-04, 2.9397e-04, 5.3740e-04,\n",
      "        5.2595e-04, 2.0695e-03, 7.4768e-04, 6.0797e-04, 4.7064e-04, 7.9107e-04,\n",
      "        1.4126e-04, 8.9455e-04, 2.7490e-04, 1.5819e-04, 2.6836e-03, 9.8228e-04,\n",
      "        3.1829e-04, 1.0576e-03, 8.2254e-04, 3.6502e-04, 5.0497e-04, 1.5974e-04,\n",
      "        3.6502e-04, 2.0332e-03, 1.0195e-03, 1.6069e-04, 1.4105e-03, 3.9291e-03,\n",
      "        1.0481e-03, 4.3907e-03, 3.5095e-04, 1.5144e-03, 2.1992e-03, 1.0338e-03,\n",
      "        1.0214e-03, 1.8559e-03, 3.9520e-03, 1.9178e-03, 8.1635e-04, 1.2245e-03,\n",
      "        1.5135e-03, 1.3170e-03, 1.1730e-03, 9.2926e-03, 1.0391e-02, 2.3804e-02,\n",
      "        5.0316e-03, 2.9488e-03, 9.4509e-04, 5.8823e-03, 2.8753e-04, 1.4229e-03,\n",
      "        2.8191e-03, 2.3594e-03, 4.3488e-03, 3.5763e-03, 1.1787e-02, 3.4924e-03,\n",
      "        4.7455e-03, 6.2256e-03, 1.3380e-03, 1.5793e-03, 3.2177e-03, 2.5158e-03,\n",
      "        7.8344e-04, 2.6627e-03, 2.2259e-03, 5.1689e-03, 4.7989e-03, 5.3482e-03,\n",
      "        9.2163e-03, 8.4763e-03, 1.8024e-03, 3.1757e-03, 1.0710e-03, 1.3351e-03,\n",
      "        4.4250e-03, 3.5152e-03, 3.4332e-03, 2.8610e-03, 1.4439e-03, 8.2445e-04,\n",
      "        8.3313e-03, 1.5907e-03, 4.5085e-04, 2.8782e-03, 2.1954e-03, 1.6565e-03,\n",
      "        3.4904e-04, 4.8294e-03, 2.8133e-03, 3.6645e-04, 6.0768e-03, 4.6272e-03,\n",
      "        7.1621e-04, 5.1165e-04, 3.6659e-03, 2.4223e-03, 2.6550e-03, 3.7956e-03,\n",
      "        7.1907e-04, 1.9608e-03, 3.0251e-03, 1.2827e-03, 1.7014e-03, 8.4209e-04,\n",
      "        2.1496e-03, 8.8096e-05, 6.7282e-04, 1.1997e-03, 9.7733e-03, 1.0666e-02,\n",
      "        3.1471e-03, 4.2267e-03, 7.3853e-03, 2.4242e-03, 6.1531e-03, 1.6165e-03,\n",
      "        7.7438e-04, 7.0238e-04, 9.0103e-03, 1.2398e-03, 1.4229e-03, 1.5488e-03,\n",
      "        2.9770e-02, 4.9194e-02, 9.7885e-03, 1.2833e-02, 2.7637e-03, 3.8719e-03,\n",
      "        1.4809e-02, 4.9591e-03, 6.6223e-03, 4.6196e-03, 2.6760e-03, 1.4372e-03,\n",
      "        7.1945e-03, 6.6109e-03, 7.9727e-03, 5.5428e-03, 5.7983e-04, 1.1139e-02,\n",
      "        1.5167e-02, 8.4686e-03, 1.2520e-02, 2.7191e-02, 6.4354e-03, 1.5358e-02,\n",
      "        7.7515e-03, 4.2000e-03, 6.4735e-03, 7.0839e-03, 5.4169e-03, 9.1019e-03,\n",
      "        3.9024e-03, 7.9727e-04, 9.6588e-03, 6.7902e-03, 8.8043e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [114] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [114] : torch.Size([1, 32, 1, 186])\n",
      "Last layer attentions for generated token [114] : tensor([2.0032e-01, 1.9995e-01, 2.1994e-04, 3.3665e-04, 1.5175e-04, 2.7294e-03,\n",
      "        1.8954e-04, 2.6107e-04, 5.0116e-04, 4.9543e-04, 1.1320e-03, 2.6274e-04,\n",
      "        2.4915e-04, 6.2132e-04, 6.7062e-03, 5.5885e-03, 6.9189e-04, 5.2977e-04,\n",
      "        1.7595e-04, 3.3259e-04, 6.7353e-05, 6.1417e-04, 1.1681e-02, 2.3403e-03,\n",
      "        4.1962e-03, 1.3588e-02, 1.5993e-03, 5.7364e-04, 1.9026e-04, 3.4809e-04,\n",
      "        3.7694e-04, 1.1082e-03, 5.6362e-04, 4.5466e-04, 4.2892e-04, 6.9475e-04,\n",
      "        2.6011e-04, 1.0014e-03, 3.4809e-04, 1.6928e-04, 2.5387e-03, 1.2074e-03,\n",
      "        6.2513e-04, 1.0977e-03, 7.5245e-04, 2.5964e-04, 5.0926e-04, 1.9789e-04,\n",
      "        3.7265e-04, 1.8682e-03, 1.2608e-03, 4.2725e-04, 2.8248e-03, 8.4229e-03,\n",
      "        1.6308e-03, 4.6463e-03, 4.1652e-04, 1.0223e-03, 1.5001e-03, 8.6451e-04,\n",
      "        7.7820e-04, 1.4877e-03, 3.1662e-03, 1.4620e-03, 8.4782e-04, 1.0738e-03,\n",
      "        1.1215e-03, 2.6894e-03, 1.4172e-03, 1.2924e-02, 1.1398e-02, 1.4046e-02,\n",
      "        3.6793e-03, 2.5654e-03, 1.0052e-03, 6.3057e-03, 8.5306e-04, 2.4567e-03,\n",
      "        2.3613e-03, 2.8877e-03, 4.4136e-03, 2.3575e-03, 6.4087e-03, 2.0294e-03,\n",
      "        3.8147e-03, 3.9291e-03, 1.0977e-03, 7.6294e-04, 2.2297e-03, 1.0862e-03,\n",
      "        5.5838e-04, 1.9388e-03, 7.7963e-04, 1.8196e-03, 2.8038e-03, 3.2024e-03,\n",
      "        3.5324e-03, 4.4594e-03, 1.4229e-03, 1.4420e-03, 8.7309e-04, 6.3515e-04,\n",
      "        3.4027e-03, 2.0351e-03, 2.2144e-03, 2.0180e-03, 6.8140e-04, 5.0926e-04,\n",
      "        3.5515e-03, 8.7833e-04, 2.6464e-04, 1.1082e-03, 1.0738e-03, 1.2341e-03,\n",
      "        2.5105e-04, 1.8559e-03, 2.0561e-03, 3.4118e-04, 3.7117e-03, 3.8643e-03,\n",
      "        6.0940e-04, 5.4407e-04, 2.1763e-03, 1.4677e-03, 1.2674e-03, 2.3613e-03,\n",
      "        3.9816e-04, 1.2102e-03, 1.7586e-03, 8.8692e-04, 9.3842e-04, 7.5245e-04,\n",
      "        1.4849e-03, 1.6725e-04, 6.7711e-04, 8.7118e-04, 5.7831e-03, 8.6823e-03,\n",
      "        2.2659e-03, 3.3741e-03, 3.9940e-03, 1.7900e-03, 6.0234e-03, 1.3523e-03,\n",
      "        7.4387e-04, 8.3780e-04, 6.2637e-03, 1.3733e-03, 1.5936e-03, 1.3494e-03,\n",
      "        1.8616e-02, 3.5797e-02, 8.1558e-03, 1.0559e-02, 2.6608e-03, 3.0174e-03,\n",
      "        1.3023e-02, 2.5311e-03, 3.6469e-03, 3.5458e-03, 2.8591e-03, 8.9550e-04,\n",
      "        4.2686e-03, 6.0425e-03, 7.5874e-03, 5.1270e-03, 3.1686e-04, 4.8828e-03,\n",
      "        1.0345e-02, 5.3787e-03, 1.2878e-02, 2.6871e-02, 4.4746e-03, 1.1398e-02,\n",
      "        5.4474e-03, 3.1281e-03, 2.9202e-03, 5.0087e-03, 4.5891e-03, 8.7357e-03,\n",
      "        2.6703e-03, 6.7329e-04, 7.5378e-03, 5.7449e-03, 5.8899e-03, 2.8152e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [115] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [115] : torch.Size([1, 32, 1, 187])\n",
      "Last layer attentions for generated token [115] : tensor([1.7871e-01, 1.7834e-01, 1.9193e-04, 4.8828e-04, 1.3506e-04, 2.4319e-03,\n",
      "        2.2757e-04, 2.1839e-04, 3.8242e-04, 4.5872e-04, 8.7547e-04, 3.5715e-04,\n",
      "        4.3941e-04, 6.7806e-04, 1.1497e-02, 8.0338e-03, 6.2323e-04, 8.0490e-04,\n",
      "        2.2614e-04, 4.3440e-04, 1.8036e-04, 1.1024e-03, 9.1019e-03, 1.9836e-03,\n",
      "        9.7809e-03, 1.5808e-02, 3.1013e-03, 1.1559e-03, 4.5872e-04, 4.4274e-04,\n",
      "        6.2227e-04, 2.0447e-03, 8.9979e-04, 4.7970e-04, 5.5981e-04, 9.3031e-04,\n",
      "        3.0851e-04, 1.1854e-03, 5.5981e-04, 3.0446e-04, 5.2528e-03, 2.3136e-03,\n",
      "        8.9264e-04, 1.4868e-03, 6.5327e-04, 5.6982e-04, 2.1589e-04, 2.1875e-04,\n",
      "        3.6931e-04, 3.8528e-03, 2.5253e-03, 3.3498e-04, 1.7843e-03, 8.8272e-03,\n",
      "        1.7500e-03, 5.9242e-03, 1.2064e-03, 2.6627e-03, 4.1389e-03, 1.4381e-03,\n",
      "        1.6928e-03, 1.2569e-03, 3.6869e-03, 2.8210e-03, 1.7824e-03, 1.2760e-03,\n",
      "        3.3627e-03, 1.9741e-03, 2.3479e-03, 1.0956e-02, 9.7275e-03, 2.3178e-02,\n",
      "        5.6915e-03, 2.6112e-03, 1.0157e-03, 2.8286e-03, 4.5419e-04, 1.3123e-03,\n",
      "        9.4318e-04, 2.5558e-03, 2.4815e-03, 3.0346e-03, 1.5129e-02, 5.1346e-03,\n",
      "        3.6049e-03, 2.7428e-03, 2.2526e-03, 1.6794e-03, 2.5806e-03, 1.3885e-03,\n",
      "        5.7554e-04, 2.7580e-03, 1.4439e-03, 2.8381e-03, 4.7493e-03, 2.5635e-03,\n",
      "        3.1910e-03, 8.4991e-03, 2.3785e-03, 1.5125e-03, 1.5221e-03, 1.1625e-03,\n",
      "        2.3689e-03, 1.4725e-03, 1.3857e-03, 1.5583e-03, 1.0233e-03, 6.0654e-04,\n",
      "        5.2071e-03, 2.2259e-03, 9.1028e-04, 1.9445e-03, 1.5402e-03, 1.9703e-03,\n",
      "        8.8406e-04, 3.3531e-03, 2.8763e-03, 8.5211e-04, 2.9526e-03, 6.2943e-03,\n",
      "        2.8210e-03, 8.4352e-04, 2.6493e-03, 1.8253e-03, 1.5459e-03, 2.4395e-03,\n",
      "        5.2595e-04, 7.6389e-04, 1.1940e-03, 6.1464e-04, 5.6314e-04, 8.3876e-04,\n",
      "        1.0481e-03, 2.7990e-04, 6.5565e-04, 1.0176e-03, 3.0804e-03, 6.2523e-03,\n",
      "        1.7700e-03, 1.6184e-03, 1.6108e-03, 1.4896e-03, 2.5387e-03, 8.9788e-04,\n",
      "        6.4802e-04, 7.8630e-04, 3.5381e-03, 1.1377e-03, 1.8091e-03, 9.7656e-04,\n",
      "        1.0208e-02, 4.2175e-02, 1.0818e-02, 2.0416e-02, 4.8561e-03, 3.8300e-03,\n",
      "        1.0895e-02, 5.0468e-03, 7.1259e-03, 3.3798e-03, 3.5553e-03, 2.1839e-03,\n",
      "        3.1166e-03, 4.6844e-03, 3.5629e-03, 2.9793e-03, 5.8317e-04, 4.0207e-03,\n",
      "        8.7891e-03, 6.6109e-03, 8.4457e-03, 1.4473e-02, 5.0392e-03, 6.7635e-03,\n",
      "        7.8201e-03, 3.2558e-03, 5.2795e-03, 3.4657e-03, 4.3488e-03, 3.6335e-03,\n",
      "        2.7905e-03, 1.3323e-03, 4.3678e-03, 4.1504e-03, 3.9635e-03, 2.0279e-02,\n",
      "        1.6953e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [116] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [116] : torch.Size([1, 32, 1, 188])\n",
      "Last layer attentions for generated token [116] : tensor([1.5173e-01, 1.5149e-01, 5.6446e-05, 1.8120e-04, 5.7340e-05, 9.4366e-04,\n",
      "        1.6725e-04, 1.6820e-04, 3.7313e-04, 2.6512e-04, 6.6376e-04, 2.5940e-04,\n",
      "        1.1677e-04, 3.1734e-04, 5.1422e-03, 3.5496e-03, 2.4629e-04, 3.0947e-04,\n",
      "        7.0274e-05, 1.2189e-04, 5.4300e-05, 1.4365e-04, 2.5101e-03, 5.5933e-04,\n",
      "        3.7842e-03, 8.8730e-03, 1.0967e-03, 3.7837e-04, 1.9360e-04, 2.4819e-04,\n",
      "        3.1972e-04, 7.8869e-04, 5.8460e-04, 4.3464e-04, 3.8576e-04, 2.5797e-04,\n",
      "        1.8907e-04, 4.9734e-04, 1.4532e-04, 1.0967e-04, 2.8839e-03, 7.9012e-04,\n",
      "        4.6349e-04, 4.6349e-04, 4.6349e-04, 1.6117e-04, 2.8610e-04, 8.5711e-05,\n",
      "        1.9026e-04, 1.8673e-03, 6.5374e-04, 2.4629e-04, 1.6050e-03, 3.8147e-03,\n",
      "        9.8515e-04, 1.5373e-03, 3.0947e-04, 5.9748e-04, 1.2188e-03, 5.4407e-04,\n",
      "        4.5180e-04, 7.5960e-04, 1.5497e-03, 4.7731e-04, 3.9792e-04, 4.6539e-04,\n",
      "        5.3549e-04, 1.1225e-03, 9.1124e-04, 9.4223e-03, 6.4659e-03, 9.5901e-03,\n",
      "        5.7144e-03, 3.7289e-03, 1.1740e-03, 5.3558e-03, 3.0279e-04, 9.4366e-04,\n",
      "        1.6985e-03, 1.8654e-03, 3.4809e-03, 3.9368e-03, 6.3896e-03, 1.0330e-02,\n",
      "        5.9090e-03, 7.4120e-03, 1.6098e-03, 4.2152e-03, 3.2482e-03, 1.6174e-03,\n",
      "        5.5933e-04, 1.8063e-03, 2.1267e-03, 5.9166e-03, 2.4223e-03, 3.9940e-03,\n",
      "        6.0043e-03, 4.9210e-03, 2.1248e-03, 3.2825e-03, 8.0729e-04, 3.2139e-03,\n",
      "        3.4008e-03, 2.7084e-03, 3.1605e-03, 2.4815e-03, 2.3212e-03, 1.0128e-03,\n",
      "        8.1100e-03, 1.1787e-03, 5.2309e-04, 1.9855e-03, 1.2369e-03, 1.1406e-03,\n",
      "        3.8815e-04, 3.9558e-03, 1.7939e-03, 3.8195e-04, 4.7874e-03, 3.7899e-03,\n",
      "        1.1740e-03, 3.7241e-04, 3.3550e-03, 3.1128e-03, 2.8667e-03, 3.7956e-03,\n",
      "        7.5960e-04, 2.0714e-03, 2.6207e-03, 1.1520e-03, 1.9226e-03, 1.1845e-03,\n",
      "        2.8286e-03, 1.6987e-04, 1.1120e-03, 2.3499e-03, 9.9335e-03, 1.1963e-02,\n",
      "        4.8218e-03, 4.6577e-03, 1.0872e-02, 3.2177e-03, 7.3891e-03, 2.3174e-03,\n",
      "        1.4372e-03, 1.2760e-03, 1.1963e-02, 2.1896e-03, 2.3384e-03, 1.9341e-03,\n",
      "        2.7100e-02, 3.4088e-02, 1.1848e-02, 1.3657e-02, 5.7564e-03, 4.4289e-03,\n",
      "        1.3611e-02, 7.9193e-03, 5.8289e-03, 5.3787e-03, 3.9444e-03, 2.4815e-03,\n",
      "        6.1455e-03, 7.6790e-03, 8.7891e-03, 4.7913e-03, 7.1812e-04, 9.8495e-03,\n",
      "        1.1589e-02, 1.8677e-02, 1.5747e-02, 2.3010e-02, 5.6953e-03, 1.2291e-02,\n",
      "        3.4981e-03, 3.9005e-03, 6.0616e-03, 5.6839e-03, 4.9210e-03, 6.4926e-03,\n",
      "        2.2163e-03, 7.9012e-04, 6.9733e-03, 8.7204e-03, 6.0120e-03, 1.6663e-02,\n",
      "        1.1597e-02, 8.7814e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [117] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [117] : torch.Size([1, 32, 1, 189])\n",
      "Last layer attentions for generated token [117] : tensor([2.5220e-01, 2.5122e-01, 9.0241e-05, 2.0421e-04, 4.8876e-05, 1.1959e-03,\n",
      "        1.0508e-04, 1.2779e-04, 1.8156e-04, 1.7130e-04, 3.1805e-04, 7.8738e-05,\n",
      "        1.3077e-04, 2.4247e-04, 4.8409e-03, 3.3302e-03, 1.4091e-04, 2.9063e-04,\n",
      "        6.7592e-05, 8.3804e-05, 2.3544e-05, 2.0814e-04, 5.4283e-03, 8.0585e-04,\n",
      "        4.3411e-03, 9.1095e-03, 9.4032e-04, 3.8362e-04, 1.7810e-04, 1.3602e-04,\n",
      "        3.0398e-04, 4.3893e-04, 2.7323e-04, 2.7895e-04, 1.5056e-04, 2.8014e-04,\n",
      "        1.2732e-04, 2.5010e-04, 1.2827e-04, 1.1450e-04, 1.2951e-03, 9.5510e-04,\n",
      "        2.7895e-04, 4.9162e-04, 3.2878e-04, 1.3602e-04, 1.2094e-04, 9.7215e-05,\n",
      "        2.0182e-04, 1.0147e-03, 9.5892e-04, 1.5903e-04, 1.3657e-03, 4.4632e-03,\n",
      "        9.3317e-04, 2.4014e-03, 2.1994e-04, 8.3637e-04, 1.5841e-03, 5.3692e-04,\n",
      "        5.8174e-04, 6.6423e-04, 9.7227e-04, 5.4407e-04, 3.6478e-04, 3.4523e-04,\n",
      "        9.4795e-04, 8.9359e-04, 9.1124e-04, 1.0956e-02, 7.3967e-03, 1.2428e-02,\n",
      "        3.5706e-03, 1.6899e-03, 8.5640e-04, 2.1706e-03, 2.2161e-04, 1.2217e-03,\n",
      "        1.1635e-03, 2.7027e-03, 3.5419e-03, 1.5440e-03, 9.5291e-03, 4.6959e-03,\n",
      "        3.4256e-03, 2.9144e-03, 1.5659e-03, 1.8444e-03, 3.5458e-03, 1.7738e-03,\n",
      "        3.9983e-04, 1.8301e-03, 1.0862e-03, 2.2411e-03, 2.1858e-03, 1.5717e-03,\n",
      "        1.4200e-03, 5.3215e-03, 1.6155e-03, 1.8301e-03, 8.5640e-04, 1.4143e-03,\n",
      "        2.6703e-03, 1.5297e-03, 2.0676e-03, 1.3790e-03, 1.3704e-03, 9.2411e-04,\n",
      "        3.3398e-03, 9.2745e-04, 3.7837e-04, 1.0405e-03, 1.0757e-03, 6.3372e-04,\n",
      "        1.8513e-04, 1.4257e-03, 1.1959e-03, 2.2602e-04, 1.1034e-03, 3.1815e-03,\n",
      "        1.5717e-03, 8.0919e-04, 3.5458e-03, 2.1610e-03, 1.8778e-03, 2.5616e-03,\n",
      "        5.4979e-04, 9.0599e-04, 1.1702e-03, 7.4387e-04, 1.1034e-03, 7.3242e-04,\n",
      "        1.2150e-03, 1.2529e-04, 5.6458e-04, 7.7200e-04, 2.7351e-03, 7.2212e-03,\n",
      "        2.2602e-03, 2.5272e-03, 1.9112e-03, 2.7637e-03, 5.6305e-03, 7.5865e-04,\n",
      "        5.3453e-04, 7.5436e-04, 3.6297e-03, 1.0290e-03, 1.7500e-03, 8.4114e-04,\n",
      "        5.5084e-03, 3.5919e-02, 9.5978e-03, 8.0490e-03, 2.8210e-03, 5.0468e-03,\n",
      "        6.6223e-03, 4.0016e-03, 3.3264e-03, 1.7881e-03, 2.0180e-03, 1.3447e-03,\n",
      "        2.8763e-03, 5.0507e-03, 4.8256e-03, 2.3136e-03, 3.4332e-04, 3.1719e-03,\n",
      "        6.3438e-03, 8.3313e-03, 8.5526e-03, 2.3346e-02, 6.6223e-03, 7.0038e-03,\n",
      "        6.9160e-03, 3.0556e-03, 4.0855e-03, 4.0321e-03, 5.3444e-03, 3.7441e-03,\n",
      "        2.3174e-03, 8.0919e-04, 4.1504e-03, 6.6872e-03, 5.2452e-03, 1.6098e-02,\n",
      "        8.0261e-03, 1.2543e-02, 8.5602e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [118] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [118] : torch.Size([1, 32, 1, 190])\n",
      "Last layer attentions for generated token [118] : tensor([2.8906e-01, 2.8906e-01, 1.1998e-04, 1.9240e-04, 7.2777e-05, 1.4849e-03,\n",
      "        1.2672e-04, 1.9026e-04, 2.3580e-04, 2.5201e-04, 3.8886e-04, 1.8370e-04,\n",
      "        1.4699e-04, 2.7943e-04, 2.5253e-03, 2.4796e-03, 2.1052e-04, 2.2769e-04,\n",
      "        7.5340e-05, 1.2136e-04, 2.1994e-05, 3.1066e-04, 7.2975e-03, 8.9359e-04,\n",
      "        2.0428e-03, 4.6997e-03, 3.4571e-04, 2.1553e-04, 1.6725e-04, 2.5845e-04,\n",
      "        3.5882e-04, 5.7125e-04, 4.2367e-04, 4.9925e-04, 2.9469e-04, 2.8014e-04,\n",
      "        6.0558e-05, 3.2043e-04, 1.7393e-04, 9.2685e-05, 1.0462e-03, 4.0340e-04,\n",
      "        1.6022e-04, 4.0913e-04, 4.4847e-04, 1.3483e-04, 2.8563e-04, 1.1009e-04,\n",
      "        2.5105e-04, 8.9169e-04, 5.0306e-04, 1.8370e-04, 9.3079e-04, 2.6131e-03,\n",
      "        4.8661e-04, 2.3251e-03, 2.2244e-04, 7.8058e-04, 7.6723e-04, 6.2370e-04,\n",
      "        3.7670e-04, 7.3481e-04, 1.3895e-03, 3.7241e-04, 4.0197e-04, 2.9516e-04,\n",
      "        4.9639e-04, 8.5402e-04, 3.8362e-04, 7.7095e-03, 3.9635e-03, 8.3847e-03,\n",
      "        2.0924e-03, 2.2831e-03, 9.5844e-04, 5.6076e-03, 4.9734e-04, 1.2751e-03,\n",
      "        1.3971e-03, 1.3075e-03, 2.0275e-03, 1.0900e-03, 3.6430e-03, 2.4853e-03,\n",
      "        4.5090e-03, 3.4828e-03, 1.0710e-03, 1.2474e-03, 2.6112e-03, 2.1191e-03,\n",
      "        9.1267e-04, 2.9354e-03, 1.5259e-03, 2.3651e-03, 2.0008e-03, 2.5368e-03,\n",
      "        3.1281e-03, 3.2845e-03, 1.0777e-03, 1.4992e-03, 4.6444e-04, 9.0933e-04,\n",
      "        1.8082e-03, 1.5774e-03, 4.1161e-03, 2.0733e-03, 1.9817e-03, 1.1520e-03,\n",
      "        2.7084e-03, 5.2023e-04, 1.4877e-04, 8.4257e-04, 9.5463e-04, 8.9169e-04,\n",
      "        1.8799e-04, 1.2455e-03, 8.8120e-04, 3.6597e-04, 1.5163e-03, 1.9531e-03,\n",
      "        5.5456e-04, 2.7776e-04, 1.3208e-03, 1.2236e-03, 1.6956e-03, 2.2640e-03,\n",
      "        5.7459e-04, 1.1425e-03, 1.2188e-03, 7.8535e-04, 5.9414e-04, 3.9792e-04,\n",
      "        6.3467e-04, 4.4286e-05, 3.6025e-04, 1.0386e-03, 2.5692e-03, 4.0741e-03,\n",
      "        2.1305e-03, 3.9043e-03, 5.4016e-03, 2.0752e-03, 6.4468e-03, 1.6727e-03,\n",
      "        6.3992e-04, 6.5613e-04, 3.8052e-03, 9.2173e-04, 1.1768e-03, 1.0929e-03,\n",
      "        6.7368e-03, 1.9135e-02, 4.5166e-03, 4.5357e-03, 1.7490e-03, 3.2578e-03,\n",
      "        7.4654e-03, 2.9316e-03, 2.8763e-03, 1.6527e-03, 2.9087e-03, 8.6927e-04,\n",
      "        2.7065e-03, 5.8594e-03, 7.4196e-03, 2.5921e-03, 2.7037e-04, 2.1992e-03,\n",
      "        5.1956e-03, 4.7836e-03, 9.3613e-03, 1.8066e-02, 4.5013e-03, 7.6256e-03,\n",
      "        5.6038e-03, 2.5578e-03, 2.5024e-03, 3.0136e-03, 3.9864e-03, 5.7030e-03,\n",
      "        1.5621e-03, 7.8535e-04, 5.4321e-03, 6.1722e-03, 5.9128e-03, 1.6296e-02,\n",
      "        7.1449e-03, 8.6441e-03, 6.7902e-03, 5.0316e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [119] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [119] : torch.Size([1, 32, 1, 191])\n",
      "Last layer attentions for generated token [119] : tensor([1.1853e-01, 1.1853e-01, 7.5758e-05, 2.4486e-04, 4.8697e-05, 1.5869e-03,\n",
      "        7.7248e-05, 8.0287e-05, 2.2519e-04, 2.6083e-04, 6.3896e-04, 1.7130e-04,\n",
      "        2.1958e-04, 5.7411e-04, 5.3596e-03, 4.7874e-03, 2.3425e-04, 4.0388e-04,\n",
      "        1.0979e-04, 1.5485e-04, 6.7651e-05, 3.4285e-04, 4.1924e-03, 8.9979e-04,\n",
      "        3.7746e-03, 7.8888e-03, 6.5041e-04, 3.5930e-04, 2.4354e-04, 2.4939e-04,\n",
      "        7.5626e-04, 1.1225e-03, 4.9782e-04, 2.9850e-04, 2.3282e-04, 5.1355e-04,\n",
      "        1.1826e-04, 4.2152e-04, 2.8586e-04, 1.2636e-04, 3.8433e-03, 1.5554e-03,\n",
      "        6.0177e-04, 6.3419e-04, 3.1161e-04, 1.9574e-04, 1.3137e-04, 1.4889e-04,\n",
      "        2.9731e-04, 2.6360e-03, 1.7042e-03, 1.4257e-04, 1.1797e-03, 3.9024e-03,\n",
      "        1.5574e-03, 3.2883e-03, 3.9220e-04, 1.2455e-03, 1.1606e-03, 6.2943e-04,\n",
      "        6.2943e-04, 9.7466e-04, 1.8635e-03, 6.9809e-04, 4.4107e-04, 5.5981e-04,\n",
      "        1.4801e-03, 9.0313e-04, 8.4496e-04, 7.7667e-03, 6.3591e-03, 1.4595e-02,\n",
      "        3.5458e-03, 3.4084e-03, 9.4843e-04, 4.0054e-03, 6.0654e-04, 1.3180e-03,\n",
      "        2.0714e-03, 4.5319e-03, 5.1956e-03, 3.5954e-03, 1.1726e-02, 6.3400e-03,\n",
      "        5.1956e-03, 5.8250e-03, 3.1471e-03, 2.1324e-03, 3.0518e-03, 1.6098e-03,\n",
      "        7.6199e-04, 3.5419e-03, 1.6708e-03, 3.3226e-03, 5.3368e-03, 2.7771e-03,\n",
      "        3.6964e-03, 7.7515e-03, 2.5845e-03, 1.9836e-03, 1.7109e-03, 1.6193e-03,\n",
      "        1.9245e-03, 1.7395e-03, 2.2259e-03, 1.7109e-03, 1.0271e-03, 7.8440e-04,\n",
      "        4.1389e-03, 2.2888e-03, 7.6628e-04, 2.3842e-03, 1.8349e-03, 1.9159e-03,\n",
      "        5.9795e-04, 2.7866e-03, 3.5095e-03, 5.9462e-04, 3.5667e-03, 7.0839e-03,\n",
      "        1.3905e-03, 4.1914e-04, 2.4261e-03, 1.8444e-03, 2.2125e-03, 4.4098e-03,\n",
      "        9.5940e-04, 1.2283e-03, 1.4858e-03, 9.0837e-04, 1.0357e-03, 9.3889e-04,\n",
      "        1.0595e-03, 1.7917e-04, 5.5313e-04, 6.8569e-04, 2.7981e-03, 6.7482e-03,\n",
      "        2.5520e-03, 2.3556e-03, 4.6005e-03, 2.7275e-03, 7.1831e-03, 2.0466e-03,\n",
      "        2.6226e-03, 1.5259e-03, 1.0193e-02, 2.0313e-03, 3.2425e-03, 2.8038e-03,\n",
      "        2.2552e-02, 5.8655e-02, 1.1261e-02, 2.8259e-02, 7.3204e-03, 6.7177e-03,\n",
      "        1.9012e-02, 8.2245e-03, 7.5951e-03, 4.8828e-03, 5.3062e-03, 2.7676e-03,\n",
      "        9.1553e-03, 7.8964e-03, 6.7711e-03, 5.7373e-03, 1.1616e-03, 6.7024e-03,\n",
      "        1.3817e-02, 1.4885e-02, 1.3634e-02, 2.6840e-02, 8.2092e-03, 1.1055e-02,\n",
      "        9.6893e-03, 4.4937e-03, 5.7220e-03, 6.3324e-03, 5.2452e-03, 4.3869e-03,\n",
      "        4.0016e-03, 2.1210e-03, 7.2403e-03, 7.3090e-03, 8.4839e-03, 2.4719e-02,\n",
      "        1.3031e-02, 1.3184e-02, 8.1177e-03, 4.7112e-03, 3.7231e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [120] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [120] : torch.Size([1, 32, 1, 192])\n",
      "Last layer attentions for generated token [120] : tensor([1.3501e-01, 1.3501e-01, 6.4790e-05, 1.3137e-04, 5.7399e-05, 1.1683e-03,\n",
      "        3.2830e-04, 3.4142e-04, 4.7398e-04, 2.4152e-04, 6.5517e-04, 2.2781e-04,\n",
      "        1.5652e-04, 3.8671e-04, 3.1204e-03, 2.7885e-03, 2.5463e-04, 2.1231e-04,\n",
      "        7.4804e-05, 8.2195e-05, 2.7001e-05, 1.7428e-04, 4.9095e-03, 7.8297e-04,\n",
      "        2.2049e-03, 7.2556e-03, 6.9475e-04, 2.1994e-04, 1.9681e-04, 2.3687e-04,\n",
      "        3.5763e-04, 6.6185e-04, 5.9891e-04, 6.1941e-04, 3.4404e-04, 2.2471e-04,\n",
      "        1.1408e-04, 3.5143e-04, 1.5235e-04, 1.2195e-04, 1.1501e-03, 5.9319e-04,\n",
      "        3.4595e-04, 4.9973e-04, 5.4979e-04, 2.2256e-04, 3.1137e-04, 1.5962e-04,\n",
      "        2.9945e-04, 1.3275e-03, 8.4162e-04, 1.8740e-04, 1.5717e-03, 2.6665e-03,\n",
      "        1.0815e-03, 1.7729e-03, 2.8133e-04, 9.2983e-04, 1.0824e-03, 6.3658e-04,\n",
      "        6.9618e-04, 1.1272e-03, 2.7008e-03, 7.6485e-04, 4.4942e-04, 5.8746e-04,\n",
      "        8.5306e-04, 6.2895e-04, 9.9182e-04, 6.3210e-03, 5.7983e-03, 9.2010e-03,\n",
      "        3.1147e-03, 2.8419e-03, 7.5865e-04, 7.4501e-03, 3.5977e-04, 1.7986e-03,\n",
      "        2.5768e-03, 3.6983e-03, 6.0577e-03, 3.9558e-03, 9.7504e-03, 5.0087e-03,\n",
      "        4.4136e-03, 6.8779e-03, 2.3460e-03, 2.7332e-03, 3.7041e-03, 1.8997e-03,\n",
      "        7.8106e-04, 3.2196e-03, 2.5864e-03, 4.3221e-03, 4.2610e-03, 4.5433e-03,\n",
      "        9.6436e-03, 8.9722e-03, 1.9264e-03, 4.0436e-03, 1.5507e-03, 2.4471e-03,\n",
      "        4.4746e-03, 3.5763e-03, 5.5313e-03, 3.6068e-03, 2.5749e-03, 1.6699e-03,\n",
      "        6.8321e-03, 1.8454e-03, 6.9332e-04, 2.0313e-03, 1.9131e-03, 1.6327e-03,\n",
      "        5.2881e-04, 3.4428e-03, 2.6798e-03, 4.3988e-04, 6.9771e-03, 5.5809e-03,\n",
      "        5.5313e-04, 2.9492e-04, 2.3880e-03, 1.8339e-03, 2.1439e-03, 3.2082e-03,\n",
      "        7.0047e-04, 2.6398e-03, 2.3212e-03, 1.2341e-03, 1.6851e-03, 7.7963e-04,\n",
      "        1.2522e-03, 8.0585e-05, 8.5497e-04, 1.5583e-03, 1.0956e-02, 1.4015e-02,\n",
      "        5.1346e-03, 6.6032e-03, 1.0681e-02, 4.5586e-03, 8.5373e-03, 3.0956e-03,\n",
      "        1.8206e-03, 1.6460e-03, 1.7212e-02, 2.3899e-03, 1.6184e-03, 1.8148e-03,\n",
      "        3.0136e-02, 3.6530e-02, 6.1607e-03, 9.5139e-03, 3.0804e-03, 6.6071e-03,\n",
      "        1.0933e-02, 8.6136e-03, 3.7994e-03, 3.9291e-03, 3.2387e-03, 2.2297e-03,\n",
      "        5.7030e-03, 6.5422e-03, 9.7504e-03, 3.8776e-03, 6.9904e-04, 8.9188e-03,\n",
      "        1.0155e-02, 9.4757e-03, 8.3008e-03, 1.6998e-02, 5.7755e-03, 1.0071e-02,\n",
      "        5.4779e-03, 3.6297e-03, 5.3215e-03, 7.2365e-03, 4.5547e-03, 8.9111e-03,\n",
      "        2.9125e-03, 1.0147e-03, 1.1353e-02, 8.5373e-03, 1.0872e-02, 3.0396e-02,\n",
      "        1.1169e-02, 1.2611e-02, 5.7144e-03, 4.3793e-03, 4.1885e-03, 5.6839e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [121] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [121] : torch.Size([1, 32, 1, 193])\n",
      "Last layer attentions for generated token [121] : tensor([2.1021e-01, 2.1021e-01, 1.4615e-04, 4.1795e-04, 1.3149e-04, 1.8883e-03,\n",
      "        7.8559e-05, 8.2612e-05, 2.0528e-04, 2.6059e-04, 4.0603e-04, 1.6057e-04,\n",
      "        1.7703e-04, 6.4230e-04, 7.8583e-03, 7.0457e-03, 2.0051e-04, 5.2929e-04,\n",
      "        8.9705e-05, 1.8764e-04, 9.2924e-05, 4.2367e-04, 5.1231e-03, 1.1673e-03,\n",
      "        4.8714e-03, 8.8654e-03, 1.3733e-03, 4.0913e-04, 2.1601e-04, 3.1543e-04,\n",
      "        7.8535e-04, 1.5469e-03, 6.8903e-04, 2.7084e-04, 2.6059e-04, 3.7837e-04,\n",
      "        1.4281e-04, 4.0913e-04, 3.2878e-04, 1.3947e-04, 5.2261e-03, 2.3708e-03,\n",
      "        9.7561e-04, 7.9298e-04, 4.0984e-04, 2.5511e-04, 1.4734e-04, 1.4329e-04,\n",
      "        2.1768e-04, 3.2597e-03, 2.0294e-03, 3.0470e-04, 1.2827e-03, 6.6338e-03,\n",
      "        1.6241e-03, 3.3360e-03, 3.1614e-04, 8.5115e-04, 1.5202e-03, 5.3358e-04,\n",
      "        7.2241e-04, 7.4100e-04, 1.3952e-03, 6.3467e-04, 3.7169e-04, 5.2118e-04,\n",
      "        1.2875e-03, 1.1406e-03, 9.4557e-04, 1.3603e-02, 8.8730e-03, 1.3115e-02,\n",
      "        3.2978e-03, 3.1090e-03, 9.7561e-04, 2.3766e-03, 3.0160e-04, 8.9693e-04,\n",
      "        1.0672e-03, 2.9049e-03, 2.6798e-03, 2.8114e-03, 1.0117e-02, 5.2338e-03,\n",
      "        2.9755e-03, 3.5343e-03, 2.6264e-03, 2.1515e-03, 2.7332e-03, 1.0023e-03,\n",
      "        5.5170e-04, 2.3537e-03, 1.0777e-03, 2.3727e-03, 2.9850e-03, 2.0275e-03,\n",
      "        2.0142e-03, 5.2834e-03, 1.5497e-03, 8.6260e-04, 1.1539e-03, 1.1473e-03,\n",
      "        1.1911e-03, 6.6662e-04, 6.8760e-04, 9.1314e-04, 4.9925e-04, 4.3797e-04,\n",
      "        3.0231e-03, 1.3542e-03, 4.2629e-04, 1.4114e-03, 8.9025e-04, 9.9087e-04,\n",
      "        4.2963e-04, 2.1667e-03, 1.8387e-03, 3.7837e-04, 2.3956e-03, 4.0321e-03,\n",
      "        1.7462e-03, 5.2404e-04, 1.4791e-03, 1.3180e-03, 1.2531e-03, 2.4815e-03,\n",
      "        7.4244e-04, 1.1473e-03, 1.9646e-03, 7.3910e-04, 7.5960e-04, 8.7261e-04,\n",
      "        9.2936e-04, 2.0695e-04, 4.7088e-04, 5.7936e-04, 2.0542e-03, 4.5776e-03,\n",
      "        1.6308e-03, 1.2236e-03, 2.3708e-03, 1.8463e-03, 2.9297e-03, 9.4175e-04,\n",
      "        1.2598e-03, 9.9850e-04, 4.9820e-03, 1.8883e-03, 1.8826e-03, 1.4534e-03,\n",
      "        1.1009e-02, 3.8391e-02, 9.1705e-03, 1.2749e-02, 4.9515e-03, 3.5820e-03,\n",
      "        1.0681e-02, 3.5515e-03, 6.2637e-03, 4.1618e-03, 3.1986e-03, 1.8206e-03,\n",
      "        4.1771e-03, 5.9547e-03, 5.6038e-03, 3.6182e-03, 5.5695e-04, 4.1695e-03,\n",
      "        7.8812e-03, 1.0475e-02, 8.4839e-03, 1.9760e-02, 5.6953e-03, 8.3923e-03,\n",
      "        6.3019e-03, 3.4084e-03, 4.9629e-03, 4.4060e-03, 4.5662e-03, 2.8114e-03,\n",
      "        2.1191e-03, 1.1406e-03, 4.4746e-03, 6.0883e-03, 5.4626e-03, 1.6678e-02,\n",
      "        1.1383e-02, 1.1833e-02, 6.4278e-03, 3.3817e-03, 3.7403e-03, 6.3095e-03,\n",
      "        1.0605e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [122] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [122] : torch.Size([1, 32, 1, 194])\n",
      "Last layer attentions for generated token [122] : tensor([1.5967e-01, 1.5967e-01, 4.7445e-05, 1.1837e-04, 3.9816e-05, 1.0996e-03,\n",
      "        2.6941e-04, 2.7752e-04, 4.6921e-04, 2.7537e-04, 6.1083e-04, 2.4629e-04,\n",
      "        1.5438e-04, 4.1175e-04, 4.1962e-03, 2.8706e-03, 2.5463e-04, 2.7370e-04,\n",
      "        8.8692e-05, 9.8944e-05, 3.9518e-05, 2.0826e-04, 4.4212e-03, 8.0585e-04,\n",
      "        3.0384e-03, 7.6408e-03, 8.6451e-04, 2.7323e-04, 1.8954e-04, 1.8704e-04,\n",
      "        3.2759e-04, 8.0109e-04, 6.0368e-04, 5.7697e-04, 3.0184e-04, 2.3782e-04,\n",
      "        1.5438e-04, 3.7932e-04, 1.5140e-04, 1.0085e-04, 1.2398e-03, 5.3358e-04,\n",
      "        2.8682e-04, 4.8590e-04, 5.7268e-04, 2.0659e-04, 3.0780e-04, 1.3053e-04,\n",
      "        2.4915e-04, 1.3323e-03, 8.0919e-04, 1.4007e-04, 1.4725e-03, 2.5291e-03,\n",
      "        1.0252e-03, 1.7824e-03, 3.3331e-04, 1.0614e-03, 1.4782e-03, 7.2384e-04,\n",
      "        8.8358e-04, 1.1187e-03, 2.4490e-03, 9.4986e-04, 5.1928e-04, 6.2418e-04,\n",
      "        9.2030e-04, 6.0368e-04, 1.1892e-03, 5.5008e-03, 4.4746e-03, 7.6904e-03,\n",
      "        2.8000e-03, 2.4052e-03, 8.6975e-04, 6.7978e-03, 2.9087e-04, 1.6174e-03,\n",
      "        1.9054e-03, 2.8515e-03, 4.8943e-03, 3.8853e-03, 1.0490e-02, 4.9744e-03,\n",
      "        4.0588e-03, 6.7062e-03, 2.3155e-03, 2.7428e-03, 3.7155e-03, 1.7710e-03,\n",
      "        5.5838e-04, 2.6512e-03, 2.0981e-03, 3.5648e-03, 3.6621e-03, 3.5553e-03,\n",
      "        5.8899e-03, 7.0572e-03, 1.6489e-03, 3.0327e-03, 1.4620e-03, 2.6188e-03,\n",
      "        4.4632e-03, 3.4523e-03, 4.6387e-03, 3.3894e-03, 2.9736e-03, 1.6766e-03,\n",
      "        6.8817e-03, 1.6174e-03, 6.3896e-04, 1.9150e-03, 1.8778e-03, 1.5278e-03,\n",
      "        5.3072e-04, 4.2267e-03, 2.1725e-03, 3.6979e-04, 5.0125e-03, 5.0697e-03,\n",
      "        8.1205e-04, 4.2391e-04, 2.9678e-03, 2.2888e-03, 2.3556e-03, 2.9430e-03,\n",
      "        7.4244e-04, 2.5196e-03, 2.1763e-03, 1.3866e-03, 1.6537e-03, 9.9564e-04,\n",
      "        1.4820e-03, 9.7036e-05, 9.0599e-04, 1.6012e-03, 8.8120e-03, 1.1887e-02,\n",
      "        4.7951e-03, 6.3133e-03, 9.5749e-03, 4.1809e-03, 8.1787e-03, 2.5139e-03,\n",
      "        1.3561e-03, 1.3914e-03, 1.2871e-02, 2.0008e-03, 1.4868e-03, 1.5478e-03,\n",
      "        2.1820e-02, 3.0457e-02, 6.1531e-03, 8.3237e-03, 3.2558e-03, 5.5084e-03,\n",
      "        1.0567e-02, 7.6828e-03, 3.9177e-03, 3.3398e-03, 3.1166e-03, 2.3041e-03,\n",
      "        4.8523e-03, 5.9586e-03, 8.1482e-03, 2.9678e-03, 6.2418e-04, 7.7820e-03,\n",
      "        9.3384e-03, 9.7351e-03, 8.7814e-03, 1.4641e-02, 5.3596e-03, 9.1858e-03,\n",
      "        4.8599e-03, 3.1624e-03, 4.7760e-03, 6.3286e-03, 4.2381e-03, 7.9422e-03,\n",
      "        2.5272e-03, 8.1205e-04, 8.8425e-03, 8.0872e-03, 8.6060e-03, 2.3804e-02,\n",
      "        1.2253e-02, 1.4488e-02, 7.2746e-03, 5.9052e-03, 5.0278e-03, 5.6877e-03,\n",
      "        1.0582e-02, 7.2670e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [123] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [123] : torch.Size([1, 32, 1, 195])\n",
      "Last layer attentions for generated token [123] : tensor([1.5405e-01, 1.5356e-01, 4.9055e-05, 7.8380e-05, 2.5928e-05, 2.3956e-03,\n",
      "        3.0088e-04, 5.5695e-04, 1.0967e-03, 4.1699e-04, 7.3910e-04, 3.4976e-04,\n",
      "        1.0502e-04, 5.3024e-04, 3.5686e-03, 3.4714e-03, 2.0289e-04, 2.0528e-04,\n",
      "        7.0810e-05, 9.1970e-05, 9.4533e-05, 3.6955e-04, 4.5509e-03, 5.9271e-04,\n",
      "        4.6158e-03, 9.5825e-03, 5.4312e-04, 3.8648e-04, 2.3353e-04, 6.0558e-04,\n",
      "        4.3368e-04, 7.3195e-04, 3.4571e-04, 5.4741e-04, 3.2163e-04, 2.2900e-04,\n",
      "        2.0242e-04, 6.4325e-04, 2.3437e-04, 1.0878e-04, 1.1559e-03, 6.6900e-04,\n",
      "        1.7011e-04, 3.7456e-04, 8.2636e-04, 2.0015e-04, 1.2674e-03, 4.2272e-04,\n",
      "        5.7125e-04, 2.5311e-03, 1.3914e-03, 1.7762e-04, 1.7643e-03, 1.7595e-03,\n",
      "        1.1625e-03, 2.7905e-03, 3.3379e-04, 1.1005e-03, 1.0862e-03, 7.2622e-04,\n",
      "        6.8378e-04, 1.0815e-03, 1.6556e-03, 1.1444e-03, 4.4560e-04, 1.1902e-03,\n",
      "        8.0729e-04, 8.3780e-04, 1.3084e-03, 6.1951e-03, 4.7188e-03, 7.0534e-03,\n",
      "        2.2507e-03, 1.2684e-03, 6.9284e-04, 1.3359e-02, 5.4312e-04, 4.7073e-03,\n",
      "        3.5820e-03, 4.0092e-03, 7.9346e-03, 9.0103e-03, 6.7291e-03, 3.8624e-03,\n",
      "        4.5509e-03, 9.5596e-03, 2.1744e-03, 2.1324e-03, 4.1008e-03, 2.0638e-03,\n",
      "        5.9509e-04, 2.5234e-03, 1.4639e-03, 2.7905e-03, 2.6569e-03, 5.2452e-03,\n",
      "        6.1531e-03, 3.9482e-03, 1.0300e-03, 3.1719e-03, 9.1457e-04, 1.4687e-03,\n",
      "        3.9825e-03, 4.2305e-03, 6.5956e-03, 2.9182e-03, 1.5078e-03, 1.2054e-03,\n",
      "        5.2567e-03, 1.1826e-03, 3.5954e-04, 1.7719e-03, 1.6737e-03, 1.6804e-03,\n",
      "        2.6774e-04, 2.0313e-03, 1.3294e-03, 2.5749e-04, 3.6869e-03, 2.7523e-03,\n",
      "        6.7425e-04, 5.6124e-04, 2.7790e-03, 2.0657e-03, 4.8332e-03, 3.5763e-03,\n",
      "        1.5154e-03, 1.9436e-03, 1.6146e-03, 1.0548e-03, 1.3847e-03, 9.0933e-04,\n",
      "        1.7252e-03, 9.2685e-05, 8.6927e-04, 1.3781e-03, 5.6839e-03, 8.8120e-03,\n",
      "        3.7193e-03, 1.1917e-02, 9.3155e-03, 2.9984e-03, 8.2397e-03, 2.2640e-03,\n",
      "        8.7929e-04, 1.4057e-03, 8.5983e-03, 2.2697e-03, 1.6146e-03, 3.3054e-03,\n",
      "        2.1378e-02, 2.2980e-02, 6.0272e-03, 5.0087e-03, 1.4696e-03, 2.3174e-03,\n",
      "        9.1400e-03, 3.9864e-03, 7.0839e-03, 2.2106e-03, 2.1706e-03, 1.2674e-03,\n",
      "        3.8319e-03, 5.0964e-03, 8.1863e-03, 2.6646e-03, 4.2129e-04, 4.3640e-03,\n",
      "        5.3520e-03, 6.4888e-03, 1.2589e-02, 1.8616e-02, 6.2828e-03, 1.1177e-02,\n",
      "        6.5498e-03, 2.9411e-03, 3.2330e-03, 5.6152e-03, 5.0697e-03, 7.7057e-03,\n",
      "        3.6392e-03, 8.4734e-04, 1.0300e-02, 7.0038e-03, 1.1124e-02, 2.2690e-02,\n",
      "        1.0048e-02, 1.5137e-02, 5.9052e-03, 7.2174e-03, 9.3536e-03, 1.0101e-02,\n",
      "        1.3466e-02, 1.1055e-02, 1.0857e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [124] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [124] : torch.Size([1, 32, 1, 196])\n",
      "Last layer attentions for generated token [124] : tensor([1.6858e-01, 1.6821e-01, 1.4186e-04, 2.1684e-04, 7.4208e-05, 2.3422e-03,\n",
      "        1.8430e-04, 2.4557e-04, 4.2534e-04, 3.8195e-04, 6.7568e-04, 2.1803e-04,\n",
      "        1.7798e-04, 7.0143e-04, 3.1948e-03, 2.9716e-03, 1.4353e-04, 1.5831e-04,\n",
      "        5.5373e-05, 1.1581e-04, 7.0274e-05, 4.1556e-04, 8.5602e-03, 1.7233e-03,\n",
      "        3.3360e-03, 6.3362e-03, 2.3580e-04, 3.2115e-04, 1.3077e-04, 4.2939e-04,\n",
      "        4.7636e-04, 3.0875e-04, 2.5105e-04, 3.6240e-04, 2.7728e-04, 4.6897e-04,\n",
      "        1.4186e-04, 5.5361e-04, 1.7655e-04, 6.8069e-05, 7.2765e-04, 3.3832e-04,\n",
      "        1.2767e-04, 2.8324e-04, 3.7670e-04, 9.5665e-05, 4.6992e-04, 3.3259e-04,\n",
      "        2.5606e-04, 9.7942e-04, 5.5790e-04, 1.7941e-04, 8.6737e-04, 2.3689e-03,\n",
      "        6.9427e-04, 3.1796e-03, 2.1219e-04, 1.0185e-03, 8.9884e-04, 7.0667e-04,\n",
      "        4.8089e-04, 8.8120e-04, 2.4052e-03, 1.0204e-03, 6.3848e-04, 1.0424e-03,\n",
      "        4.8280e-04, 7.4196e-04, 8.4734e-04, 4.0817e-03, 3.3360e-03, 7.0992e-03,\n",
      "        1.2255e-03, 1.1311e-03, 5.3978e-04, 7.0457e-03, 1.0843e-03, 2.4662e-03,\n",
      "        5.4893e-03, 2.8114e-03, 2.8610e-03, 2.2068e-03, 5.8441e-03, 1.9531e-03,\n",
      "        2.5768e-03, 6.5002e-03, 1.6499e-03, 1.0862e-03, 1.5745e-03, 1.1139e-03,\n",
      "        1.1425e-03, 2.5730e-03, 9.0027e-04, 1.8272e-03, 2.8896e-03, 3.6144e-03,\n",
      "        4.6768e-03, 5.0545e-03, 1.2646e-03, 1.9417e-03, 1.0815e-03, 1.0672e-03,\n",
      "        3.1853e-03, 3.0727e-03, 4.3869e-03, 2.8553e-03, 1.3571e-03, 7.5960e-04,\n",
      "        6.3362e-03, 1.5984e-03, 4.4227e-04, 2.2411e-03, 1.7643e-03, 2.2030e-03,\n",
      "        3.4714e-04, 3.0785e-03, 2.9297e-03, 6.9427e-04, 5.8899e-03, 3.0670e-03,\n",
      "        5.9414e-04, 5.5456e-04, 1.4610e-03, 1.5802e-03, 2.2621e-03, 2.4624e-03,\n",
      "        7.8535e-04, 1.1101e-03, 1.7700e-03, 1.1520e-03, 1.3256e-03, 8.4400e-04,\n",
      "        2.2926e-03, 1.7047e-04, 1.3781e-03, 1.8330e-03, 6.4964e-03, 7.8583e-03,\n",
      "        2.6054e-03, 5.4169e-03, 4.4098e-03, 2.9202e-03, 5.9624e-03, 1.5984e-03,\n",
      "        1.4000e-03, 1.1034e-03, 1.3695e-02, 2.4834e-03, 1.5802e-03, 2.2411e-03,\n",
      "        3.2684e-02, 2.5482e-02, 4.3221e-03, 4.5395e-03, 1.0004e-03, 1.5554e-03,\n",
      "        4.7188e-03, 1.6670e-03, 2.8191e-03, 2.1114e-03, 1.8454e-03, 1.1721e-03,\n",
      "        3.7537e-03, 4.6806e-03, 7.4005e-03, 4.3373e-03, 5.3024e-04, 7.2746e-03,\n",
      "        6.6109e-03, 6.0463e-03, 8.6212e-03, 1.6434e-02, 4.3755e-03, 7.9956e-03,\n",
      "        4.1618e-03, 1.6432e-03, 2.7809e-03, 5.7220e-03, 4.7379e-03, 1.0101e-02,\n",
      "        3.7098e-03, 1.6642e-03, 1.1642e-02, 7.0534e-03, 9.8724e-03, 3.2837e-02,\n",
      "        2.0920e-02, 1.3954e-02, 6.2065e-03, 4.5891e-03, 1.3580e-02, 1.6785e-02,\n",
      "        1.4122e-02, 1.4755e-02, 8.3618e-03, 1.2405e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [125] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [125] : torch.Size([1, 32, 1, 197])\n",
      "Last layer attentions for generated token [125] : tensor([2.2937e-01, 2.2937e-01, 1.8346e-04, 3.5238e-04, 9.7871e-05, 1.9312e-03,\n",
      "        1.7583e-04, 1.9085e-04, 4.1604e-04, 4.1604e-04, 6.0749e-04, 2.4891e-04,\n",
      "        1.9383e-04, 5.7077e-04, 3.0804e-03, 2.3594e-03, 1.2958e-04, 1.6320e-04,\n",
      "        5.1737e-05, 1.0097e-04, 6.6221e-05, 6.5422e-04, 1.1597e-02, 1.9588e-03,\n",
      "        4.1008e-03, 6.1111e-03, 3.4094e-04, 2.2662e-04, 1.6975e-04, 3.6430e-04,\n",
      "        4.5943e-04, 5.2071e-04, 2.9969e-04, 2.9325e-04, 2.0790e-04, 3.8838e-04,\n",
      "        1.9312e-04, 4.9686e-04, 2.4557e-04, 1.0455e-04, 8.5831e-04, 2.9206e-04,\n",
      "        9.7454e-05, 3.1042e-04, 3.7146e-04, 1.1128e-04, 3.8028e-04, 2.9373e-04,\n",
      "        3.0375e-04, 1.1988e-03, 5.9128e-04, 1.7440e-04, 7.6818e-04, 2.6779e-03,\n",
      "        6.6996e-04, 2.6169e-03, 1.5759e-04, 1.2159e-03, 1.0643e-03, 8.5354e-04,\n",
      "        6.7902e-04, 1.0176e-03, 2.0981e-03, 8.8406e-04, 6.9237e-04, 9.0170e-04,\n",
      "        4.1366e-04, 5.7888e-04, 5.9462e-04, 4.4899e-03, 3.6049e-03, 5.3673e-03,\n",
      "        9.0170e-04, 9.7084e-04, 4.0960e-04, 3.9215e-03, 8.3351e-04, 2.2526e-03,\n",
      "        3.0823e-03, 2.6817e-03, 3.9043e-03, 1.9817e-03, 5.3253e-03, 1.0519e-03,\n",
      "        1.4982e-03, 4.0169e-03, 1.4515e-03, 6.6233e-04, 1.4601e-03, 8.2397e-04,\n",
      "        5.3406e-04, 2.0313e-03, 8.6355e-04, 1.5850e-03, 2.6131e-03, 2.8992e-03,\n",
      "        3.4466e-03, 4.1161e-03, 6.3324e-04, 1.0433e-03, 6.9809e-04, 3.4094e-04,\n",
      "        1.6937e-03, 1.3828e-03, 2.6054e-03, 1.9989e-03, 9.9945e-04, 6.8998e-04,\n",
      "        4.3259e-03, 1.1196e-03, 3.0017e-04, 1.6546e-03, 1.0662e-03, 1.6451e-03,\n",
      "        2.7704e-04, 2.5978e-03, 2.1210e-03, 5.0783e-04, 4.2839e-03, 2.2469e-03,\n",
      "        3.8934e-04, 3.4952e-04, 7.5006e-04, 1.0481e-03, 1.9684e-03, 1.8482e-03,\n",
      "        6.0272e-04, 1.0538e-03, 1.5574e-03, 8.1921e-04, 9.3937e-04, 5.2166e-04,\n",
      "        1.5888e-03, 1.1086e-04, 8.0013e-04, 1.5421e-03, 4.2686e-03, 6.3362e-03,\n",
      "        2.4700e-03, 4.4708e-03, 3.9330e-03, 2.9526e-03, 5.3253e-03, 1.4019e-03,\n",
      "        1.1730e-03, 1.3380e-03, 8.1482e-03, 2.0542e-03, 1.2760e-03, 1.7443e-03,\n",
      "        2.4887e-02, 1.7914e-02, 2.9507e-03, 2.4986e-03, 7.1859e-04, 9.7084e-04,\n",
      "        2.8934e-03, 1.3018e-03, 1.7443e-03, 1.0834e-03, 1.6747e-03, 8.7738e-04,\n",
      "        2.2316e-03, 3.6678e-03, 5.0964e-03, 2.0294e-03, 2.8419e-04, 3.8853e-03,\n",
      "        4.2229e-03, 2.2812e-03, 3.5896e-03, 8.7280e-03, 2.9869e-03, 5.4741e-03,\n",
      "        4.5280e-03, 1.5459e-03, 2.6779e-03, 3.7403e-03, 2.3975e-03, 7.7171e-03,\n",
      "        2.3918e-03, 9.3555e-04, 8.9264e-03, 5.3329e-03, 7.6637e-03, 2.6276e-02,\n",
      "        1.9409e-02, 1.4793e-02, 5.0049e-03, 4.3602e-03, 1.2634e-02, 1.4755e-02,\n",
      "        1.5137e-02, 1.3062e-02, 7.3929e-03, 1.1177e-02, 1.6693e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [126] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [126] : torch.Size([1, 32, 1, 198])\n",
      "Last layer attentions for generated token [126] : tensor([1.3171e-01, 1.3171e-01, 1.9681e-04, 4.8637e-04, 1.5700e-04, 1.8053e-03,\n",
      "        1.8609e-04, 1.6320e-04, 3.6359e-04, 3.1710e-04, 5.2595e-04, 1.8787e-04,\n",
      "        1.9050e-04, 5.0449e-04, 7.2250e-03, 4.5509e-03, 3.4142e-04, 5.6171e-04,\n",
      "        1.3578e-04, 2.7227e-04, 1.4460e-04, 5.9938e-04, 5.4245e-03, 1.2093e-03,\n",
      "        6.2599e-03, 1.4496e-02, 1.9569e-03, 4.9210e-04, 2.5272e-04, 2.9087e-04,\n",
      "        5.0163e-04, 1.2398e-03, 6.7377e-04, 4.4012e-04, 4.3845e-04, 4.9782e-04,\n",
      "        1.5366e-04, 5.6171e-04, 2.9969e-04, 1.5187e-04, 3.6068e-03, 1.6479e-03,\n",
      "        6.2799e-04, 9.4461e-04, 5.5885e-04, 3.2830e-04, 1.5759e-04, 1.2565e-04,\n",
      "        3.4213e-04, 3.7727e-03, 2.4719e-03, 5.6505e-04, 1.2598e-03, 9.4299e-03,\n",
      "        1.1520e-03, 3.7594e-03, 5.6839e-04, 1.5974e-03, 3.0365e-03, 1.1034e-03,\n",
      "        1.3494e-03, 1.4057e-03, 2.8286e-03, 1.5526e-03, 7.4005e-04, 9.0504e-04,\n",
      "        1.8482e-03, 2.3918e-03, 1.7395e-03, 1.2894e-02, 9.4833e-03, 1.7197e-02,\n",
      "        5.4741e-03, 2.4147e-03, 1.1892e-03, 4.3144e-03, 3.4213e-04, 1.1940e-03,\n",
      "        1.2188e-03, 3.0365e-03, 3.3226e-03, 3.3741e-03, 1.4931e-02, 4.3144e-03,\n",
      "        2.5539e-03, 3.8624e-03, 3.0632e-03, 1.7567e-03, 2.8629e-03, 1.0662e-03,\n",
      "        3.9840e-04, 2.4891e-03, 9.4128e-04, 2.3155e-03, 4.0436e-03, 2.5291e-03,\n",
      "        2.5883e-03, 8.3923e-03, 2.3365e-03, 1.1101e-03, 1.7290e-03, 1.3962e-03,\n",
      "        2.1133e-03, 1.5335e-03, 1.5440e-03, 1.4734e-03, 1.1101e-03, 6.5947e-04,\n",
      "        4.5547e-03, 1.5497e-03, 8.8072e-04, 1.5593e-03, 1.1320e-03, 1.4238e-03,\n",
      "        8.2541e-04, 3.5095e-03, 2.7905e-03, 4.9877e-04, 3.5267e-03, 7.2556e-03,\n",
      "        2.0142e-03, 5.7173e-04, 3.0823e-03, 2.1038e-03, 2.5654e-03, 3.1128e-03,\n",
      "        7.6962e-04, 1.1177e-03, 1.4772e-03, 9.1028e-04, 9.5797e-04, 1.4277e-03,\n",
      "        1.2178e-03, 3.3808e-04, 9.9754e-04, 8.2397e-04, 2.4509e-03, 6.5613e-03,\n",
      "        1.7462e-03, 1.4048e-03, 1.7824e-03, 1.2980e-03, 2.9411e-03, 8.9979e-04,\n",
      "        1.0920e-03, 7.0477e-04, 4.9133e-03, 1.3971e-03, 1.4057e-03, 1.3676e-03,\n",
      "        1.0040e-02, 5.1392e-02, 9.0256e-03, 1.6464e-02, 4.6692e-03, 3.5095e-03,\n",
      "        1.3000e-02, 6.8092e-03, 9.2392e-03, 3.5744e-03, 4.2915e-03, 3.3512e-03,\n",
      "        5.2605e-03, 7.2937e-03, 5.4817e-03, 3.5477e-03, 8.4019e-04, 4.8447e-03,\n",
      "        1.0933e-02, 7.3738e-03, 9.0103e-03, 1.9485e-02, 5.4054e-03, 9.0408e-03,\n",
      "        7.6828e-03, 4.2610e-03, 6.6872e-03, 5.4092e-03, 5.4703e-03, 4.5052e-03,\n",
      "        3.6049e-03, 1.3428e-03, 5.4131e-03, 7.7553e-03, 5.4855e-03, 2.7344e-02,\n",
      "        1.5778e-02, 2.1042e-02, 8.6365e-03, 5.7869e-03, 3.5954e-03, 8.1558e-03,\n",
      "        7.4883e-03, 9.4452e-03, 8.5373e-03, 4.1656e-03, 3.4752e-03, 4.0359e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [127] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [127] : torch.Size([1, 32, 1, 199])\n",
      "Last layer attentions for generated token [127] : tensor([1.3013e-01, 1.3013e-01, 4.2498e-05, 1.5330e-04, 4.5419e-05, 5.9557e-04,\n",
      "        1.2660e-04, 1.3793e-04, 1.9073e-04, 1.6928e-04, 3.4189e-04, 2.2733e-04,\n",
      "        8.8513e-05, 2.4021e-04, 4.5547e-03, 2.5616e-03, 1.3626e-04, 2.6846e-04,\n",
      "        5.6267e-05, 7.7844e-05, 3.7909e-05, 1.0228e-04, 1.3199e-03, 4.0388e-04,\n",
      "        2.3670e-03, 6.2065e-03, 8.5306e-04, 1.7917e-04, 1.2910e-04, 1.9145e-04,\n",
      "        2.7800e-04, 8.4496e-04, 5.4979e-04, 3.7932e-04, 4.0698e-04, 1.5450e-04,\n",
      "        9.4235e-05, 2.9421e-04, 8.3148e-05, 7.8142e-05, 2.2221e-03, 5.1832e-04,\n",
      "        4.1914e-04, 3.3092e-04, 4.1485e-04, 1.4234e-04, 2.3365e-04, 5.2631e-05,\n",
      "        1.4067e-04, 2.0351e-03, 6.6423e-04, 2.1994e-04, 1.2474e-03, 2.8400e-03,\n",
      "        6.3133e-04, 1.3199e-03, 1.8966e-04, 4.3583e-04, 1.1044e-03, 4.3154e-04,\n",
      "        4.0603e-04, 5.8842e-04, 1.2197e-03, 3.2496e-04, 2.2912e-04, 3.6907e-04,\n",
      "        4.7398e-04, 1.2197e-03, 7.3099e-04, 1.2230e-02, 6.1760e-03, 8.7585e-03,\n",
      "        6.4201e-03, 4.7073e-03, 1.0891e-03, 3.8700e-03, 9.7215e-05, 6.3753e-04,\n",
      "        1.0281e-03, 8.6975e-04, 2.2888e-03, 3.9787e-03, 7.7667e-03, 7.9956e-03,\n",
      "        3.6221e-03, 7.3357e-03, 1.7977e-03, 4.2419e-03, 3.8395e-03, 1.3838e-03,\n",
      "        3.7050e-04, 2.2526e-03, 1.8711e-03, 6.4392e-03, 2.8172e-03, 3.7079e-03,\n",
      "        5.9128e-03, 6.4240e-03, 2.2354e-03, 2.8973e-03, 8.3160e-04, 3.7289e-03,\n",
      "        2.9411e-03, 2.3422e-03, 2.6360e-03, 2.0828e-03, 2.2068e-03, 9.5272e-04,\n",
      "        6.7062e-03, 8.5497e-04, 4.2057e-04, 1.7710e-03, 1.0490e-03, 9.1505e-04,\n",
      "        4.1914e-04, 5.8136e-03, 1.4572e-03, 2.9135e-04, 4.8141e-03, 4.1466e-03,\n",
      "        2.0390e-03, 2.9540e-04, 6.0921e-03, 3.6640e-03, 3.4962e-03, 4.8752e-03,\n",
      "        1.0710e-03, 3.4866e-03, 2.5654e-03, 1.1044e-03, 1.7729e-03, 1.1234e-03,\n",
      "        2.1000e-03, 9.0241e-05, 8.1396e-04, 1.4772e-03, 8.6594e-03, 1.0780e-02,\n",
      "        3.9253e-03, 3.6774e-03, 1.4168e-02, 2.9640e-03, 5.7526e-03, 1.9245e-03,\n",
      "        1.1959e-03, 1.1101e-03, 1.3962e-02, 1.9512e-03, 1.7977e-03, 1.6146e-03,\n",
      "        2.3254e-02, 4.0192e-02, 1.1932e-02, 1.4954e-02, 6.3515e-03, 3.6182e-03,\n",
      "        1.7578e-02, 8.5068e-03, 1.0437e-02, 6.8703e-03, 3.9749e-03, 2.8229e-03,\n",
      "        5.5199e-03, 8.0338e-03, 7.7209e-03, 3.1433e-03, 7.1430e-04, 8.6594e-03,\n",
      "        7.7972e-03, 1.6449e-02, 9.3079e-03, 1.8417e-02, 5.4703e-03, 1.4122e-02,\n",
      "        2.7523e-03, 4.2572e-03, 6.1798e-03, 5.0583e-03, 3.9215e-03, 5.2071e-03,\n",
      "        1.2474e-03, 4.6563e-04, 7.0305e-03, 8.8730e-03, 4.6387e-03, 1.3893e-02,\n",
      "        8.2092e-03, 1.1406e-02, 8.9493e-03, 6.0730e-03, 4.4174e-03, 5.4131e-03,\n",
      "        1.2024e-02, 6.1760e-03, 1.2817e-02, 2.8591e-03, 2.3994e-03, 3.4218e-03,\n",
      "        9.0942e-03], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "i = -1\n",
    "for j in range(len(outputs.hidden_states)):\n",
    "    print(f\" Number of elements : {len(outputs.hidden_states[j][1:])}\")\n",
    "    print(f\"Hidden states shape for generated token [{j}] : {outputs.hidden_states[j][i].shape}\")\n",
    "    #print(f\"Some values: {outputs.hidden_states[j][i][:5]}\")\n",
    "    \n",
    "    print(f\"Attention weights shape for generated token [{j}] : {outputs.attentions[j][i].shape}\")\n",
    "    print(f\"Last layer attentions for generated token [{j}] : {outputs.attentions[j][i][0,-1,-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.3076,  1.5225, -0.2357,  ...,  0.3049, -1.8154, -2.0996],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.2549, -1.9189,  2.3262,  ...,  4.1211,  0.2391, -4.5898],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-7.0078,  3.7402,  1.7207,  ...,  2.6328, -0.7285, -1.0400],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-6.5820,  2.1250, -0.2280,  ...,  1.4365, -1.4990, -2.0879],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.5742,  2.7383, -4.1289,  ...,  2.5371,  2.4668, -3.2344],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.5391,  1.0801, -3.5723,  ...,  4.4727,  1.9434, -0.9648],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.5215, -0.0200, -5.4922,  ...,  1.8125,  2.1230, -1.4883],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.4434,  0.4512,  1.7158,  ...,  0.8379,  0.0867,  0.3909],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.3223, -1.1777, -3.2461,  ...,  1.0430,  3.0293, -1.5127],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.9727, -1.4287, -2.3398,  ...,  1.8730,  1.6533, -1.0537],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.0221, -0.7612, -0.3638,  ..., -1.1699, -2.8262, -1.0381],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.4902, -0.3723,  1.5635,  ..., -1.4912, -2.2285, -2.2129],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.5234,  0.4651, -0.3257,  ...,  0.6719,  0.1152,  1.7002],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.1797, -1.2617,  0.4956,  ..., -0.1542, -1.9229, -0.0884],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.8467,  1.7705, -3.3809,  ..., -4.4180, -4.7422, -1.0352],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.3926, -0.4995,  0.2522,  ..., -1.9219, -1.4805, -0.7974],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.0430,  1.0986, -0.6006,  ..., -0.5879, -0.6421,  1.3027],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.3730, -1.1768, -1.3252,  ...,  0.1825, -0.9121,  1.0674],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.4141,  1.4297, -1.9912,  ..., -1.1016, -3.5977, -0.6094],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.3994, -0.4722, -0.5029,  ..., -1.0518, -0.7744,  1.2520],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.7632, -0.9824, -0.2690,  ..., -0.2881, -5.1953,  1.5996],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.1611, -1.8525,  0.1932,  ..., -0.3191, -5.4180,  0.6440],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.5078,  1.0430, -0.6426,  ...,  3.5508, -4.8867,  0.6997],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.3430, -1.9883,  0.5688,  ...,  0.7422,  0.5103, -1.3936],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.1221, -0.1881, -0.1951,  ...,  2.5430, -2.1523, -3.3438],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.5312,  0.8521,  0.1479,  ...,  1.0850, -4.9023, -1.5264],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.4375, -3.0332,  4.0273,  ..., -0.9312, -2.8789, -0.9170],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.4551, -2.0215,  0.7539,  ...,  2.2559, -2.4219, -0.1963],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.1687, -0.4829,  1.9688,  ..., -1.5352, -4.4727, -0.8389],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.4824, -2.8242,  3.2832,  ..., -2.2012, -1.3623, -0.1500],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.8164,  0.4702,  0.6523,  ...,  0.1757, -1.6084,  1.2422],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.1172, -3.9238, -1.1709,  ...,  2.2461, -3.5801, -1.1240],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.5889,  2.7930, -1.9912,  ...,  1.5459, -2.5723,  2.9824],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.5605,  0.7642, -2.7559,  ...,  3.2871, -4.8906,  5.2148],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.0879, -0.3650, -0.3760,  ..., -0.0152, -0.7676,  1.0078],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-5.5508,  0.0000,  1.0527,  ..., -0.8794, -2.4883, -0.4790],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-5.5312,  2.1074,  0.1333,  ..., -2.6738, -2.3281, -1.2500],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.4941,  2.1973,  2.5137,  ...,  1.1055, -0.6157, -2.7852],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.2090, -2.5723,  1.7920,  ..., -0.5518,  3.6660, -3.1133],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.5830, -1.7217,  3.3516,  ...,  0.2837,  1.2881, -0.2922],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.8652, -0.1284,  3.4180,  ..., -0.5703,  0.2559,  1.5771],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.4141,  1.6621,  6.4062,  ...,  1.2041,  0.5547, -2.6309],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.0533, -3.4824,  3.0215,  ..., -1.3867,  2.4062, -0.9111],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.7324, -1.0996,  0.6382,  ..., -1.1553,  2.9160,  1.3955],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.9736,  0.3000,  0.4744,  ..., -1.5967, -0.1078, -1.1680],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.5547, -2.0371, -0.5273,  ..., -2.3340,  2.9844, -1.4014],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.8223, -4.0703, -1.0498,  ..., -2.5996, -0.0044, -1.4600],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.4082,  0.3018,  2.3652,  ..., -2.1094, -1.8203,  0.3052],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.4434, -2.4258,  1.7402,  ..., -3.6309, -0.9146, -0.6064],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.0723, -2.5000, -1.9229,  ..., -1.3262, -1.1279,  1.3809],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.6270, -1.3135, -5.0391,  ..., -2.7910,  0.4778, -0.1600],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.6328, -0.9551, -0.6519,  ..., -2.5898,  1.6807, -2.5254],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-5.8633, -1.7285,  0.0705,  ..., -1.1895, -0.5112, -1.0400],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-6.0820, -1.4697, -0.7310,  ..., -1.0430, -2.2402, -3.5820],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-5.3359, -2.2012, -0.8579,  ...,  2.1367,  0.6152, -1.8506],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-6.9727, -1.2803, -0.7739,  ..., -1.2900, -1.7305, -2.6738],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-7.6406,  0.5532,  1.3350,  ..., -1.0117, -3.0449, -1.1172],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.6729, -1.7588,  4.1797,  ...,  0.1210, -0.1964,  2.7578],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.9883, -0.1124,  0.5225,  ...,  0.3201,  2.6582,  1.8408],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.4355,  0.9746, -3.7109,  ..., -2.6191, -0.6025,  0.4695],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.8047, -0.2947,  0.3398,  ..., -1.5928,  1.3252,  2.0273],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.0479,  1.0176, -1.1621,  ..., -1.3252, -0.3743,  2.1953],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.8936, -0.0655,  0.0427,  ..., -1.4346,  0.2654,  0.6387],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.7188, -1.1035, -0.4419,  ...,  0.0763, -0.8706,  1.0068],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-5.8984, -2.0684, -0.0113,  ...,  2.6250, -1.0498,  1.9062],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-6.6250,  1.8828, -0.5581,  ...,  0.0287, -1.3271,  0.7085],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-5.2188,  0.6611, -0.2998,  ..., -0.7871, -0.7642,  0.5889],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.7080, -4.1484,  1.8652,  ...,  0.0988,  4.3789, -0.0746],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.9297, -2.5938,  0.7578,  ...,  1.4131,  3.2227, -0.3037],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-6.0156, -1.0986, -0.3083,  ...,  2.0684,  0.5737, -1.8037],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.5996,  0.8848, -1.4658,  ...,  0.0677, -0.2321,  0.1564],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.1866, -1.1045, -1.2529,  ...,  1.5205, -0.7754,  2.3672],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.2559, -0.4631, -0.1241,  ...,  0.1796, -1.2725, -0.6147],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.4319, -1.0234,  0.2455,  ..., -0.4314,  2.1699, -2.5586],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.0020, -2.7422,  0.5718,  ..., -0.4338,  1.9131, -2.7480],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.5586,  2.3945,  0.4390,  ..., -0.4922, -0.9355,  1.9463],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.8945,  0.8916, -1.4629,  ..., -2.3633,  3.3730, -1.3350],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.4797,  0.0486,  0.4502,  ..., -1.4180,  4.1211, -4.6484],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.8599, -3.7500, -2.5879,  ..., -2.5684,  2.3203, -3.5703],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.0450,  1.1758,  2.3047,  ...,  0.8115, -0.8398,  0.4819],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.4121, -1.8242,  1.8574,  ..., -4.1133, -2.2129, -1.7695],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.9336,  1.6729,  0.7104,  ..., -3.9219, -2.5605, -0.7769],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.4668,  1.8057,  0.0616,  ..., -2.5449, -3.5391, -0.2396],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 2.1484,  2.2520,  0.2900,  ..., -0.7227, -1.2334,  1.5566],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.5850,  3.2012,  0.2351,  ..., -0.0873, -1.4102,  2.2891],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.2888,  1.3203,  0.5405,  ..., -1.2041,  1.7549, -1.7695],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 5.7227, -1.0410, -0.7788,  ..., -1.6436,  0.9531, -0.7515],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 3.2793,  0.1680, -1.0000,  ..., -1.9268,  0.9722, -1.8311],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.9502,  4.1328,  0.0706,  ..., -3.3809, -3.1250, -0.0510],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.9712, -2.7891,  0.1100,  ..., -0.9092,  1.2197, -1.7695],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 3.9844, -0.8579,  2.0176,  ...,  0.2776,  2.6641, -2.1406],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.8838,  6.0508,  1.8066,  ..., -0.1541,  1.9609,  0.8525],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.5312,  2.2637,  4.1992,  ...,  0.4421, -0.8984, -3.6523],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.6006,  2.6465,  2.9961,  ...,  0.1946,  0.5498, -2.4551],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.0731, -3.0371,  0.5244,  ..., -1.7607,  4.3633, -1.9346],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 2.5820, -0.7529,  0.9043,  ..., -1.7686,  3.6465, -0.1020],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.1406,  1.0586, -1.9590,  ...,  0.1798, -0.1691, -2.1426],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.2010,  0.8730,  3.1270,  ..., -1.4072,  1.6299,  0.5947],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.8105,  0.4890,  2.1094,  ..., -0.1078, -1.0811, -1.3066],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.3623, -3.1641,  1.7764,  ..., -0.3386, -2.9043,  2.4219],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.3145,  0.7183, -0.9448,  ..., -1.7646,  2.8711, -1.8594],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.0537, -2.8359, -1.4463,  ..., -1.0361,  2.3965,  0.5186],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.8164,  2.5703,  2.3945,  ..., -1.6963,  0.9692,  0.2988],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.1133, -3.7090,  2.1992,  ..., -4.7656,  0.2404, -1.9990],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-7.5312,  1.8125,  2.7051,  ..., -2.6289,  1.7402, -0.2517],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 2.4766,  0.1514,  1.9326,  ...,  0.8413,  3.3867, -2.5410],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.3115, -0.4985,  3.0332,  ..., -1.3301,  2.0391, -1.6699],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.5605,  0.4309,  2.0840,  ..., -1.0488, -0.3037, -3.4434],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.7012,  1.2627,  3.7852,  ..., -1.2598,  0.9678,  0.1343],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.3091, -0.9614, -0.8003,  ..., -4.0664,  2.8965, -2.2285],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.9355, -2.6562, -0.9634,  ..., -0.9487,  1.4248, -1.6475],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.8730,  3.1348,  5.8008,  ..., -0.0595,  0.2610,  1.2080],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 2.8164, -0.9507,  1.5693,  ..., -1.9219,  0.0817, -1.5684],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.0225,  1.8662,  0.2460,  ...,  0.3928,  1.5479, -3.7891],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.3027,  2.1719,  5.4766,  ..., -0.5605,  4.3242, -0.5806],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.2656, -1.1299,  4.7500,  ..., -2.8008, -2.9219, -1.0850],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.7734, -0.2900,  4.1445,  ..., -2.3262,  0.5107, -3.0332],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.3477, -1.5361,  1.1709,  ...,  4.0469,  1.0195, -3.3438],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.5420,  1.1836, -0.8188,  ..., -1.6768,  0.1945, -4.3828],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.2754,  3.8027, -0.4431,  ..., -0.4651, -3.6699, -1.3398],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.1895,  1.2031,  1.2471,  ...,  0.4443, -2.6992, -2.1934],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.4902,  1.3877, -0.4177,  ..., -1.0479, -3.6348, -0.1242],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.1857, -0.1895,  1.8184,  ..., -0.9497, -1.3564, -3.9922],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 3.6953, -1.0928, -0.2576,  ...,  1.5557,  1.6650, -2.4727],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.4998,  0.4644, -2.1250,  ...,  1.1738,  1.3506, -0.7837],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.3135, -1.3145, -1.3906,  ...,  1.7129,  1.6660, -1.3018],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.2148, -0.0028,  2.0293,  ..., -2.7207, -2.7734, -2.7266],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.6289, -2.4004,  2.9844,  ..., -3.4922, -0.1779, -1.4766],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "j = 1\n",
    "i = -1\n",
    "for j in range(len(outputs.hidden_states)):\n",
    "    print(f\" Number of elements : {len(outputs.hidden_states[j])}\")\n",
    "    #print(f\"Hidden states shape for generated token [{j}] : {outputs.hidden_states[j][i].shape}\")\n",
    "    print(f\"Last hidden state : {outputs.hidden_states[j][i][0,-1,:4096]}\")\n",
    "    #print(f\"Some values: {outputs.hidden_states[j][i][:5]}\")\n",
    "    \n",
    "    #print(f\"Attention weights shape for generated token [{j}] : {outputs.attentions[j][i].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   32,  3488,   430,  5334,   311,   279,  4851,   315,   279, 32677,\n",
       "            13,   358,  2011, 17113,    11,   358,  3077,  2646,  2586,  4028,\n",
       "           904,  3495,   389,   420,  8712,    13,   763,  2144,    11,   358,\n",
       "          2846,   539,  1524,  2771,   433,   596,  3284,   369,   264,  3823,\n",
       "           311,  8343,   264, 36125,   304,   832, 11961,    13, 16183, 24904,\n",
       "           388,   527,  3544,    11,  6485, 12933,  1903,   315,  9501,    11,\n",
       "         12466,    11,   323,  1023,  7384,    11,   539,  7041,  2555,   499,\n",
       "          4265,  1505,   389,   279,  5130,   520,   701,  2254, 89206,   382,\n",
       "         11458,    11,   422,   584,  9855,   264, 59159, 15398,  1405,   264,\n",
       "          3823,  1436, 17354, 25024,   264, 36125,    11,   358,  4265, 16430,\n",
       "           279,  1396,   315, 59432,   264,  1732,  1436,  8343,   304,   832,\n",
       "         11961,  1053,   387,  1131,  7315,    13,  3011,   596,  1314,    11,\n",
       "         15687,    11,   264,  2466,  8834,  7315,    13,  2876]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['sequences'][:,nb_tokens_in:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (1): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (2): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (3): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (4): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (5): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (6): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (7): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (8): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (9): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (10): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (11): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (12): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (13): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (14): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (15): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (16): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (17): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (18): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (19): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (20): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (21): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (22): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (23): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (24): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (25): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (26): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (27): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (28): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (29): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (30): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (31): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A question that gets to the heart of the absurd. I must admit, I've never come across any research on this topic. In fact, I'm not even sure it's possible for a human to eat a helicopter in one sitting. Helicopters are large, complex machines made of metal, plastic, and other materials, not exactly something you'd find on the menu at your local diner.\n",
      "\n",
      "However, if we assume a hypothetical scenario where a human could somehow consume a helicopter, I'd estimate the number of helicopters a person could eat in one sitting would be... zero. That's right, folks, a big fat zero. Not\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs['sequences'][0][nb_tokens_in:], skip_special_tokens=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ktit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
