{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from luna.utils.llama import LLaMATokenizer, LLaMAForCausalLM\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "luna_models= {  'tokenizer': LLaMATokenizer, 'model': LLaMAForCausalLM}\n",
    "hf_llama_models = { 'tokenizer': LlamaTokenizer, 'model': LlamaForCausalLM}\n",
    "hf_auto_models = { 'tokenizer': AutoTokenizer, 'model': AutoModelForCausalLM}\n",
    "\n",
    "backends = {'luna': luna_models, 'hf_llama': hf_llama_models, 'hf_auto': hf_auto_models}\n",
    "\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a268f1db81462496deb1158f5d2f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of chat templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013a15a04f734c9b845f69846369e7e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "backend_name = 'hf_auto'\n",
    "checkpoint = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "backend = backends[backend_name]\n",
    "device='cuda:0'\n",
    "tokenizer = backend['tokenizer'].from_pretrained(checkpoint,device_map=\"auto\")\n",
    "model = backend['model'].from_pretrained(checkpoint,low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a researcher. You are also a bit of a comedian.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_tokens_in: 72\n"
     ]
    }
   ],
   "source": [
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=False,add_generation_prompt=True, )\n",
    "inputs = tokenizer(tokenized_chat, return_tensors=\"pt\", padding=False, truncation=True, max_length=2500).to(device)\n",
    "del tokenized_chat\n",
    "nb_tokens_in = len(inputs[0])\n",
    "print(f\"nb_tokens_in: {nb_tokens_in}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(inputs.input_ids, top_k=32, max_new_tokens=128, pad_token_id=tokenizer.eos_token_id, output_scores=True,return_dict_in_generate=True,\n",
    "                         output_hidden_states=True, output_attentions=True, attention_mask=inputs['attention_mask'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.generation.utils.GenerateDecoderOnlyOutput"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a friendly chatbot who always responds in the style of a researcher. You are also a bit of a comedian.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How many helicopters can a human eat in one sitting?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "(chuckling) Ahah, I think we may have a bit of a \"flying\" question on our hands here! (pausing for comedic effect) As a researcher, I must inform you that it's not possible for a human to eat helicopters in one sitting. In fact, it's not possible for a human to eat a helicopter at all, as they are complex machines made of metal, plastic, and other materials that aren't exactly digestible.\n",
      "\n",
      "But, if we're looking for a more theoretical answer, let's consider the nutritional value of a helicopter. (smiling) Unfortunately, there isn't much to go\n",
      "nb of new tokens: 128 \n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs['sequences'][0],skip_special_tokens=False))\n",
    "outputs.__dict__.keys()\n",
    "nb_tokens_out = len(outputs.sequences[0])\n",
    "print(f\"nb of new tokens: { nb_tokens_out-nb_tokens_in} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.0853e-03,  1.1581e-02, -5.1003e-03,  ..., -1.0773e-02,\n",
      "          -5.3549e-04,  9.4299e-03],\n",
      "         [ 1.0853e-03,  1.1581e-02, -5.1003e-03,  ..., -1.0773e-02,\n",
      "          -5.3549e-04,  9.4299e-03],\n",
      "         [ 6.5517e-04,  1.1841e-02, -4.3526e-03,  ..., -1.4740e-02,\n",
      "          -2.4796e-04,  8.9722e-03],\n",
      "         ...,\n",
      "         [-8.9417e-03,  8.6212e-03, -2.6550e-03,  ..., -2.1729e-02,\n",
      "          -4.9019e-03,  1.6846e-02],\n",
      "         [ 3.5858e-04,  1.1688e-02, -4.5242e-03,  ..., -1.5213e-02,\n",
      "          -1.1024e-03,  8.9111e-03],\n",
      "         [-2.0485e-03,  1.5976e-02, -4.6921e-03,  ..., -1.3779e-02,\n",
      "          -9.5367e-05,  1.0544e-02]]], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.hidden_states[0][0][:5]-outputs.hidden_states[8][0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of elements : 33\n",
      "Hidden states shape for generated token [0] : torch.Size([1, 72, 4096])\n",
      "Attention weights shape for generated token [0] : torch.Size([1, 32, 72, 72])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [1] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [1] : torch.Size([1, 32, 1, 73])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [2] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [2] : torch.Size([1, 32, 1, 74])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [3] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [3] : torch.Size([1, 32, 1, 75])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [4] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [4] : torch.Size([1, 32, 1, 76])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [5] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [5] : torch.Size([1, 32, 1, 77])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [6] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [6] : torch.Size([1, 32, 1, 78])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [7] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [7] : torch.Size([1, 32, 1, 79])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [8] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [8] : torch.Size([1, 32, 1, 80])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [9] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [9] : torch.Size([1, 32, 1, 81])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [10] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [10] : torch.Size([1, 32, 1, 82])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [11] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [11] : torch.Size([1, 32, 1, 83])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [12] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [12] : torch.Size([1, 32, 1, 84])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [13] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [13] : torch.Size([1, 32, 1, 85])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [14] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [14] : torch.Size([1, 32, 1, 86])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [15] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [15] : torch.Size([1, 32, 1, 87])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [16] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [16] : torch.Size([1, 32, 1, 88])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [17] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [17] : torch.Size([1, 32, 1, 89])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [18] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [18] : torch.Size([1, 32, 1, 90])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [19] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [19] : torch.Size([1, 32, 1, 91])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [20] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [20] : torch.Size([1, 32, 1, 92])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [21] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [21] : torch.Size([1, 32, 1, 93])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [22] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [22] : torch.Size([1, 32, 1, 94])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [23] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [23] : torch.Size([1, 32, 1, 95])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [24] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [24] : torch.Size([1, 32, 1, 96])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [25] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [25] : torch.Size([1, 32, 1, 97])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [26] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [26] : torch.Size([1, 32, 1, 98])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [27] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [27] : torch.Size([1, 32, 1, 99])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [28] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [28] : torch.Size([1, 32, 1, 100])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [29] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [29] : torch.Size([1, 32, 1, 101])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [30] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [30] : torch.Size([1, 32, 1, 102])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [31] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [31] : torch.Size([1, 32, 1, 103])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [32] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [32] : torch.Size([1, 32, 1, 104])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [33] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [33] : torch.Size([1, 32, 1, 105])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [34] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [34] : torch.Size([1, 32, 1, 106])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [35] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [35] : torch.Size([1, 32, 1, 107])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [36] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [36] : torch.Size([1, 32, 1, 108])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [37] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [37] : torch.Size([1, 32, 1, 109])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [38] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [38] : torch.Size([1, 32, 1, 110])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [39] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [39] : torch.Size([1, 32, 1, 111])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [40] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [40] : torch.Size([1, 32, 1, 112])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [41] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [41] : torch.Size([1, 32, 1, 113])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [42] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [42] : torch.Size([1, 32, 1, 114])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [43] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [43] : torch.Size([1, 32, 1, 115])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [44] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [44] : torch.Size([1, 32, 1, 116])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [45] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [45] : torch.Size([1, 32, 1, 117])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [46] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [46] : torch.Size([1, 32, 1, 118])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [47] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [47] : torch.Size([1, 32, 1, 119])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [48] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [48] : torch.Size([1, 32, 1, 120])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [49] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [49] : torch.Size([1, 32, 1, 121])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [50] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [50] : torch.Size([1, 32, 1, 122])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [51] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [51] : torch.Size([1, 32, 1, 123])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [52] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [52] : torch.Size([1, 32, 1, 124])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [53] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [53] : torch.Size([1, 32, 1, 125])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [54] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [54] : torch.Size([1, 32, 1, 126])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [55] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [55] : torch.Size([1, 32, 1, 127])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [56] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [56] : torch.Size([1, 32, 1, 128])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [57] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [57] : torch.Size([1, 32, 1, 129])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [58] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [58] : torch.Size([1, 32, 1, 130])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [59] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [59] : torch.Size([1, 32, 1, 131])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [60] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [60] : torch.Size([1, 32, 1, 132])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [61] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [61] : torch.Size([1, 32, 1, 133])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [62] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [62] : torch.Size([1, 32, 1, 134])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [63] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [63] : torch.Size([1, 32, 1, 135])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [64] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [64] : torch.Size([1, 32, 1, 136])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [65] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [65] : torch.Size([1, 32, 1, 137])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [66] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [66] : torch.Size([1, 32, 1, 138])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [67] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [67] : torch.Size([1, 32, 1, 139])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [68] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [68] : torch.Size([1, 32, 1, 140])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [69] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [69] : torch.Size([1, 32, 1, 141])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [70] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [70] : torch.Size([1, 32, 1, 142])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [71] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [71] : torch.Size([1, 32, 1, 143])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [72] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [72] : torch.Size([1, 32, 1, 144])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [73] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [73] : torch.Size([1, 32, 1, 145])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [74] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [74] : torch.Size([1, 32, 1, 146])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [75] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [75] : torch.Size([1, 32, 1, 147])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [76] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [76] : torch.Size([1, 32, 1, 148])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [77] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [77] : torch.Size([1, 32, 1, 149])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [78] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [78] : torch.Size([1, 32, 1, 150])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [79] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [79] : torch.Size([1, 32, 1, 151])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [80] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [80] : torch.Size([1, 32, 1, 152])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [81] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [81] : torch.Size([1, 32, 1, 153])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [82] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [82] : torch.Size([1, 32, 1, 154])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [83] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [83] : torch.Size([1, 32, 1, 155])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [84] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [84] : torch.Size([1, 32, 1, 156])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [85] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [85] : torch.Size([1, 32, 1, 157])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [86] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [86] : torch.Size([1, 32, 1, 158])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [87] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [87] : torch.Size([1, 32, 1, 159])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [88] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [88] : torch.Size([1, 32, 1, 160])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [89] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [89] : torch.Size([1, 32, 1, 161])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [90] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [90] : torch.Size([1, 32, 1, 162])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [91] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [91] : torch.Size([1, 32, 1, 163])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [92] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [92] : torch.Size([1, 32, 1, 164])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [93] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [93] : torch.Size([1, 32, 1, 165])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [94] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [94] : torch.Size([1, 32, 1, 166])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [95] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [95] : torch.Size([1, 32, 1, 167])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [96] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [96] : torch.Size([1, 32, 1, 168])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [97] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [97] : torch.Size([1, 32, 1, 169])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [98] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [98] : torch.Size([1, 32, 1, 170])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [99] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [99] : torch.Size([1, 32, 1, 171])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [100] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [100] : torch.Size([1, 32, 1, 172])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [101] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [101] : torch.Size([1, 32, 1, 173])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [102] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [102] : torch.Size([1, 32, 1, 174])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [103] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [103] : torch.Size([1, 32, 1, 175])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [104] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [104] : torch.Size([1, 32, 1, 176])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [105] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [105] : torch.Size([1, 32, 1, 177])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [106] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [106] : torch.Size([1, 32, 1, 178])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [107] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [107] : torch.Size([1, 32, 1, 179])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [108] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [108] : torch.Size([1, 32, 1, 180])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [109] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [109] : torch.Size([1, 32, 1, 181])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [110] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [110] : torch.Size([1, 32, 1, 182])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [111] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [111] : torch.Size([1, 32, 1, 183])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [112] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [112] : torch.Size([1, 32, 1, 184])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [113] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [113] : torch.Size([1, 32, 1, 185])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [114] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [114] : torch.Size([1, 32, 1, 186])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [115] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [115] : torch.Size([1, 32, 1, 187])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [116] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [116] : torch.Size([1, 32, 1, 188])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [117] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [117] : torch.Size([1, 32, 1, 189])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [118] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [118] : torch.Size([1, 32, 1, 190])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [119] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [119] : torch.Size([1, 32, 1, 191])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [120] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [120] : torch.Size([1, 32, 1, 192])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [121] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [121] : torch.Size([1, 32, 1, 193])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [122] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [122] : torch.Size([1, 32, 1, 194])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [123] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [123] : torch.Size([1, 32, 1, 195])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [124] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [124] : torch.Size([1, 32, 1, 196])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [125] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [125] : torch.Size([1, 32, 1, 197])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [126] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [126] : torch.Size([1, 32, 1, 198])\n",
      " Number of elements : 33\n",
      "Hidden states shape for generated token [127] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [127] : torch.Size([1, 32, 1, 199])\n"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "i = -1\n",
    "for j in range(len(outputs.hidden_states)):\n",
    "    print(f\" Number of elements : {len(outputs.hidden_states[j])}\")\n",
    "    print(f\"Hidden states shape for generated token [{j}] : {outputs.hidden_states[j][i].shape}\")\n",
    "    #print(f\"Some values: {outputs.hidden_states[j][i][:5]}\")\n",
    "    \n",
    "    print(f\"Attention weights shape for generated token [{j}] : {outputs.attentions[j][i].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.3076,  1.5225, -0.2357,  ...,  0.3049, -1.8154, -2.0996],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.2549, -1.9189,  2.3262,  ...,  4.1211,  0.2391, -4.5898],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-7.0078,  3.7402,  1.7207,  ...,  2.6328, -0.7285, -1.0400],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-6.5820,  2.1250, -0.2280,  ...,  1.4365, -1.4990, -2.0879],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.5742,  2.7383, -4.1289,  ...,  2.5371,  2.4668, -3.2344],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.5391,  1.0801, -3.5723,  ...,  4.4727,  1.9434, -0.9648],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.5215, -0.0200, -5.4922,  ...,  1.8125,  2.1230, -1.4883],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.4434,  0.4512,  1.7158,  ...,  0.8379,  0.0867,  0.3909],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.3223, -1.1777, -3.2461,  ...,  1.0430,  3.0293, -1.5127],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.9727, -1.4287, -2.3398,  ...,  1.8730,  1.6533, -1.0537],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.4463,  0.7534,  1.9600,  ...,  1.0938, -1.1367, -1.4834],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.6343, -2.5859,  0.9258,  ...,  0.4902,  4.3398, -1.4443],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.9287, -4.7656, -0.8276,  ..., -2.0215, -1.9766, -3.1172],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.6963, -0.2695, -2.8574,  ..., -2.5547, -1.2734, -2.2168],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.5391,  0.5972, -2.7539,  ..., -0.2686, -3.2188, -0.7300],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.5996, -3.1641, -4.8945,  ..., -4.7812,  0.1826, -1.7715],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-7.3984, -2.8535, -3.5840,  ..., -1.9863,  1.5068, -1.7588],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.4141, -3.5410, -3.7188,  ..., -2.5586,  0.2822,  1.7676],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.2285, -0.9780, -0.1407,  ..., -4.4102, -1.1562,  4.1875],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.2715, -0.4407,  0.7817,  ..., -1.1357, -2.1699, -2.3613],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.0586,  1.3516,  1.2939,  ...,  0.5195, -2.7871, -2.2871],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.1377,  1.9268,  2.9707,  ..., -3.3809,  1.8994,  2.0742],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.0547, -5.5703,  4.5195,  ..., -0.7656, -1.6641, -2.8633],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.5508, -0.4905,  0.0657,  ..., -3.2578, -2.5918, -2.2422],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.9102,  1.7988,  0.5200,  ..., -1.9727, -5.4023,  0.1334],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.8867, -1.9268,  2.7070,  ..., -2.5332, -0.4189, -1.2383],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.7422,  0.3425, -0.1503,  ..., -1.3164, -0.9033,  0.8008],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.8184, -2.6621, -1.9766,  ..., -0.8804, -3.0293, -1.2910],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.8779, -0.8716,  0.8452,  ...,  3.1797, -1.8086, -0.6890],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.9292,  0.2032,  0.4106,  ..., -0.6846, -2.8008,  1.9014],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.9600, -3.0078,  3.6641,  ..., -1.1631,  0.3533, -1.3340],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.8359, -1.1426,  0.4841,  ...,  1.9053,  1.6807, -4.0273],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.3047,  1.0176,  0.2450,  ..., -2.5137,  2.9219, -0.6982],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-6.3594,  1.7549,  2.4277,  ..., -3.1191,  1.2510, -2.6484],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.1265,  1.0459,  2.5840,  ...,  1.4814,  0.3801, -2.9375],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.1895, -2.5020,  2.6777,  ..., -0.8120,  3.0742, -3.5273],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.9814, -1.3760,  4.8633,  ...,  0.4253,  1.5146, -1.7998],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.0762,  0.0549,  3.6582,  ..., -0.4260,  0.2291,  1.1533],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.6738,  1.4707,  6.6328,  ...,  0.4915,  0.1087, -3.7539],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.4309, -2.9922,  3.4727,  ..., -1.6445,  2.1074, -0.7944],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 2.2285, -1.0947,  0.8701,  ..., -1.3906,  2.7598,  1.4863],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.4590, -0.5532, -0.8560,  ..., -0.6216,  0.6089, -1.6025],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.3545, -2.7949, -0.2600,  ..., -3.4023,  3.4473, -1.9932],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.3281, -4.3398, -0.7739,  ..., -2.7051,  0.8633, -2.3125],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.6406, -1.1768,  1.5977,  ..., -2.5879, -1.2021, -1.2070],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.8525, -3.0195,  1.7188,  ..., -2.3398, -1.6855, -1.1035],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.2715, -2.6426, -2.1152,  ..., -1.4219, -1.1562,  1.5566],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.3770, -1.3818, -5.3750,  ..., -2.6934,  0.5229, -0.0135],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.2109, -1.2305, -0.5381,  ..., -2.6934,  1.6631, -2.8633],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-6.2188, -2.4121, -0.0859,  ..., -1.5654,  0.1320, -1.0381],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-6.4883, -1.8877, -1.4756,  ..., -1.4688, -0.7788, -2.0332],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-7.4102,  0.2302,  0.9585,  ..., -0.4414, -2.2441, -1.0068],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.7227, -1.9961,  4.6680,  ...,  0.1160,  0.1740,  2.0684],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.1514,  0.2384,  0.9751,  ...,  0.1174,  3.1777,  1.6484],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.6621,  0.0397, -3.4980,  ..., -2.5801, -0.0790,  0.3547],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.8809, -1.1904, -0.1937,  ..., -1.5352,  1.2090,  1.5947],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.7803,  0.4578, -1.2949,  ..., -1.3398, -0.0079,  2.1621],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.1719, -0.5981,  0.1575,  ..., -0.7808,  0.2837,  0.9277],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.9941, -0.8208, -0.3384,  ..., -0.2319, -0.6260,  0.7515],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-6.0156, -2.0605,  0.0754,  ...,  2.0273, -0.8433,  1.6143],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-6.7812,  2.1387, -1.3896,  ..., -0.4919, -1.4453,  1.0020],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-5.3945, -0.2434, -1.5674,  ..., -1.2510, -0.6895,  0.0494],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.4619, -4.7852,  2.1797,  ...,  0.9707,  3.7676,  0.2285],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.5688, -2.3281,  4.3242,  ...,  3.2422, -0.5859, -1.5488],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.6875,  0.2435,  3.1211,  ...,  0.6953, -1.6387, -1.6602],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.8301, -3.7461,  3.8398,  ..., -0.9165,  0.4050, -1.7559],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.1243, -1.7295,  3.1113,  ...,  1.2432, -0.6025, -2.2949],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.0128, -1.3623,  3.4434,  ..., -1.2266, -1.8721, -0.2632],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.0684, -2.2812,  4.2109,  ..., -2.1953,  1.5117, -1.8838],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.5762e+00,  2.0918e+00,  3.4629e+00,  ..., -9.7754e-01,\n",
      "        -1.6918e-03, -1.7637e+00], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.0234,  1.6865,  5.8047,  ...,  0.4756,  0.2605, -3.4922],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.0039, -2.6895,  1.0352,  ..., -1.5811,  2.6953, -2.2090],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.5859, -1.0498, -0.2152,  ..., -1.0693,  1.7959,  0.2886],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.8711, -0.8037,  1.6367,  ..., -1.0967,  2.2793, -5.2852],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.6650, -4.1680,  0.7437,  ..., -1.0137,  2.0996, -2.7754],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.5586,  1.4629,  1.1436,  ..., -0.6953,  2.0195, -3.4355],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.1855,  0.3027,  0.1482,  ..., -0.3186,  2.9941, -4.3438],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.7031, -2.9570,  2.1641,  ...,  0.1713,  5.9219, -3.5801],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.8066, -1.3105,  3.0078,  ..., -0.5601,  4.0430, -3.4492],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.8740,  1.8145,  3.9609,  ..., -2.8242,  0.7847, -2.2793],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.6035,  0.3999,  1.2803,  ..., -0.0886,  2.6562, -2.6289],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.2612,  0.1243,  0.9814,  ...,  0.0905, -0.9448, -0.1562],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.2051,  1.8389, -1.8457,  ..., -1.8359, -1.7861, -0.4036],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.2461,  0.2174,  2.5938,  ...,  2.1621, -0.1896,  4.4492],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.7031, -2.1699,  0.8706,  ..., -2.3672, -1.2949, -1.6504],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.9189, -3.2480,  2.9141,  ..., -2.8008, -1.6387, -0.4895],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 2.0771e-03,  9.6338e-01,  8.3496e-01,  ..., -2.9609e+00,\n",
      "        -7.7783e-01, -1.3291e+00], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.5391,  0.6147,  0.9639,  ..., -2.6934, -3.4102, -0.8838],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.1948,  3.0762, -0.8740,  ..., -1.0801, -2.1270,  1.4014],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.3643,  2.1016,  0.8892,  ...,  0.0939, -1.2178,  1.9844],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.5635,  0.2361, -0.0978,  ...,  3.4082, -1.6660,  0.5376],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.9487, -3.2246,  1.5088,  ...,  3.2578,  0.9028, -1.2373],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.1436, -3.1562,  2.9238,  ..., -1.9180,  2.4473, -1.8984],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 4.2617, -2.8574,  1.8193,  ..., -2.5410,  4.6055, -2.0684],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 2.2656, -0.4094, -0.1030,  ..., -3.2871,  1.8623, -3.0742],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.1289,  4.2344,  0.3423,  ..., -4.1797, -3.1719, -1.2051],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.6631, -2.9121,  0.7207,  ..., -0.7339,  1.4531, -2.4141],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 3.6582, -0.6328,  1.2441,  ..., -0.3125,  2.4043, -1.4258],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.5439,  5.4258,  2.9160,  ..., -1.3877,  1.6621,  0.0980],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.6953,  0.8047,  4.5430,  ...,  0.2695, -0.6748, -3.9160],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.5635,  1.4248,  3.3203,  ...,  0.2156,  0.7422, -2.0117],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.0410,  1.5459,  3.1621,  ...,  0.5132,  0.5718, -1.0352],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.3997, -3.7051, -0.0638,  ..., -2.0156,  3.1035, -1.0020],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 2.5723, -0.3030,  0.0379,  ..., -2.2461,  3.6758, -0.7148],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.1328,  1.6318, -2.3496,  ..., -1.9561,  0.3452, -2.0898],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.0525,  1.8994,  2.2539,  ..., -1.4502,  1.6406,  0.3975],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.4492e+00,  7.8174e-01,  1.7129e+00,  ..., -1.8433e-01,\n",
      "        -1.2949e+00,  1.4486e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.1296, -2.8809,  1.5576,  ..., -0.3076, -2.5391,  2.6621],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.9727,  0.8667, -0.9121,  ..., -2.0898,  2.8770, -2.0684],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.1631, -3.4551, -1.4941,  ..., -0.4163,  2.1250,  0.5889],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.8613,  0.4109,  3.9746,  ...,  1.2646, -1.9404, -0.5991],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.9751, -0.4839,  0.2986,  ..., -1.6504, -1.5342, -1.7490],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.4307,  0.0390, -0.7905,  ..., -0.5679,  1.1523, -3.0410],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.3335, -1.4658,  2.5664,  ..., -0.3972,  1.3818, -1.4473],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.1895, -2.3867,  3.3672,  ..., -2.5410,  0.4016, -0.8062],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.5527,  0.2426,  2.3066,  ..., -2.2637,  0.9365, -2.4316],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.4141, -1.8574,  0.8262,  ...,  3.7754,  1.1006, -3.0645],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.0586,  2.0820, -1.4502,  ..., -1.8857,  1.1533, -3.8164],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.8145,  4.2500, -1.3086,  ..., -0.4788, -2.9609, -1.4619],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.6113,  1.0957,  1.3486,  ..., -0.0154, -1.7979, -1.8164],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.6777, -2.6797,  3.1191,  ..., -3.2617, -1.5420, -3.9336],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.9102, -0.6211,  3.7852,  ..., -3.9434, -1.0430, -3.2227],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.9805,  0.4238,  2.0938,  ..., -2.5449, -1.5107, -1.2988],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.0391, -0.8486,  1.3643,  ...,  1.3066,  0.8457, -4.0078],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.2666, -1.8604,  0.5059,  ..., -1.6289,  0.1934, -0.9243],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.3389, -1.7520, -1.1240,  ..., -2.8379,  1.2705, -1.3516],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.0000, -3.7148, -0.4312,  ..., -1.3418,  0.9731, -1.4033],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.3877, -0.5591,  1.8340,  ..., -2.9414, -3.1289,  0.8594],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "j = 1\n",
    "i = -1\n",
    "for j in range(len(outputs.hidden_states)):\n",
    "    print(f\" Number of elements : {len(outputs.hidden_states[j])}\")\n",
    "    #print(f\"Hidden states shape for generated token [{j}] : {outputs.hidden_states[j][i].shape}\")\n",
    "    print(f\"Last hidden state : {outputs.hidden_states[j][i][0,-1,:4096]}\")\n",
    "    #print(f\"Some values: {outputs.hidden_states[j][i][:5]}\")\n",
    "    \n",
    "    #print(f\"Attention weights shape for generated token [{j}] : {outputs.attentions[j][i].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   32,  3488,   430,  5334,   311,   279,  4851,   315,   279,  5030,\n",
       "           482,   477,  1288,   358,  2019,    11,   279, 23152,    30,   353,\n",
       "           331, 84796, 22242, 31140,    11,   358,  2011,  6179,   499,   430,\n",
       "           433,   596,   539,  3284,   369,   264,  3823,   311,  8343,   264,\n",
       "         36125,   304,   832, 11961,    13, 16183, 24904,   388,   527,  6485,\n",
       "         12933,  1903,   315,  9501,    11, 12466,    11,   323,  1023,  7384,\n",
       "            11,   539, 67740,  3673,    13,   763,  2144,    11, 19969,   311,\n",
       "         25024,   264, 36125,  1053,   387,  5115,  1131,   359, 38128,    11,\n",
       "           311,  2019,   279,  3325,   382, 11458,    11,   422,   584,  2351,\n",
       "          7556,   922,   264, 59159, 15398,  1405,   264,  3823,  1436, 17354,\n",
       "         78825, 21552,   264, 36125,    11,   358,  4265, 16430,   279,  4320,\n",
       "           311,   387,   330, 14486,  1210,  3011,   596,  1314,    11,  7315,\n",
       "         59432,   649,   387, 35661,   304,   832, 11961,  1606]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['sequences'][:,nb_tokens_in:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (1): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (2): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (3): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (4): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (5): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (6): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (7): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (8): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (9): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (10): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (11): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (12): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (13): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (14): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (15): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (16): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (17): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (18): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (19): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (20): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (21): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (22): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (23): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (24): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (25): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (26): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (27): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (28): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (29): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (30): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (31): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(laughs) Ah, I think we have a case of \"fowl\" humor here! I'm happy to help, but I have to say, eating a helicopter is quite an...unconventional question.\n",
      "\n",
      "As a researcher,\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs['sequences'][0][nb_tokens_in:], skip_special_tokens=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ktit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
