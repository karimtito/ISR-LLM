{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from luna.utils.llama import LLaMATokenizer, LLaMAForCausalLM\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "luna_models= {  'tokenizer': LLaMATokenizer, 'model': LLaMAForCausalLM}\n",
    "hf_llama_models = { 'tokenizer': LlamaTokenizer, 'model': LlamaForCausalLM}\n",
    "hf_auto_models = { 'tokenizer': AutoTokenizer, 'model': AutoModelForCausalLM}\n",
    "\n",
    "backends = {'luna': luna_models, 'hf_llama': hf_llama_models, 'hf_auto': hf_auto_models}\n",
    "\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of chat templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d2bd82b8694cc6828cf3433715bca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "backend_name = 'hf_auto'\n",
    "\n",
    "model_dict= {\"llama_8B\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "             \"meta_8B\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "              \"llama_3B\": \"meta-llama/Llama-3.2-3B-Instruct\", \n",
    "              \"mistral_nemo\": \"mistralai/Mistral-Nemo-Instruct-2407\",\n",
    "              \"gemma_9B\": \"google/gemma-2-9b-it\",\n",
    "              \"gemma_27B\": \"google/gemma-2-27b-it\",\n",
    "              \"gemma_7B\": \"google/gemma-7b-it\",\n",
    "              \"qwen_14B\": \"Qwen/Qwen2.5-14B-Instruct\",\n",
    "              \"qwen_7B\": \"Qwen/Qwen2.5-7B-Instruct\"}\n",
    "model_name= \"llama_8B\"\n",
    "checkpoint = model_dict[model_name]\n",
    "temperature=0.0001\n",
    "backend = backends[backend_name]\n",
    "device='cuda:0'\n",
    "tokenizer = backend['tokenizer'].from_pretrained(checkpoint,device_map=\"cuda:0\")\n",
    "model = backend['model'].from_pretrained(checkpoint,low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a researcher. You are also a bit of a comedian.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_tokens_in: 72\n"
     ]
    }
   ],
   "source": [
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=False,add_generation_prompt=True, )\n",
    "inputs = tokenizer(tokenized_chat, return_tensors=\"pt\", padding=False, truncation=True, max_length=2500).to(device)\n",
    "#del tokenized_chat\n",
    "nb_tokens_in = len(inputs[0])\n",
    "print(f\"nb_tokens_in: {nb_tokens_in}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\nYou are a friendly chatbot who always responds in the style of a researcher. You are also a bit of a comedian.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow many helicopters can a human eat in one sitting?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a friendly chatbot who always responds in the style of a researcher. You are also a bit of a comedian.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How many helicopters can a human eat in one sitting?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the tokens in natural language\n",
    "print(tokenizer.decode(inputs['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(inputs.input_ids, top_k=128, max_new_tokens=128, pad_token_id=tokenizer.eos_token_id, output_scores=True,return_dict_in_generate=True,\n",
    "                         output_hidden_states=True, output_attentions=True, attention_mask=inputs['attention_mask'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266 ms ± 4.02 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for _ in range(100):\n",
    "    tokenizer.batch_decode(outputs['sequences'][0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.26 ms ± 105 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for _ in range(100):\n",
    "    tokenizer.decode(outputs['sequences'][0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.generation.utils.GenerateDecoderOnlyOutput"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a friendly chatbot who always responds in the style of a researcher. You are also a bit of a comedian.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How many helicopters can a human eat in one sitting?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "What an intriguing question, my friend.  I must admit, I've never come across any credible research on this topic before. However, I can try to provide some insights based on what I know about human physiology and the size of helicopters.\n",
      "\n",
      "Firstly, let's consider the average size of a helicopter. A small helicopter, like a Robinson R22, weighs around 1,100 pounds (500 kg). A larger helicopter, like a Sikorsky S-92, can weigh up to 26,000 pounds (11,800 kg). For the sake of this thought experiment, let's assume we're talking about a small\n",
      "nb of new tokens: 128 \n"
     ]
    }
   ],
   "source": [
    "c = tokenizer.decode(outputs['sequences'][0],skip_special_tokens=False)\n",
    "print(c)\n",
    "outputs.__dict__.keys()\n",
    "nb_tokens_out = len(outputs.sequences[0])\n",
    "print(f\"nb of new tokens: { nb_tokens_out-nb_tokens_in} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 9.4032e-04,  7.6294e-05, -1.0605e-03,  ...,  4.8828e-04,\n",
      "          -3.2759e-04,  7.1526e-04],\n",
      "         [ 9.4032e-04,  7.6294e-05, -1.0605e-03,  ...,  4.8828e-04,\n",
      "          -3.2759e-04,  7.1526e-04],\n",
      "         [ 5.1022e-04,  3.3283e-04, -3.1281e-04,  ..., -3.4790e-03,\n",
      "          -4.0054e-05,  2.5749e-04],\n",
      "         ...,\n",
      "         [-9.0942e-03, -2.8877e-03,  1.3847e-03,  ..., -1.0468e-02,\n",
      "          -4.6921e-03,  8.1329e-03],\n",
      "         [ 2.1362e-04,  1.8311e-04, -4.8351e-04,  ..., -3.9520e-03,\n",
      "          -8.9455e-04,  1.9264e-04],\n",
      "         [-2.1935e-03,  4.4670e-03, -6.5041e-04,  ..., -2.5177e-03,\n",
      "           1.1253e-04,  1.8291e-03]]], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.hidden_states[0][0][:5]-outputs.hidden_states[8][0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of elements : 32\n",
      "Hidden states shape for generated token [0] : torch.Size([1, 72, 4096])\n",
      "Attention weights shape for generated token [0] : torch.Size([1, 32, 72, 72])\n",
      "Last layer attentions for generated token [0] : tensor([0.2468, 0.2473, 0.0012, 0.0014, 0.0008, 0.0104, 0.0011, 0.0009, 0.0013,\n",
      "        0.0018, 0.0077, 0.0013, 0.0009, 0.0012, 0.0284, 0.0298, 0.0022, 0.0029,\n",
      "        0.0009, 0.0016, 0.0009, 0.0014, 0.0110, 0.0027, 0.0120, 0.0504, 0.0099,\n",
      "        0.0026, 0.0011, 0.0026, 0.0029, 0.0064, 0.0024, 0.0027, 0.0018, 0.0025,\n",
      "        0.0010, 0.0018, 0.0012, 0.0005, 0.0066, 0.0024, 0.0024, 0.0025, 0.0025,\n",
      "        0.0009, 0.0016, 0.0009, 0.0009, 0.0058, 0.0038, 0.0023, 0.0047, 0.0137,\n",
      "        0.0028, 0.0212, 0.0036, 0.0040, 0.0104, 0.0047, 0.0037, 0.0049, 0.0091,\n",
      "        0.0052, 0.0026, 0.0030, 0.0032, 0.0151, 0.0027, 0.0438, 0.0311, 0.0702],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [1] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [1] : torch.Size([1, 32, 1, 73])\n",
      "Last layer attentions for generated token [1] : tensor([2.8882e-01, 2.8809e-01, 6.7902e-04, 6.0272e-04, 4.0054e-04, 9.2392e-03,\n",
      "        6.3419e-04, 5.9700e-04, 1.0033e-03, 1.0738e-03, 4.1237e-03, 4.9686e-04,\n",
      "        5.0449e-04, 1.2760e-03, 9.6664e-03, 1.7288e-02, 6.2895e-04, 6.7472e-04,\n",
      "        2.9898e-04, 5.8651e-04, 1.6189e-04, 4.1914e-04, 1.4610e-02, 2.1000e-03,\n",
      "        6.5804e-03, 3.1891e-02, 3.0212e-03, 1.1501e-03, 1.0071e-03, 1.7271e-03,\n",
      "        1.1282e-03, 1.9455e-03, 2.2602e-03, 1.4744e-03, 8.6164e-04, 2.0351e-03,\n",
      "        1.0176e-03, 1.0576e-03, 1.4124e-03, 7.0047e-04, 3.2902e-03, 3.9406e-03,\n",
      "        2.5978e-03, 1.8559e-03, 1.9150e-03, 1.3952e-03, 1.1082e-03, 8.9741e-04,\n",
      "        1.5211e-03, 2.7428e-03, 3.8910e-03, 6.2037e-04, 6.4774e-03, 6.5346e-03,\n",
      "        5.7907e-03, 2.0981e-02, 2.9697e-03, 4.2114e-03, 8.1100e-03, 4.3449e-03,\n",
      "        4.1161e-03, 3.5286e-03, 3.7785e-03, 3.2902e-03, 1.6317e-03, 9.8038e-04,\n",
      "        8.1177e-03, 1.1330e-02, 4.1542e-03, 4.0680e-02, 3.3661e-02, 7.9224e-02,\n",
      "        1.7075e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [2] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [2] : torch.Size([1, 32, 1, 74])\n",
      "Last layer attentions for generated token [2] : tensor([0.2747, 0.2747, 0.0008, 0.0007, 0.0004, 0.0071, 0.0005, 0.0006, 0.0012,\n",
      "        0.0010, 0.0045, 0.0005, 0.0004, 0.0007, 0.0103, 0.0121, 0.0008, 0.0008,\n",
      "        0.0004, 0.0006, 0.0003, 0.0010, 0.0129, 0.0017, 0.0073, 0.0230, 0.0013,\n",
      "        0.0012, 0.0004, 0.0021, 0.0011, 0.0025, 0.0008, 0.0012, 0.0010, 0.0008,\n",
      "        0.0004, 0.0010, 0.0008, 0.0004, 0.0054, 0.0031, 0.0008, 0.0017, 0.0018,\n",
      "        0.0006, 0.0013, 0.0012, 0.0016, 0.0051, 0.0035, 0.0006, 0.0047, 0.0093,\n",
      "        0.0024, 0.0164, 0.0011, 0.0031, 0.0072, 0.0040, 0.0020, 0.0039, 0.0062,\n",
      "        0.0023, 0.0013, 0.0013, 0.0046, 0.0141, 0.0024, 0.0599, 0.0370, 0.0978,\n",
      "        0.0114, 0.0267], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [3] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [3] : torch.Size([1, 32, 1, 75])\n",
      "Last layer attentions for generated token [3] : tensor([0.2452, 0.2452, 0.0013, 0.0007, 0.0007, 0.0082, 0.0006, 0.0007, 0.0012,\n",
      "        0.0012, 0.0053, 0.0007, 0.0013, 0.0021, 0.0190, 0.0188, 0.0017, 0.0012,\n",
      "        0.0006, 0.0010, 0.0003, 0.0015, 0.0143, 0.0044, 0.0085, 0.0275, 0.0050,\n",
      "        0.0014, 0.0012, 0.0018, 0.0016, 0.0044, 0.0026, 0.0020, 0.0014, 0.0018,\n",
      "        0.0008, 0.0018, 0.0011, 0.0007, 0.0086, 0.0049, 0.0025, 0.0026, 0.0021,\n",
      "        0.0011, 0.0010, 0.0008, 0.0011, 0.0037, 0.0037, 0.0010, 0.0048, 0.0108,\n",
      "        0.0034, 0.0195, 0.0022, 0.0054, 0.0064, 0.0042, 0.0031, 0.0045, 0.0074,\n",
      "        0.0050, 0.0037, 0.0032, 0.0071, 0.0117, 0.0027, 0.0532, 0.0323, 0.0825,\n",
      "        0.0230, 0.0162, 0.0135], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [4] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [4] : torch.Size([1, 32, 1, 76])\n",
      "Last layer attentions for generated token [4] : tensor([2.7539e-01, 2.7539e-01, 5.7364e-04, 9.1887e-04, 3.1996e-04, 4.9820e-03,\n",
      "        4.6563e-04, 5.6839e-04, 1.1616e-03, 2.2545e-03, 5.2299e-03, 7.3385e-04,\n",
      "        5.9080e-04, 9.4604e-04, 1.4191e-02, 1.3756e-02, 1.5383e-03, 1.1635e-03,\n",
      "        5.6028e-04, 7.1812e-04, 2.3365e-04, 1.2465e-03, 1.2329e-02, 2.1992e-03,\n",
      "        6.0463e-03, 1.6418e-02, 7.1106e-03, 1.0910e-03, 1.1215e-03, 1.7433e-03,\n",
      "        1.0347e-03, 2.6531e-03, 2.2755e-03, 1.5059e-03, 1.0805e-03, 2.7618e-03,\n",
      "        1.0958e-03, 1.7471e-03, 1.6994e-03, 7.5436e-04, 7.5493e-03, 4.5967e-03,\n",
      "        3.1166e-03, 2.7962e-03, 2.2087e-03, 1.7643e-03, 9.0647e-04, 7.9823e-04,\n",
      "        1.2312e-03, 5.0507e-03, 4.8904e-03, 9.0265e-04, 4.8218e-03, 8.6899e-03,\n",
      "        4.2229e-03, 1.4870e-02, 6.5994e-03, 6.2981e-03, 8.9340e-03, 5.6458e-03,\n",
      "        4.9667e-03, 4.2839e-03, 5.9395e-03, 8.8959e-03, 4.4479e-03, 2.5768e-03,\n",
      "        1.1520e-02, 1.0231e-02, 3.3970e-03, 3.6957e-02, 1.6693e-02, 5.1544e-02,\n",
      "        1.9073e-02, 1.3107e-02, 1.3649e-02, 2.9114e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [5] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [5] : torch.Size([1, 32, 1, 77])\n",
      "Last layer attentions for generated token [5] : tensor([0.1810, 0.1810, 0.0008, 0.0011, 0.0005, 0.0084, 0.0007, 0.0009, 0.0019,\n",
      "        0.0029, 0.0065, 0.0013, 0.0005, 0.0010, 0.0132, 0.0186, 0.0022, 0.0016,\n",
      "        0.0008, 0.0010, 0.0007, 0.0027, 0.0122, 0.0031, 0.0097, 0.0316, 0.0060,\n",
      "        0.0014, 0.0014, 0.0029, 0.0018, 0.0040, 0.0040, 0.0023, 0.0019, 0.0034,\n",
      "        0.0011, 0.0025, 0.0015, 0.0008, 0.0091, 0.0052, 0.0024, 0.0028, 0.0027,\n",
      "        0.0011, 0.0020, 0.0008, 0.0014, 0.0070, 0.0068, 0.0015, 0.0069, 0.0103,\n",
      "        0.0046, 0.0195, 0.0035, 0.0041, 0.0079, 0.0053, 0.0040, 0.0050, 0.0067,\n",
      "        0.0070, 0.0038, 0.0025, 0.0095, 0.0217, 0.0038, 0.0622, 0.0381, 0.0939,\n",
      "        0.0148, 0.0122, 0.0124, 0.0366, 0.0400], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [6] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [6] : torch.Size([1, 32, 1, 78])\n",
      "Last layer attentions for generated token [6] : tensor([0.2299, 0.2302, 0.0008, 0.0007, 0.0006, 0.0070, 0.0007, 0.0012, 0.0021,\n",
      "        0.0014, 0.0039, 0.0007, 0.0005, 0.0010, 0.0064, 0.0122, 0.0011, 0.0006,\n",
      "        0.0003, 0.0005, 0.0003, 0.0009, 0.0141, 0.0021, 0.0068, 0.0217, 0.0022,\n",
      "        0.0010, 0.0011, 0.0038, 0.0039, 0.0042, 0.0023, 0.0025, 0.0018, 0.0015,\n",
      "        0.0008, 0.0013, 0.0010, 0.0008, 0.0063, 0.0037, 0.0012, 0.0017, 0.0020,\n",
      "        0.0012, 0.0026, 0.0013, 0.0028, 0.0077, 0.0068, 0.0018, 0.0084, 0.0077,\n",
      "        0.0039, 0.0124, 0.0020, 0.0029, 0.0070, 0.0047, 0.0037, 0.0077, 0.0063,\n",
      "        0.0029, 0.0020, 0.0022, 0.0053, 0.0147, 0.0039, 0.0700, 0.0335, 0.0594,\n",
      "        0.0102, 0.0159, 0.0214, 0.0327, 0.0268, 0.0175], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [7] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [7] : torch.Size([1, 32, 1, 79])\n",
      "Last layer attentions for generated token [7] : tensor([2.7710e-01, 2.7661e-01, 9.2077e-04, 1.1044e-03, 5.9080e-04, 5.1460e-03,\n",
      "        4.1914e-04, 4.5395e-04, 1.0233e-03, 2.1706e-03, 3.8528e-03, 6.2895e-04,\n",
      "        6.3276e-04, 7.3528e-04, 1.0902e-02, 1.3023e-02, 1.1253e-03, 1.1826e-03,\n",
      "        6.7616e-04, 8.1539e-04, 2.5320e-04, 1.6413e-03, 1.2039e-02, 1.9341e-03,\n",
      "        5.6267e-03, 1.3786e-02, 5.7983e-03, 8.3160e-04, 5.5408e-04, 1.4830e-03,\n",
      "        1.3609e-03, 2.1343e-03, 1.6966e-03, 1.1015e-03, 7.7057e-04, 9.9373e-04,\n",
      "        7.1430e-04, 1.3504e-03, 9.3508e-04, 4.5657e-04, 5.8212e-03, 3.5362e-03,\n",
      "        2.0065e-03, 1.9035e-03, 1.3952e-03, 6.4373e-04, 5.6839e-04, 7.3671e-04,\n",
      "        7.4244e-04, 4.4785e-03, 5.3062e-03, 1.1635e-03, 3.5877e-03, 1.4153e-02,\n",
      "        3.2520e-03, 1.3412e-02, 3.8795e-03, 3.9062e-03, 7.4883e-03, 4.5700e-03,\n",
      "        2.9221e-03, 3.6812e-03, 5.2223e-03, 3.9215e-03, 2.8057e-03, 1.5450e-03,\n",
      "        8.0032e-03, 9.6664e-03, 3.1509e-03, 4.3976e-02, 1.8906e-02, 3.8757e-02,\n",
      "        1.4923e-02, 9.7275e-03, 1.1360e-02, 1.8677e-02, 3.0579e-02, 1.7685e-02,\n",
      "        1.7456e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [8] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [8] : torch.Size([1, 32, 1, 80])\n",
      "Last layer attentions for generated token [8] : tensor([2.0825e-01, 2.0825e-01, 5.2357e-04, 6.7616e-04, 4.0936e-04, 4.1046e-03,\n",
      "        3.8004e-04, 6.0940e-04, 6.9332e-04, 1.1549e-03, 3.0422e-03, 5.5265e-04,\n",
      "        2.5868e-04, 4.8876e-04, 1.2138e-02, 1.4503e-02, 7.5150e-04, 8.3303e-04,\n",
      "        4.8041e-04, 5.6839e-04, 1.9944e-04, 5.5170e-04, 6.6185e-03, 2.0523e-03,\n",
      "        4.6349e-03, 2.3529e-02, 5.7945e-03, 7.2956e-04, 5.3692e-04, 1.0090e-03,\n",
      "        5.6696e-04, 2.0962e-03, 1.6155e-03, 1.2684e-03, 1.2121e-03, 1.4095e-03,\n",
      "        5.1928e-04, 1.4038e-03, 8.7833e-04, 5.4455e-04, 5.7793e-03, 3.1261e-03,\n",
      "        2.3022e-03, 2.8267e-03, 2.4376e-03, 9.0599e-04, 1.0719e-03, 4.5824e-04,\n",
      "        6.9904e-04, 3.2501e-03, 3.4103e-03, 1.2627e-03, 5.0201e-03, 1.1902e-02,\n",
      "        3.2787e-03, 1.4198e-02, 2.3804e-03, 3.1757e-03, 5.9547e-03, 3.1166e-03,\n",
      "        2.6016e-03, 2.6112e-03, 5.4245e-03, 4.0245e-03, 1.7281e-03, 1.5144e-03,\n",
      "        6.8169e-03, 1.6830e-02, 2.6550e-03, 3.8788e-02, 3.6621e-02, 7.9712e-02,\n",
      "        2.2400e-02, 1.0208e-02, 7.0763e-03, 3.7537e-02, 4.1748e-02, 1.4046e-02,\n",
      "        2.0264e-02, 5.4871e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [9] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [9] : torch.Size([1, 32, 1, 81])\n",
      "Last layer attentions for generated token [9] : tensor([3.1738e-01, 3.1738e-01, 5.1069e-04, 8.5068e-04, 2.5988e-04, 5.7144e-03,\n",
      "        3.2544e-04, 3.6716e-04, 1.1091e-03, 9.3794e-04, 1.7519e-03, 5.2786e-04,\n",
      "        4.3273e-04, 1.0777e-03, 6.4354e-03, 1.2558e-02, 3.7670e-04, 2.6608e-04,\n",
      "        2.1720e-04, 4.5443e-04, 3.0136e-04, 2.7776e-04, 9.0103e-03, 1.2522e-03,\n",
      "        8.1329e-03, 2.4414e-02, 3.5381e-03, 4.7421e-04, 2.8777e-04, 3.9864e-04,\n",
      "        6.0797e-04, 1.7834e-03, 9.3937e-04, 6.8998e-04, 3.5238e-04, 6.6376e-04,\n",
      "        4.5252e-04, 7.2908e-04, 2.9969e-04, 1.2231e-04, 3.5152e-03, 2.2316e-03,\n",
      "        1.3151e-03, 9.0170e-04, 1.1759e-03, 3.1900e-04, 7.1764e-04, 3.2973e-04,\n",
      "        3.5238e-04, 3.0117e-03, 2.7008e-03, 9.4700e-04, 4.7607e-03, 6.4964e-03,\n",
      "        2.3022e-03, 6.8588e-03, 1.1158e-03, 1.9093e-03, 6.2523e-03, 2.2602e-03,\n",
      "        1.6460e-03, 3.5152e-03, 5.6152e-03, 2.9259e-03, 1.2045e-03, 1.3943e-03,\n",
      "        3.8033e-03, 8.3084e-03, 5.0163e-03, 2.2247e-02, 1.3336e-02, 2.7496e-02,\n",
      "        5.4970e-03, 5.4474e-03, 4.6463e-03, 1.8860e-02, 1.7838e-02, 7.4120e-03,\n",
      "        1.1429e-02, 2.0645e-02, 3.8696e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [10] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [10] : torch.Size([1, 32, 1, 82])\n",
      "Last layer attentions for generated token [10] : tensor([2.3499e-01, 2.3499e-01, 5.4073e-04, 8.1015e-04, 3.7980e-04, 4.4174e-03,\n",
      "        7.1383e-04, 1.1320e-03, 2.5997e-03, 1.4677e-03, 3.9482e-03, 6.1655e-04,\n",
      "        4.1699e-04, 1.5163e-03, 1.4679e-02, 1.1780e-02, 9.9087e-04, 6.0225e-04,\n",
      "        1.5926e-04, 2.9802e-04, 1.3673e-04, 2.3627e-04, 7.6981e-03, 1.0490e-03,\n",
      "        8.8196e-03, 1.8295e-02, 3.2024e-03, 6.4707e-04, 7.9155e-04, 9.7942e-04,\n",
      "        1.3676e-03, 2.4052e-03, 1.3752e-03, 1.5049e-03, 8.5258e-04, 9.7752e-04,\n",
      "        4.8184e-04, 8.1491e-04, 5.8937e-04, 3.0518e-04, 5.0621e-03, 3.2387e-03,\n",
      "        1.8387e-03, 1.0862e-03, 1.7023e-03, 6.9427e-04, 1.2627e-03, 6.1035e-04,\n",
      "        1.0424e-03, 4.2610e-03, 3.8471e-03, 7.6246e-04, 5.5351e-03, 6.8092e-03,\n",
      "        5.1003e-03, 9.2545e-03, 2.7866e-03, 2.3289e-03, 7.6866e-03, 3.5229e-03,\n",
      "        3.4752e-03, 5.9814e-03, 4.2496e-03, 2.1992e-03, 1.1406e-03, 1.5678e-03,\n",
      "        4.8256e-03, 9.0485e-03, 4.5509e-03, 4.8645e-02, 2.6978e-02, 3.8513e-02,\n",
      "        1.3504e-02, 1.1421e-02, 1.0284e-02, 3.7537e-02, 3.1433e-02, 1.2268e-02,\n",
      "        2.3895e-02, 3.0869e-02, 1.8295e-02, 2.1301e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [11] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [11] : torch.Size([1, 32, 1, 83])\n",
      "Last layer attentions for generated token [11] : tensor([2.7368e-01, 2.7368e-01, 2.6321e-04, 4.5037e-04, 1.6284e-04, 2.9221e-03,\n",
      "        5.8746e-04, 8.9025e-04, 1.3733e-03, 1.6413e-03, 3.7384e-03, 9.3842e-04,\n",
      "        4.1485e-04, 9.2411e-04, 9.2392e-03, 8.2703e-03, 7.7677e-04, 7.8249e-04,\n",
      "        2.9016e-04, 3.1996e-04, 1.5533e-04, 4.9162e-04, 6.9427e-03, 1.2684e-03,\n",
      "        6.6833e-03, 8.4610e-03, 2.0428e-03, 8.0919e-04, 5.3263e-04, 1.4067e-03,\n",
      "        1.8377e-03, 2.3079e-03, 1.5593e-03, 1.9875e-03, 9.4223e-04, 9.0599e-04,\n",
      "        2.8229e-04, 1.2074e-03, 6.2513e-04, 2.8801e-04, 3.5362e-03, 2.1152e-03,\n",
      "        7.8726e-04, 9.8133e-04, 1.7881e-03, 5.4741e-04, 1.6088e-03, 7.7820e-04,\n",
      "        1.1034e-03, 4.3526e-03, 2.7046e-03, 2.8563e-04, 3.9024e-03, 2.6016e-03,\n",
      "        2.9392e-03, 7.9575e-03, 2.1648e-03, 2.9888e-03, 4.6272e-03, 3.4599e-03,\n",
      "        1.7815e-03, 3.6087e-03, 5.4741e-03, 2.1400e-03, 1.7843e-03, 1.6928e-03,\n",
      "        4.0321e-03, 4.6501e-03, 2.3766e-03, 2.7802e-02, 1.5465e-02, 3.4149e-02,\n",
      "        1.1475e-02, 1.2436e-02, 1.3557e-02, 3.5156e-02, 2.5543e-02, 1.3008e-02,\n",
      "        3.8605e-02, 2.1347e-02, 1.0895e-02, 1.4404e-02, 2.5024e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [12] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [12] : torch.Size([1, 32, 1, 84])\n",
      "Last layer attentions for generated token [12] : tensor([1.9299e-01, 1.9299e-01, 2.7108e-04, 4.6372e-04, 2.2423e-04, 3.4561e-03,\n",
      "        6.0844e-04, 5.2452e-04, 7.9489e-04, 1.5087e-03, 4.1618e-03, 4.3559e-04,\n",
      "        8.0442e-04, 7.9823e-04, 7.4654e-03, 1.0788e-02, 7.8440e-04, 9.5701e-04,\n",
      "        3.8671e-04, 6.2037e-04, 1.6594e-04, 4.1890e-04, 8.2932e-03, 1.5144e-03,\n",
      "        4.5128e-03, 1.2077e-02, 3.6945e-03, 6.6280e-04, 7.4816e-04, 6.7711e-04,\n",
      "        1.0509e-03, 1.8644e-03, 2.4929e-03, 1.2627e-03, 7.1812e-04, 1.7881e-03,\n",
      "        5.8746e-04, 9.9087e-04, 9.0790e-04, 6.4278e-04, 3.8338e-03, 2.9945e-03,\n",
      "        2.0733e-03, 2.2240e-03, 1.7633e-03, 1.2827e-03, 6.4135e-04, 6.6423e-04,\n",
      "        1.5230e-03, 3.7518e-03, 4.2229e-03, 4.6563e-04, 3.3970e-03, 3.7193e-03,\n",
      "        3.8033e-03, 1.2627e-02, 2.8763e-03, 4.4174e-03, 6.0921e-03, 4.0741e-03,\n",
      "        5.2147e-03, 3.4943e-03, 3.8071e-03, 4.2267e-03, 3.0766e-03, 1.0328e-03,\n",
      "        8.7738e-03, 6.7253e-03, 2.2182e-03, 3.1769e-02, 1.8677e-02, 7.2449e-02,\n",
      "        1.7410e-02, 1.4290e-02, 1.3245e-02, 2.9053e-02, 4.7119e-02, 1.4679e-02,\n",
      "        2.0432e-02, 7.5500e-02, 2.9587e-02, 2.3788e-02, 1.0712e-02, 1.0078e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [13] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [13] : torch.Size([1, 32, 1, 85])\n",
      "Last layer attentions for generated token [13] : tensor([2.0532e-01, 2.0496e-01, 3.9244e-04, 5.5790e-04, 2.6703e-04, 3.5248e-03,\n",
      "        6.3848e-04, 6.0558e-04, 1.0061e-03, 1.5926e-03, 4.6654e-03, 8.1015e-04,\n",
      "        7.5340e-04, 8.9645e-04, 9.6207e-03, 1.3496e-02, 7.9441e-04, 7.7438e-04,\n",
      "        5.3024e-04, 7.2622e-04, 1.9693e-04, 3.5954e-04, 4.6082e-03, 1.3914e-03,\n",
      "        8.6670e-03, 2.0294e-02, 3.4199e-03, 5.6982e-04, 5.8699e-04, 6.0177e-04,\n",
      "        8.1015e-04, 1.7796e-03, 1.6947e-03, 9.8705e-04, 7.1621e-04, 1.1339e-03,\n",
      "        5.3453e-04, 1.1177e-03, 6.8235e-04, 5.5981e-04, 4.8370e-03, 2.8839e-03,\n",
      "        1.5650e-03, 1.7796e-03, 1.8873e-03, 8.4877e-04, 9.1982e-04, 5.3740e-04,\n",
      "        1.0567e-03, 3.7193e-03, 3.1242e-03, 7.5960e-04, 4.7035e-03, 5.2223e-03,\n",
      "        3.8948e-03, 1.1307e-02, 3.1662e-03, 3.8090e-03, 1.0948e-02, 4.6349e-03,\n",
      "        5.3406e-03, 2.9430e-03, 5.4398e-03, 4.9896e-03, 4.5090e-03, 1.9665e-03,\n",
      "        8.8120e-03, 1.2550e-02, 6.2828e-03, 2.8763e-02, 2.1988e-02, 5.3040e-02,\n",
      "        1.8494e-02, 1.4488e-02, 7.4463e-03, 3.2654e-02, 3.0212e-02, 1.3390e-02,\n",
      "        2.1515e-02, 4.9011e-02, 3.3936e-02, 1.9241e-02, 8.6060e-03, 9.5062e-03,\n",
      "        1.0719e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [14] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [14] : torch.Size([1, 32, 1, 86])\n",
      "Last layer attentions for generated token [14] : tensor([2.4109e-01, 2.4060e-01, 2.6202e-04, 5.8603e-04, 1.5163e-04, 4.2305e-03,\n",
      "        5.2118e-04, 6.5374e-04, 1.6184e-03, 1.1492e-03, 4.1771e-03, 7.7343e-04,\n",
      "        5.1498e-04, 1.9140e-03, 1.8143e-02, 1.3512e-02, 8.2970e-04, 4.9067e-04,\n",
      "        4.2462e-04, 7.1812e-04, 3.5262e-04, 8.0109e-04, 1.0948e-02, 2.2640e-03,\n",
      "        1.2009e-02, 1.6663e-02, 2.7199e-03, 6.7186e-04, 5.4312e-04, 8.0395e-04,\n",
      "        9.6416e-04, 1.9474e-03, 8.7643e-04, 6.1655e-04, 3.5191e-04, 4.5276e-04,\n",
      "        3.2544e-04, 8.8120e-04, 6.0797e-04, 3.0756e-04, 4.6234e-03, 2.9469e-03,\n",
      "        1.4935e-03, 1.0042e-03, 1.1816e-03, 4.8590e-04, 8.5926e-04, 5.5695e-04,\n",
      "        7.3195e-04, 3.3092e-03, 3.0022e-03, 4.2462e-04, 4.3259e-03, 3.3207e-03,\n",
      "        4.5929e-03, 8.6136e-03, 1.7118e-03, 1.9341e-03, 6.7177e-03, 3.5782e-03,\n",
      "        3.0766e-03, 5.1651e-03, 3.9291e-03, 4.0665e-03, 1.6117e-03, 1.7185e-03,\n",
      "        6.2218e-03, 7.1793e-03, 5.4054e-03, 3.6713e-02, 1.9897e-02, 3.2532e-02,\n",
      "        1.0460e-02, 7.6904e-03, 6.7825e-03, 2.2873e-02, 4.2877e-02, 1.0719e-02,\n",
      "        2.1622e-02, 2.7817e-02, 1.3901e-02, 1.4252e-02, 1.4610e-02, 1.7578e-02,\n",
      "        8.9874e-03, 9.4833e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [15] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [15] : torch.Size([1, 32, 1, 87])\n",
      "Last layer attentions for generated token [15] : tensor([3.1396e-01, 3.1396e-01, 3.5858e-04, 5.6744e-04, 2.2221e-04, 4.9019e-03,\n",
      "        4.8923e-04, 6.6042e-04, 1.4601e-03, 1.7071e-03, 4.0512e-03, 7.8011e-04,\n",
      "        4.1175e-04, 1.2760e-03, 1.2924e-02, 1.4145e-02, 5.8174e-04, 5.9462e-04,\n",
      "        2.5272e-04, 4.4966e-04, 2.4676e-04, 6.9666e-04, 7.7133e-03, 1.5335e-03,\n",
      "        7.6256e-03, 1.1993e-02, 1.4973e-03, 5.6076e-04, 5.4455e-04, 9.3365e-04,\n",
      "        1.1148e-03, 3.3417e-03, 1.4544e-03, 1.1921e-03, 7.8440e-04, 7.6914e-04,\n",
      "        2.1195e-04, 9.9945e-04, 5.9938e-04, 2.8634e-04, 4.0054e-03, 1.8454e-03,\n",
      "        6.0272e-04, 1.1044e-03, 1.5240e-03, 5.0831e-04, 1.1177e-03, 4.4966e-04,\n",
      "        7.5006e-04, 3.4580e-03, 2.2850e-03, 4.3416e-04, 2.3994e-03, 3.0231e-03,\n",
      "        2.1286e-03, 7.1487e-03, 1.1330e-03, 2.2945e-03, 6.8054e-03, 3.5362e-03,\n",
      "        2.0752e-03, 3.5706e-03, 4.5090e-03, 2.4986e-03, 1.5974e-03, 1.1263e-03,\n",
      "        3.8204e-03, 5.1918e-03, 3.5610e-03, 2.0691e-02, 1.0826e-02, 2.2522e-02,\n",
      "        5.4016e-03, 5.3825e-03, 6.8970e-03, 1.4938e-02, 1.6098e-02, 6.1607e-03,\n",
      "        1.8570e-02, 1.7349e-02, 1.0071e-02, 7.1297e-03, 1.2245e-02, 1.1665e-02,\n",
      "        5.6076e-03, 6.8665e-03, 9.0637e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [16] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [16] : torch.Size([1, 32, 1, 88])\n",
      "Last layer attentions for generated token [16] : tensor([2.7319e-01, 2.7319e-01, 5.4169e-04, 5.8126e-04, 3.7313e-04, 3.8910e-03,\n",
      "        4.6706e-04, 6.6900e-04, 1.1053e-03, 1.4896e-03, 4.6310e-03, 6.2609e-04,\n",
      "        3.2806e-04, 9.1076e-04, 1.9470e-02, 1.6663e-02, 6.0225e-04, 8.6260e-04,\n",
      "        3.6955e-04, 3.1900e-04, 2.0802e-04, 1.0443e-03, 9.3613e-03, 2.9469e-03,\n",
      "        9.0179e-03, 1.4862e-02, 1.8005e-03, 7.2622e-04, 5.7793e-04, 8.4734e-04,\n",
      "        9.1600e-04, 2.8019e-03, 1.4582e-03, 1.1053e-03, 1.2569e-03, 6.7282e-04,\n",
      "        1.7583e-04, 1.0729e-03, 5.0879e-04, 3.2663e-04, 3.2825e-03, 2.3556e-03,\n",
      "        6.4230e-04, 1.5526e-03, 1.6012e-03, 4.9639e-04, 9.3985e-04, 7.7581e-04,\n",
      "        8.4734e-04, 3.0365e-03, 3.1796e-03, 9.0027e-04, 2.3327e-03, 4.3106e-03,\n",
      "        2.2697e-03, 9.9945e-03, 1.3199e-03, 2.6531e-03, 5.2681e-03, 3.1052e-03,\n",
      "        2.1782e-03, 2.9716e-03, 4.3449e-03, 3.1242e-03, 1.9169e-03, 1.3647e-03,\n",
      "        3.4008e-03, 6.9275e-03, 2.6455e-03, 1.9287e-02, 1.5640e-02, 3.0273e-02,\n",
      "        6.0081e-03, 7.8964e-03, 1.1398e-02, 1.8097e-02, 1.7639e-02, 5.5656e-03,\n",
      "        2.2217e-02, 2.6978e-02, 2.0264e-02, 7.1526e-03, 9.7961e-03, 1.7197e-02,\n",
      "        9.4604e-03, 5.8403e-03, 8.4457e-03, 9.3689e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [17] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [17] : torch.Size([1, 32, 1, 89])\n",
      "Last layer attentions for generated token [17] : tensor([3.1738e-01, 3.1738e-01, 1.1530e-03, 7.2193e-04, 3.6001e-04, 6.9122e-03,\n",
      "        2.9159e-04, 5.4884e-04, 8.1921e-04, 1.1024e-03, 2.8591e-03, 2.6965e-04,\n",
      "        5.1355e-04, 8.5211e-04, 7.7133e-03, 1.0368e-02, 3.9315e-04, 2.1589e-04,\n",
      "        2.4366e-04, 3.1662e-04, 1.3351e-04, 9.8419e-04, 3.0060e-02, 3.3169e-03,\n",
      "        3.7193e-03, 1.3016e-02, 8.3399e-04, 5.1355e-04, 4.3774e-04, 3.6716e-04,\n",
      "        6.6900e-04, 9.9850e-04, 1.1044e-03, 7.4005e-04, 5.9032e-04, 5.0402e-04,\n",
      "        2.1172e-04, 1.0004e-03, 4.1771e-04, 2.9397e-04, 2.1591e-03, 1.7796e-03,\n",
      "        4.8065e-04, 8.0347e-04, 8.9121e-04, 2.8038e-04, 6.3944e-04, 6.3467e-04,\n",
      "        4.0007e-04, 9.2316e-04, 1.5612e-03, 4.7994e-04, 2.6627e-03, 4.2877e-03,\n",
      "        1.3781e-03, 9.8648e-03, 5.7220e-04, 3.2310e-03, 5.1956e-03, 2.7294e-03,\n",
      "        1.0195e-03, 1.7796e-03, 4.6234e-03, 1.7242e-03, 2.2984e-03, 8.1158e-04,\n",
      "        2.4929e-03, 2.6531e-03, 1.8797e-03, 1.9287e-02, 1.4130e-02, 2.7725e-02,\n",
      "        7.9269e-03, 5.3902e-03, 5.6000e-03, 1.0017e-02, 7.3776e-03, 1.8644e-03,\n",
      "        1.0628e-02, 2.3438e-02, 1.9440e-02, 6.6681e-03, 8.7738e-03, 1.2230e-02,\n",
      "        8.4610e-03, 6.7520e-03, 3.0861e-03, 6.2103e-03, 4.6120e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [18] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [18] : torch.Size([1, 32, 1, 90])\n",
      "Last layer attentions for generated token [18] : tensor([2.3730e-01, 2.3730e-01, 3.0637e-04, 5.2643e-04, 2.7633e-04, 3.8967e-03,\n",
      "        2.5558e-04, 3.0279e-04, 1.0824e-03, 1.1234e-03, 3.3169e-03, 7.5817e-04,\n",
      "        4.0030e-04, 7.7629e-04, 1.2321e-02, 1.5625e-02, 7.5531e-04, 7.2670e-04,\n",
      "        4.5824e-04, 4.2057e-04, 1.7703e-04, 5.2309e-04, 7.4120e-03, 1.9703e-03,\n",
      "        8.5907e-03, 1.3863e-02, 1.9951e-03, 7.9012e-04, 9.2554e-04, 1.2751e-03,\n",
      "        1.6994e-03, 3.0518e-03, 1.4763e-03, 1.0309e-03, 7.7486e-04, 6.6423e-04,\n",
      "        2.5964e-04, 1.4648e-03, 5.0926e-04, 3.8123e-04, 5.8098e-03, 2.7332e-03,\n",
      "        1.0929e-03, 1.5526e-03, 1.7595e-03, 7.7629e-04, 1.2999e-03, 6.5517e-04,\n",
      "        1.1568e-03, 4.9973e-03, 3.4924e-03, 8.6260e-04, 2.5311e-03, 4.3335e-03,\n",
      "        2.8782e-03, 7.1983e-03, 1.5173e-03, 2.7409e-03, 4.9324e-03, 2.8629e-03,\n",
      "        3.1967e-03, 3.1223e-03, 3.2711e-03, 2.7046e-03, 2.1095e-03, 1.4763e-03,\n",
      "        4.4861e-03, 7.8888e-03, 2.6970e-03, 3.4729e-02, 2.0615e-02, 3.8239e-02,\n",
      "        1.3130e-02, 9.4757e-03, 1.1642e-02, 2.3773e-02, 1.8494e-02, 1.1559e-02,\n",
      "        2.1484e-02, 2.7863e-02, 1.3489e-02, 1.8066e-02, 1.1108e-02, 1.1932e-02,\n",
      "        8.4686e-03, 1.4961e-02, 1.2833e-02, 1.2413e-02, 2.6894e-03, 1.0056e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [19] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [19] : torch.Size([1, 32, 1, 91])\n",
      "Last layer attentions for generated token [19] : tensor([1.8542e-01, 1.8506e-01, 3.0422e-04, 5.8079e-04, 2.1124e-04, 3.6774e-03,\n",
      "        3.6716e-04, 6.9094e-04, 1.7176e-03, 1.0920e-03, 2.7275e-03, 5.5122e-04,\n",
      "        2.6751e-04, 3.8910e-04, 1.0193e-02, 1.3138e-02, 1.2274e-03, 7.2718e-04,\n",
      "        3.7074e-04, 3.0541e-04, 1.3959e-04, 5.7745e-04, 6.8970e-03, 1.2560e-03,\n",
      "        5.2948e-03, 1.4877e-02, 2.0752e-03, 4.0936e-04, 7.1049e-04, 2.2907e-03,\n",
      "        1.6384e-03, 3.0499e-03, 1.4486e-03, 1.0891e-03, 1.3714e-03, 5.7745e-04,\n",
      "        1.6832e-04, 1.9588e-03, 6.1607e-04, 3.4070e-04, 8.3542e-03, 3.2902e-03,\n",
      "        6.5804e-04, 1.0557e-03, 1.6985e-03, 6.3419e-04, 1.4267e-03, 7.4434e-04,\n",
      "        1.1396e-03, 7.1793e-03, 3.9787e-03, 6.9094e-04, 3.0651e-03, 4.8103e-03,\n",
      "        3.1376e-03, 7.7820e-03, 2.2411e-03, 2.8458e-03, 8.1329e-03, 3.0403e-03,\n",
      "        2.6627e-03, 3.7327e-03, 4.7836e-03, 2.0752e-03, 1.4744e-03, 1.6975e-03,\n",
      "        4.8561e-03, 1.1467e-02, 3.5038e-03, 4.4373e-02, 2.6672e-02, 4.5166e-02,\n",
      "        1.0590e-02, 1.1406e-02, 2.2781e-02, 3.5370e-02, 2.1027e-02, 8.1558e-03,\n",
      "        3.0670e-02, 4.4769e-02, 2.0996e-02, 1.0857e-02, 1.0429e-02, 1.9852e-02,\n",
      "        1.1002e-02, 9.5367e-03, 1.1009e-02, 1.0902e-02, 1.7786e-03, 1.4732e-02,\n",
      "        2.0081e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [20] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [20] : torch.Size([1, 32, 1, 92])\n",
      "Last layer attentions for generated token [20] : tensor([2.5562e-01, 2.5562e-01, 4.4584e-04, 5.9414e-04, 3.5405e-04, 3.4790e-03,\n",
      "        3.2949e-04, 4.7278e-04, 1.0490e-03, 1.0223e-03, 1.9217e-03, 4.1962e-04,\n",
      "        3.9196e-04, 6.1798e-04, 9.5825e-03, 1.4687e-02, 5.2929e-04, 5.3596e-04,\n",
      "        2.5296e-04, 2.7966e-04, 1.3709e-04, 5.6934e-04, 8.9417e-03, 1.8997e-03,\n",
      "        6.5727e-03, 1.1780e-02, 2.4624e-03, 4.1556e-04, 5.4121e-04, 1.1387e-03,\n",
      "        1.0967e-03, 2.6245e-03, 1.5564e-03, 8.7118e-04, 9.6083e-04, 4.0841e-04,\n",
      "        1.9634e-04, 1.0653e-03, 4.0293e-04, 2.3043e-04, 5.4359e-03, 2.7599e-03,\n",
      "        6.9189e-04, 7.5531e-04, 8.6451e-04, 3.8433e-04, 4.6372e-04, 4.9448e-04,\n",
      "        4.9829e-04, 4.0321e-03, 3.2406e-03, 7.5245e-04, 2.4776e-03, 6.7406e-03,\n",
      "        2.6474e-03, 7.1182e-03, 1.7500e-03, 2.7962e-03, 6.8245e-03, 2.5635e-03,\n",
      "        2.5120e-03, 2.5883e-03, 4.0627e-03, 2.0618e-03, 1.6699e-03, 1.4362e-03,\n",
      "        4.3526e-03, 7.0114e-03, 2.5425e-03, 3.8116e-02, 2.2964e-02, 3.0716e-02,\n",
      "        1.0048e-02, 4.6387e-03, 1.0323e-02, 2.6047e-02, 1.5152e-02, 6.1150e-03,\n",
      "        1.7548e-02, 3.6133e-02, 1.7014e-02, 8.9722e-03, 7.7248e-03, 9.3994e-03,\n",
      "        7.6523e-03, 7.0381e-03, 7.8125e-03, 7.0457e-03, 1.8120e-03, 9.7427e-03,\n",
      "        1.1963e-02, 1.2337e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [21] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [21] : torch.Size([1, 32, 1, 93])\n",
      "Last layer attentions for generated token [21] : tensor([2.1460e-01, 2.1460e-01, 4.7112e-04, 4.8041e-04, 3.6979e-04, 3.1090e-03,\n",
      "        4.3416e-04, 5.3930e-04, 1.0719e-03, 1.1644e-03, 2.0638e-03, 3.9363e-04,\n",
      "        4.9019e-04, 7.9679e-04, 9.2239e-03, 1.0735e-02, 6.5660e-04, 7.2861e-04,\n",
      "        3.0375e-04, 3.8600e-04, 1.2267e-04, 4.9400e-04, 8.1940e-03, 1.7033e-03,\n",
      "        5.1651e-03, 1.1536e-02, 2.3232e-03, 4.4346e-04, 7.2575e-04, 8.3685e-04,\n",
      "        1.1568e-03, 2.1324e-03, 2.6073e-03, 1.4229e-03, 1.0977e-03, 7.1144e-04,\n",
      "        3.7050e-04, 1.1797e-03, 5.9080e-04, 4.0698e-04, 4.5128e-03, 3.4676e-03,\n",
      "        9.0647e-04, 1.1892e-03, 1.8101e-03, 8.0919e-04, 7.2289e-04, 5.4884e-04,\n",
      "        8.8882e-04, 4.1389e-03, 3.5744e-03, 7.6485e-04, 4.0588e-03, 5.7449e-03,\n",
      "        4.2839e-03, 8.1940e-03, 1.7958e-03, 2.7676e-03, 6.3095e-03, 3.0975e-03,\n",
      "        2.8725e-03, 2.6817e-03, 4.2000e-03, 2.9373e-03, 1.7271e-03, 1.4772e-03,\n",
      "        8.2626e-03, 6.1226e-03, 3.6125e-03, 3.7170e-02, 2.4673e-02, 3.0243e-02,\n",
      "        8.6746e-03, 6.3591e-03, 1.2642e-02, 2.4399e-02, 2.2141e-02, 7.6752e-03,\n",
      "        2.5009e-02, 4.3152e-02, 1.9287e-02, 1.0551e-02, 8.7051e-03, 1.4442e-02,\n",
      "        1.0941e-02, 1.2665e-02, 8.6288e-03, 8.0032e-03, 2.5406e-03, 1.8219e-02,\n",
      "        1.5251e-02, 1.5671e-02, 1.3710e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [22] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [22] : torch.Size([1, 32, 1, 94])\n",
      "Last layer attentions for generated token [22] : tensor([2.0605e-01, 2.0605e-01, 3.6073e-04, 3.4094e-04, 2.4259e-04, 4.3182e-03,\n",
      "        3.2091e-04, 5.3835e-04, 1.0557e-03, 1.0939e-03, 2.5082e-03, 4.6945e-04,\n",
      "        5.7650e-04, 9.2268e-04, 6.2370e-03, 1.3588e-02, 5.8889e-04, 4.9973e-04,\n",
      "        2.9612e-04, 4.9496e-04, 1.7071e-04, 4.0960e-04, 8.7433e-03, 2.1992e-03,\n",
      "        6.2256e-03, 1.6647e-02, 1.6489e-03, 6.4802e-04, 7.6199e-04, 9.9373e-04,\n",
      "        1.2865e-03, 2.9182e-03, 2.0657e-03, 1.2817e-03, 7.1335e-04, 6.5041e-04,\n",
      "        3.0422e-04, 6.5708e-04, 3.8028e-04, 3.5381e-04, 4.0131e-03, 2.2469e-03,\n",
      "        8.3208e-04, 1.0433e-03, 1.4124e-03, 5.8794e-04, 9.8038e-04, 5.2881e-04,\n",
      "        1.0014e-03, 2.8896e-03, 2.6360e-03, 9.1219e-04, 2.8820e-03, 4.0512e-03,\n",
      "        3.1471e-03, 8.0795e-03, 1.3771e-03, 3.3360e-03, 6.9885e-03, 3.6793e-03,\n",
      "        5.3520e-03, 4.1084e-03, 3.7193e-03, 2.9221e-03, 4.0016e-03, 1.5697e-03,\n",
      "        5.8060e-03, 7.3204e-03, 3.4752e-03, 2.3987e-02, 1.7365e-02, 3.4576e-02,\n",
      "        1.1551e-02, 7.8506e-03, 7.8506e-03, 1.8829e-02, 1.6815e-02, 1.0872e-02,\n",
      "        2.0386e-02, 2.8412e-02, 1.3901e-02, 1.2840e-02, 1.0559e-02, 9.1171e-03,\n",
      "        1.1024e-02, 1.4137e-02, 1.4122e-02, 1.4061e-02, 3.8891e-03, 1.6235e-02,\n",
      "        2.8625e-02, 1.9318e-02, 1.5060e-02, 3.2166e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [23] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [23] : torch.Size([1, 32, 1, 95])\n",
      "Last layer attentions for generated token [23] : tensor([1.5735e-01, 1.5698e-01, 3.2711e-04, 3.2973e-04, 2.7323e-04, 3.1967e-03,\n",
      "        3.7432e-04, 5.8794e-04, 5.4550e-04, 6.4182e-04, 1.3990e-03, 1.6260e-04,\n",
      "        1.3018e-04, 4.1199e-04, 5.7755e-03, 9.0027e-03, 6.0272e-04, 3.6788e-04,\n",
      "        1.2177e-04, 9.3699e-05, 3.7134e-05, 2.5868e-04, 1.0048e-02, 1.5917e-03,\n",
      "        2.8839e-03, 1.1932e-02, 1.2016e-03, 2.4986e-04, 4.0483e-04, 1.0710e-03,\n",
      "        6.9952e-04, 1.2712e-03, 1.4992e-03, 1.3256e-03, 1.3332e-03, 4.4441e-04,\n",
      "        2.3293e-04, 9.5940e-04, 4.2176e-04, 2.9325e-04, 2.4834e-03, 1.5116e-03,\n",
      "        5.2071e-04, 8.4543e-04, 1.3561e-03, 4.6492e-04, 9.8610e-04, 3.2711e-04,\n",
      "        7.9536e-04, 3.4161e-03, 2.3918e-03, 6.1941e-04, 2.4071e-03, 7.6752e-03,\n",
      "        2.2717e-03, 8.5526e-03, 9.8610e-04, 1.2760e-03, 3.1986e-03, 1.4849e-03,\n",
      "        1.8826e-03, 2.7447e-03, 3.7022e-03, 1.3628e-03, 1.0576e-03, 9.4843e-04,\n",
      "        2.7866e-03, 6.5689e-03, 1.5182e-03, 3.8391e-02, 4.2389e-02, 4.7607e-02,\n",
      "        1.0963e-02, 1.0040e-02, 1.5060e-02, 2.3453e-02, 1.7090e-02, 7.5722e-03,\n",
      "        3.4241e-02, 4.3976e-02, 1.2825e-02, 1.1307e-02, 1.3588e-02, 1.8494e-02,\n",
      "        1.0391e-02, 1.4549e-02, 1.0948e-02, 1.2711e-02, 2.8095e-03, 1.5732e-02,\n",
      "        1.9714e-02, 1.5297e-02, 9.2468e-03, 2.7740e-02, 7.0862e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [24] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [24] : torch.Size([1, 32, 1, 96])\n",
      "Last layer attentions for generated token [24] : tensor([0.1732, 0.1726, 0.0004, 0.0005, 0.0003, 0.0029, 0.0003, 0.0004, 0.0007,\n",
      "        0.0012, 0.0022, 0.0005, 0.0003, 0.0004, 0.0089, 0.0094, 0.0010, 0.0010,\n",
      "        0.0005, 0.0004, 0.0002, 0.0013, 0.0102, 0.0033, 0.0093, 0.0133, 0.0034,\n",
      "        0.0007, 0.0008, 0.0014, 0.0014, 0.0032, 0.0009, 0.0007, 0.0008, 0.0009,\n",
      "        0.0004, 0.0011, 0.0005, 0.0003, 0.0039, 0.0027, 0.0011, 0.0011, 0.0010,\n",
      "        0.0005, 0.0006, 0.0005, 0.0007, 0.0041, 0.0034, 0.0005, 0.0021, 0.0073,\n",
      "        0.0020, 0.0096, 0.0023, 0.0033, 0.0063, 0.0028, 0.0033, 0.0033, 0.0042,\n",
      "        0.0040, 0.0032, 0.0017, 0.0072, 0.0085, 0.0032, 0.0435, 0.0221, 0.0366,\n",
      "        0.0078, 0.0095, 0.0143, 0.0303, 0.0219, 0.0118, 0.0221, 0.0439, 0.0186,\n",
      "        0.0153, 0.0107, 0.0106, 0.0089, 0.0125, 0.0133, 0.0124, 0.0030, 0.0099,\n",
      "        0.0099, 0.0127, 0.0106, 0.0310, 0.0229, 0.0082], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [25] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [25] : torch.Size([1, 32, 1, 97])\n",
      "Last layer attentions for generated token [25] : tensor([2.1692e-01, 2.1643e-01, 4.8971e-04, 4.1056e-04, 2.9230e-04, 3.1528e-03,\n",
      "        2.2638e-04, 2.5606e-04, 5.2023e-04, 1.1854e-03, 2.2507e-03, 4.7922e-04,\n",
      "        4.9829e-04, 5.9843e-04, 9.2392e-03, 1.0239e-02, 6.6805e-04, 8.9359e-04,\n",
      "        4.2439e-04, 5.3453e-04, 2.0444e-04, 1.0900e-03, 9.2392e-03, 2.0256e-03,\n",
      "        8.4991e-03, 1.0635e-02, 3.3875e-03, 6.9189e-04, 7.0143e-04, 1.0443e-03,\n",
      "        1.2379e-03, 2.9087e-03, 1.1272e-03, 7.3910e-04, 5.7697e-04, 6.5899e-04,\n",
      "        2.5439e-04, 9.7370e-04, 5.6887e-04, 3.4118e-04, 3.7727e-03, 2.1782e-03,\n",
      "        1.0633e-03, 9.7942e-04, 8.1968e-04, 4.7350e-04, 3.5739e-04, 3.1543e-04,\n",
      "        5.1308e-04, 2.7714e-03, 3.1834e-03, 6.2227e-04, 1.8549e-03, 7.7629e-03,\n",
      "        1.8730e-03, 8.8882e-03, 2.1343e-03, 2.7771e-03, 5.0507e-03, 2.3842e-03,\n",
      "        2.4242e-03, 2.9430e-03, 3.6030e-03, 2.4662e-03, 2.4242e-03, 9.9277e-04,\n",
      "        7.1907e-03, 5.7259e-03, 2.2621e-03, 3.2471e-02, 1.9958e-02, 3.3417e-02,\n",
      "        8.8959e-03, 9.7885e-03, 1.2413e-02, 1.7395e-02, 2.9053e-02, 1.4122e-02,\n",
      "        1.9699e-02, 3.6957e-02, 1.4732e-02, 1.8051e-02, 1.2688e-02, 9.0027e-03,\n",
      "        5.5466e-03, 1.1108e-02, 9.7504e-03, 1.0880e-02, 2.7523e-03, 6.0196e-03,\n",
      "        6.6605e-03, 6.8321e-03, 6.0959e-03, 2.0233e-02, 1.5747e-02, 5.8441e-03,\n",
      "        1.1406e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [26] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [26] : torch.Size([1, 32, 1, 98])\n",
      "Last layer attentions for generated token [26] : tensor([1.8530e-01, 1.8457e-01, 2.9945e-04, 3.8147e-04, 1.9956e-04, 2.8172e-03,\n",
      "        2.9016e-04, 3.8671e-04, 6.6853e-04, 7.5865e-04, 1.8730e-03, 5.8746e-04,\n",
      "        3.9434e-04, 5.7507e-04, 8.3313e-03, 9.8572e-03, 3.8838e-04, 4.7851e-04,\n",
      "        2.9707e-04, 4.9973e-04, 1.8239e-04, 4.1509e-04, 4.3869e-03, 1.1501e-03,\n",
      "        5.2490e-03, 1.6556e-02, 2.6150e-03, 4.7660e-04, 4.2725e-04, 6.1321e-04,\n",
      "        5.7173e-04, 2.0866e-03, 9.8610e-04, 7.2670e-04, 6.3753e-04, 4.6921e-04,\n",
      "        3.3092e-04, 6.1798e-04, 2.9612e-04, 2.1446e-04, 3.9558e-03, 1.8024e-03,\n",
      "        1.1568e-03, 1.0033e-03, 9.9945e-04, 4.5228e-04, 5.6505e-04, 3.8528e-04,\n",
      "        6.2895e-04, 3.0193e-03, 2.5921e-03, 7.5865e-04, 3.5992e-03, 6.9351e-03,\n",
      "        2.1496e-03, 9.1019e-03, 1.5087e-03, 2.3460e-03, 6.3095e-03, 2.8362e-03,\n",
      "        2.3441e-03, 2.7122e-03, 3.9444e-03, 3.0575e-03, 1.7834e-03, 1.0309e-03,\n",
      "        4.2953e-03, 7.6103e-03, 2.6646e-03, 2.4551e-02, 1.9119e-02, 4.4312e-02,\n",
      "        1.1131e-02, 8.5831e-03, 7.4921e-03, 3.0106e-02, 2.9404e-02, 1.0414e-02,\n",
      "        1.9363e-02, 3.7598e-02, 4.0070e-02, 1.7776e-02, 1.1620e-02, 1.1719e-02,\n",
      "        7.7362e-03, 1.1589e-02, 1.0254e-02, 6.7368e-03, 1.6508e-03, 8.7280e-03,\n",
      "        8.0490e-03, 9.5139e-03, 1.5312e-02, 2.8442e-02, 1.2917e-02, 9.0866e-03,\n",
      "        1.3153e-02, 1.3947e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [27] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [27] : torch.Size([1, 32, 1, 99])\n",
      "Last layer attentions for generated token [27] : tensor([2.4463e-01, 2.4512e-01, 2.8086e-04, 4.8542e-04, 1.6415e-04, 2.2755e-03,\n",
      "        1.8537e-04, 2.2793e-04, 2.6560e-04, 8.0681e-04, 1.2569e-03, 2.2185e-04,\n",
      "        5.9032e-04, 1.0004e-03, 6.5231e-03, 6.3515e-03, 4.6062e-04, 6.1369e-04,\n",
      "        2.3067e-04, 3.8171e-04, 1.5855e-04, 5.2881e-04, 8.7585e-03, 1.2541e-03,\n",
      "        4.4327e-03, 8.2779e-03, 1.5697e-03, 4.0555e-04, 3.1352e-04, 4.6134e-04,\n",
      "        4.9877e-04, 1.4868e-03, 6.7806e-04, 5.8889e-04, 5.5027e-04, 7.3576e-04,\n",
      "        5.3406e-04, 7.1049e-04, 8.0061e-04, 3.3617e-04, 3.0117e-03, 3.1719e-03,\n",
      "        1.0872e-03, 9.0361e-04, 6.8617e-04, 4.4799e-04, 2.5630e-04, 3.6502e-04,\n",
      "        3.4428e-04, 2.8934e-03, 4.7188e-03, 5.9605e-04, 3.1433e-03, 5.7755e-03,\n",
      "        1.9627e-03, 7.3853e-03, 1.3065e-03, 2.4090e-03, 5.9853e-03, 2.1496e-03,\n",
      "        1.9627e-03, 2.6779e-03, 3.3150e-03, 2.9106e-03, 2.4300e-03, 1.0624e-03,\n",
      "        7.8659e-03, 5.4588e-03, 4.3793e-03, 3.9703e-02, 2.4139e-02, 3.4210e-02,\n",
      "        1.3626e-02, 9.1476e-03, 9.1019e-03, 1.1192e-02, 2.7039e-02, 8.9035e-03,\n",
      "        9.8038e-03, 3.8391e-02, 1.3176e-02, 7.8506e-03, 5.7220e-03, 4.7798e-03,\n",
      "        6.9008e-03, 5.1270e-03, 5.2338e-03, 7.8125e-03, 2.3613e-03, 6.2981e-03,\n",
      "        4.9858e-03, 4.6959e-03, 4.8027e-03, 1.8585e-02, 1.1520e-02, 3.0499e-03,\n",
      "        5.5618e-03, 2.0477e-02, 9.8953e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [28] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [28] : torch.Size([1, 32, 1, 100])\n",
      "Last layer attentions for generated token [28] : tensor([2.0447e-01, 2.0447e-01, 3.8862e-04, 5.3120e-04, 2.1088e-04, 4.6196e-03,\n",
      "        2.8992e-04, 4.1294e-04, 8.4209e-04, 1.0195e-03, 2.5101e-03, 6.7425e-04,\n",
      "        8.1301e-04, 1.1444e-03, 9.2468e-03, 1.3115e-02, 6.6090e-04, 4.5872e-04,\n",
      "        3.2663e-04, 6.3181e-04, 1.6356e-04, 3.8266e-04, 5.2834e-03, 1.0424e-03,\n",
      "        5.0583e-03, 1.6815e-02, 2.1839e-03, 7.4768e-04, 7.4482e-04, 6.5184e-04,\n",
      "        4.8637e-04, 1.4076e-03, 1.5335e-03, 1.2693e-03, 8.4877e-04, 9.1410e-04,\n",
      "        3.6716e-04, 7.4196e-04, 3.8266e-04, 2.8205e-04, 2.9793e-03, 1.8616e-03,\n",
      "        1.0281e-03, 9.7132e-04, 1.3323e-03, 5.6076e-04, 7.7152e-04, 4.3440e-04,\n",
      "        8.4543e-04, 3.4561e-03, 2.3441e-03, 5.8699e-04, 4.3449e-03, 3.6755e-03,\n",
      "        2.1667e-03, 1.0933e-02, 2.6608e-03, 2.5940e-03, 5.8784e-03, 3.2215e-03,\n",
      "        3.3150e-03, 2.8248e-03, 3.6087e-03, 3.2215e-03, 2.5997e-03, 1.0262e-03,\n",
      "        5.2795e-03, 8.1406e-03, 3.7594e-03, 1.3931e-02, 1.3329e-02, 3.9764e-02,\n",
      "        9.8038e-03, 9.9564e-03, 7.9269e-03, 1.7578e-02, 2.0523e-02, 7.2937e-03,\n",
      "        1.5411e-02, 2.5040e-02, 1.9180e-02, 1.5808e-02, 1.4771e-02, 1.0338e-02,\n",
      "        8.4991e-03, 1.3100e-02, 1.0361e-02, 9.6436e-03, 2.8343e-03, 7.4883e-03,\n",
      "        9.4910e-03, 1.1215e-02, 1.1497e-02, 2.1210e-02, 1.2466e-02, 1.1360e-02,\n",
      "        1.6235e-02, 1.1299e-02, 1.1497e-02, 1.6586e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [29] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [29] : torch.Size([1, 32, 1, 101])\n",
      "Last layer attentions for generated token [29] : tensor([1.8555e-01, 1.8518e-01, 2.3806e-04, 3.0279e-04, 1.9431e-04, 3.5229e-03,\n",
      "        3.9649e-04, 7.1526e-04, 1.1272e-03, 8.4925e-04, 2.4357e-03, 5.4502e-04,\n",
      "        3.4571e-04, 1.4191e-03, 1.0216e-02, 1.0971e-02, 9.0027e-04, 4.1699e-04,\n",
      "        2.1422e-04, 2.7251e-04, 1.2493e-04, 3.5405e-04, 7.1754e-03, 1.5774e-03,\n",
      "        6.4507e-03, 1.5129e-02, 1.9264e-03, 8.1825e-04, 7.5674e-04, 1.2894e-03,\n",
      "        1.1101e-03, 1.7271e-03, 1.5106e-03, 1.7729e-03, 1.0366e-03, 9.0218e-04,\n",
      "        5.0879e-04, 9.4557e-04, 4.5180e-04, 3.0875e-04, 3.2749e-03, 2.6665e-03,\n",
      "        1.4496e-03, 1.1673e-03, 1.5860e-03, 4.2939e-04, 1.2159e-03, 5.0020e-04,\n",
      "        7.6866e-04, 3.3188e-03, 2.9526e-03, 6.5994e-04, 6.1531e-03, 5.9662e-03,\n",
      "        3.4523e-03, 9.2010e-03, 1.4133e-03, 1.6260e-03, 4.2763e-03, 2.8915e-03,\n",
      "        2.0866e-03, 5.2795e-03, 3.3455e-03, 2.4376e-03, 1.3103e-03, 1.2970e-03,\n",
      "        4.2343e-03, 8.4915e-03, 3.9139e-03, 2.7008e-02, 2.2934e-02, 2.8351e-02,\n",
      "        9.1476e-03, 8.3160e-03, 1.0963e-02, 2.1820e-02, 2.0035e-02, 9.1705e-03,\n",
      "        1.9333e-02, 2.3743e-02, 9.9945e-03, 1.1879e-02, 1.5419e-02, 2.4734e-02,\n",
      "        8.4991e-03, 9.9564e-03, 1.1475e-02, 6.9466e-03, 2.1152e-03, 7.4539e-03,\n",
      "        7.4272e-03, 1.1360e-02, 1.2794e-02, 1.9608e-02, 1.2184e-02, 1.1726e-02,\n",
      "        1.1864e-02, 1.5495e-02, 1.1948e-02, 2.2736e-02, 1.4587e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [30] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [30] : torch.Size([1, 32, 1, 102])\n",
      "Last layer attentions for generated token [30] : tensor([1.9995e-01, 1.9958e-01, 1.8418e-04, 4.2820e-04, 1.2410e-04, 4.0665e-03,\n",
      "        5.6934e-04, 9.3699e-04, 1.3628e-03, 1.2903e-03, 2.2869e-03, 9.0456e-04,\n",
      "        3.1447e-04, 7.9966e-04, 6.9466e-03, 1.0094e-02, 9.9754e-04, 7.2527e-04,\n",
      "        4.0388e-04, 4.0841e-04, 1.5271e-04, 5.6171e-04, 5.7297e-03, 1.3399e-03,\n",
      "        4.1275e-03, 1.3054e-02, 2.1858e-03, 8.0156e-04, 6.3419e-04, 1.8911e-03,\n",
      "        1.2760e-03, 2.0714e-03, 1.6699e-03, 1.9169e-03, 1.2121e-03, 6.6042e-04,\n",
      "        3.8385e-04, 1.4257e-03, 3.9291e-04, 2.1446e-04, 3.4676e-03, 1.7252e-03,\n",
      "        5.3072e-04, 8.8167e-04, 1.4572e-03, 3.0708e-04, 1.2636e-03, 5.0449e-04,\n",
      "        7.4577e-04, 4.5967e-03, 1.9588e-03, 2.2388e-04, 3.3607e-03, 2.9316e-03,\n",
      "        2.3289e-03, 8.4915e-03, 1.4172e-03, 2.3880e-03, 5.4741e-03, 2.5978e-03,\n",
      "        1.2007e-03, 3.2616e-03, 4.4403e-03, 1.7958e-03, 1.3504e-03, 1.0195e-03,\n",
      "        2.7752e-03, 3.0155e-03, 2.3117e-03, 1.0582e-02, 7.7286e-03, 2.1225e-02,\n",
      "        8.4839e-03, 9.6588e-03, 1.7136e-02, 2.5894e-02, 1.3870e-02, 6.7978e-03,\n",
      "        2.7359e-02, 1.8677e-02, 1.1795e-02, 6.7329e-03, 2.0233e-02, 2.3911e-02,\n",
      "        8.6746e-03, 6.7940e-03, 1.0658e-02, 1.1703e-02, 3.4885e-03, 9.4833e-03,\n",
      "        7.4310e-03, 1.5327e-02, 1.4656e-02, 2.1011e-02, 1.7807e-02, 1.6922e-02,\n",
      "        1.5388e-02, 9.4681e-03, 1.3245e-02, 1.6190e-02, 7.7057e-03, 2.2049e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [31] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [31] : torch.Size([1, 32, 1, 103])\n",
      "Last layer attentions for generated token [31] : tensor([2.1130e-01, 2.1094e-01, 2.4271e-04, 7.6532e-04, 1.6105e-04, 2.4681e-03,\n",
      "        6.9809e-04, 8.0681e-04, 1.1463e-03, 1.2274e-03, 1.1854e-03, 6.3181e-04,\n",
      "        4.2343e-04, 9.0313e-04, 5.1155e-03, 7.1106e-03, 7.2575e-04, 5.2691e-04,\n",
      "        2.7776e-04, 2.8086e-04, 8.8215e-05, 4.4537e-04, 7.1144e-03, 1.7443e-03,\n",
      "        5.1994e-03, 1.0780e-02, 2.9106e-03, 1.0138e-03, 5.6410e-04, 9.0313e-04,\n",
      "        6.3562e-04, 1.3504e-03, 2.2373e-03, 1.5821e-03, 9.2316e-04, 1.0643e-03,\n",
      "        6.4802e-04, 1.2398e-03, 6.6328e-04, 3.6573e-04, 2.9736e-03, 2.9049e-03,\n",
      "        1.1988e-03, 1.7042e-03, 1.8177e-03, 6.0081e-04, 5.8889e-04, 4.7612e-04,\n",
      "        6.3562e-04, 2.8763e-03, 2.4166e-03, 3.7646e-04, 4.8599e-03, 9.3689e-03,\n",
      "        3.1586e-03, 8.2855e-03, 1.2589e-03, 4.0550e-03, 6.3744e-03, 3.5934e-03,\n",
      "        1.7929e-03, 3.2406e-03, 5.1346e-03, 3.2597e-03, 1.5640e-03, 1.0300e-03,\n",
      "        9.6054e-03, 7.3776e-03, 5.5351e-03, 3.3722e-02, 1.8219e-02, 3.5034e-02,\n",
      "        1.1688e-02, 8.4839e-03, 9.4223e-03, 1.8341e-02, 1.5297e-02, 5.0392e-03,\n",
      "        1.2810e-02, 4.0222e-02, 1.3947e-02, 9.5139e-03, 7.9880e-03, 6.8283e-03,\n",
      "        7.9956e-03, 7.3586e-03, 8.2397e-03, 7.1182e-03, 2.7771e-03, 7.6637e-03,\n",
      "        5.4169e-03, 6.2790e-03, 8.1711e-03, 1.6815e-02, 1.1414e-02, 7.8812e-03,\n",
      "        1.0590e-02, 1.4214e-02, 7.3013e-03, 1.0910e-02, 6.3896e-03, 9.1705e-03,\n",
      "        1.1253e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [32] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [32] : torch.Size([1, 32, 1, 104])\n",
      "Last layer attentions for generated token [32] : tensor([1.7969e-01, 1.7969e-01, 2.0909e-04, 3.6764e-04, 1.1826e-04, 3.0994e-03,\n",
      "        6.2418e-04, 9.8801e-04, 1.9417e-03, 1.5669e-03, 3.1528e-03, 1.1892e-03,\n",
      "        3.8385e-04, 8.8930e-04, 9.2316e-03, 1.0880e-02, 9.5177e-04, 6.8855e-04,\n",
      "        4.4441e-04, 5.0259e-04, 2.1076e-04, 7.1859e-04, 5.9891e-03, 1.3008e-03,\n",
      "        5.2681e-03, 1.3481e-02, 2.0180e-03, 7.3576e-04, 4.7874e-04, 1.8625e-03,\n",
      "        1.6041e-03, 2.1763e-03, 2.5406e-03, 1.8787e-03, 1.5450e-03, 7.6342e-04,\n",
      "        2.9159e-04, 1.6146e-03, 6.9237e-04, 4.2319e-04, 4.4289e-03, 1.9894e-03,\n",
      "        6.0749e-04, 7.7105e-04, 1.3008e-03, 2.6584e-04, 8.3542e-04, 6.5422e-04,\n",
      "        6.6710e-04, 4.9438e-03, 2.6798e-03, 2.6894e-04, 3.6716e-03, 3.1853e-03,\n",
      "        2.1992e-03, 7.8201e-03, 5.8746e-04, 2.0370e-03, 5.5847e-03, 3.2043e-03,\n",
      "        1.4429e-03, 3.5515e-03, 6.0883e-03, 3.3188e-03, 1.6270e-03, 1.6289e-03,\n",
      "        3.8052e-03, 5.4741e-03, 3.3550e-03, 2.2369e-02, 1.0986e-02, 2.8122e-02,\n",
      "        5.0125e-03, 8.4915e-03, 1.2459e-02, 2.2232e-02, 2.0599e-02, 5.7640e-03,\n",
      "        2.7557e-02, 2.2171e-02, 1.0246e-02, 3.7498e-03, 1.1063e-02, 1.7914e-02,\n",
      "        8.9264e-03, 4.6844e-03, 9.2316e-03, 9.1171e-03, 2.1706e-03, 1.0483e-02,\n",
      "        7.0267e-03, 1.0323e-02, 1.1375e-02, 2.5192e-02, 2.5635e-02, 1.8448e-02,\n",
      "        2.2903e-02, 1.2367e-02, 8.2703e-03, 1.2383e-02, 4.8943e-03, 1.6449e-02,\n",
      "        2.0218e-02, 1.1017e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [33] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [33] : torch.Size([1, 32, 1, 105])\n",
      "Last layer attentions for generated token [33] : tensor([2.3730e-01, 2.3730e-01, 2.5201e-04, 3.9101e-04, 1.7941e-04, 4.5319e-03,\n",
      "        3.6240e-04, 3.9101e-04, 9.0933e-04, 6.4230e-04, 1.8120e-03, 4.5276e-04,\n",
      "        5.0497e-04, 1.2722e-03, 7.6981e-03, 1.5854e-02, 5.1212e-04, 3.5334e-04,\n",
      "        1.9705e-04, 3.1114e-04, 1.7118e-04, 3.1972e-04, 7.9422e-03, 1.4820e-03,\n",
      "        6.8130e-03, 1.6983e-02, 1.8473e-03, 8.3303e-04, 8.0872e-04, 9.4938e-04,\n",
      "        8.9884e-04, 1.8549e-03, 1.2846e-03, 9.1839e-04, 5.9271e-04, 1.0185e-03,\n",
      "        5.5170e-04, 8.3303e-04, 4.5109e-04, 3.8815e-04, 3.5973e-03, 2.4643e-03,\n",
      "        8.3447e-04, 6.8760e-04, 9.7179e-04, 5.0020e-04, 5.0211e-04, 2.7037e-04,\n",
      "        4.4918e-04, 2.6550e-03, 2.1267e-03, 3.0637e-04, 3.7537e-03, 3.9406e-03,\n",
      "        2.4567e-03, 6.0234e-03, 1.2550e-03, 2.4738e-03, 5.1155e-03, 2.2545e-03,\n",
      "        1.6527e-03, 1.9703e-03, 2.5311e-03, 1.6727e-03, 1.6403e-03, 7.6580e-04,\n",
      "        5.3368e-03, 3.6774e-03, 2.9640e-03, 2.8610e-02, 1.8845e-02, 2.6642e-02,\n",
      "        1.1543e-02, 6.0463e-03, 8.5831e-03, 1.5259e-02, 1.0605e-02, 6.8054e-03,\n",
      "        1.2657e-02, 2.2110e-02, 7.2517e-03, 1.3985e-02, 1.1528e-02, 5.5428e-03,\n",
      "        4.1237e-03, 1.0559e-02, 7.0534e-03, 6.9275e-03, 2.5482e-03, 3.8662e-03,\n",
      "        6.6795e-03, 8.6899e-03, 1.0529e-02, 7.6637e-03, 8.4534e-03, 6.2294e-03,\n",
      "        8.9111e-03, 1.4282e-02, 8.4915e-03, 9.3231e-03, 1.2856e-02, 1.2703e-02,\n",
      "        1.0078e-02, 4.6730e-03, 6.3019e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [34] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [34] : torch.Size([1, 32, 1, 106])\n",
      "Last layer attentions for generated token [34] : tensor([1.3599e-01, 1.3599e-01, 1.8287e-04, 4.0984e-04, 1.3256e-04, 3.4046e-03,\n",
      "        7.7581e-04, 1.3504e-03, 2.0142e-03, 8.5545e-04, 1.3189e-03, 6.5994e-04,\n",
      "        2.2984e-04, 6.3848e-04, 9.0027e-03, 9.6130e-03, 1.4277e-03, 4.6611e-04,\n",
      "        1.6177e-04, 1.7583e-04, 1.7726e-04, 6.8331e-04, 5.3291e-03, 1.2550e-03,\n",
      "        7.3891e-03, 1.6129e-02, 2.1114e-03, 9.4891e-04, 6.2132e-04, 2.7428e-03,\n",
      "        1.7080e-03, 2.6131e-03, 1.4753e-03, 1.7691e-03, 2.0294e-03, 8.7404e-04,\n",
      "        4.4560e-04, 1.2932e-03, 5.3644e-04, 4.0269e-04, 4.9591e-03, 1.9798e-03,\n",
      "        5.6648e-04, 8.8120e-04, 1.4286e-03, 3.4046e-04, 9.2173e-04, 4.9114e-04,\n",
      "        6.1607e-04, 5.7945e-03, 2.3289e-03, 3.5596e-04, 2.7752e-03, 5.6419e-03,\n",
      "        1.8911e-03, 5.7373e-03, 8.9836e-04, 2.3308e-03, 6.0425e-03, 2.5272e-03,\n",
      "        1.0777e-03, 3.0880e-03, 5.2681e-03, 1.6365e-03, 1.2140e-03, 2.2507e-03,\n",
      "        3.3741e-03, 6.5308e-03, 3.0308e-03, 5.9631e-02, 2.6932e-02, 3.6987e-02,\n",
      "        8.7433e-03, 9.7961e-03, 2.4597e-02, 2.6825e-02, 1.0231e-02, 4.5204e-03,\n",
      "        2.2507e-02, 2.8030e-02, 8.1024e-03, 6.8588e-03, 1.2726e-02, 2.2614e-02,\n",
      "        7.3624e-03, 6.5613e-03, 9.2239e-03, 9.0637e-03, 3.2520e-03, 1.0956e-02,\n",
      "        7.8278e-03, 2.0859e-02, 1.4740e-02, 1.4206e-02, 2.4551e-02, 1.5991e-02,\n",
      "        1.3672e-02, 1.6235e-02, 8.7662e-03, 1.2589e-02, 8.0185e-03, 1.6724e-02,\n",
      "        1.7151e-02, 7.0114e-03, 7.0076e-03, 7.8506e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [35] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [35] : torch.Size([1, 32, 1, 107])\n",
      "Last layer attentions for generated token [35] : tensor([0.1705, 0.1709, 0.0002, 0.0005, 0.0002, 0.0036, 0.0002, 0.0004, 0.0010,\n",
      "        0.0012, 0.0013, 0.0008, 0.0004, 0.0009, 0.0130, 0.0128, 0.0012, 0.0010,\n",
      "        0.0002, 0.0004, 0.0002, 0.0006, 0.0064, 0.0019, 0.0065, 0.0134, 0.0027,\n",
      "        0.0007, 0.0007, 0.0013, 0.0010, 0.0034, 0.0016, 0.0010, 0.0013, 0.0015,\n",
      "        0.0006, 0.0015, 0.0006, 0.0003, 0.0068, 0.0027, 0.0010, 0.0011, 0.0012,\n",
      "        0.0004, 0.0006, 0.0003, 0.0004, 0.0058, 0.0031, 0.0004, 0.0032, 0.0062,\n",
      "        0.0021, 0.0063, 0.0008, 0.0019, 0.0059, 0.0018, 0.0019, 0.0017, 0.0030,\n",
      "        0.0019, 0.0020, 0.0013, 0.0049, 0.0051, 0.0034, 0.0380, 0.0233, 0.0356,\n",
      "        0.0074, 0.0068, 0.0128, 0.0195, 0.0175, 0.0080, 0.0179, 0.0309, 0.0142,\n",
      "        0.0065, 0.0078, 0.0066, 0.0054, 0.0054, 0.0063, 0.0057, 0.0021, 0.0058,\n",
      "        0.0067, 0.0113, 0.0142, 0.0113, 0.0143, 0.0091, 0.0136, 0.0191, 0.0110,\n",
      "        0.0160, 0.0093, 0.0132, 0.0156, 0.0060, 0.0073, 0.0057, 0.0277],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [36] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [36] : torch.Size([1, 32, 1, 108])\n",
      "Last layer attentions for generated token [36] : tensor([2.7661e-01, 2.7661e-01, 1.7273e-04, 4.1437e-04, 9.5785e-05, 2.1324e-03,\n",
      "        2.4557e-04, 3.9840e-04, 7.9107e-04, 9.6130e-04, 1.2321e-03, 3.4833e-04,\n",
      "        2.9278e-04, 7.9727e-04, 5.8899e-03, 6.5842e-03, 5.4550e-04, 4.8923e-04,\n",
      "        1.6284e-04, 2.8706e-04, 1.5128e-04, 5.2071e-04, 1.2718e-02, 1.4896e-03,\n",
      "        4.1695e-03, 8.8348e-03, 9.4128e-04, 5.9366e-04, 6.9952e-04, 7.8011e-04,\n",
      "        7.5006e-04, 1.2760e-03, 1.0500e-03, 9.6130e-04, 7.4005e-04, 7.1335e-04,\n",
      "        7.7105e-04, 7.2002e-04, 6.3038e-04, 4.9973e-04, 3.5896e-03, 2.5177e-03,\n",
      "        5.7650e-04, 7.2575e-04, 9.7275e-04, 7.4434e-04, 4.9973e-04, 5.2500e-04,\n",
      "        5.6839e-04, 2.3003e-03, 1.9007e-03, 1.8680e-04, 4.5929e-03, 2.9106e-03,\n",
      "        2.5291e-03, 3.2654e-03, 5.8222e-04, 2.1210e-03, 5.2490e-03, 1.4858e-03,\n",
      "        1.1692e-03, 1.4095e-03, 2.3727e-03, 1.3475e-03, 1.9083e-03, 9.5415e-04,\n",
      "        4.7417e-03, 2.4052e-03, 2.6398e-03, 2.4750e-02, 1.3557e-02, 2.1378e-02,\n",
      "        6.6795e-03, 5.7716e-03, 6.1455e-03, 9.9030e-03, 9.2697e-03, 4.1618e-03,\n",
      "        8.8730e-03, 1.9028e-02, 6.6490e-03, 4.2343e-03, 6.2714e-03, 4.1656e-03,\n",
      "        4.6310e-03, 3.9749e-03, 4.4823e-03, 6.3400e-03, 3.0346e-03, 7.9803e-03,\n",
      "        6.4354e-03, 1.1749e-02, 8.7357e-03, 1.3252e-02, 8.9493e-03, 4.6997e-03,\n",
      "        9.2239e-03, 1.2573e-02, 5.5504e-03, 7.4348e-03, 5.7411e-03, 8.2245e-03,\n",
      "        7.8888e-03, 4.6425e-03, 3.7804e-03, 5.3101e-03, 1.1330e-02, 1.2207e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [37] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [37] : torch.Size([1, 32, 1, 109])\n",
      "Last layer attentions for generated token [37] : tensor([1.8323e-01, 1.8323e-01, 1.2338e-04, 2.2876e-04, 8.8871e-05, 3.4218e-03,\n",
      "        3.2759e-04, 6.6090e-04, 1.1396e-03, 6.5660e-04, 1.5068e-03, 6.8712e-04,\n",
      "        3.3355e-04, 5.3692e-04, 6.4087e-03, 9.5596e-03, 8.2874e-04, 5.0449e-04,\n",
      "        2.1529e-04, 2.7227e-04, 2.2876e-04, 4.9973e-04, 5.2490e-03, 1.0271e-03,\n",
      "        7.0610e-03, 1.3161e-02, 9.7847e-04, 5.6171e-04, 7.2145e-04, 1.4372e-03,\n",
      "        9.8991e-04, 1.9646e-03, 1.3685e-03, 1.1044e-03, 1.0271e-03, 9.2268e-04,\n",
      "        3.4332e-04, 8.4829e-04, 4.4274e-04, 4.2248e-04, 4.0283e-03, 1.6127e-03,\n",
      "        4.3082e-04, 6.6090e-04, 1.1082e-03, 6.6710e-04, 8.7357e-04, 3.8695e-04,\n",
      "        7.6056e-04, 3.9597e-03, 2.2583e-03, 2.9445e-04, 3.6964e-03, 2.2793e-03,\n",
      "        1.9188e-03, 4.2267e-03, 7.9250e-04, 1.8730e-03, 6.6223e-03, 2.1687e-03,\n",
      "        2.4471e-03, 2.9316e-03, 3.7880e-03, 2.1935e-03, 2.0714e-03, 1.4515e-03,\n",
      "        3.9978e-03, 6.0349e-03, 3.2845e-03, 2.8412e-02, 1.4076e-02, 3.4302e-02,\n",
      "        6.3438e-03, 8.1406e-03, 1.0155e-02, 1.7776e-02, 1.1368e-02, 7.1640e-03,\n",
      "        1.7517e-02, 2.2003e-02, 9.4833e-03, 7.1449e-03, 9.8648e-03, 6.6833e-03,\n",
      "        3.9978e-03, 6.1150e-03, 7.7362e-03, 8.7738e-03, 1.8930e-03, 6.8779e-03,\n",
      "        1.2688e-02, 2.1088e-02, 1.6220e-02, 1.5549e-02, 1.5747e-02, 8.8043e-03,\n",
      "        1.2100e-02, 1.2856e-02, 9.8495e-03, 1.0803e-02, 1.2573e-02, 1.4450e-02,\n",
      "        1.1559e-02, 7.6256e-03, 6.7863e-03, 1.2405e-02, 2.7557e-02, 1.5495e-02,\n",
      "        6.9504e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [38] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [38] : torch.Size([1, 32, 1, 110])\n",
      "Last layer attentions for generated token [38] : tensor([2.3804e-01, 2.3755e-01, 2.7108e-04, 4.8804e-04, 1.4341e-04, 2.5578e-03,\n",
      "        5.8508e-04, 7.6914e-04, 9.0837e-04, 9.6893e-04, 1.2388e-03, 5.6839e-04,\n",
      "        2.3282e-04, 5.1165e-04, 7.3624e-03, 7.2250e-03, 1.4009e-03, 7.0763e-04,\n",
      "        2.9731e-04, 4.1270e-04, 2.1112e-04, 6.9094e-04, 1.0605e-02, 1.6451e-03,\n",
      "        6.0997e-03, 1.0239e-02, 2.5597e-03, 8.1921e-04, 6.7520e-04, 1.0624e-03,\n",
      "        1.3247e-03, 1.9264e-03, 1.4629e-03, 1.1549e-03, 9.2602e-04, 7.9393e-04,\n",
      "        4.7493e-04, 1.1024e-03, 7.7868e-04, 4.7207e-04, 3.8662e-03, 2.0046e-03,\n",
      "        1.0853e-03, 1.2417e-03, 1.3533e-03, 7.9203e-04, 8.0967e-04, 5.1546e-04,\n",
      "        7.2145e-04, 3.2101e-03, 2.3212e-03, 4.3917e-04, 4.2229e-03, 4.4746e-03,\n",
      "        2.4567e-03, 4.5052e-03, 1.3962e-03, 2.7866e-03, 5.3139e-03, 2.4586e-03,\n",
      "        1.5602e-03, 2.7256e-03, 3.2387e-03, 2.1667e-03, 2.1458e-03, 1.0309e-03,\n",
      "        5.2414e-03, 5.8479e-03, 3.7060e-03, 2.8717e-02, 1.2184e-02, 2.5909e-02,\n",
      "        7.6637e-03, 4.7035e-03, 5.4893e-03, 9.3460e-03, 1.4465e-02, 5.0087e-03,\n",
      "        1.0391e-02, 1.6541e-02, 7.7896e-03, 6.3057e-03, 6.9809e-03, 4.3983e-03,\n",
      "        4.0283e-03, 5.6572e-03, 5.9242e-03, 7.1220e-03, 1.8425e-03, 7.0877e-03,\n",
      "        6.7406e-03, 8.1558e-03, 6.7177e-03, 1.2756e-02, 8.2245e-03, 4.8943e-03,\n",
      "        8.1635e-03, 1.1093e-02, 6.8626e-03, 1.1787e-02, 9.5215e-03, 1.2505e-02,\n",
      "        1.0948e-02, 6.3438e-03, 4.6120e-03, 6.2714e-03, 1.2825e-02, 1.0323e-02,\n",
      "        7.7553e-03, 1.5114e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [39] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [39] : torch.Size([1, 32, 1, 111])\n",
      "Last layer attentions for generated token [39] : tensor([1.7432e-01, 1.7395e-01, 3.2997e-04, 4.2367e-04, 1.6367e-04, 3.1033e-03,\n",
      "        5.3883e-04, 1.1806e-03, 1.5469e-03, 9.6941e-04, 1.7624e-03, 4.5013e-04,\n",
      "        2.3031e-04, 6.9427e-04, 8.9188e-03, 9.1782e-03, 7.7152e-04, 3.2854e-04,\n",
      "        1.7691e-04, 3.5739e-04, 2.8372e-04, 1.2941e-03, 1.2711e-02, 4.2763e-03,\n",
      "        6.4201e-03, 1.2589e-02, 4.8447e-03, 1.2817e-03, 8.4734e-04, 1.4935e-03,\n",
      "        1.9798e-03, 2.7809e-03, 2.7409e-03, 1.9131e-03, 1.3313e-03, 9.1457e-04,\n",
      "        6.1512e-04, 1.5402e-03, 9.4366e-04, 6.0081e-04, 5.8289e-03, 2.6512e-03,\n",
      "        1.3237e-03, 1.7319e-03, 1.9321e-03, 6.1989e-04, 1.2922e-03, 7.6723e-04,\n",
      "        1.0386e-03, 4.7836e-03, 2.6321e-03, 6.7282e-04, 3.2444e-03, 6.5155e-03,\n",
      "        3.1033e-03, 6.5956e-03, 8.0204e-04, 2.0523e-03, 3.3512e-03, 1.5850e-03,\n",
      "        1.0948e-03, 2.8038e-03, 3.4809e-03, 1.8272e-03, 2.0046e-03, 1.7538e-03,\n",
      "        3.5057e-03, 4.5700e-03, 2.0924e-03, 3.6102e-02, 1.6129e-02, 2.1851e-02,\n",
      "        4.5891e-03, 4.1008e-03, 5.5542e-03, 1.6144e-02, 1.2794e-02, 6.5231e-03,\n",
      "        1.9699e-02, 1.9836e-02, 9.7351e-03, 5.9052e-03, 8.2703e-03, 9.5978e-03,\n",
      "        4.9019e-03, 5.3177e-03, 6.2256e-03, 4.9286e-03, 1.5631e-03, 9.2392e-03,\n",
      "        6.6299e-03, 9.2316e-03, 1.0742e-02, 1.4771e-02, 1.1597e-02, 1.0658e-02,\n",
      "        1.3504e-02, 1.3107e-02, 5.8632e-03, 1.4275e-02, 8.0872e-03, 1.3184e-02,\n",
      "        1.8707e-02, 1.0681e-02, 7.0229e-03, 8.3237e-03, 2.3499e-02, 1.1612e-02,\n",
      "        1.0269e-02, 2.3468e-02, 9.4147e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [40] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [40] : torch.Size([1, 32, 1, 112])\n",
      "Last layer attentions for generated token [40] : tensor([1.8652e-01, 1.8652e-01, 3.0160e-04, 3.8648e-04, 1.3351e-04, 1.7776e-03,\n",
      "        1.9503e-04, 3.9101e-04, 7.4053e-04, 5.0497e-04, 1.0319e-03, 3.0613e-04,\n",
      "        2.6250e-04, 5.6553e-04, 9.9869e-03, 7.8812e-03, 4.1604e-04, 4.3011e-04,\n",
      "        1.7893e-04, 3.9411e-04, 2.5249e-04, 1.1997e-03, 1.0574e-02, 2.7237e-03,\n",
      "        6.0043e-03, 9.0332e-03, 3.0994e-03, 8.6212e-04, 8.3876e-04, 1.1024e-03,\n",
      "        1.5306e-03, 3.3875e-03, 1.4839e-03, 9.9087e-04, 8.3733e-04, 9.5987e-04,\n",
      "        5.5981e-04, 9.1791e-04, 6.4945e-04, 5.8365e-04, 6.7329e-03, 2.3556e-03,\n",
      "        8.4734e-04, 8.8978e-04, 8.8453e-04, 6.5327e-04, 4.7898e-04, 4.6968e-04,\n",
      "        6.5708e-04, 4.0054e-03, 3.0251e-03, 2.9325e-04, 3.3894e-03, 6.2981e-03,\n",
      "        2.0256e-03, 4.0016e-03, 9.6560e-04, 1.7214e-03, 5.7831e-03, 2.0866e-03,\n",
      "        1.4496e-03, 2.1877e-03, 3.0174e-03, 1.9464e-03, 1.4782e-03, 1.0338e-03,\n",
      "        6.3324e-03, 3.7098e-03, 3.5152e-03, 3.9307e-02, 1.6708e-02, 2.7390e-02,\n",
      "        6.8626e-03, 5.3711e-03, 9.6970e-03, 1.5259e-02, 1.4961e-02, 8.6288e-03,\n",
      "        1.4343e-02, 1.9852e-02, 8.5373e-03, 8.7738e-03, 8.9722e-03, 5.3978e-03,\n",
      "        5.5504e-03, 6.7177e-03, 6.7329e-03, 7.0267e-03, 2.0294e-03, 8.6975e-03,\n",
      "        5.7068e-03, 9.7198e-03, 1.2520e-02, 1.2619e-02, 5.2757e-03, 6.1951e-03,\n",
      "        1.2032e-02, 1.6632e-02, 9.4604e-03, 1.5305e-02, 1.0666e-02, 1.0948e-02,\n",
      "        9.9869e-03, 5.2910e-03, 6.0501e-03, 4.9973e-03, 2.1881e-02, 1.6479e-02,\n",
      "        6.9618e-03, 1.4587e-02, 9.6512e-03, 1.1406e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [41] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [41] : torch.Size([1, 32, 1, 113])\n",
      "Last layer attentions for generated token [41] : tensor([2.6367e-01, 2.6318e-01, 3.9434e-04, 3.7622e-04, 1.8477e-04, 3.4122e-03,\n",
      "        2.8181e-04, 5.7793e-04, 8.4925e-04, 5.4836e-04, 1.9007e-03, 4.9448e-04,\n",
      "        4.1723e-04, 4.7565e-04, 8.3084e-03, 9.3536e-03, 4.1485e-04, 2.4772e-04,\n",
      "        2.1768e-04, 2.4486e-04, 2.1517e-04, 6.3133e-04, 7.8964e-03, 1.6565e-03,\n",
      "        7.1640e-03, 1.1627e-02, 2.3727e-03, 5.0926e-04, 6.2990e-04, 6.3992e-04,\n",
      "        8.9884e-04, 1.5745e-03, 1.7090e-03, 5.6696e-04, 5.3787e-04, 4.2796e-04,\n",
      "        1.4555e-04, 5.1212e-04, 3.2187e-04, 3.2997e-04, 4.1046e-03, 1.5650e-03,\n",
      "        5.6696e-04, 4.9829e-04, 5.7697e-04, 3.2759e-04, 3.3855e-04, 3.0231e-04,\n",
      "        4.3988e-04, 2.6760e-03, 1.7910e-03, 4.9925e-04, 2.1057e-03, 6.5117e-03,\n",
      "        1.3657e-03, 4.5280e-03, 7.3814e-04, 2.3613e-03, 7.4768e-03, 2.2659e-03,\n",
      "        2.1687e-03, 1.9503e-03, 5.0468e-03, 1.8263e-03, 1.9627e-03, 1.3418e-03,\n",
      "        3.0823e-03, 6.4278e-03, 2.8934e-03, 3.5583e-02, 1.2199e-02, 1.9226e-02,\n",
      "        3.7041e-03, 2.3842e-03, 3.1223e-03, 1.0803e-02, 7.2441e-03, 3.7632e-03,\n",
      "        1.1406e-02, 1.5457e-02, 6.1989e-03, 5.4054e-03, 4.9057e-03, 3.1643e-03,\n",
      "        2.8324e-03, 4.9667e-03, 5.4245e-03, 4.6921e-03, 8.1062e-04, 3.3360e-03,\n",
      "        6.8588e-03, 4.7531e-03, 8.3847e-03, 1.1856e-02, 9.4147e-03, 6.4201e-03,\n",
      "        1.1879e-02, 9.1553e-03, 4.9706e-03, 5.4436e-03, 5.2681e-03, 7.1030e-03,\n",
      "        6.2904e-03, 4.0321e-03, 3.9940e-03, 6.0844e-03, 1.1604e-02, 9.3384e-03,\n",
      "        6.1684e-03, 1.0674e-02, 4.9324e-03, 8.5983e-03, 6.3286e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [42] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [42] : torch.Size([1, 32, 1, 114])\n",
      "Last layer attentions for generated token [42] : tensor([2.5952e-01, 2.5952e-01, 3.3689e-04, 2.2268e-04, 1.4412e-04, 3.3379e-03,\n",
      "        3.5596e-04, 7.1764e-04, 1.2970e-03, 7.1478e-04, 2.6436e-03, 5.3024e-04,\n",
      "        4.2510e-04, 5.9128e-04, 7.0343e-03, 1.0078e-02, 3.7670e-04, 2.4509e-04,\n",
      "        1.5402e-04, 2.2757e-04, 2.1374e-04, 4.6968e-04, 5.7564e-03, 8.1968e-04,\n",
      "        6.8436e-03, 9.6359e-03, 1.4105e-03, 3.8338e-04, 5.4169e-04, 6.6233e-04,\n",
      "        1.0729e-03, 1.5869e-03, 1.8215e-03, 4.6968e-04, 5.2595e-04, 4.9305e-04,\n",
      "        1.5700e-04, 5.8222e-04, 4.0007e-04, 2.4939e-04, 4.7951e-03, 1.6317e-03,\n",
      "        3.5810e-04, 4.0174e-04, 4.9686e-04, 2.6703e-04, 2.8205e-04, 3.7813e-04,\n",
      "        5.0879e-04, 3.4409e-03, 2.2984e-03, 3.6001e-04, 2.6360e-03, 4.2152e-03,\n",
      "        1.5554e-03, 3.4599e-03, 5.4169e-04, 1.5554e-03, 6.6986e-03, 2.3365e-03,\n",
      "        1.4896e-03, 2.6550e-03, 4.4594e-03, 1.6975e-03, 2.0142e-03, 1.5554e-03,\n",
      "        3.6831e-03, 5.7411e-03, 3.3569e-03, 2.7145e-02, 1.0574e-02, 1.7624e-02,\n",
      "        3.0956e-03, 2.4662e-03, 5.4817e-03, 1.2627e-02, 7.3814e-03, 2.9221e-03,\n",
      "        8.5068e-03, 1.1711e-02, 4.4594e-03, 2.9373e-03, 2.8362e-03, 3.7823e-03,\n",
      "        4.2076e-03, 4.0321e-03, 4.3144e-03, 3.4332e-03, 8.0204e-04, 3.5210e-03,\n",
      "        6.2866e-03, 5.4359e-03, 1.0277e-02, 1.7883e-02, 1.4969e-02, 8.0109e-03,\n",
      "        8.0948e-03, 8.8348e-03, 4.2496e-03, 6.5002e-03, 3.8223e-03, 6.0272e-03,\n",
      "        7.3509e-03, 5.1003e-03, 6.2103e-03, 6.3972e-03, 1.8600e-02, 1.2398e-02,\n",
      "        9.5673e-03, 1.4328e-02, 4.7379e-03, 1.0651e-02, 9.8343e-03, 3.1471e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [43] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [43] : torch.Size([1, 32, 1, 115])\n",
      "Last layer attentions for generated token [43] : tensor([0.1753, 0.1753, 0.0004, 0.0006, 0.0003, 0.0028, 0.0003, 0.0003, 0.0006,\n",
      "        0.0010, 0.0022, 0.0006, 0.0004, 0.0006, 0.0132, 0.0126, 0.0007, 0.0009,\n",
      "        0.0003, 0.0005, 0.0005, 0.0010, 0.0084, 0.0025, 0.0116, 0.0119, 0.0033,\n",
      "        0.0009, 0.0008, 0.0010, 0.0012, 0.0034, 0.0012, 0.0005, 0.0006, 0.0011,\n",
      "        0.0004, 0.0012, 0.0008, 0.0004, 0.0087, 0.0033, 0.0006, 0.0012, 0.0010,\n",
      "        0.0005, 0.0003, 0.0003, 0.0004, 0.0055, 0.0033, 0.0005, 0.0028, 0.0067,\n",
      "        0.0022, 0.0052, 0.0014, 0.0026, 0.0080, 0.0024, 0.0021, 0.0032, 0.0043,\n",
      "        0.0024, 0.0029, 0.0019, 0.0062, 0.0059, 0.0036, 0.0385, 0.0193, 0.0299,\n",
      "        0.0052, 0.0040, 0.0134, 0.0148, 0.0194, 0.0079, 0.0141, 0.0239, 0.0141,\n",
      "        0.0058, 0.0051, 0.0037, 0.0048, 0.0044, 0.0044, 0.0047, 0.0014, 0.0036,\n",
      "        0.0047, 0.0067, 0.0102, 0.0159, 0.0102, 0.0076, 0.0102, 0.0145, 0.0107,\n",
      "        0.0139, 0.0049, 0.0068, 0.0080, 0.0030, 0.0037, 0.0031, 0.0202, 0.0148,\n",
      "        0.0064, 0.0116, 0.0048, 0.0087, 0.0091, 0.0036, 0.0128],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [44] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [44] : torch.Size([1, 32, 1, 116])\n",
      "Last layer attentions for generated token [44] : tensor([2.6392e-01, 2.6392e-01, 2.3377e-04, 2.6798e-04, 1.5152e-04, 3.1490e-03,\n",
      "        4.2653e-04, 6.5947e-04, 7.6771e-04, 5.3596e-04, 1.6356e-03, 6.7616e-04,\n",
      "        4.2081e-04, 6.6710e-04, 6.3515e-03, 7.4196e-03, 3.4332e-04, 2.4307e-04,\n",
      "        1.7989e-04, 1.7989e-04, 1.3578e-04, 2.8253e-04, 6.5651e-03, 1.0681e-03,\n",
      "        8.4229e-03, 1.2596e-02, 1.0662e-03, 3.6407e-04, 4.4966e-04, 5.3310e-04,\n",
      "        7.2289e-04, 1.0939e-03, 1.3962e-03, 6.2561e-04, 6.4898e-04, 4.4537e-04,\n",
      "        2.8419e-04, 4.7302e-04, 2.8253e-04, 2.5868e-04, 3.2673e-03, 1.4257e-03,\n",
      "        3.8314e-04, 5.9128e-04, 7.3290e-04, 3.3355e-04, 4.2486e-04, 3.8314e-04,\n",
      "        6.5565e-04, 3.0880e-03, 2.3766e-03, 4.6492e-04, 3.7136e-03, 6.5231e-03,\n",
      "        1.8387e-03, 6.1951e-03, 8.1396e-04, 1.7033e-03, 6.2141e-03, 2.3403e-03,\n",
      "        2.1667e-03, 1.8206e-03, 5.1765e-03, 2.5768e-03, 1.5335e-03, 1.4257e-03,\n",
      "        2.8782e-03, 6.7558e-03, 4.5204e-03, 2.7161e-02, 1.2062e-02, 2.2430e-02,\n",
      "        3.3646e-03, 2.9964e-03, 3.3169e-03, 9.3460e-03, 6.7329e-03, 2.9068e-03,\n",
      "        9.2850e-03, 1.5007e-02, 8.0109e-03, 3.5915e-03, 3.3169e-03, 3.5686e-03,\n",
      "        2.9640e-03, 3.4828e-03, 4.2381e-03, 3.3970e-03, 8.9931e-04, 2.8439e-03,\n",
      "        5.6114e-03, 4.1389e-03, 8.9645e-03, 1.1948e-02, 1.2093e-02, 7.3586e-03,\n",
      "        1.2184e-02, 9.3307e-03, 5.7220e-03, 6.7978e-03, 4.0627e-03, 5.5466e-03,\n",
      "        7.0610e-03, 3.8910e-03, 4.6730e-03, 6.8398e-03, 1.1726e-02, 1.1765e-02,\n",
      "        7.1487e-03, 8.2779e-03, 3.7022e-03, 8.7967e-03, 5.8937e-03, 2.1324e-03,\n",
      "        6.1836e-03, 5.0163e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [45] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [45] : torch.Size([1, 32, 1, 117])\n",
      "Last layer attentions for generated token [45] : tensor([2.1448e-01, 2.1448e-01, 1.7738e-04, 1.5533e-04, 1.2243e-04, 2.2221e-03,\n",
      "        3.3140e-04, 7.5388e-04, 8.3303e-04, 4.2129e-04, 1.1616e-03, 2.5558e-04,\n",
      "        2.0254e-04, 4.4513e-04, 4.9591e-03, 4.5929e-03, 2.2423e-04, 2.2423e-04,\n",
      "        1.2982e-04, 1.3077e-04, 7.3910e-05, 1.7536e-04, 5.1880e-03, 4.6277e-04,\n",
      "        4.2381e-03, 7.4692e-03, 7.1812e-04, 3.0828e-04, 2.7680e-04, 6.6423e-04,\n",
      "        5.5838e-04, 6.5756e-04, 1.0309e-03, 6.5517e-04, 6.4993e-04, 2.3592e-04,\n",
      "        1.8513e-04, 7.0286e-04, 3.0756e-04, 2.1815e-04, 2.7122e-03, 1.2608e-03,\n",
      "        2.4247e-04, 4.9353e-04, 8.0442e-04, 2.6202e-04, 4.7469e-04, 3.1185e-04,\n",
      "        4.9448e-04, 2.5806e-03, 1.6994e-03, 2.8563e-04, 2.5463e-03, 3.2978e-03,\n",
      "        1.6155e-03, 3.5496e-03, 6.1178e-04, 1.1148e-03, 4.9515e-03, 1.6956e-03,\n",
      "        1.2455e-03, 1.8253e-03, 3.4161e-03, 2.0123e-03, 7.7343e-04, 1.0033e-03,\n",
      "        3.0003e-03, 3.1757e-03, 3.6583e-03, 1.2436e-02, 8.8577e-03, 1.6434e-02,\n",
      "        2.3594e-03, 2.7161e-03, 6.1073e-03, 9.1553e-03, 8.6670e-03, 1.9073e-03,\n",
      "        8.6212e-03, 1.6083e-02, 6.6566e-03, 2.2850e-03, 4.3182e-03, 6.2180e-03,\n",
      "        5.6114e-03, 3.7842e-03, 5.7716e-03, 5.0354e-03, 1.0633e-03, 5.7449e-03,\n",
      "        7.0496e-03, 5.6953e-03, 9.3155e-03, 1.4778e-02, 2.1805e-02, 1.2085e-02,\n",
      "        1.0468e-02, 1.2558e-02, 3.9787e-03, 1.1826e-02, 4.6959e-03, 1.2184e-02,\n",
      "        1.6754e-02, 9.5749e-03, 8.0643e-03, 8.7509e-03, 2.2308e-02, 1.4893e-02,\n",
      "        1.6861e-02, 1.7975e-02, 6.6605e-03, 2.1973e-02, 1.8250e-02, 6.0272e-03,\n",
      "        1.6983e-02, 1.3428e-02, 1.3077e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [46] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [46] : torch.Size([1, 32, 1, 118])\n",
      "Last layer attentions for generated token [46] : tensor([2.0129e-01, 2.0093e-01, 1.7309e-04, 1.3745e-04, 1.0139e-04, 1.9646e-03,\n",
      "        3.1042e-04, 4.3511e-04, 6.1512e-04, 7.7581e-04, 2.3766e-03, 3.8862e-04,\n",
      "        3.8552e-04, 7.7581e-04, 9.7504e-03, 7.5455e-03, 1.9467e-04, 4.7421e-04,\n",
      "        2.4605e-04, 2.3484e-04, 1.4186e-04, 5.1785e-04, 7.1678e-03, 1.1377e-03,\n",
      "        6.1607e-03, 7.2060e-03, 8.2254e-04, 6.2704e-04, 5.4598e-04, 5.0306e-04,\n",
      "        8.8072e-04, 1.2302e-03, 9.3746e-04, 4.2343e-04, 5.1785e-04, 6.1035e-04,\n",
      "        5.9843e-04, 1.0424e-03, 6.1274e-04, 3.9864e-04, 3.1452e-03, 1.7757e-03,\n",
      "        3.6430e-04, 5.6314e-04, 5.2786e-04, 4.2677e-04, 2.4509e-04, 5.2691e-04,\n",
      "        4.4465e-04, 3.0842e-03, 3.2864e-03, 1.8287e-04, 3.2253e-03, 3.8109e-03,\n",
      "        2.7943e-03, 5.0354e-03, 8.3065e-04, 2.0180e-03, 8.6365e-03, 2.4567e-03,\n",
      "        2.5883e-03, 1.8539e-03, 4.5090e-03, 4.0779e-03, 1.7042e-03, 2.1801e-03,\n",
      "        6.7101e-03, 2.1687e-03, 6.3324e-03, 1.4252e-02, 8.7891e-03, 1.4427e-02,\n",
      "        1.5888e-03, 1.8215e-03, 4.0436e-03, 9.9335e-03, 1.1345e-02, 2.4128e-03,\n",
      "        9.9182e-03, 1.8906e-02, 6.7825e-03, 3.1204e-03, 2.8992e-03, 4.0627e-03,\n",
      "        4.4250e-03, 5.0507e-03, 5.2185e-03, 4.4746e-03, 1.5583e-03, 4.8981e-03,\n",
      "        5.1422e-03, 4.7569e-03, 8.4686e-03, 1.6846e-02, 1.1780e-02, 7.6180e-03,\n",
      "        1.8234e-02, 1.4648e-02, 6.4850e-03, 1.0574e-02, 5.0545e-03, 5.7487e-03,\n",
      "        9.5520e-03, 4.6196e-03, 5.7373e-03, 3.7422e-03, 1.7776e-02, 1.4542e-02,\n",
      "        1.2436e-02, 1.2566e-02, 7.6027e-03, 1.7822e-02, 1.7166e-02, 5.7678e-03,\n",
      "        2.4399e-02, 2.1667e-02, 1.9669e-02, 1.2970e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [47] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [47] : torch.Size([1, 32, 1, 119])\n",
      "Last layer attentions for generated token [47] : tensor([2.4585e-01, 2.4585e-01, 3.1447e-04, 5.5265e-04, 2.6989e-04, 2.5654e-03,\n",
      "        1.6892e-04, 4.8780e-04, 7.0858e-04, 4.7851e-04, 1.8663e-03, 3.8886e-04,\n",
      "        3.6407e-04, 5.1832e-04, 1.4702e-02, 1.2878e-02, 3.7408e-04, 4.3058e-04,\n",
      "        3.0112e-04, 1.9908e-04, 1.0532e-04, 7.6628e-04, 8.6746e-03, 1.4391e-03,\n",
      "        5.5161e-03, 1.0460e-02, 1.1148e-03, 3.4523e-04, 4.2892e-04, 6.6805e-04,\n",
      "        1.1215e-03, 2.0885e-03, 8.0919e-04, 3.9744e-04, 4.8494e-04, 2.8396e-04,\n",
      "        1.2267e-04, 5.6171e-04, 1.6832e-04, 1.8120e-04, 4.2229e-03, 1.1101e-03,\n",
      "        3.9744e-04, 4.3297e-04, 5.6505e-04, 2.2554e-04, 4.9353e-04, 2.9755e-04,\n",
      "        2.4486e-04, 3.0632e-03, 1.6861e-03, 3.5834e-04, 2.2945e-03, 7.5760e-03,\n",
      "        1.6060e-03, 4.8103e-03, 6.9904e-04, 1.7061e-03, 1.0185e-02, 2.1305e-03,\n",
      "        1.9608e-03, 1.8740e-03, 5.5923e-03, 2.5482e-03, 1.5354e-03, 1.7061e-03,\n",
      "        3.2864e-03, 2.8076e-03, 4.5624e-03, 3.2990e-02, 1.2894e-02, 1.3412e-02,\n",
      "        3.7041e-03, 1.5087e-03, 4.5433e-03, 9.1095e-03, 6.1340e-03, 2.6913e-03,\n",
      "        1.0857e-02, 1.4038e-02, 6.3972e-03, 2.6455e-03, 3.0422e-03, 2.7313e-03,\n",
      "        1.8301e-03, 3.4771e-03, 4.4479e-03, 3.4904e-03, 8.1062e-04, 3.1452e-03,\n",
      "        5.1537e-03, 4.3221e-03, 7.2441e-03, 8.9569e-03, 1.0788e-02, 7.6790e-03,\n",
      "        1.2741e-02, 9.9258e-03, 5.4817e-03, 6.1913e-03, 3.7746e-03, 5.0735e-03,\n",
      "        7.7019e-03, 3.4637e-03, 3.6755e-03, 3.5248e-03, 1.3260e-02, 6.3133e-03,\n",
      "        4.9133e-03, 8.3237e-03, 3.6697e-03, 8.6365e-03, 8.6441e-03, 2.7161e-03,\n",
      "        1.6418e-02, 1.0559e-02, 9.6970e-03, 8.6594e-03, 1.3489e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [48] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [48] : torch.Size([1, 32, 1, 120])\n",
      "Last layer attentions for generated token [48] : tensor([2.2559e-01, 2.2559e-01, 5.5695e-04, 7.6103e-04, 4.3464e-04, 4.4632e-03,\n",
      "        2.1136e-04, 3.1662e-04, 4.8089e-04, 8.1635e-04, 1.9321e-03, 5.3740e-04,\n",
      "        4.6802e-04, 4.7064e-04, 1.4862e-02, 1.2672e-02, 5.8699e-04, 1.0061e-03,\n",
      "        4.5371e-04, 3.6383e-04, 2.5010e-04, 1.7424e-03, 1.1726e-02, 2.7981e-03,\n",
      "        1.2985e-02, 1.4267e-02, 2.6779e-03, 5.4312e-04, 8.3590e-04, 9.1076e-04,\n",
      "        1.0815e-03, 3.6983e-03, 8.9836e-04, 4.3535e-04, 4.5276e-04, 7.9155e-04,\n",
      "        3.0994e-04, 9.8133e-04, 4.4751e-04, 2.6608e-04, 5.2376e-03, 2.5482e-03,\n",
      "        4.7731e-04, 7.5674e-04, 5.3549e-04, 3.5477e-04, 3.1781e-04, 2.9516e-04,\n",
      "        3.3975e-04, 5.6992e-03, 4.1618e-03, 4.8470e-04, 2.7351e-03, 1.3062e-02,\n",
      "        1.7805e-03, 6.1951e-03, 1.1272e-03, 2.0313e-03, 7.6599e-03, 1.5316e-03,\n",
      "        1.7624e-03, 1.6661e-03, 4.3068e-03, 2.7676e-03, 1.5736e-03, 1.0900e-03,\n",
      "        7.2975e-03, 3.8395e-03, 3.6774e-03, 2.5146e-02, 1.8753e-02, 2.1637e-02,\n",
      "        3.8586e-03, 2.6150e-03, 7.0457e-03, 9.4452e-03, 1.1787e-02, 3.9520e-03,\n",
      "        9.7961e-03, 2.3941e-02, 1.0239e-02, 3.9062e-03, 3.7727e-03, 1.6174e-03,\n",
      "        2.2812e-03, 2.6875e-03, 3.7251e-03, 3.4237e-03, 7.1239e-04, 2.5005e-03,\n",
      "        2.8896e-03, 4.0855e-03, 5.7678e-03, 9.6664e-03, 6.3934e-03, 4.5052e-03,\n",
      "        8.5068e-03, 1.3161e-02, 8.1787e-03, 8.4534e-03, 2.9488e-03, 5.0278e-03,\n",
      "        4.7874e-03, 1.9741e-03, 2.2583e-03, 1.7700e-03, 7.7820e-03, 6.7101e-03,\n",
      "        3.2177e-03, 4.8027e-03, 2.9697e-03, 5.3596e-03, 7.7629e-03, 2.5291e-03,\n",
      "        9.4223e-03, 1.0170e-02, 1.1909e-02, 6.0730e-03, 1.0666e-02, 1.1528e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [49] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [49] : torch.Size([1, 32, 1, 121])\n",
      "Last layer attentions for generated token [49] : tensor([1.5662e-01, 1.5662e-01, 2.6321e-04, 4.1580e-04, 1.7643e-04, 2.7084e-03,\n",
      "        1.7166e-04, 3.2306e-04, 4.7851e-04, 3.9983e-04, 2.2125e-03, 3.1638e-04,\n",
      "        2.3782e-04, 4.4179e-04, 1.1024e-02, 1.2169e-02, 2.7204e-04, 3.0184e-04,\n",
      "        1.8060e-04, 3.7718e-04, 1.4591e-04, 3.4261e-04, 3.8071e-03, 1.0433e-03,\n",
      "        4.2152e-03, 1.7838e-02, 2.4948e-03, 3.5691e-04, 2.3508e-04, 4.9543e-04,\n",
      "        6.2037e-04, 1.7881e-03, 1.0109e-03, 6.7902e-04, 5.0259e-04, 2.5320e-04,\n",
      "        2.0301e-04, 5.1546e-04, 1.5533e-04, 1.3554e-04, 4.1046e-03, 1.2856e-03,\n",
      "        8.3828e-04, 7.1001e-04, 9.3126e-04, 2.1696e-04, 6.7234e-04, 3.0231e-04,\n",
      "        3.5071e-04, 2.9335e-03, 1.3275e-03, 4.1175e-04, 2.3918e-03, 6.8474e-03,\n",
      "        1.7347e-03, 4.1275e-03, 6.7091e-04, 8.8167e-04, 5.2376e-03, 1.4696e-03,\n",
      "        8.4639e-04, 1.5182e-03, 2.4261e-03, 1.0414e-03, 7.7820e-04, 6.4659e-04,\n",
      "        1.7490e-03, 4.7112e-03, 2.2678e-03, 2.4384e-02, 1.5350e-02, 2.1866e-02,\n",
      "        1.0773e-02, 5.8250e-03, 7.7171e-03, 1.8539e-02, 1.2032e-02, 6.2637e-03,\n",
      "        1.4290e-02, 1.6785e-02, 1.7548e-02, 8.7814e-03, 1.1093e-02, 9.6588e-03,\n",
      "        3.6602e-03, 6.0654e-03, 7.2479e-03, 4.6997e-03, 1.2722e-03, 4.6692e-03,\n",
      "        4.9782e-03, 6.0005e-03, 1.0162e-02, 1.0910e-02, 9.1095e-03, 9.5215e-03,\n",
      "        1.0757e-02, 9.1629e-03, 1.1246e-02, 1.4862e-02, 9.2850e-03, 1.3390e-02,\n",
      "        1.5419e-02, 5.3253e-03, 6.2904e-03, 7.7171e-03, 3.9368e-02, 1.3985e-02,\n",
      "        6.1951e-03, 7.7934e-03, 3.7289e-03, 1.6281e-02, 5.0888e-03, 1.7605e-03,\n",
      "        1.4511e-02, 8.1558e-03, 7.5340e-03, 1.1429e-02, 5.8823e-03, 1.1940e-02,\n",
      "        2.7649e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [50] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [50] : torch.Size([1, 32, 1, 122])\n",
      "Last layer attentions for generated token [50] : tensor([1.5759e-01, 1.5759e-01, 1.8704e-04, 2.7323e-04, 1.0914e-04, 2.8706e-03,\n",
      "        1.8418e-04, 3.8242e-04, 4.3917e-04, 5.6505e-04, 2.5730e-03, 2.3603e-04,\n",
      "        4.4179e-04, 1.0452e-03, 5.0964e-03, 7.9651e-03, 3.1090e-04, 3.7003e-04,\n",
      "        2.0993e-04, 5.7316e-04, 1.1045e-04, 2.7919e-04, 7.7896e-03, 1.0786e-03,\n",
      "        1.6804e-03, 8.0109e-03, 8.5831e-04, 4.9496e-04, 5.6505e-04, 5.1880e-04,\n",
      "        6.9094e-04, 8.4496e-04, 8.2731e-04, 7.3004e-04, 4.0627e-04, 4.4799e-04,\n",
      "        5.0259e-04, 5.9366e-04, 2.8253e-04, 3.3164e-04, 2.6798e-03, 1.2512e-03,\n",
      "        4.9210e-04, 6.7759e-04, 7.0333e-04, 4.6587e-04, 4.0388e-04, 3.6263e-04,\n",
      "        5.6744e-04, 1.7424e-03, 1.2465e-03, 2.0111e-04, 2.9488e-03, 2.3308e-03,\n",
      "        1.9045e-03, 6.1073e-03, 1.0481e-03, 1.6356e-03, 3.0632e-03, 1.7786e-03,\n",
      "        1.8082e-03, 2.1286e-03, 3.0823e-03, 1.5306e-03, 2.1706e-03, 1.0414e-03,\n",
      "        3.1242e-03, 2.4509e-03, 1.3266e-03, 1.3672e-02, 8.6288e-03, 2.8198e-02,\n",
      "        7.7591e-03, 8.8120e-03, 1.0483e-02, 1.7654e-02, 1.4893e-02, 7.8087e-03,\n",
      "        1.9867e-02, 2.1408e-02, 8.6441e-03, 5.0201e-03, 9.7046e-03, 8.6823e-03,\n",
      "        6.1417e-03, 6.4087e-03, 6.5918e-03, 6.9237e-03, 3.6888e-03, 7.1869e-03,\n",
      "        5.1689e-03, 6.2065e-03, 1.0513e-02, 1.3161e-02, 1.3725e-02, 9.5901e-03,\n",
      "        1.1375e-02, 1.5068e-02, 7.7438e-03, 1.4206e-02, 5.8517e-03, 8.8501e-03,\n",
      "        1.0262e-02, 4.1161e-03, 7.3318e-03, 4.6082e-03, 1.8631e-02, 1.1864e-02,\n",
      "        6.8283e-03, 9.1858e-03, 5.2643e-03, 1.3794e-02, 7.8087e-03, 4.4250e-03,\n",
      "        1.2093e-02, 8.5907e-03, 8.5297e-03, 9.8877e-03, 7.3433e-03, 6.5575e-03,\n",
      "        4.6997e-02, 2.9877e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [51] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [51] : torch.Size([1, 32, 1, 123])\n",
      "Last layer attentions for generated token [51] : tensor([0.1260, 0.1260, 0.0004, 0.0005, 0.0003, 0.0032, 0.0002, 0.0002, 0.0002,\n",
      "        0.0008, 0.0023, 0.0002, 0.0007, 0.0007, 0.0056, 0.0086, 0.0004, 0.0007,\n",
      "        0.0004, 0.0006, 0.0001, 0.0004, 0.0052, 0.0012, 0.0026, 0.0095, 0.0023,\n",
      "        0.0007, 0.0009, 0.0004, 0.0005, 0.0015, 0.0008, 0.0005, 0.0004, 0.0005,\n",
      "        0.0004, 0.0005, 0.0004, 0.0003, 0.0028, 0.0021, 0.0007, 0.0011, 0.0007,\n",
      "        0.0005, 0.0002, 0.0002, 0.0005, 0.0023, 0.0019, 0.0006, 0.0022, 0.0064,\n",
      "        0.0015, 0.0078, 0.0011, 0.0027, 0.0044, 0.0013, 0.0020, 0.0013, 0.0020,\n",
      "        0.0012, 0.0017, 0.0005, 0.0051, 0.0055, 0.0014, 0.0261, 0.0160, 0.0359,\n",
      "        0.0074, 0.0060, 0.0069, 0.0101, 0.0214, 0.0051, 0.0112, 0.0555, 0.0325,\n",
      "        0.0085, 0.0070, 0.0054, 0.0098, 0.0092, 0.0068, 0.0083, 0.0027, 0.0049,\n",
      "        0.0060, 0.0041, 0.0061, 0.0169, 0.0108, 0.0054, 0.0085, 0.0262, 0.0087,\n",
      "        0.0216, 0.0047, 0.0087, 0.0070, 0.0037, 0.0034, 0.0023, 0.0082, 0.0097,\n",
      "        0.0060, 0.0030, 0.0032, 0.0053, 0.0109, 0.0035, 0.0087, 0.0116, 0.0087,\n",
      "        0.0048, 0.0069, 0.0043, 0.0571, 0.0233, 0.0152], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [52] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [52] : torch.Size([1, 32, 1, 124])\n",
      "Last layer attentions for generated token [52] : tensor([1.6748e-01, 1.6748e-01, 2.4354e-04, 4.4799e-04, 2.0385e-04, 1.9064e-03,\n",
      "        2.1279e-04, 3.2783e-04, 3.2020e-04, 5.0545e-04, 2.1954e-03, 4.7779e-04,\n",
      "        6.6566e-04, 6.7902e-04, 8.4534e-03, 1.1581e-02, 3.9005e-04, 5.5075e-04,\n",
      "        3.2401e-04, 4.5776e-04, 8.8036e-05, 2.5415e-04, 3.7766e-03, 8.1110e-04,\n",
      "        3.2978e-03, 1.1299e-02, 1.9970e-03, 4.1747e-04, 3.9601e-04, 3.4142e-04,\n",
      "        4.5156e-04, 1.2293e-03, 1.2960e-03, 7.5006e-04, 4.7398e-04, 3.5357e-04,\n",
      "        2.3973e-04, 4.0007e-04, 1.9681e-04, 2.1875e-04, 2.0390e-03, 1.3351e-03,\n",
      "        9.9373e-04, 5.4550e-04, 7.2002e-04, 2.5535e-04, 4.3511e-04, 2.1362e-04,\n",
      "        2.2566e-04, 1.6098e-03, 1.2960e-03, 4.8518e-04, 2.9411e-03, 6.0196e-03,\n",
      "        1.7443e-03, 5.5466e-03, 1.5669e-03, 1.6756e-03, 4.8828e-03, 1.8787e-03,\n",
      "        2.2907e-03, 1.5240e-03, 3.0766e-03, 1.7138e-03, 1.6708e-03, 9.6893e-04,\n",
      "        3.3092e-03, 5.5733e-03, 3.3493e-03, 2.8046e-02, 1.4961e-02, 1.9943e-02,\n",
      "        1.0750e-02, 4.1847e-03, 5.8556e-03, 1.1162e-02, 9.6436e-03, 6.4812e-03,\n",
      "        1.1681e-02, 1.6998e-02, 1.3596e-02, 9.6130e-03, 8.5297e-03, 6.7329e-03,\n",
      "        4.0131e-03, 8.3542e-03, 7.8964e-03, 5.9738e-03, 1.7223e-03, 4.2725e-03,\n",
      "        7.0114e-03, 4.3755e-03, 6.0539e-03, 7.8735e-03, 8.5526e-03, 6.7177e-03,\n",
      "        9.4833e-03, 1.0506e-02, 1.2840e-02, 1.1070e-02, 8.2397e-03, 7.3586e-03,\n",
      "        9.7351e-03, 3.6640e-03, 4.4708e-03, 7.3090e-03, 1.3756e-02, 8.8654e-03,\n",
      "        6.0158e-03, 8.4381e-03, 3.6640e-03, 8.3466e-03, 6.3095e-03, 2.4815e-03,\n",
      "        8.8882e-03, 8.1482e-03, 7.1869e-03, 7.0343e-03, 6.8436e-03, 1.9043e-02,\n",
      "        4.4464e-02, 2.1484e-02, 1.9424e-02, 1.0948e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [53] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [53] : torch.Size([1, 32, 1, 125])\n",
      "Last layer attentions for generated token [53] : tensor([1.7957e-01, 1.7920e-01, 5.6028e-04, 5.8508e-04, 2.9254e-04, 5.3825e-03,\n",
      "        1.9372e-04, 3.1257e-04, 4.1413e-04, 8.0729e-04, 2.9659e-03, 2.2161e-04,\n",
      "        6.8808e-04, 1.2436e-03, 6.2485e-03, 1.3466e-02, 4.1413e-04, 5.5170e-04,\n",
      "        3.0637e-04, 5.3167e-04, 1.2505e-04, 3.5334e-04, 1.2299e-02, 1.3285e-03,\n",
      "        3.3264e-03, 1.5198e-02, 3.1357e-03, 1.2674e-03, 1.0824e-03, 8.4114e-04,\n",
      "        9.4795e-04, 1.7233e-03, 9.7418e-04, 7.6771e-04, 3.3021e-04, 4.9925e-04,\n",
      "        5.4407e-04, 7.2813e-04, 4.4322e-04, 4.0054e-04, 3.5801e-03, 2.5158e-03,\n",
      "        9.4938e-04, 7.6151e-04, 6.5517e-04, 3.8981e-04, 2.9874e-04, 3.5000e-04,\n",
      "        4.7088e-04, 2.2697e-03, 2.1954e-03, 4.2462e-04, 3.7155e-03, 4.8218e-03,\n",
      "        2.8973e-03, 6.8512e-03, 6.3610e-04, 1.4257e-03, 3.5419e-03, 1.1654e-03,\n",
      "        1.4172e-03, 1.5659e-03, 1.4391e-03, 9.5510e-04, 9.6464e-04, 5.4979e-04,\n",
      "        3.2825e-03, 3.2635e-03, 1.5440e-03, 2.0813e-02, 1.3115e-02, 2.0676e-02,\n",
      "        4.7188e-03, 2.6531e-03, 5.0697e-03, 8.2550e-03, 1.4687e-02, 6.2866e-03,\n",
      "        1.1757e-02, 1.7059e-02, 7.3357e-03, 6.3248e-03, 1.1406e-02, 4.8866e-03,\n",
      "        3.4103e-03, 4.3259e-03, 6.1722e-03, 5.4016e-03, 2.4986e-03, 3.2749e-03,\n",
      "        3.0003e-03, 2.6112e-03, 5.5847e-03, 6.3972e-03, 5.5656e-03, 3.5973e-03,\n",
      "        5.1804e-03, 1.2978e-02, 6.7444e-03, 1.0674e-02, 5.7640e-03, 6.5804e-03,\n",
      "        6.6032e-03, 3.3340e-03, 3.2654e-03, 2.7561e-03, 8.0261e-03, 8.3923e-03,\n",
      "        6.8169e-03, 5.9700e-03, 5.7869e-03, 9.2697e-03, 8.7204e-03, 4.6844e-03,\n",
      "        8.5831e-03, 1.0292e-02, 7.8735e-03, 5.8212e-03, 7.0381e-03, 5.5084e-03,\n",
      "        6.6956e-02, 2.7908e-02, 1.3992e-02, 1.4534e-02, 7.8964e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [54] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [54] : torch.Size([1, 32, 1, 126])\n",
      "Last layer attentions for generated token [54] : tensor([1.6125e-01, 1.6101e-01, 8.1897e-05, 2.5821e-04, 7.6294e-05, 1.6012e-03,\n",
      "        3.7575e-04, 5.5933e-04, 6.0368e-04, 8.0776e-04, 2.5215e-03, 5.7173e-04,\n",
      "        3.4952e-04, 8.1873e-04, 8.4915e-03, 8.3771e-03, 4.7660e-04, 5.8079e-04,\n",
      "        2.4247e-04, 2.5463e-04, 5.6028e-05, 2.1362e-04, 4.7569e-03, 7.2002e-04,\n",
      "        2.4052e-03, 9.1553e-03, 1.2121e-03, 4.8804e-04, 4.6468e-04, 6.4135e-04,\n",
      "        7.0572e-04, 7.3814e-04, 1.2884e-03, 1.3771e-03, 9.3508e-04, 4.4179e-04,\n",
      "        2.9898e-04, 6.9761e-04, 2.3055e-04, 2.4390e-04, 1.3742e-03, 9.3317e-04,\n",
      "        6.0272e-04, 5.7173e-04, 9.4986e-04, 3.7861e-04, 5.9795e-04, 3.2377e-04,\n",
      "        5.8651e-04, 2.1763e-03, 1.1826e-03, 2.1994e-04, 2.7275e-03, 3.0003e-03,\n",
      "        1.4830e-03, 5.1117e-03, 9.7418e-04, 1.5211e-03, 2.5368e-03, 1.3847e-03,\n",
      "        1.0614e-03, 1.3161e-03, 2.5196e-03, 1.2417e-03, 1.2341e-03, 7.5293e-04,\n",
      "        1.8854e-03, 2.6360e-03, 1.4915e-03, 1.4900e-02, 7.4844e-03, 1.6281e-02,\n",
      "        5.3215e-03, 4.2877e-03, 9.8648e-03, 1.1871e-02, 6.7482e-03, 4.2419e-03,\n",
      "        1.2947e-02, 1.1391e-02, 4.3869e-03, 3.9062e-03, 8.4686e-03, 1.3649e-02,\n",
      "        4.4060e-03, 6.1455e-03, 7.5569e-03, 7.2174e-03, 2.9011e-03, 8.5526e-03,\n",
      "        6.6566e-03, 1.1108e-02, 7.7782e-03, 1.1681e-02, 1.3824e-02, 7.7629e-03,\n",
      "        9.5520e-03, 7.5912e-03, 8.6060e-03, 1.0574e-02, 7.2556e-03, 1.3405e-02,\n",
      "        1.9547e-02, 7.6904e-03, 9.2010e-03, 1.0246e-02, 2.1561e-02, 9.8877e-03,\n",
      "        7.5989e-03, 1.4969e-02, 5.1041e-03, 2.0004e-02, 5.9738e-03, 3.3131e-03,\n",
      "        1.0040e-02, 9.2316e-03, 1.6144e-02, 1.1368e-02, 8.5754e-03, 7.2517e-03,\n",
      "        3.2227e-02, 1.9913e-02, 1.3557e-02, 1.0643e-02, 5.7487e-03, 1.2665e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [55] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [55] : torch.Size([1, 32, 1, 127])\n",
      "Last layer attentions for generated token [55] : tensor([2.2437e-01, 2.2388e-01, 1.0908e-04, 2.0218e-04, 7.7367e-05, 1.9417e-03,\n",
      "        2.3091e-04, 2.4295e-04, 2.8634e-04, 5.0068e-04, 1.9913e-03, 3.8671e-04,\n",
      "        4.5562e-04, 7.2241e-04, 7.4615e-03, 8.6899e-03, 3.5214e-04, 4.9639e-04,\n",
      "        2.7156e-04, 2.1696e-04, 7.3254e-05, 2.3544e-04, 5.8632e-03, 7.9823e-04,\n",
      "        3.4676e-03, 1.0956e-02, 9.2077e-04, 4.8232e-04, 5.3263e-04, 2.9540e-04,\n",
      "        3.3092e-04, 6.2275e-04, 7.9489e-04, 4.5919e-04, 3.3140e-04, 5.3501e-04,\n",
      "        4.1580e-04, 4.4179e-04, 2.7275e-04, 3.0947e-04, 1.3552e-03, 1.2684e-03,\n",
      "        6.2275e-04, 4.5562e-04, 4.8137e-04, 4.3559e-04, 2.2733e-04, 2.2650e-04,\n",
      "        3.8457e-04, 1.5030e-03, 1.3742e-03, 1.1122e-04, 3.2864e-03, 2.4929e-03,\n",
      "        2.2621e-03, 4.2191e-03, 8.5974e-04, 2.0523e-03, 3.0899e-03, 1.2245e-03,\n",
      "        1.6699e-03, 8.9550e-04, 2.0809e-03, 1.5783e-03, 1.4038e-03, 5.7602e-04,\n",
      "        3.9673e-03, 1.9035e-03, 2.6169e-03, 1.0887e-02, 5.8479e-03, 1.5930e-02,\n",
      "        6.9885e-03, 3.5229e-03, 4.2267e-03, 6.2256e-03, 6.4354e-03, 3.5362e-03,\n",
      "        5.9929e-03, 1.0971e-02, 5.5923e-03, 5.7907e-03, 4.5891e-03, 2.4910e-03,\n",
      "        2.5425e-03, 6.1264e-03, 4.1008e-03, 4.5815e-03, 1.4801e-03, 3.3092e-03,\n",
      "        7.7171e-03, 5.1842e-03, 5.0583e-03, 1.0170e-02, 8.3008e-03, 3.7975e-03,\n",
      "        8.2703e-03, 7.6981e-03, 7.1335e-03, 6.8169e-03, 7.1182e-03, 5.4512e-03,\n",
      "        5.0468e-03, 2.9888e-03, 3.3245e-03, 4.9553e-03, 7.5417e-03, 7.5455e-03,\n",
      "        5.4321e-03, 7.9193e-03, 4.6234e-03, 7.0381e-03, 5.9776e-03, 2.3117e-03,\n",
      "        7.6714e-03, 8.3771e-03, 1.2344e-02, 7.3814e-03, 7.7820e-03, 8.1329e-03,\n",
      "        5.0507e-02, 3.0350e-02, 1.3878e-02, 9.1171e-03, 8.3160e-03, 1.7120e-02,\n",
      "        4.7340e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [56] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [56] : torch.Size([1, 32, 1, 128])\n",
      "Last layer attentions for generated token [56] : tensor([2.3035e-01, 2.3035e-01, 1.2398e-04, 1.2445e-04, 7.4327e-05, 1.7185e-03,\n",
      "        2.3079e-04, 4.0817e-04, 4.8280e-04, 4.8828e-04, 2.4185e-03, 4.5967e-04,\n",
      "        2.7823e-04, 4.6873e-04, 1.0551e-02, 7.3166e-03, 4.1294e-04, 5.6887e-04,\n",
      "        3.0208e-04, 1.8036e-04, 7.8201e-05, 2.8205e-04, 4.6997e-03, 5.1785e-04,\n",
      "        3.8567e-03, 9.0332e-03, 8.2302e-04, 3.9101e-04, 3.6955e-04, 4.8923e-04,\n",
      "        3.5191e-04, 7.5340e-04, 8.0395e-04, 4.8280e-04, 4.8089e-04, 3.7384e-04,\n",
      "        1.6487e-04, 6.5994e-04, 4.0984e-04, 2.4939e-04, 1.6203e-03, 1.1740e-03,\n",
      "        3.0875e-04, 4.3869e-04, 5.7459e-04, 2.7895e-04, 3.3188e-04, 3.1471e-04,\n",
      "        4.3964e-04, 2.1591e-03, 1.8415e-03, 1.8036e-04, 2.5520e-03, 2.9068e-03,\n",
      "        1.3380e-03, 3.8719e-03, 6.4707e-04, 1.9684e-03, 3.3379e-03, 1.3943e-03,\n",
      "        1.3304e-03, 1.6623e-03, 2.5425e-03, 1.9817e-03, 1.0281e-03, 8.1491e-04,\n",
      "        3.8128e-03, 2.6150e-03, 3.0136e-03, 1.0811e-02, 5.6419e-03, 1.3260e-02,\n",
      "        3.0365e-03, 2.4376e-03, 6.1569e-03, 6.5956e-03, 7.8125e-03, 1.6460e-03,\n",
      "        5.9204e-03, 1.2215e-02, 3.9978e-03, 1.4334e-03, 2.6894e-03, 3.6297e-03,\n",
      "        3.2578e-03, 2.3460e-03, 4.2191e-03, 4.3030e-03, 7.8821e-04, 4.6654e-03,\n",
      "        6.0043e-03, 4.6692e-03, 4.6463e-03, 1.2260e-02, 1.2993e-02, 5.9853e-03,\n",
      "        6.1684e-03, 7.2784e-03, 3.3512e-03, 6.1798e-03, 2.9583e-03, 6.5117e-03,\n",
      "        7.4997e-03, 4.4174e-03, 3.8643e-03, 4.9896e-03, 1.0902e-02, 5.1613e-03,\n",
      "        7.8583e-03, 7.6714e-03, 3.2635e-03, 1.0017e-02, 8.8501e-03, 2.3232e-03,\n",
      "        8.7585e-03, 1.0300e-02, 1.2398e-02, 1.2672e-02, 1.6861e-02, 8.9645e-03,\n",
      "        4.3549e-02, 1.6037e-02, 9.9869e-03, 1.2398e-02, 3.6850e-03, 1.1589e-02,\n",
      "        6.1111e-03, 4.8637e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [57] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [57] : torch.Size([1, 32, 1, 129])\n",
      "Last layer attentions for generated token [57] : tensor([2.6050e-01, 2.6050e-01, 2.8157e-04, 2.5129e-04, 1.3399e-04, 1.9894e-03,\n",
      "        1.6880e-04, 2.8491e-04, 7.0620e-04, 5.5456e-04, 1.8215e-03, 2.6965e-04,\n",
      "        2.3711e-04, 5.1975e-04, 1.4069e-02, 8.1482e-03, 2.8253e-04, 5.8222e-04,\n",
      "        2.8253e-04, 3.3879e-04, 1.9574e-04, 5.8079e-04, 9.7198e-03, 1.2941e-03,\n",
      "        5.6725e-03, 9.1019e-03, 1.0176e-03, 5.1355e-04, 4.6325e-04, 4.0078e-04,\n",
      "        5.8317e-04, 1.1759e-03, 5.7888e-04, 2.3615e-04, 2.1672e-04, 3.2973e-04,\n",
      "        2.6560e-04, 4.7064e-04, 3.4428e-04, 2.5535e-04, 1.8721e-03, 1.2274e-03,\n",
      "        3.6716e-04, 3.9244e-04, 3.7289e-04, 2.6965e-04, 2.4557e-04, 3.6573e-04,\n",
      "        3.3307e-04, 1.2274e-03, 1.3514e-03, 1.5855e-04, 2.6302e-03, 3.8528e-03,\n",
      "        1.3695e-03, 3.4809e-03, 5.6314e-04, 2.7103e-03, 5.4283e-03, 2.2087e-03,\n",
      "        1.7519e-03, 1.8978e-03, 3.5725e-03, 2.6302e-03, 1.1263e-03, 1.2131e-03,\n",
      "        3.3836e-03, 1.3351e-03, 3.5629e-03, 1.0307e-02, 5.8212e-03, 9.4452e-03,\n",
      "        2.7180e-03, 1.2712e-03, 3.3150e-03, 4.6844e-03, 6.4888e-03, 1.7967e-03,\n",
      "        5.1727e-03, 9.3079e-03, 4.0970e-03, 9.1410e-04, 1.8005e-03, 2.2507e-03,\n",
      "        2.2659e-03, 1.3695e-03, 2.9335e-03, 2.8381e-03, 1.1902e-03, 4.5929e-03,\n",
      "        3.7003e-03, 3.7518e-03, 5.5237e-03, 7.9117e-03, 6.0120e-03, 3.3207e-03,\n",
      "        5.5542e-03, 4.9324e-03, 2.6417e-03, 2.9945e-03, 1.6613e-03, 3.1586e-03,\n",
      "        4.4022e-03, 2.4986e-03, 2.6455e-03, 2.9659e-03, 5.8899e-03, 4.4098e-03,\n",
      "        4.8180e-03, 6.8436e-03, 2.9945e-03, 5.6229e-03, 7.8812e-03, 1.6556e-03,\n",
      "        6.7825e-03, 7.1793e-03, 6.3019e-03, 1.1749e-02, 1.7426e-02, 7.5226e-03,\n",
      "        3.3508e-02, 1.3359e-02, 9.6436e-03, 1.3695e-02, 5.5771e-03, 1.2421e-02,\n",
      "        8.1558e-03, 6.8512e-03, 1.5282e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [58] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [58] : torch.Size([1, 32, 1, 130])\n",
      "Last layer attentions for generated token [58] : tensor([1.6748e-01, 1.6748e-01, 2.1207e-04, 2.6250e-04, 1.1086e-04, 1.7519e-03,\n",
      "        3.1400e-04, 4.0555e-04, 5.3644e-04, 1.0500e-03, 3.0384e-03, 3.2473e-04,\n",
      "        6.1703e-04, 8.5354e-04, 1.2054e-02, 7.8049e-03, 4.0245e-04, 9.7847e-04,\n",
      "        5.7077e-04, 7.0620e-04, 2.6798e-04, 7.2575e-04, 1.0262e-02, 1.6489e-03,\n",
      "        5.5923e-03, 9.3842e-03, 1.2016e-03, 6.4707e-04, 8.6212e-04, 2.8253e-04,\n",
      "        4.3249e-04, 8.4352e-04, 9.1028e-04, 3.4761e-04, 3.3236e-04, 7.0477e-04,\n",
      "        6.4564e-04, 6.8045e-04, 6.1226e-04, 6.6996e-04, 2.1896e-03, 1.9369e-03,\n",
      "        5.3406e-04, 7.0620e-04, 5.7077e-04, 6.8045e-04, 1.4353e-04, 2.9683e-04,\n",
      "        4.1938e-04, 1.7395e-03, 2.0161e-03, 1.7107e-04, 3.1319e-03, 4.3526e-03,\n",
      "        2.0828e-03, 5.3978e-03, 8.0204e-04, 3.7193e-03, 9.6588e-03, 2.9221e-03,\n",
      "        3.5686e-03, 2.2888e-03, 5.8861e-03, 5.9700e-03, 3.6964e-03, 1.8015e-03,\n",
      "        8.1635e-03, 2.0218e-03, 4.3335e-03, 1.6602e-02, 7.5531e-03, 1.3000e-02,\n",
      "        3.6621e-03, 1.8196e-03, 3.2444e-03, 4.7646e-03, 7.9651e-03, 1.9913e-03,\n",
      "        4.7951e-03, 1.6998e-02, 7.4043e-03, 1.9121e-03, 1.8463e-03, 2.0180e-03,\n",
      "        3.3836e-03, 2.5787e-03, 2.9297e-03, 3.8929e-03, 1.8625e-03, 6.7635e-03,\n",
      "        5.5199e-03, 4.2915e-03, 6.2561e-03, 1.5434e-02, 1.0826e-02, 4.8332e-03,\n",
      "        9.6207e-03, 1.0605e-02, 4.7188e-03, 4.5547e-03, 3.1567e-03, 3.5591e-03,\n",
      "        4.7112e-03, 2.6665e-03, 2.3899e-03, 2.4052e-03, 8.0185e-03, 8.3694e-03,\n",
      "        7.3509e-03, 8.7280e-03, 3.6507e-03, 6.8169e-03, 1.0185e-02, 2.5139e-03,\n",
      "        1.1078e-02, 1.5457e-02, 1.1452e-02, 1.2115e-02, 2.7008e-02, 1.2978e-02,\n",
      "        4.9469e-02, 2.2354e-02, 1.1337e-02, 1.7014e-02, 5.0964e-03, 1.2062e-02,\n",
      "        8.1253e-03, 1.0696e-02, 1.2863e-02, 7.4654e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [59] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [59] : torch.Size([1, 32, 1, 131])\n",
      "Last layer attentions for generated token [59] : tensor([2.3938e-01, 2.3938e-01, 2.6441e-04, 4.8137e-04, 1.4889e-04, 1.6193e-03,\n",
      "        9.3162e-05, 2.4354e-04, 4.4703e-04, 3.4547e-04, 1.3371e-03, 2.5773e-04,\n",
      "        4.1676e-04, 7.0858e-04, 1.3176e-02, 9.7809e-03, 4.3058e-04, 5.1546e-04,\n",
      "        3.4666e-04, 2.6369e-04, 1.7202e-04, 6.4135e-04, 1.0025e-02, 1.7958e-03,\n",
      "        5.7564e-03, 1.1635e-02, 9.1171e-04, 3.7575e-04, 6.2561e-04, 3.8457e-04,\n",
      "        4.5037e-04, 1.2436e-03, 3.9530e-04, 2.0587e-04, 1.6999e-04, 3.0422e-04,\n",
      "        1.2684e-04, 3.1018e-04, 1.8454e-04, 2.4939e-04, 3.0537e-03, 1.6127e-03,\n",
      "        4.2248e-04, 4.1270e-04, 4.6945e-04, 3.7575e-04, 2.2697e-04, 1.6475e-04,\n",
      "        1.9264e-04, 1.1044e-03, 9.5940e-04, 1.8597e-04, 2.8553e-03, 5.4131e-03,\n",
      "        1.5879e-03, 3.3741e-03, 6.0844e-04, 2.5253e-03, 9.5367e-03, 1.7710e-03,\n",
      "        1.9093e-03, 2.0847e-03, 4.0741e-03, 2.3861e-03, 1.8597e-03, 1.2884e-03,\n",
      "        3.7498e-03, 2.0542e-03, 4.0665e-03, 2.8152e-02, 1.0452e-02, 1.1322e-02,\n",
      "        8.0566e-03, 1.6575e-03, 3.8719e-03, 4.7493e-03, 4.3526e-03, 2.4376e-03,\n",
      "        5.4817e-03, 1.0239e-02, 4.0932e-03, 1.9855e-03, 2.8305e-03, 1.8206e-03,\n",
      "        1.3847e-03, 1.4744e-03, 2.4090e-03, 2.6588e-03, 8.0919e-04, 2.9659e-03,\n",
      "        5.5656e-03, 4.6616e-03, 7.8506e-03, 6.8359e-03, 6.8893e-03, 4.6959e-03,\n",
      "        6.0349e-03, 6.3438e-03, 5.8861e-03, 3.4752e-03, 2.6360e-03, 3.6106e-03,\n",
      "        3.3913e-03, 1.8597e-03, 1.7300e-03, 2.3270e-03, 5.1918e-03, 5.0125e-03,\n",
      "        3.0994e-03, 6.9122e-03, 1.8206e-03, 3.2139e-03, 4.8904e-03, 1.5545e-03,\n",
      "        4.9744e-03, 6.4697e-03, 5.5237e-03, 3.9062e-03, 9.9792e-03, 1.7761e-02,\n",
      "        3.2837e-02, 1.4236e-02, 1.2978e-02, 9.3613e-03, 4.8523e-03, 1.3893e-02,\n",
      "        3.9787e-03, 4.8523e-03, 1.7151e-02, 5.9547e-03, 6.7024e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [60] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [60] : torch.Size([1, 32, 1, 132])\n",
      "Last layer attentions for generated token [60] : tensor([2.7515e-01, 2.7515e-01, 3.8481e-04, 7.7581e-04, 1.7178e-04, 3.5381e-03,\n",
      "        1.2958e-04, 3.1829e-04, 6.8855e-04, 5.3501e-04, 1.2836e-03, 4.2343e-04,\n",
      "        2.7061e-04, 6.4039e-04, 2.0462e-02, 1.0750e-02, 8.2588e-04, 6.5851e-04,\n",
      "        4.2844e-04, 2.4128e-04, 3.3355e-04, 1.4181e-03, 1.2306e-02, 2.7103e-03,\n",
      "        1.0063e-02, 1.6129e-02, 1.3857e-03, 4.0722e-04, 6.9380e-04, 6.8998e-04,\n",
      "        5.0974e-04, 1.5726e-03, 5.7411e-04, 2.1970e-04, 2.1207e-04, 2.7490e-04,\n",
      "        1.0663e-04, 4.6873e-04, 2.2483e-04, 1.8859e-04, 2.6951e-03, 1.1311e-03,\n",
      "        3.4213e-04, 4.1270e-04, 4.5872e-04, 2.9898e-04, 3.5858e-04, 2.0158e-04,\n",
      "        2.4128e-04, 1.7309e-03, 1.1024e-03, 1.5640e-04, 2.4853e-03, 7.2403e-03,\n",
      "        8.1778e-04, 4.7112e-03, 5.4789e-04, 2.5330e-03, 7.3395e-03, 1.9197e-03,\n",
      "        1.7281e-03, 2.2125e-03, 6.5613e-03, 2.4700e-03, 1.4629e-03, 1.7614e-03,\n",
      "        2.5978e-03, 2.0638e-03, 3.2654e-03, 2.2202e-02, 8.4152e-03, 1.1627e-02,\n",
      "        5.6190e-03, 1.7080e-03, 4.6196e-03, 4.6654e-03, 3.7727e-03, 2.1896e-03,\n",
      "        6.3477e-03, 8.7280e-03, 3.2558e-03, 1.2989e-03, 2.0294e-03, 1.3351e-03,\n",
      "        8.4877e-04, 1.0061e-03, 1.8749e-03, 2.0847e-03, 4.9400e-04, 1.2417e-03,\n",
      "        2.2964e-03, 2.4452e-03, 3.4466e-03, 3.7003e-03, 7.6561e-03, 3.8185e-03,\n",
      "        4.5395e-03, 3.3588e-03, 4.1695e-03, 1.6518e-03, 1.3561e-03, 2.3632e-03,\n",
      "        2.5501e-03, 1.3247e-03, 1.1759e-03, 1.5640e-03, 3.2501e-03, 2.4109e-03,\n",
      "        1.6041e-03, 4.5891e-03, 1.4687e-03, 2.4853e-03, 2.7695e-03, 6.9284e-04,\n",
      "        3.0899e-03, 3.5839e-03, 4.4174e-03, 4.5395e-03, 1.0498e-02, 1.1597e-02,\n",
      "        1.9440e-02, 1.0872e-02, 9.3994e-03, 6.6528e-03, 3.0346e-03, 6.8779e-03,\n",
      "        2.5864e-03, 2.8687e-03, 1.4229e-02, 7.2784e-03, 7.3357e-03, 5.6190e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [61] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [61] : torch.Size([1, 32, 1, 133])\n",
      "Last layer attentions for generated token [61] : tensor([1.7896e-01, 1.7859e-01, 2.6846e-04, 4.7302e-04, 1.6379e-04, 3.3627e-03,\n",
      "        2.3830e-04, 2.9945e-04, 4.8614e-04, 8.3351e-04, 3.1490e-03, 4.7874e-04,\n",
      "        5.3406e-04, 9.3699e-04, 1.8127e-02, 1.5450e-02, 5.2261e-04, 1.0138e-03,\n",
      "        7.3862e-04, 6.5184e-04, 4.0007e-04, 1.0252e-03, 1.0803e-02, 2.0466e-03,\n",
      "        9.5978e-03, 1.7776e-02, 2.2469e-03, 8.9073e-04, 1.0157e-03, 5.6934e-04,\n",
      "        6.6090e-04, 1.6356e-03, 9.9564e-04, 4.0698e-04, 3.1710e-04, 6.4039e-04,\n",
      "        3.1090e-04, 7.5579e-04, 5.4550e-04, 5.2691e-04, 3.7975e-03, 2.6646e-03,\n",
      "        7.2002e-04, 1.0176e-03, 7.4148e-04, 6.6090e-04, 1.9455e-04, 2.7752e-04,\n",
      "        3.4809e-04, 2.7523e-03, 2.0332e-03, 1.8418e-04, 2.9545e-03, 5.5275e-03,\n",
      "        2.0790e-03, 5.9357e-03, 9.3889e-04, 2.1667e-03, 7.0000e-03, 1.8330e-03,\n",
      "        1.9855e-03, 1.6165e-03, 4.6806e-03, 3.9101e-03, 1.3247e-03, 1.2932e-03,\n",
      "        6.8436e-03, 2.3460e-03, 3.3627e-03, 1.3046e-02, 7.6561e-03, 1.6876e-02,\n",
      "        4.5815e-03, 1.7385e-03, 4.5815e-03, 4.5052e-03, 8.2016e-03, 2.3003e-03,\n",
      "        5.6839e-03, 2.0126e-02, 7.6790e-03, 2.4986e-03, 3.1223e-03, 1.4038e-03,\n",
      "        1.9159e-03, 2.0142e-03, 3.5191e-03, 2.8915e-03, 8.1444e-04, 2.2755e-03,\n",
      "        2.9736e-03, 2.9240e-03, 4.5967e-03, 7.1640e-03, 7.2784e-03, 3.7956e-03,\n",
      "        6.4163e-03, 1.0521e-02, 5.4665e-03, 6.2523e-03, 3.7575e-03, 5.4245e-03,\n",
      "        5.2109e-03, 2.7618e-03, 2.5253e-03, 2.4986e-03, 6.4850e-03, 7.4730e-03,\n",
      "        4.4899e-03, 6.5613e-03, 3.5439e-03, 6.6223e-03, 6.2981e-03, 1.8711e-03,\n",
      "        6.9275e-03, 1.0742e-02, 8.6594e-03, 6.5041e-03, 1.1841e-02, 9.3231e-03,\n",
      "        5.0049e-02, 2.6123e-02, 1.3908e-02, 1.4534e-02, 6.3515e-03, 1.3863e-02,\n",
      "        5.8403e-03, 5.8289e-03, 1.2314e-02, 6.4468e-03, 7.0000e-03, 5.4550e-03,\n",
      "        8.3237e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [62] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [62] : torch.Size([1, 32, 1, 134])\n",
      "Last layer attentions for generated token [62] : tensor([1.1737e-01, 1.1737e-01, 1.7715e-04, 3.2568e-04, 1.9956e-04, 2.1248e-03,\n",
      "        1.1390e-04, 2.2566e-04, 5.3215e-04, 3.8981e-04, 1.4591e-03, 5.0449e-04,\n",
      "        3.6144e-04, 8.6021e-04, 1.6098e-02, 1.1513e-02, 2.5415e-04, 5.5838e-04,\n",
      "        2.3007e-04, 5.1546e-04, 1.7571e-04, 3.6693e-04, 5.0735e-03, 1.6222e-03,\n",
      "        5.8289e-03, 1.5701e-02, 9.8419e-04, 2.3329e-04, 2.9373e-04, 3.6621e-04,\n",
      "        4.3583e-04, 1.0595e-03, 7.5769e-04, 3.8457e-04, 3.4547e-04, 3.1447e-04,\n",
      "        2.6965e-04, 5.0163e-04, 1.4460e-04, 1.6320e-04, 2.7008e-03, 1.0376e-03,\n",
      "        7.4148e-04, 5.5552e-04, 6.6710e-04, 2.8634e-04, 3.1018e-04, 1.5819e-04,\n",
      "        3.0494e-04, 1.8167e-03, 9.7275e-04, 3.2973e-04, 2.8648e-03, 5.1117e-03,\n",
      "        2.0142e-03, 4.8141e-03, 4.6301e-04, 9.6321e-04, 3.1776e-03, 1.2083e-03,\n",
      "        1.1196e-03, 1.5574e-03, 2.6913e-03, 1.4238e-03, 1.0939e-03, 8.2874e-04,\n",
      "        2.1114e-03, 2.2488e-03, 2.0828e-03, 1.7441e-02, 1.1154e-02, 1.6541e-02,\n",
      "        5.9471e-03, 2.5005e-03, 6.6299e-03, 1.1421e-02, 9.1400e-03, 5.5962e-03,\n",
      "        8.9798e-03, 1.4160e-02, 8.8959e-03, 4.3297e-03, 5.6152e-03, 5.0201e-03,\n",
      "        2.3651e-03, 4.7798e-03, 6.4735e-03, 3.8891e-03, 1.6785e-03, 4.3678e-03,\n",
      "        6.5231e-03, 8.4991e-03, 1.4511e-02, 1.0300e-02, 1.1505e-02, 1.0757e-02,\n",
      "        1.2054e-02, 9.2087e-03, 9.1400e-03, 9.0103e-03, 9.2926e-03, 9.7351e-03,\n",
      "        1.2260e-02, 4.1771e-03, 4.7302e-03, 7.0152e-03, 2.6154e-02, 1.2741e-02,\n",
      "        5.5618e-03, 9.8190e-03, 4.7569e-03, 1.1726e-02, 5.6992e-03, 2.3556e-03,\n",
      "        9.5444e-03, 8.3313e-03, 1.2512e-02, 1.1955e-02, 9.5673e-03, 1.4107e-02,\n",
      "        5.3253e-02, 2.0538e-02, 1.4420e-02, 9.0332e-03, 6.5727e-03, 1.9043e-02,\n",
      "        7.5493e-03, 5.1498e-03, 8.9645e-03, 7.1640e-03, 4.4785e-03, 3.8891e-03,\n",
      "        9.7961e-03, 2.7817e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [63] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [63] : torch.Size([1, 32, 1, 135])\n",
      "Last layer attentions for generated token [63] : tensor([1.7737e-01, 1.7737e-01, 2.4748e-04, 3.8934e-04, 1.6809e-04, 4.2953e-03,\n",
      "        2.9516e-04, 6.0415e-04, 1.1950e-03, 6.0654e-04, 1.7557e-03, 5.9271e-04,\n",
      "        3.0518e-04, 9.0551e-04, 2.2400e-02, 1.2863e-02, 6.7139e-04, 6.1131e-04,\n",
      "        2.2364e-04, 4.0960e-04, 3.6168e-04, 9.3985e-04, 1.1948e-02, 2.2011e-03,\n",
      "        1.0643e-02, 2.7435e-02, 1.0300e-03, 3.8934e-04, 5.7554e-04, 7.1621e-04,\n",
      "        6.7949e-04, 1.3857e-03, 7.2622e-04, 4.3178e-04, 3.3498e-04, 4.6873e-04,\n",
      "        3.4642e-04, 7.5483e-04, 2.3615e-04, 2.1672e-04, 2.4815e-03, 1.0204e-03,\n",
      "        3.5191e-04, 4.6802e-04, 6.5708e-04, 3.4499e-04, 6.5851e-04, 2.8610e-04,\n",
      "        4.2939e-04, 1.7214e-03, 1.1225e-03, 2.0444e-04, 3.3569e-03, 4.3144e-03,\n",
      "        1.3647e-03, 4.5891e-03, 4.8828e-04, 1.8892e-03, 5.1842e-03, 1.5526e-03,\n",
      "        1.4191e-03, 2.2678e-03, 4.1580e-03, 1.9131e-03, 1.4105e-03, 1.2846e-03,\n",
      "        1.7900e-03, 2.0466e-03, 3.4142e-03, 1.3321e-02, 8.1711e-03, 1.3077e-02,\n",
      "        2.8515e-03, 2.1515e-03, 7.4196e-03, 7.0305e-03, 6.1493e-03, 3.3150e-03,\n",
      "        9.8038e-03, 8.4610e-03, 5.3673e-03, 1.7414e-03, 3.4599e-03, 4.0054e-03,\n",
      "        1.2045e-03, 1.8797e-03, 3.2692e-03, 2.8839e-03, 1.0900e-03, 2.7599e-03,\n",
      "        3.8834e-03, 5.9052e-03, 6.7177e-03, 4.5166e-03, 1.2238e-02, 6.7711e-03,\n",
      "        6.6986e-03, 4.3602e-03, 5.7564e-03, 2.7695e-03, 2.8267e-03, 4.2267e-03,\n",
      "        8.1863e-03, 2.5139e-03, 3.3970e-03, 4.4403e-03, 1.4175e-02, 6.0310e-03,\n",
      "        3.6201e-03, 6.9084e-03, 3.1948e-03, 8.8120e-03, 4.6768e-03, 1.6298e-03,\n",
      "        7.8278e-03, 4.9706e-03, 7.5569e-03, 1.3725e-02, 1.1299e-02, 1.4938e-02,\n",
      "        3.0914e-02, 1.9150e-02, 1.5656e-02, 7.0267e-03, 4.8409e-03, 1.2222e-02,\n",
      "        5.2223e-03, 3.2101e-03, 1.4252e-02, 8.1253e-03, 5.6839e-03, 4.9820e-03,\n",
      "        1.2978e-02, 2.0065e-02, 1.4458e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [64] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [64] : torch.Size([1, 32, 1, 136])\n",
      "Last layer attentions for generated token [64] : tensor([2.4084e-01, 2.4084e-01, 3.5644e-04, 5.3215e-04, 2.0504e-04, 3.5553e-03,\n",
      "        2.0194e-04, 4.4799e-04, 1.0500e-03, 4.9210e-04, 1.4801e-03, 4.5061e-04,\n",
      "        2.9373e-04, 9.9754e-04, 1.5472e-02, 8.3237e-03, 2.8801e-04, 4.2415e-04,\n",
      "        1.9038e-04, 3.1281e-04, 3.4356e-04, 5.7316e-04, 1.6495e-02, 2.5520e-03,\n",
      "        1.0071e-02, 1.7029e-02, 7.3290e-04, 6.2704e-04, 7.7391e-04, 6.0177e-04,\n",
      "        1.2083e-03, 2.0924e-03, 6.9237e-04, 2.9039e-04, 2.3699e-04, 4.1914e-04,\n",
      "        3.7146e-04, 4.1437e-04, 2.5821e-04, 1.7548e-04, 2.8992e-03, 8.5163e-04,\n",
      "        2.2435e-04, 3.0017e-04, 2.6035e-04, 2.5320e-04, 3.0851e-04, 4.0627e-04,\n",
      "        3.4022e-04, 1.6708e-03, 9.7275e-04, 1.5187e-04, 3.3894e-03, 3.1700e-03,\n",
      "        1.3714e-03, 3.1490e-03, 2.7537e-04, 1.5068e-03, 5.5580e-03, 1.4124e-03,\n",
      "        1.3933e-03, 1.7271e-03, 4.2610e-03, 1.7138e-03, 1.2131e-03, 1.3800e-03,\n",
      "        1.6479e-03, 1.3018e-03, 2.9793e-03, 8.6670e-03, 5.8517e-03, 9.1324e-03,\n",
      "        2.7466e-03, 1.3933e-03, 4.4975e-03, 6.6872e-03, 6.1378e-03, 1.9379e-03,\n",
      "        6.0501e-03, 5.5313e-03, 3.1376e-03, 8.5163e-04, 1.8568e-03, 1.8206e-03,\n",
      "        1.1415e-03, 1.2274e-03, 2.7657e-03, 1.9121e-03, 9.6512e-04, 2.4147e-03,\n",
      "        2.2602e-03, 3.0861e-03, 6.1073e-03, 3.4389e-03, 7.4883e-03, 3.4122e-03,\n",
      "        4.7188e-03, 2.9507e-03, 2.7847e-03, 1.5945e-03, 1.2102e-03, 2.0485e-03,\n",
      "        3.4962e-03, 1.4019e-03, 2.2297e-03, 2.1324e-03, 6.0158e-03, 3.9368e-03,\n",
      "        2.3079e-03, 4.8447e-03, 2.6417e-03, 5.5733e-03, 4.5967e-03, 1.4400e-03,\n",
      "        8.2245e-03, 3.9215e-03, 5.0392e-03, 1.0071e-02, 7.3357e-03, 1.1452e-02,\n",
      "        2.2049e-02, 1.3039e-02, 8.2169e-03, 5.9738e-03, 4.1389e-03, 9.9945e-03,\n",
      "        3.6106e-03, 2.0294e-03, 7.7133e-03, 8.9111e-03, 7.5493e-03, 6.4507e-03,\n",
      "        1.4503e-02, 2.2781e-02, 1.6846e-02, 2.2903e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [65] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [65] : torch.Size([1, 32, 1, 137])\n",
      "Last layer attentions for generated token [65] : tensor([1.7554e-01, 1.7554e-01, 2.4509e-04, 3.7217e-04, 1.7166e-04, 2.3956e-03,\n",
      "        1.8859e-04, 2.9969e-04, 5.6648e-04, 6.0272e-04, 2.9469e-03, 2.6917e-04,\n",
      "        4.5156e-04, 8.2254e-04, 1.8845e-02, 1.3641e-02, 3.3760e-04, 6.1464e-04,\n",
      "        3.7432e-04, 6.2561e-04, 1.7715e-04, 7.7724e-04, 9.2392e-03, 2.5101e-03,\n",
      "        5.1270e-03, 1.8066e-02, 1.2293e-03, 6.2704e-04, 7.1049e-04, 4.9973e-04,\n",
      "        8.2254e-04, 1.4744e-03, 9.9564e-04, 6.0987e-04, 2.7227e-04, 4.9400e-04,\n",
      "        2.2924e-04, 3.4213e-04, 2.5129e-04, 2.8753e-04, 1.9836e-03, 1.4019e-03,\n",
      "        4.3082e-04, 5.4359e-04, 5.8889e-04, 4.9591e-04, 3.1590e-04, 2.8467e-04,\n",
      "        5.3310e-04, 2.1019e-03, 1.3065e-03, 1.5092e-04, 3.1071e-03, 3.1166e-03,\n",
      "        1.8625e-03, 5.3902e-03, 1.0214e-03, 2.5063e-03, 6.9580e-03, 1.8921e-03,\n",
      "        2.0618e-03, 1.8883e-03, 3.3035e-03, 2.2526e-03, 9.8228e-04, 9.1743e-04,\n",
      "        3.7479e-03, 1.8921e-03, 2.7065e-03, 1.1665e-02, 6.8932e-03, 1.3832e-02,\n",
      "        3.9215e-03, 2.1515e-03, 4.3716e-03, 5.8289e-03, 6.4735e-03, 2.6512e-03,\n",
      "        7.4654e-03, 1.0597e-02, 4.0474e-03, 1.9464e-03, 3.6354e-03, 2.3003e-03,\n",
      "        1.9646e-03, 2.4834e-03, 4.7913e-03, 4.5929e-03, 1.2789e-03, 4.8790e-03,\n",
      "        5.9929e-03, 4.7150e-03, 5.8212e-03, 6.3515e-03, 6.5117e-03, 3.7518e-03,\n",
      "        6.3248e-03, 5.3749e-03, 5.2681e-03, 5.1384e-03, 4.0627e-03, 5.7602e-03,\n",
      "        6.4011e-03, 3.5591e-03, 3.7880e-03, 3.9864e-03, 6.1035e-03, 6.2141e-03,\n",
      "        4.9858e-03, 9.7275e-03, 4.4937e-03, 7.8583e-03, 5.4665e-03, 1.7509e-03,\n",
      "        7.4234e-03, 1.0048e-02, 1.2642e-02, 1.0414e-02, 1.4114e-02, 1.0658e-02,\n",
      "        3.3264e-02, 1.5602e-02, 1.0162e-02, 1.0521e-02, 4.5700e-03, 1.3290e-02,\n",
      "        6.3286e-03, 6.5460e-03, 1.3832e-02, 6.4163e-03, 5.3825e-03, 5.2299e-03,\n",
      "        9.7427e-03, 2.3956e-02, 1.4168e-02, 8.4457e-03, 1.4771e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [66] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [66] : torch.Size([1, 32, 1, 138])\n",
      "Last layer attentions for generated token [66] : tensor([1.7859e-01, 1.7859e-01, 2.1958e-04, 4.0007e-04, 1.7107e-04, 2.0294e-03,\n",
      "        2.5725e-04, 3.8552e-04, 7.8630e-04, 7.1573e-04, 2.1992e-03, 3.7789e-04,\n",
      "        5.3120e-04, 9.9945e-04, 9.6436e-03, 1.0002e-02, 2.8706e-04, 6.2180e-04,\n",
      "        2.4211e-04, 6.2943e-04, 9.6679e-05, 5.5552e-04, 1.0376e-02, 2.7580e-03,\n",
      "        4.2725e-03, 1.3008e-02, 1.1482e-03, 3.2067e-04, 3.7646e-04, 4.0555e-04,\n",
      "        6.0892e-04, 1.3371e-03, 1.2245e-03, 5.3120e-04, 2.6488e-04, 3.0303e-04,\n",
      "        2.5630e-04, 4.7326e-04, 2.2483e-04, 1.9383e-04, 2.2755e-03, 1.3504e-03,\n",
      "        3.6645e-04, 5.1260e-04, 5.8651e-04, 3.7003e-04, 3.0732e-04, 2.0635e-04,\n",
      "        3.7718e-04, 2.1210e-03, 1.2245e-03, 2.1875e-04, 3.8357e-03, 2.9469e-03,\n",
      "        2.1935e-03, 4.5128e-03, 7.9393e-04, 1.1845e-03, 4.5433e-03, 1.3828e-03,\n",
      "        1.4153e-03, 2.2984e-03, 3.4180e-03, 1.9236e-03, 1.0138e-03, 1.1530e-03,\n",
      "        3.4924e-03, 2.1877e-03, 2.2831e-03, 1.3054e-02, 7.8812e-03, 1.2321e-02,\n",
      "        2.6264e-03, 1.6003e-03, 3.7327e-03, 9.4452e-03, 7.5111e-03, 2.9392e-03,\n",
      "        9.0866e-03, 1.2505e-02, 4.4098e-03, 2.2888e-03, 4.1466e-03, 2.6970e-03,\n",
      "        1.9083e-03, 2.9335e-03, 6.4545e-03, 4.8103e-03, 1.6747e-03, 5.2452e-03,\n",
      "        5.4855e-03, 5.3940e-03, 9.3307e-03, 6.2675e-03, 8.0032e-03, 8.2245e-03,\n",
      "        9.8801e-03, 5.9319e-03, 6.1417e-03, 5.6572e-03, 3.7613e-03, 4.7340e-03,\n",
      "        8.5220e-03, 2.8343e-03, 3.8357e-03, 4.1962e-03, 1.2856e-02, 9.6817e-03,\n",
      "        4.9629e-03, 9.6359e-03, 3.8033e-03, 1.1711e-02, 3.7556e-03, 2.3041e-03,\n",
      "        7.4081e-03, 6.3629e-03, 1.0414e-02, 1.0002e-02, 7.8583e-03, 1.4076e-02,\n",
      "        3.6743e-02, 1.4755e-02, 9.3460e-03, 7.0000e-03, 4.9171e-03, 1.4458e-02,\n",
      "        6.4240e-03, 4.1122e-03, 6.8665e-03, 5.3139e-03, 3.7880e-03, 3.0384e-03,\n",
      "        8.4763e-03, 2.3163e-02, 8.3923e-03, 4.3678e-03, 1.0963e-02, 1.8570e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [67] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [67] : torch.Size([1, 32, 1, 139])\n",
      "Last layer attentions for generated token [67] : tensor([2.1973e-01, 2.1973e-01, 1.9455e-04, 5.4359e-04, 1.8418e-04, 1.2808e-03,\n",
      "        1.6832e-04, 2.6894e-04, 6.8855e-04, 5.0879e-04, 1.2760e-03, 3.8695e-04,\n",
      "        4.8327e-04, 1.0662e-03, 1.3351e-02, 6.8626e-03, 1.6642e-04, 4.7135e-04,\n",
      "        1.3745e-04, 2.7275e-04, 1.7715e-04, 3.4881e-04, 1.2680e-02, 2.8667e-03,\n",
      "        7.8888e-03, 9.1934e-03, 8.9788e-04, 3.2330e-04, 5.8317e-04, 5.0354e-04,\n",
      "        9.4461e-04, 1.3905e-03, 6.6996e-04, 3.0422e-04, 2.4891e-04, 4.0078e-04,\n",
      "        3.1567e-04, 3.0422e-04, 2.0468e-04, 2.3556e-04, 2.1935e-03, 9.7466e-04,\n",
      "        4.3678e-04, 2.6703e-04, 2.2829e-04, 3.7646e-04, 1.4853e-04, 2.4116e-04,\n",
      "        3.1757e-04, 1.9207e-03, 1.0729e-03, 1.4007e-04, 2.7905e-03, 3.8395e-03,\n",
      "        1.7309e-03, 3.1681e-03, 4.9210e-04, 1.5850e-03, 6.1951e-03, 1.0395e-03,\n",
      "        1.9436e-03, 1.3742e-03, 4.8714e-03, 1.9989e-03, 1.5602e-03, 1.3666e-03,\n",
      "        2.4147e-03, 1.4400e-03, 2.3289e-03, 1.4206e-02, 7.9041e-03, 8.6212e-03,\n",
      "        4.0627e-03, 1.7681e-03, 5.4817e-03, 6.8703e-03, 5.1537e-03, 2.8191e-03,\n",
      "        6.0387e-03, 6.6338e-03, 3.0727e-03, 2.4986e-03, 2.0256e-03, 1.5545e-03,\n",
      "        1.1368e-03, 2.4605e-03, 2.6646e-03, 2.2678e-03, 1.2465e-03, 2.7676e-03,\n",
      "        3.2272e-03, 4.6768e-03, 6.9275e-03, 4.1313e-03, 6.8893e-03, 4.3716e-03,\n",
      "        7.7248e-03, 3.8509e-03, 4.7874e-03, 2.0847e-03, 2.9736e-03, 2.1210e-03,\n",
      "        3.7212e-03, 1.0996e-03, 1.7271e-03, 2.9030e-03, 8.3466e-03, 5.8899e-03,\n",
      "        2.2812e-03, 9.8190e-03, 4.5090e-03, 6.1760e-03, 3.8967e-03, 2.1458e-03,\n",
      "        1.0979e-02, 4.4899e-03, 6.4659e-03, 8.2474e-03, 7.0000e-03, 1.8356e-02,\n",
      "        1.7319e-02, 1.1299e-02, 9.1324e-03, 4.7150e-03, 4.2992e-03, 8.7891e-03,\n",
      "        3.3360e-03, 2.7180e-03, 8.3160e-03, 7.6218e-03, 4.6921e-03, 6.4468e-03,\n",
      "        1.3504e-02, 1.3672e-02, 1.4244e-02, 1.1986e-02, 1.8646e-02, 1.8616e-02,\n",
      "        1.0963e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [68] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [68] : torch.Size([1, 32, 1, 140])\n",
      "Last layer attentions for generated token [68] : tensor([3.0493e-01, 3.0493e-01, 3.3474e-04, 5.6934e-04, 1.9908e-04, 2.3117e-03,\n",
      "        1.6379e-04, 4.7398e-04, 1.6222e-03, 8.4305e-04, 1.3847e-03, 1.1730e-03,\n",
      "        4.4250e-04, 1.3475e-03, 1.4305e-02, 5.4588e-03, 2.9302e-04, 3.2139e-04,\n",
      "        1.2851e-04, 2.3007e-04, 4.9257e-04, 1.0719e-03, 1.7593e-02, 3.1757e-03,\n",
      "        1.1024e-02, 1.1284e-02, 7.9346e-04, 3.9053e-04, 7.3242e-04, 7.9966e-04,\n",
      "        9.8801e-04, 1.6670e-03, 7.4530e-04, 2.8396e-04, 2.5773e-04, 3.2759e-04,\n",
      "        2.9898e-04, 4.3559e-04, 2.3007e-04, 1.8132e-04, 2.7637e-03, 7.5579e-04,\n",
      "        2.4104e-04, 2.0218e-04, 1.9145e-04, 1.6117e-04, 2.4486e-04, 3.5071e-04,\n",
      "        2.4199e-04, 1.7710e-03, 9.5177e-04, 1.6510e-04, 3.6335e-03, 3.4142e-03,\n",
      "        1.4172e-03, 3.0518e-03, 2.4867e-04, 1.1988e-03, 4.7379e-03, 1.0147e-03,\n",
      "        1.1663e-03, 1.5812e-03, 5.3978e-03, 1.3447e-03, 1.3742e-03, 1.4515e-03,\n",
      "        9.6464e-04, 1.1549e-03, 2.4128e-03, 7.0724e-03, 4.7684e-03, 4.7340e-03,\n",
      "        1.4744e-03, 9.0981e-04, 4.0474e-03, 4.6883e-03, 3.5229e-03, 1.8778e-03,\n",
      "        6.9237e-03, 2.8610e-03, 1.7061e-03, 5.4455e-04, 8.8358e-04, 1.2312e-03,\n",
      "        4.7946e-04, 6.0987e-04, 1.0824e-03, 7.5579e-04, 3.7193e-04, 8.2207e-04,\n",
      "        1.2245e-03, 1.5211e-03, 3.2253e-03, 1.3447e-03, 6.3667e-03, 2.7695e-03,\n",
      "        3.8490e-03, 1.2560e-03, 2.1648e-03, 6.1464e-04, 5.0259e-04, 7.4863e-04,\n",
      "        2.0065e-03, 5.0640e-04, 8.3828e-04, 1.4143e-03, 4.6463e-03, 2.0618e-03,\n",
      "        8.7166e-04, 4.7455e-03, 2.0180e-03, 3.6831e-03, 1.5059e-03, 7.7534e-04,\n",
      "        5.6725e-03, 1.9951e-03, 2.9316e-03, 5.4283e-03, 3.4504e-03, 1.0666e-02,\n",
      "        6.0730e-03, 6.0158e-03, 3.8605e-03, 1.8520e-03, 2.4643e-03, 3.6030e-03,\n",
      "        1.5030e-03, 7.8917e-04, 4.0779e-03, 3.8910e-03, 2.6855e-03, 2.8896e-03,\n",
      "        9.7656e-03, 7.2746e-03, 7.4730e-03, 1.5205e-02, 1.7746e-02, 1.2772e-02,\n",
      "        1.0765e-02, 9.3307e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [69] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [69] : torch.Size([1, 32, 1, 141])\n",
      "Last layer attentions for generated token [69] : tensor([3.3740e-01, 3.3740e-01, 3.0112e-04, 2.8849e-04, 1.8549e-04, 1.4591e-03,\n",
      "        1.3459e-04, 3.1924e-04, 7.3624e-04, 6.6423e-04, 1.1034e-03, 6.8665e-04,\n",
      "        2.8849e-04, 7.9632e-04, 1.5930e-02, 4.6959e-03, 1.5199e-04, 3.7551e-04,\n",
      "        1.1700e-04, 1.5807e-04, 1.7357e-04, 9.4557e-04, 1.6541e-02, 3.1891e-03,\n",
      "        8.7051e-03, 6.9885e-03, 8.2302e-04, 5.6028e-04, 5.7030e-04, 7.1239e-04,\n",
      "        2.0103e-03, 2.3441e-03, 4.2295e-04, 1.7428e-04, 1.9288e-04, 3.7766e-04,\n",
      "        2.9063e-04, 4.7922e-04, 2.7943e-04, 1.2898e-04, 1.8082e-03, 8.1825e-04,\n",
      "        2.0933e-04, 1.6046e-04, 1.2302e-04, 1.0365e-04, 1.5926e-04, 5.0211e-04,\n",
      "        1.2118e-04, 1.1168e-03, 1.0128e-03, 1.6236e-04, 2.5654e-03, 2.5406e-03,\n",
      "        1.2722e-03, 2.9335e-03, 2.8944e-04, 1.1053e-03, 4.3030e-03, 8.1348e-04,\n",
      "        9.6607e-04, 7.3910e-04, 2.9011e-03, 1.3599e-03, 8.9025e-04, 1.2407e-03,\n",
      "        9.3269e-04, 6.0940e-04, 2.3746e-03, 4.7379e-03, 4.0169e-03, 3.4523e-03,\n",
      "        5.5790e-04, 3.4189e-04, 2.2945e-03, 3.0708e-03, 3.3855e-03, 8.0109e-04,\n",
      "        4.0169e-03, 3.3760e-03, 2.2945e-03, 3.6526e-04, 5.0926e-04, 1.6499e-03,\n",
      "        7.2241e-04, 4.2033e-04, 8.1825e-04, 3.5548e-04, 2.5153e-04, 9.6035e-04,\n",
      "        5.1498e-04, 1.1587e-03, 1.7023e-03, 1.4563e-03, 1.9779e-03, 1.1292e-03,\n",
      "        3.6221e-03, 1.3599e-03, 1.5869e-03, 6.4850e-04, 2.4772e-04, 5.8699e-04,\n",
      "        1.8663e-03, 4.1080e-04, 6.0558e-04, 3.7456e-04, 3.1223e-03, 1.0548e-03,\n",
      "        6.4230e-04, 1.5259e-03, 1.1187e-03, 2.7828e-03, 1.3733e-03, 2.9516e-04,\n",
      "        4.3106e-03, 1.2331e-03, 1.1015e-03, 2.6989e-03, 2.1152e-03, 6.9885e-03,\n",
      "        7.0534e-03, 3.4046e-03, 2.1477e-03, 2.1229e-03, 1.2045e-03, 1.7357e-03,\n",
      "        1.1339e-03, 4.7374e-04, 2.6207e-03, 3.0708e-03, 2.1610e-03, 1.7424e-03,\n",
      "        9.1324e-03, 1.2146e-02, 5.9357e-03, 1.0025e-02, 1.8784e-02, 1.2741e-02,\n",
      "        8.7967e-03, 6.4621e-03, 1.6052e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [70] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [70] : torch.Size([1, 32, 1, 142])\n",
      "Last layer attentions for generated token [70] : tensor([2.6392e-01, 2.6343e-01, 9.9754e-04, 9.4986e-04, 7.1144e-04, 1.6603e-03,\n",
      "        2.0576e-04, 3.6049e-04, 1.1168e-03, 9.9754e-04, 2.3594e-03, 1.1568e-03,\n",
      "        9.4414e-04, 1.7605e-03, 7.4196e-03, 2.7237e-03, 1.2434e-04, 6.2037e-04,\n",
      "        2.1994e-04, 3.0303e-04, 3.2115e-04, 2.3460e-03, 2.1286e-02, 4.9019e-03,\n",
      "        9.7504e-03, 5.2299e-03, 5.0640e-04, 4.2892e-04, 5.3883e-04, 5.4216e-04,\n",
      "        1.1911e-03, 1.6155e-03, 3.0303e-04, 1.3232e-04, 1.4198e-04, 5.2547e-04,\n",
      "        9.6083e-04, 3.4881e-04, 4.4084e-04, 1.6606e-04, 2.1229e-03, 1.6928e-03,\n",
      "        2.0826e-04, 1.6665e-04, 7.5102e-05, 1.0431e-04, 1.4138e-04, 8.4782e-04,\n",
      "        1.9109e-04, 7.3385e-04, 1.3103e-03, 2.9707e-04, 4.6387e-03, 6.5079e-03,\n",
      "        2.6798e-03, 3.0994e-03, 2.0254e-04, 2.1229e-03, 6.3820e-03, 1.8377e-03,\n",
      "        1.3447e-03, 2.0065e-03, 3.4981e-03, 1.8778e-03, 1.0633e-03, 1.5631e-03,\n",
      "        1.6832e-03, 1.4677e-03, 5.3329e-03, 5.5847e-03, 4.2686e-03, 3.0384e-03,\n",
      "        1.1063e-03, 6.9332e-04, 3.6163e-03, 3.9864e-03, 4.2191e-03, 7.4816e-04,\n",
      "        3.2635e-03, 3.8414e-03, 2.6970e-03, 4.8137e-04, 8.0919e-04, 1.4791e-03,\n",
      "        1.2217e-03, 6.7472e-04, 1.6441e-03, 6.8140e-04, 5.0163e-04, 1.1082e-03,\n",
      "        9.3842e-04, 1.0653e-03, 2.9507e-03, 1.6994e-03, 1.9226e-03, 1.1082e-03,\n",
      "        2.7275e-03, 1.6603e-03, 1.3685e-03, 6.9332e-04, 2.4247e-04, 7.9203e-04,\n",
      "        1.5268e-03, 6.1941e-04, 1.0633e-03, 5.7602e-04, 2.7542e-03, 1.4038e-03,\n",
      "        1.1015e-03, 1.7710e-03, 1.4038e-03, 2.6627e-03, 3.3703e-03, 7.9966e-04,\n",
      "        4.5090e-03, 2.0065e-03, 1.8024e-03, 3.8261e-03, 3.8528e-03, 8.0490e-03,\n",
      "        4.8027e-03, 2.1400e-03, 2.0695e-03, 2.4834e-03, 1.3733e-03, 1.9226e-03,\n",
      "        1.5240e-03, 7.4816e-04, 3.1624e-03, 5.8746e-03, 5.7564e-03, 2.9488e-03,\n",
      "        9.4833e-03, 1.2901e-02, 6.5460e-03, 1.9226e-02, 2.5436e-02, 1.6907e-02,\n",
      "        1.1459e-02, 1.1902e-02, 2.7176e-02, 5.9570e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [71] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [71] : torch.Size([1, 32, 1, 143])\n",
      "Last layer attentions for generated token [71] : tensor([1.9592e-01, 1.9592e-01, 4.6802e-04, 4.9114e-04, 3.6430e-04, 2.2526e-03,\n",
      "        1.8716e-04, 3.0923e-04, 6.2370e-04, 7.5054e-04, 2.4319e-03, 5.0306e-04,\n",
      "        6.1989e-04, 1.2693e-03, 2.1835e-02, 1.1864e-02, 2.1470e-04, 8.2588e-04,\n",
      "        2.3758e-04, 3.5954e-04, 1.8942e-04, 1.1806e-03, 1.3680e-02, 4.2038e-03,\n",
      "        5.7297e-03, 1.5358e-02, 1.2112e-03, 4.8828e-04, 1.0281e-03, 6.8617e-04,\n",
      "        1.1806e-03, 1.9283e-03, 9.9277e-04, 4.5705e-04, 4.1533e-04, 1.0405e-03,\n",
      "        6.4564e-04, 7.4053e-04, 4.7708e-04, 3.2592e-04, 2.0599e-03, 9.6750e-04,\n",
      "        2.4414e-04, 3.7527e-04, 2.9969e-04, 4.3607e-04, 2.1136e-04, 4.0102e-04,\n",
      "        4.6420e-04, 1.5583e-03, 1.2016e-03, 2.4700e-04, 2.6360e-03, 5.5962e-03,\n",
      "        1.6489e-03, 6.0539e-03, 3.9411e-04, 2.0809e-03, 4.2000e-03, 8.7595e-04,\n",
      "        9.8705e-04, 6.2132e-04, 2.8858e-03, 1.4868e-03, 8.3733e-04, 1.0567e-03,\n",
      "        2.0370e-03, 1.3723e-03, 1.4753e-03, 6.0654e-03, 6.2180e-03, 8.4686e-03,\n",
      "        1.5888e-03, 9.0170e-04, 5.2567e-03, 4.9515e-03, 5.4703e-03, 1.1806e-03,\n",
      "        5.6953e-03, 1.2093e-02, 4.9820e-03, 1.0118e-03, 1.5278e-03, 2.2678e-03,\n",
      "        1.2426e-03, 1.3227e-03, 2.5902e-03, 1.9245e-03, 1.2426e-03, 2.1992e-03,\n",
      "        2.2545e-03, 3.9330e-03, 6.0692e-03, 3.6545e-03, 4.9438e-03, 3.9673e-03,\n",
      "        8.6136e-03, 4.5166e-03, 4.7684e-03, 2.0809e-03, 1.0691e-03, 1.6489e-03,\n",
      "        6.8741e-03, 1.0748e-03, 2.5291e-03, 2.2221e-03, 9.5978e-03, 6.1226e-03,\n",
      "        2.1725e-03, 5.2567e-03, 3.0479e-03, 6.2866e-03, 4.3182e-03, 9.4509e-04,\n",
      "        7.7553e-03, 5.0240e-03, 1.0315e-02, 7.1602e-03, 6.1646e-03, 8.4381e-03,\n",
      "        2.5116e-02, 1.1055e-02, 1.2627e-02, 5.8250e-03, 4.1542e-03, 5.6725e-03,\n",
      "        4.6844e-03, 3.2253e-03, 7.4196e-03, 6.8245e-03, 5.6877e-03, 3.4885e-03,\n",
      "        7.6408e-03, 2.4673e-02, 1.3535e-02, 1.0925e-02, 1.2871e-02, 1.6525e-02,\n",
      "        9.2239e-03, 6.7139e-03, 1.5167e-02, 8.1787e-03, 2.3651e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [72] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [72] : torch.Size([1, 32, 1, 144])\n",
      "Last layer attentions for generated token [72] : tensor([2.0007e-01, 2.0007e-01, 1.4412e-04, 2.1791e-04, 1.0216e-04, 9.4700e-04,\n",
      "        1.6522e-04, 3.6144e-04, 5.8126e-04, 6.6233e-04, 1.6356e-03, 2.7084e-04,\n",
      "        3.4904e-04, 4.8542e-04, 1.2558e-02, 7.2975e-03, 2.5940e-04, 6.7282e-04,\n",
      "        2.2137e-04, 3.2210e-04, 6.3658e-05, 3.2902e-04, 7.7896e-03, 1.3533e-03,\n",
      "        3.7270e-03, 8.2016e-03, 8.6355e-04, 3.9458e-04, 5.3835e-04, 3.6573e-04,\n",
      "        6.2704e-04, 6.4564e-04, 6.6233e-04, 5.0068e-04, 2.7442e-04, 4.9782e-04,\n",
      "        1.1438e-04, 2.3746e-04, 2.4176e-04, 2.3615e-04, 1.0834e-03, 6.9523e-04,\n",
      "        2.3711e-04, 3.9172e-04, 4.1199e-04, 3.5381e-04, 1.9383e-04, 1.7512e-04,\n",
      "        3.8171e-04, 7.8011e-04, 6.6757e-04, 1.3697e-04, 2.4719e-03, 1.8215e-03,\n",
      "        1.0767e-03, 3.5629e-03, 7.3719e-04, 2.2659e-03, 4.2915e-03, 1.5545e-03,\n",
      "        1.5697e-03, 1.5430e-03, 2.9621e-03, 1.5640e-03, 1.0986e-03, 6.4182e-04,\n",
      "        2.4281e-03, 1.2159e-03, 1.8072e-03, 6.2943e-03, 4.5052e-03, 1.0147e-02,\n",
      "        1.7376e-03, 1.5097e-03, 4.9171e-03, 4.4136e-03, 7.7362e-03, 2.1648e-03,\n",
      "        5.5542e-03, 8.5526e-03, 3.9101e-03, 1.5306e-03, 2.5444e-03, 3.2349e-03,\n",
      "        1.5793e-03, 1.9932e-03, 3.1662e-03, 2.9507e-03, 1.1158e-03, 5.8022e-03,\n",
      "        4.3907e-03, 6.2447e-03, 5.1956e-03, 5.9929e-03, 3.9101e-03, 3.2501e-03,\n",
      "        5.4893e-03, 3.6125e-03, 3.4332e-03, 3.0289e-03, 2.0809e-03, 3.1872e-03,\n",
      "        8.3923e-03, 2.5578e-03, 3.4733e-03, 4.5319e-03, 8.3694e-03, 5.2681e-03,\n",
      "        4.8752e-03, 6.8054e-03, 3.3436e-03, 8.2855e-03, 4.7302e-03, 1.7242e-03,\n",
      "        6.2065e-03, 6.2904e-03, 1.0162e-02, 7.4959e-03, 8.5983e-03, 8.3923e-03,\n",
      "        2.2919e-02, 1.2535e-02, 9.5520e-03, 9.5825e-03, 5.2643e-03, 1.2840e-02,\n",
      "        1.0460e-02, 4.9744e-03, 1.8143e-02, 7.7248e-03, 8.0032e-03, 4.7417e-03,\n",
      "        8.2855e-03, 2.5299e-02, 1.5961e-02, 8.1100e-03, 1.2352e-02, 1.6754e-02,\n",
      "        8.6746e-03, 4.9591e-03, 4.0436e-03, 1.7014e-03, 1.2550e-02, 1.6647e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [73] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [73] : torch.Size([1, 32, 1, 145])\n",
      "Last layer attentions for generated token [73] : tensor([1.4355e-01, 1.4355e-01, 1.5986e-04, 3.1734e-04, 1.2398e-04, 2.8324e-03,\n",
      "        2.2936e-04, 4.6968e-04, 7.4482e-04, 5.3835e-04, 2.5673e-03, 5.5695e-04,\n",
      "        4.9210e-04, 9.9850e-04, 1.7776e-02, 1.3138e-02, 3.1424e-04, 4.4131e-04,\n",
      "        2.3758e-04, 3.8648e-04, 1.1379e-04, 6.4707e-04, 1.1467e-02, 2.7924e-03,\n",
      "        4.2458e-03, 1.9928e-02, 1.1292e-03, 5.6791e-04, 1.0042e-03, 6.9714e-04,\n",
      "        9.1600e-04, 1.5612e-03, 1.0691e-03, 5.7220e-04, 2.4748e-04, 7.2765e-04,\n",
      "        2.2852e-04, 6.0081e-04, 3.2282e-04, 2.2233e-04, 1.8911e-03, 1.0529e-03,\n",
      "        2.9278e-04, 6.1989e-04, 5.1403e-04, 5.8126e-04, 3.5048e-04, 2.1088e-04,\n",
      "        3.5739e-04, 1.1024e-03, 7.8344e-04, 8.5890e-05, 2.6245e-03, 1.1444e-03,\n",
      "        1.1311e-03, 4.9629e-03, 6.6900e-04, 2.2469e-03, 6.1989e-03, 1.7281e-03,\n",
      "        1.6365e-03, 2.2278e-03, 2.9030e-03, 1.7557e-03, 1.0862e-03, 7.6389e-04,\n",
      "        2.6226e-03, 9.2173e-04, 1.7433e-03, 5.7907e-03, 5.2414e-03, 1.3672e-02,\n",
      "        1.6603e-03, 9.7942e-04, 4.0894e-03, 6.3591e-03, 7.7095e-03, 1.8024e-03,\n",
      "        7.1373e-03, 1.1780e-02, 3.8338e-03, 1.9093e-03, 2.7828e-03, 1.9569e-03,\n",
      "        1.3056e-03, 2.2049e-03, 3.3092e-03, 2.9716e-03, 9.8515e-04, 2.4109e-03,\n",
      "        3.3932e-03, 4.8752e-03, 5.5466e-03, 3.7060e-03, 2.7561e-03, 3.8853e-03,\n",
      "        4.7836e-03, 5.1117e-03, 3.7155e-03, 3.2482e-03, 2.4319e-03, 2.9144e-03,\n",
      "        4.9934e-03, 2.0409e-03, 2.4605e-03, 2.9049e-03, 8.6899e-03, 5.2948e-03,\n",
      "        3.2196e-03, 6.1531e-03, 3.8795e-03, 1.0567e-02, 4.2839e-03, 1.8024e-03,\n",
      "        5.4245e-03, 5.3787e-03, 8.6670e-03, 9.5978e-03, 4.9095e-03, 1.1177e-02,\n",
      "        4.3274e-02, 1.4915e-02, 8.3313e-03, 7.6637e-03, 3.0537e-03, 1.1559e-02,\n",
      "        5.6725e-03, 3.3016e-03, 1.7426e-02, 7.7820e-03, 5.9319e-03, 4.4594e-03,\n",
      "        1.0689e-02, 3.3691e-02, 1.6556e-02, 1.5945e-02, 1.6739e-02, 2.7466e-02,\n",
      "        1.0818e-02, 7.9498e-03, 8.3847e-03, 2.1400e-03, 1.5778e-02, 1.7380e-02,\n",
      "        2.2583e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [74] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [74] : torch.Size([1, 32, 1, 146])\n",
      "Last layer attentions for generated token [74] : tensor([2.0337e-01, 2.0300e-01, 5.0735e-04, 4.9067e-04, 2.6560e-04, 2.5902e-03,\n",
      "        1.3649e-04, 2.0409e-04, 4.6444e-04, 5.1928e-04, 1.9951e-03, 4.1080e-04,\n",
      "        1.1587e-03, 1.1253e-03, 1.1574e-02, 1.0384e-02, 3.8218e-04, 5.7030e-04,\n",
      "        3.4928e-04, 6.6662e-04, 2.5249e-04, 1.2951e-03, 1.8616e-02, 3.0918e-03,\n",
      "        5.0049e-03, 1.4206e-02, 9.6273e-04, 4.2796e-04, 6.6137e-04, 3.3927e-04,\n",
      "        6.6042e-04, 1.1253e-03, 7.6008e-04, 2.4819e-04, 1.5771e-04, 7.4673e-04,\n",
      "        3.6740e-04, 2.9945e-04, 4.4489e-04, 2.4056e-04, 2.3022e-03, 1.3208e-03,\n",
      "        3.7980e-04, 4.9543e-04, 2.7418e-04, 3.8743e-04, 1.0186e-04, 3.6740e-04,\n",
      "        2.6155e-04, 8.8167e-04, 1.1320e-03, 1.3971e-04, 3.6831e-03, 2.8782e-03,\n",
      "        1.7290e-03, 4.3335e-03, 2.6679e-04, 1.8358e-03, 4.9248e-03, 1.5717e-03,\n",
      "        1.6279e-03, 1.4229e-03, 2.3403e-03, 1.3466e-03, 1.2026e-03, 5.9414e-04,\n",
      "        2.7428e-03, 1.2331e-03, 2.4776e-03, 7.6408e-03, 6.1607e-03, 1.1559e-02,\n",
      "        1.2655e-03, 8.9359e-04, 2.9697e-03, 3.1471e-03, 6.7978e-03, 1.7881e-03,\n",
      "        4.6310e-03, 9.2850e-03, 4.1885e-03, 9.8705e-04, 1.3733e-03, 1.1473e-03,\n",
      "        1.3866e-03, 8.7643e-04, 1.8444e-03, 1.1520e-03, 5.7793e-04, 1.5440e-03,\n",
      "        2.0523e-03, 1.7633e-03, 2.1706e-03, 3.1776e-03, 1.6441e-03, 1.7223e-03,\n",
      "        3.7518e-03, 4.6234e-03, 3.0327e-03, 2.8839e-03, 1.1797e-03, 1.4000e-03,\n",
      "        1.8644e-03, 8.5258e-04, 1.4620e-03, 1.0862e-03, 4.1389e-03, 3.2177e-03,\n",
      "        3.1681e-03, 2.8782e-03, 1.7881e-03, 4.3983e-03, 4.5738e-03, 1.0004e-03,\n",
      "        4.2801e-03, 7.5722e-03, 5.6305e-03, 4.9362e-03, 6.5804e-03, 8.8501e-03,\n",
      "        4.1931e-02, 9.5215e-03, 5.8327e-03, 6.9427e-03, 3.7193e-03, 7.6294e-03,\n",
      "        4.9629e-03, 3.2501e-03, 9.9258e-03, 3.9978e-03, 5.3825e-03, 3.9825e-03,\n",
      "        9.5062e-03, 2.1423e-02, 9.7351e-03, 1.0529e-02, 1.4511e-02, 2.0706e-02,\n",
      "        9.1019e-03, 6.8512e-03, 7.4043e-03, 2.9354e-03, 2.2598e-02, 2.0386e-02,\n",
      "        2.2308e-02, 1.1612e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [75] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [75] : torch.Size([1, 32, 1, 147])\n",
      "Last layer attentions for generated token [75] : tensor([3.2471e-01, 3.2471e-01, 2.3067e-04, 2.7084e-04, 1.8466e-04, 1.2589e-03,\n",
      "        1.0854e-04, 1.6165e-04, 3.6287e-04, 2.0921e-04, 5.1355e-04, 1.0834e-03,\n",
      "        4.3440e-04, 1.3456e-03, 1.0788e-02, 2.1477e-03, 9.7275e-05, 1.8609e-04,\n",
      "        6.6876e-05, 2.2006e-04, 2.6131e-04, 3.5858e-04, 7.3891e-03, 1.5917e-03,\n",
      "        5.5046e-03, 6.2218e-03, 2.1672e-04, 1.2153e-04, 3.4499e-04, 3.6073e-04,\n",
      "        4.8161e-04, 1.0481e-03, 2.3985e-04, 1.0157e-04, 1.1289e-04, 1.7893e-04,\n",
      "        2.8658e-04, 1.5855e-04, 2.0516e-04, 9.3222e-05, 1.0567e-03, 5.6887e-04,\n",
      "        1.9205e-04, 8.5175e-05, 6.0916e-05, 1.6046e-04, 6.6876e-05, 2.5940e-04,\n",
      "        1.0276e-04, 1.0376e-03, 5.7888e-04, 1.2302e-04, 2.0599e-03, 1.2541e-03,\n",
      "        8.4209e-04, 1.4839e-03, 7.6950e-05, 7.5340e-04, 4.2801e-03, 4.2844e-04,\n",
      "        1.1311e-03, 6.4087e-04, 3.0766e-03, 7.2861e-04, 8.4209e-04, 8.8406e-04,\n",
      "        3.6073e-04, 9.4700e-04, 2.5368e-03, 5.0621e-03, 2.6608e-03, 2.7027e-03,\n",
      "        5.3740e-04, 4.6420e-04, 1.9646e-03, 1.7519e-03, 1.9722e-03, 1.0643e-03,\n",
      "        3.0804e-03, 1.0900e-03, 1.0815e-03, 5.8222e-04, 6.5851e-04, 1.3170e-03,\n",
      "        3.7599e-04, 4.0245e-04, 7.6818e-04, 5.3120e-04, 3.4690e-04, 7.3576e-04,\n",
      "        6.5708e-04, 9.8228e-04, 1.3723e-03, 8.2445e-04, 7.7868e-04, 1.0195e-03,\n",
      "        2.2392e-03, 8.5211e-04, 2.2640e-03, 5.6076e-04, 3.2020e-04, 4.6229e-04,\n",
      "        8.0824e-04, 2.1672e-04, 4.6158e-04, 3.5930e-04, 2.4319e-03, 1.3943e-03,\n",
      "        5.9271e-04, 5.2786e-04, 4.4465e-04, 2.2640e-03, 7.8821e-04, 3.0327e-04,\n",
      "        4.7760e-03, 8.5020e-04, 9.7847e-04, 2.5635e-03, 9.5224e-04, 1.1108e-02,\n",
      "        4.1580e-03, 4.2648e-03, 2.3289e-03, 1.1578e-03, 1.0242e-03, 1.6556e-03,\n",
      "        1.5945e-03, 4.9686e-04, 2.9221e-03, 1.9569e-03, 1.1578e-03, 1.4076e-03,\n",
      "        1.0384e-02, 5.4932e-03, 3.3340e-03, 1.0315e-02, 1.0445e-02, 5.6496e-03,\n",
      "        9.0485e-03, 4.8027e-03, 6.5880e-03, 4.1809e-03, 4.9866e-02, 1.0719e-02,\n",
      "        1.4946e-02, 1.3252e-02, 2.2324e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [76] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [76] : torch.Size([1, 32, 1, 148])\n",
      "Last layer attentions for generated token [76] : tensor([2.6318e-01, 2.6318e-01, 4.2725e-04, 4.6921e-04, 3.9053e-04, 1.4763e-03,\n",
      "        1.1837e-04, 1.6117e-04, 2.3448e-04, 2.8300e-04, 7.1526e-04, 4.6015e-04,\n",
      "        4.1986e-04, 7.8726e-04, 1.0139e-02, 5.8098e-03, 3.0541e-04, 4.8494e-04,\n",
      "        1.0288e-04, 1.4055e-04, 7.5281e-05, 5.3692e-04, 1.9348e-02, 3.5172e-03,\n",
      "        3.6583e-03, 1.0620e-02, 6.4611e-04, 1.8191e-04, 4.2796e-04, 2.5868e-04,\n",
      "        4.0674e-04, 8.5640e-04, 4.4513e-04, 1.0169e-04, 1.1081e-04, 2.9302e-04,\n",
      "        6.0606e-04, 2.2995e-04, 3.6263e-04, 2.4390e-04, 1.2674e-03, 9.3842e-04,\n",
      "        4.4942e-04, 3.1137e-04, 1.0532e-04, 2.2209e-04, 6.9618e-05, 3.2759e-04,\n",
      "        9.7036e-05, 3.2306e-04, 5.9652e-04, 1.1164e-04, 2.3346e-03, 3.4733e-03,\n",
      "        1.2875e-03, 2.0409e-03, 1.0246e-04, 9.8324e-04, 2.1801e-03, 5.0116e-04,\n",
      "        1.0509e-03, 5.2357e-04, 1.4057e-03, 5.5933e-04, 6.1893e-04, 3.8671e-04,\n",
      "        9.0265e-04, 6.8283e-04, 1.8415e-03, 4.8561e-03, 4.7264e-03, 4.6806e-03,\n",
      "        8.1682e-04, 3.7265e-04, 1.4534e-03, 1.9178e-03, 3.4370e-03, 1.2627e-03,\n",
      "        3.9482e-03, 4.9629e-03, 3.1757e-03, 5.6171e-04, 5.5599e-04, 9.7609e-04,\n",
      "        8.0585e-04, 6.1655e-04, 1.1501e-03, 5.1546e-04, 3.7050e-04, 1.0509e-03,\n",
      "        9.5510e-04, 5.6171e-04, 9.8324e-04, 9.8705e-04, 1.2054e-03, 8.4448e-04,\n",
      "        1.9741e-03, 2.7618e-03, 1.6994e-03, 1.3027e-03, 5.5075e-04, 4.4346e-04,\n",
      "        9.5129e-04, 2.3091e-04, 4.1080e-04, 3.9506e-04, 2.9392e-03, 1.2703e-03,\n",
      "        9.3842e-04, 1.5631e-03, 9.8515e-04, 3.7594e-03, 1.8301e-03, 5.5170e-04,\n",
      "        3.5706e-03, 1.8415e-03, 2.1439e-03, 2.1229e-03, 2.2888e-03, 7.8659e-03,\n",
      "        1.9485e-02, 4.3221e-03, 3.7384e-03, 2.2545e-03, 1.7633e-03, 3.5152e-03,\n",
      "        1.6251e-03, 1.3866e-03, 3.2768e-03, 2.4757e-03, 1.6441e-03, 1.8024e-03,\n",
      "        5.7793e-03, 1.4160e-02, 4.9934e-03, 5.8441e-03, 1.4397e-02, 1.0925e-02,\n",
      "        6.4201e-03, 5.9128e-03, 6.6757e-03, 2.6169e-03, 2.7023e-02, 2.5650e-02,\n",
      "        2.3514e-02, 1.4801e-02, 3.9215e-02, 3.4515e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [77] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [77] : torch.Size([1, 32, 1, 149])\n",
      "Last layer attentions for generated token [77] : tensor([2.7466e-01, 2.7417e-01, 2.7609e-04, 1.8942e-04, 2.2149e-04, 1.4896e-03,\n",
      "        4.9233e-05, 8.1182e-05, 4.8828e-04, 1.7655e-04, 6.3848e-04, 5.2309e-04,\n",
      "        3.9721e-04, 8.3590e-04, 1.7288e-02, 5.4588e-03, 9.6738e-05, 2.8157e-04,\n",
      "        1.1992e-04, 4.3607e-04, 3.0684e-04, 8.7070e-04, 7.2517e-03, 3.6278e-03,\n",
      "        9.8648e-03, 1.1086e-02, 1.9312e-04, 2.1219e-04, 3.0518e-04, 4.0960e-04,\n",
      "        5.5695e-04, 1.8473e-03, 2.4271e-04, 9.7156e-05, 2.0242e-04, 7.4482e-04,\n",
      "        3.8481e-04, 2.7084e-04, 3.5596e-04, 1.6260e-04, 2.6531e-03, 1.7252e-03,\n",
      "        3.9864e-04, 1.5891e-04, 6.6519e-05, 1.4353e-04, 8.4698e-05, 5.2929e-04,\n",
      "        8.7082e-05, 9.5463e-04, 1.2188e-03, 4.6790e-05, 3.1643e-03, 2.1935e-03,\n",
      "        1.2426e-03, 2.6741e-03, 4.0948e-05, 6.3324e-04, 1.5831e-03, 2.6512e-04,\n",
      "        9.6178e-04, 4.3440e-04, 3.6278e-03, 8.7261e-04, 7.9298e-04, 1.4076e-03,\n",
      "        7.3624e-04, 8.0061e-04, 2.7618e-03, 5.1117e-03, 6.0959e-03, 6.1684e-03,\n",
      "        4.5538e-04, 8.2111e-04, 2.6073e-03, 3.7098e-03, 3.3112e-03, 1.6813e-03,\n",
      "        5.8746e-03, 1.9283e-03, 1.9150e-03, 7.7152e-04, 6.3086e-04, 2.3327e-03,\n",
      "        8.0395e-04, 7.3624e-04, 1.2112e-03, 3.3760e-04, 2.1052e-04, 9.2888e-04,\n",
      "        4.1127e-04, 9.0551e-04, 1.4582e-03, 1.1244e-03, 4.0889e-04, 9.2697e-04,\n",
      "        3.2349e-03, 2.1152e-03, 1.1358e-03, 5.4693e-04, 3.0923e-04, 3.9482e-04,\n",
      "        9.0551e-04, 2.1303e-04, 6.2609e-04, 3.1972e-04, 5.4588e-03, 1.4410e-03,\n",
      "        6.3467e-04, 4.0579e-04, 7.5674e-04, 4.3526e-03, 1.0834e-03, 2.4652e-04,\n",
      "        7.5531e-03, 8.7261e-04, 7.3910e-04, 2.2106e-03, 1.0710e-03, 1.0193e-02,\n",
      "        5.0087e-03, 1.4582e-03, 1.3695e-03, 9.3412e-04, 7.3481e-04, 1.6413e-03,\n",
      "        1.4496e-03, 5.1212e-04, 1.7014e-03, 1.8091e-03, 1.0223e-03, 8.0204e-04,\n",
      "        6.2408e-03, 1.0551e-02, 3.1166e-03, 6.5575e-03, 7.4310e-03, 4.8981e-03,\n",
      "        7.4120e-03, 7.0610e-03, 6.2332e-03, 3.1796e-03, 4.2694e-02, 1.5617e-02,\n",
      "        1.6678e-02, 1.6052e-02, 3.4821e-02, 2.2827e-02, 2.1408e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [78] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [78] : torch.Size([1, 32, 1, 150])\n",
      "Last layer attentions for generated token [78] : tensor([2.0654e-01, 2.0654e-01, 5.3120e-04, 6.9284e-04, 4.7255e-04, 1.1644e-03,\n",
      "        1.6141e-04, 2.2984e-04, 3.9864e-04, 4.9114e-04, 9.1076e-04, 2.7442e-04,\n",
      "        2.4319e-04, 5.7220e-04, 1.0269e-02, 6.7596e-03, 3.9864e-04, 5.6124e-04,\n",
      "        1.4746e-04, 2.1636e-04, 9.4891e-05, 8.8596e-04, 2.0599e-02, 5.5237e-03,\n",
      "        4.2343e-03, 1.4603e-02, 8.9312e-04, 3.9935e-04, 4.9019e-04, 3.5810e-04,\n",
      "        7.5960e-04, 1.1396e-03, 6.8855e-04, 2.8038e-04, 2.3293e-04, 7.8201e-04,\n",
      "        5.2834e-04, 2.4986e-04, 4.3178e-04, 4.5872e-04, 1.4524e-03, 1.0939e-03,\n",
      "        5.1785e-04, 5.3978e-04, 2.1935e-04, 4.4823e-04, 1.4639e-04, 5.6887e-04,\n",
      "        2.2316e-04, 4.9305e-04, 7.5817e-04, 1.7309e-04, 3.6259e-03, 4.2076e-03,\n",
      "        1.3885e-03, 2.1343e-03, 1.1671e-04, 1.5488e-03, 3.6449e-03, 9.3985e-04,\n",
      "        1.2922e-03, 1.0004e-03, 2.4929e-03, 1.2894e-03, 6.9427e-04, 4.7064e-04,\n",
      "        1.2112e-03, 1.3199e-03, 1.9207e-03, 6.4049e-03, 6.3133e-03, 5.5504e-03,\n",
      "        9.7656e-04, 4.9114e-04, 1.3199e-03, 2.0123e-03, 3.9215e-03, 1.5678e-03,\n",
      "        3.3321e-03, 7.4539e-03, 4.0817e-03, 4.8828e-04, 7.5817e-04, 8.7595e-04,\n",
      "        7.4482e-04, 5.5218e-04, 1.7490e-03, 9.9468e-04, 4.1056e-04, 1.9341e-03,\n",
      "        1.2646e-03, 1.6270e-03, 1.4639e-03, 1.7300e-03, 1.1177e-03, 7.1764e-04,\n",
      "        2.1439e-03, 3.1700e-03, 1.4219e-03, 1.0691e-03, 4.8184e-04, 8.1968e-04,\n",
      "        1.6651e-03, 4.9686e-04, 6.8855e-04, 4.7708e-04, 2.2850e-03, 1.2722e-03,\n",
      "        1.0443e-03, 2.2221e-03, 1.1158e-03, 3.0899e-03, 1.7672e-03, 5.8126e-04,\n",
      "        3.2158e-03, 2.3174e-03, 2.7008e-03, 2.4567e-03, 3.3417e-03, 8.1100e-03,\n",
      "        1.9928e-02, 4.2877e-03, 2.9049e-03, 2.6798e-03, 1.6232e-03, 4.4098e-03,\n",
      "        2.4300e-03, 2.1439e-03, 4.6997e-03, 2.9202e-03, 2.5482e-03, 2.7580e-03,\n",
      "        7.8201e-03, 2.0645e-02, 7.2975e-03, 7.1487e-03, 2.0096e-02, 1.2184e-02,\n",
      "        7.5531e-03, 6.4049e-03, 9.8724e-03, 2.6436e-03, 2.3849e-02, 2.1027e-02,\n",
      "        2.0233e-02, 1.4946e-02, 4.6814e-02, 4.3335e-02, 9.5062e-03, 3.0472e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [79] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [79] : torch.Size([1, 32, 1, 151])\n",
      "Last layer attentions for generated token [79] : tensor([0.1508, 0.1508, 0.0004, 0.0008, 0.0005, 0.0017, 0.0002, 0.0003, 0.0006,\n",
      "        0.0006, 0.0011, 0.0005, 0.0006, 0.0008, 0.0144, 0.0086, 0.0005, 0.0011,\n",
      "        0.0003, 0.0003, 0.0003, 0.0013, 0.0108, 0.0043, 0.0084, 0.0139, 0.0024,\n",
      "        0.0007, 0.0006, 0.0007, 0.0014, 0.0018, 0.0015, 0.0006, 0.0005, 0.0007,\n",
      "        0.0004, 0.0005, 0.0004, 0.0003, 0.0031, 0.0013, 0.0006, 0.0010, 0.0004,\n",
      "        0.0005, 0.0002, 0.0003, 0.0004, 0.0014, 0.0011, 0.0004, 0.0022, 0.0084,\n",
      "        0.0014, 0.0035, 0.0004, 0.0018, 0.0036, 0.0012, 0.0017, 0.0013, 0.0031,\n",
      "        0.0015, 0.0012, 0.0007, 0.0017, 0.0015, 0.0018, 0.0100, 0.0072, 0.0103,\n",
      "        0.0016, 0.0008, 0.0038, 0.0053, 0.0065, 0.0026, 0.0075, 0.0118, 0.0052,\n",
      "        0.0016, 0.0018, 0.0016, 0.0013, 0.0014, 0.0029, 0.0015, 0.0008, 0.0018,\n",
      "        0.0020, 0.0029, 0.0039, 0.0022, 0.0018, 0.0015, 0.0035, 0.0044, 0.0040,\n",
      "        0.0026, 0.0016, 0.0020, 0.0032, 0.0008, 0.0017, 0.0014, 0.0043, 0.0031,\n",
      "        0.0017, 0.0032, 0.0026, 0.0057, 0.0034, 0.0015, 0.0079, 0.0045, 0.0068,\n",
      "        0.0052, 0.0051, 0.0090, 0.0294, 0.0166, 0.0089, 0.0049, 0.0032, 0.0067,\n",
      "        0.0040, 0.0038, 0.0075, 0.0040, 0.0023, 0.0028, 0.0075, 0.0157, 0.0079,\n",
      "        0.0074, 0.0108, 0.0094, 0.0045, 0.0036, 0.0067, 0.0027, 0.0212, 0.0147,\n",
      "        0.0263, 0.0104, 0.0307, 0.0425, 0.0075, 0.0356, 0.0158],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [80] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [80] : torch.Size([1, 32, 1, 152])\n",
      "Last layer attentions for generated token [80] : tensor([2.3071e-01, 2.3071e-01, 7.8773e-04, 1.1091e-03, 3.8099e-04, 4.3907e-03,\n",
      "        1.2958e-04, 3.0971e-04, 6.5041e-04, 7.5340e-04, 1.3933e-03, 1.1892e-03,\n",
      "        9.6321e-04, 1.2131e-03, 1.4694e-02, 4.6005e-03, 4.3416e-04, 4.5943e-04,\n",
      "        1.9920e-04, 4.3845e-04, 2.5988e-04, 1.8606e-03, 1.2054e-02, 5.0888e-03,\n",
      "        6.9351e-03, 1.3298e-02, 7.1335e-04, 1.8346e-04, 5.0068e-04, 4.1270e-04,\n",
      "        8.1587e-04, 1.5068e-03, 7.0906e-04, 3.3164e-04, 1.9765e-04, 4.5514e-04,\n",
      "        3.1042e-04, 3.0136e-04, 2.2924e-04, 1.2612e-04, 1.7614e-03, 7.5769e-04,\n",
      "        3.7146e-04, 3.3808e-04, 2.4796e-04, 2.6608e-04, 1.8489e-04, 3.4213e-04,\n",
      "        2.4700e-04, 1.2493e-03, 1.2159e-03, 4.1771e-04, 2.4414e-03, 2.2202e-03,\n",
      "        1.0662e-03, 4.0855e-03, 1.3638e-04, 1.3962e-03, 2.9621e-03, 7.7724e-04,\n",
      "        8.8930e-04, 1.2617e-03, 4.1389e-03, 1.0576e-03, 9.6893e-04, 9.4271e-04,\n",
      "        8.9645e-04, 1.2083e-03, 1.8282e-03, 9.2926e-03, 5.7106e-03, 9.4376e-03,\n",
      "        1.5068e-03, 1.1415e-03, 1.8501e-03, 3.4828e-03, 3.5801e-03, 1.8463e-03,\n",
      "        6.4735e-03, 5.2605e-03, 2.2945e-03, 9.2649e-04, 1.4353e-03, 1.3638e-03,\n",
      "        4.3511e-04, 6.1846e-04, 1.0605e-03, 1.3142e-03, 4.6134e-04, 1.2569e-03,\n",
      "        1.3094e-03, 1.9388e-03, 3.7651e-03, 1.6775e-03, 1.8282e-03, 2.2316e-03,\n",
      "        4.6654e-03, 3.1624e-03, 5.4054e-03, 2.3060e-03, 1.2016e-03, 1.6165e-03,\n",
      "        1.8282e-03, 6.8569e-04, 7.7868e-04, 9.7466e-04, 4.2267e-03, 3.0193e-03,\n",
      "        1.1377e-03, 1.7042e-03, 1.3857e-03, 3.1643e-03, 1.0014e-03, 2.8753e-04,\n",
      "        3.5076e-03, 1.1415e-03, 1.1625e-03, 4.1580e-03, 1.8530e-03, 1.2100e-02,\n",
      "        1.1703e-02, 8.8196e-03, 5.2986e-03, 1.8787e-03, 1.9083e-03, 2.9583e-03,\n",
      "        2.4548e-03, 9.2459e-04, 3.7117e-03, 2.3727e-03, 1.5574e-03, 1.0939e-03,\n",
      "        1.0788e-02, 7.7515e-03, 3.6793e-03, 4.8218e-03, 5.0964e-03, 5.7640e-03,\n",
      "        4.2267e-03, 1.7824e-03, 4.4937e-03, 2.6684e-03, 2.1378e-02, 9.5291e-03,\n",
      "        1.0658e-02, 8.6365e-03, 1.3405e-02, 1.4923e-02, 1.2924e-02, 4.8279e-02,\n",
      "        3.3325e-02, 1.7731e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [81] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [81] : torch.Size([1, 32, 1, 153])\n",
      "Last layer attentions for generated token [81] : tensor([1.8372e-01, 1.8372e-01, 2.1267e-03, 1.8349e-03, 1.7891e-03, 2.2182e-03,\n",
      "        1.7667e-04, 3.0565e-04, 4.6706e-04, 7.2050e-04, 1.3466e-03, 4.1294e-04,\n",
      "        7.3051e-04, 1.3046e-03, 8.3694e-03, 6.4201e-03, 2.5892e-04, 9.1648e-04,\n",
      "        1.7595e-04, 5.8460e-04, 1.1951e-04, 5.6553e-04, 2.6321e-02, 8.7051e-03,\n",
      "        5.3368e-03, 1.7181e-02, 8.7976e-04, 2.3901e-04, 6.3372e-04, 2.5249e-04,\n",
      "        5.6458e-04, 1.1253e-03, 8.5592e-04, 2.0969e-04, 1.7798e-04, 4.3964e-04,\n",
      "        4.6086e-04, 2.4140e-04, 3.5882e-04, 3.0398e-04, 1.3809e-03, 1.2846e-03,\n",
      "        7.4816e-04, 4.9257e-04, 1.8299e-04, 4.4322e-04, 8.8453e-05, 4.8566e-04,\n",
      "        3.0684e-04, 5.2500e-04, 1.1768e-03, 1.0042e-03, 3.3989e-03, 4.0665e-03,\n",
      "        1.6727e-03, 2.9545e-03, 4.0829e-05, 6.3372e-04, 1.2894e-03, 3.8195e-04,\n",
      "        5.7220e-04, 4.9162e-04, 1.4219e-03, 5.1308e-04, 3.0446e-04, 2.8276e-04,\n",
      "        8.5926e-04, 2.0580e-03, 8.1825e-04, 8.4000e-03, 7.2899e-03, 6.8855e-03,\n",
      "        1.5593e-03, 5.0926e-04, 1.3514e-03, 2.0828e-03, 5.0278e-03, 1.5926e-03,\n",
      "        3.1242e-03, 6.8436e-03, 8.2474e-03, 8.5068e-04, 7.4339e-04, 7.6580e-04,\n",
      "        6.2513e-04, 5.2929e-04, 1.0328e-03, 7.7343e-04, 4.8280e-04, 1.8969e-03,\n",
      "        1.3514e-03, 1.8911e-03, 2.9545e-03, 1.6565e-03, 1.0653e-03, 1.2703e-03,\n",
      "        2.5349e-03, 4.2877e-03, 2.4471e-03, 1.2503e-03, 1.0347e-03, 8.3113e-04,\n",
      "        1.3838e-03, 4.0269e-04, 8.0729e-04, 5.6553e-04, 2.4452e-03, 1.7643e-03,\n",
      "        8.9693e-04, 1.6594e-03, 9.7561e-04, 1.9474e-03, 1.4820e-03, 4.7827e-04,\n",
      "        2.4643e-03, 1.4677e-03, 1.2455e-03, 2.5234e-03, 1.9951e-03, 6.3705e-03,\n",
      "        2.3270e-02, 3.9558e-03, 3.8052e-03, 2.5883e-03, 3.0079e-03, 3.8548e-03,\n",
      "        2.7122e-03, 1.4362e-03, 2.9755e-03, 2.1687e-03, 2.2812e-03, 2.8248e-03,\n",
      "        7.9651e-03, 1.7761e-02, 8.6823e-03, 3.3817e-03, 8.3313e-03, 9.1095e-03,\n",
      "        4.7569e-03, 3.4657e-03, 4.2305e-03, 1.7853e-03, 2.2186e-02, 1.4641e-02,\n",
      "        1.8295e-02, 1.2955e-02, 1.6632e-02, 2.9114e-02, 8.8348e-03, 4.0710e-02,\n",
      "        2.4200e-02, 4.4952e-02, 3.6621e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [82] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [82] : torch.Size([1, 32, 1, 154])\n",
      "Last layer attentions for generated token [82] : tensor([0.1078, 0.1078, 0.0004, 0.0006, 0.0003, 0.0013, 0.0001, 0.0002, 0.0002,\n",
      "        0.0005, 0.0008, 0.0003, 0.0005, 0.0008, 0.0156, 0.0064, 0.0001, 0.0008,\n",
      "        0.0001, 0.0002, 0.0002, 0.0003, 0.0073, 0.0027, 0.0047, 0.0100, 0.0021,\n",
      "        0.0004, 0.0005, 0.0005, 0.0007, 0.0014, 0.0006, 0.0003, 0.0003, 0.0004,\n",
      "        0.0003, 0.0004, 0.0003, 0.0002, 0.0034, 0.0011, 0.0008, 0.0009, 0.0005,\n",
      "        0.0005, 0.0001, 0.0002, 0.0003, 0.0018, 0.0012, 0.0003, 0.0016, 0.0040,\n",
      "        0.0011, 0.0039, 0.0002, 0.0011, 0.0025, 0.0004, 0.0010, 0.0005, 0.0019,\n",
      "        0.0006, 0.0005, 0.0007, 0.0010, 0.0013, 0.0007, 0.0068, 0.0089, 0.0144,\n",
      "        0.0014, 0.0007, 0.0032, 0.0072, 0.0067, 0.0021, 0.0100, 0.0132, 0.0105,\n",
      "        0.0018, 0.0014, 0.0027, 0.0017, 0.0014, 0.0019, 0.0011, 0.0006, 0.0021,\n",
      "        0.0015, 0.0044, 0.0103, 0.0032, 0.0025, 0.0036, 0.0046, 0.0090, 0.0056,\n",
      "        0.0049, 0.0027, 0.0022, 0.0060, 0.0008, 0.0015, 0.0017, 0.0083, 0.0061,\n",
      "        0.0025, 0.0060, 0.0032, 0.0094, 0.0036, 0.0028, 0.0176, 0.0044, 0.0062,\n",
      "        0.0090, 0.0037, 0.0161, 0.0451, 0.0299, 0.0166, 0.0067, 0.0065, 0.0071,\n",
      "        0.0090, 0.0052, 0.0096, 0.0069, 0.0038, 0.0025, 0.0134, 0.0320, 0.0107,\n",
      "        0.0080, 0.0128, 0.0135, 0.0072, 0.0037, 0.0048, 0.0016, 0.0147, 0.0104,\n",
      "        0.0183, 0.0079, 0.0206, 0.0214, 0.0074, 0.0230, 0.0139, 0.0072, 0.0083,\n",
      "        0.0143], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [83] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [83] : torch.Size([1, 32, 1, 155])\n",
      "Last layer attentions for generated token [83] : tensor([1.6016e-01, 1.6016e-01, 1.3876e-04, 2.6917e-04, 1.4830e-04, 7.7438e-04,\n",
      "        1.6487e-04, 2.0432e-04, 3.1638e-04, 3.3498e-04, 1.1778e-03, 2.5177e-04,\n",
      "        2.6917e-04, 4.1199e-04, 1.1543e-02, 6.0158e-03, 1.4722e-04, 6.5184e-04,\n",
      "        1.2153e-04, 2.0158e-04, 7.0095e-05, 2.2447e-04, 3.0594e-03, 1.0519e-03,\n",
      "        3.5458e-03, 9.9792e-03, 1.1759e-03, 2.3663e-04, 2.4557e-04, 2.6083e-04,\n",
      "        3.9315e-04, 7.9107e-04, 5.9128e-04, 3.5644e-04, 3.3092e-04, 2.3293e-04,\n",
      "        1.8978e-04, 3.5787e-04, 1.2791e-04, 1.3041e-04, 2.2602e-03, 7.5483e-04,\n",
      "        2.9612e-04, 3.8314e-04, 4.0889e-04, 1.9276e-04, 1.8108e-04, 1.3769e-04,\n",
      "        1.6809e-04, 1.3485e-03, 6.3086e-04, 1.6165e-04, 1.2760e-03, 3.7441e-03,\n",
      "        1.0900e-03, 2.5539e-03, 3.4976e-04, 6.5708e-04, 2.3994e-03, 5.7077e-04,\n",
      "        6.9141e-04, 6.5184e-04, 1.8139e-03, 6.5708e-04, 4.6134e-04, 4.8351e-04,\n",
      "        6.8331e-04, 9.5797e-04, 9.8038e-04, 8.6060e-03, 6.2141e-03, 8.0643e-03,\n",
      "        3.6755e-03, 1.3771e-03, 5.0583e-03, 8.0948e-03, 6.2752e-03, 3.6144e-03,\n",
      "        1.1070e-02, 7.1983e-03, 6.2370e-03, 3.1872e-03, 3.1586e-03, 4.8332e-03,\n",
      "        1.5030e-03, 3.4428e-03, 3.7556e-03, 1.9064e-03, 8.4877e-04, 2.5196e-03,\n",
      "        3.5286e-03, 4.5738e-03, 8.0566e-03, 3.4943e-03, 5.9814e-03, 5.3749e-03,\n",
      "        7.5989e-03, 3.6793e-03, 1.0056e-02, 3.4695e-03, 3.4695e-03, 3.6831e-03,\n",
      "        9.9258e-03, 1.2417e-03, 3.5629e-03, 3.5725e-03, 1.7380e-02, 8.3466e-03,\n",
      "        2.4490e-03, 6.5079e-03, 3.1662e-03, 1.0246e-02, 4.1656e-03, 2.3270e-03,\n",
      "        1.2344e-02, 6.5155e-03, 8.7433e-03, 8.2245e-03, 3.9558e-03, 1.2909e-02,\n",
      "        2.8229e-02, 2.0599e-02, 1.4709e-02, 4.4060e-03, 5.3101e-03, 9.8419e-03,\n",
      "        6.9122e-03, 3.7155e-03, 8.1635e-03, 4.1580e-03, 2.0180e-03, 1.5564e-03,\n",
      "        8.8196e-03, 1.5556e-02, 1.1353e-02, 5.9853e-03, 1.1116e-02, 1.3863e-02,\n",
      "        8.2779e-03, 3.1681e-03, 3.7041e-03, 1.2589e-03, 7.8506e-03, 1.2222e-02,\n",
      "        2.4292e-02, 8.2474e-03, 1.7853e-02, 7.4501e-03, 1.9703e-03, 5.4703e-03,\n",
      "        7.0038e-03, 3.7441e-03, 1.5688e-03, 1.0307e-02, 1.6357e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [84] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [84] : torch.Size([1, 32, 1, 156])\n",
      "Last layer attentions for generated token [84] : tensor([2.5586e-01, 2.5537e-01, 2.3794e-04, 2.3603e-04, 1.8454e-04, 2.0924e-03,\n",
      "        2.2697e-04, 4.3416e-04, 4.7493e-04, 3.3808e-04, 7.9060e-04, 3.9291e-04,\n",
      "        1.6737e-04, 4.8614e-04, 7.4730e-03, 5.9357e-03, 3.9148e-04, 4.7398e-04,\n",
      "        1.3137e-04, 1.7679e-04, 1.1504e-04, 6.2180e-04, 9.4757e-03, 1.3218e-03,\n",
      "        2.7733e-03, 1.1742e-02, 5.7173e-04, 2.0826e-04, 4.0460e-04, 6.0987e-04,\n",
      "        4.7040e-04, 7.2289e-04, 5.3167e-04, 3.3092e-04, 3.4404e-04, 2.1243e-04,\n",
      "        3.9148e-04, 4.2915e-04, 1.5056e-04, 1.9717e-04, 1.2341e-03, 5.8746e-04,\n",
      "        2.5916e-04, 3.0541e-04, 4.4346e-04, 3.3689e-04, 4.9591e-04, 3.0303e-04,\n",
      "        4.1580e-04, 1.0357e-03, 7.4720e-04, 1.9562e-04, 2.8706e-03, 5.6419e-03,\n",
      "        1.0786e-03, 3.1090e-03, 3.0732e-04, 1.0338e-03, 2.3346e-03, 9.8228e-04,\n",
      "        1.0338e-03, 8.5354e-04, 1.7204e-03, 9.4986e-04, 4.7970e-04, 6.4898e-04,\n",
      "        7.6771e-04, 1.3247e-03, 1.7071e-03, 7.9803e-03, 4.5547e-03, 7.1106e-03,\n",
      "        1.4772e-03, 1.1644e-03, 3.6812e-03, 2.9697e-03, 2.6493e-03, 1.9722e-03,\n",
      "        4.9057e-03, 3.6240e-03, 2.1744e-03, 9.5367e-04, 1.6966e-03, 1.9798e-03,\n",
      "        6.5804e-04, 1.0939e-03, 1.7786e-03, 1.2712e-03, 5.8651e-04, 1.3609e-03,\n",
      "        2.5482e-03, 3.4485e-03, 4.1885e-03, 2.0676e-03, 8.9493e-03, 4.5052e-03,\n",
      "        5.4054e-03, 2.0466e-03, 4.1580e-03, 1.6260e-03, 1.6317e-03, 1.9798e-03,\n",
      "        4.7073e-03, 1.0176e-03, 2.2964e-03, 2.1458e-03, 7.9651e-03, 3.8853e-03,\n",
      "        1.6356e-03, 4.2000e-03, 1.8826e-03, 3.4351e-03, 1.7271e-03, 8.3685e-04,\n",
      "        4.7722e-03, 2.0542e-03, 4.7607e-03, 6.6223e-03, 3.4351e-03, 6.8970e-03,\n",
      "        1.4458e-02, 1.2131e-02, 7.7477e-03, 2.4700e-03, 4.6349e-03, 4.8332e-03,\n",
      "        3.1090e-03, 2.0275e-03, 8.6975e-03, 4.3945e-03, 1.4458e-03, 2.3441e-03,\n",
      "        6.5269e-03, 6.2943e-03, 6.2332e-03, 6.6071e-03, 8.5831e-03, 5.8098e-03,\n",
      "        5.1956e-03, 3.2940e-03, 4.6120e-03, 1.4400e-03, 8.0032e-03, 7.7477e-03,\n",
      "        1.1368e-02, 8.1558e-03, 1.5213e-02, 7.4272e-03, 3.2139e-03, 1.3969e-02,\n",
      "        8.3694e-03, 3.1796e-03, 3.4924e-03, 7.7477e-03, 1.4702e-02, 5.4283e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [85] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [85] : torch.Size([1, 32, 1, 157])\n",
      "Last layer attentions for generated token [85] : tensor([2.1838e-01, 2.1802e-01, 1.8382e-04, 2.5129e-04, 1.1504e-04, 1.6747e-03,\n",
      "        1.5008e-04, 2.6441e-04, 3.9768e-04, 3.2330e-04, 1.2131e-03, 2.2435e-04,\n",
      "        2.3246e-04, 6.3038e-04, 1.0117e-02, 6.8817e-03, 1.9419e-04, 5.0068e-04,\n",
      "        1.7476e-04, 1.9646e-04, 9.0659e-05, 5.1451e-04, 8.7891e-03, 1.3990e-03,\n",
      "        3.9043e-03, 1.1627e-02, 5.0259e-04, 2.7657e-04, 3.6216e-04, 3.2711e-04,\n",
      "        5.2500e-04, 1.1244e-03, 7.6818e-04, 2.6751e-04, 2.5129e-04, 2.4557e-04,\n",
      "        3.1042e-04, 4.6039e-04, 2.1410e-04, 2.0432e-04, 1.8682e-03, 8.0967e-04,\n",
      "        3.0375e-04, 4.6039e-04, 3.8481e-04, 2.5630e-04, 3.5501e-04, 4.7326e-04,\n",
      "        4.5419e-04, 1.5011e-03, 1.1940e-03, 1.4663e-04, 1.9197e-03, 4.3411e-03,\n",
      "        1.0624e-03, 3.3684e-03, 2.4652e-04, 1.0891e-03, 2.4910e-03, 1.0538e-03,\n",
      "        1.0576e-03, 8.4829e-04, 2.1896e-03, 1.0519e-03, 5.7411e-04, 9.8610e-04,\n",
      "        9.5940e-04, 7.6199e-04, 1.5011e-03, 6.1378e-03, 4.6349e-03, 6.1264e-03,\n",
      "        1.3876e-03, 8.8596e-04, 3.1071e-03, 3.7766e-03, 3.7174e-03, 1.1063e-03,\n",
      "        4.6196e-03, 4.6082e-03, 2.2354e-03, 6.4182e-04, 1.3218e-03, 1.9722e-03,\n",
      "        1.0157e-03, 7.8011e-04, 2.3174e-03, 1.3037e-03, 5.9700e-04, 2.1973e-03,\n",
      "        2.0828e-03, 2.6684e-03, 5.0774e-03, 3.2806e-03, 6.8130e-03, 5.1727e-03,\n",
      "        6.5994e-03, 2.3861e-03, 3.2902e-03, 1.7586e-03, 1.1806e-03, 1.9855e-03,\n",
      "        4.0398e-03, 9.8801e-04, 2.9068e-03, 1.7614e-03, 1.0002e-02, 4.7569e-03,\n",
      "        2.1191e-03, 4.1084e-03, 1.6747e-03, 4.9591e-03, 2.7771e-03, 7.4005e-04,\n",
      "        7.4615e-03, 3.0556e-03, 4.0131e-03, 9.1629e-03, 6.1951e-03, 9.7656e-03,\n",
      "        1.7670e-02, 9.6741e-03, 9.0256e-03, 4.3259e-03, 4.9782e-03, 7.5493e-03,\n",
      "        4.2763e-03, 1.8139e-03, 5.9776e-03, 5.6076e-03, 2.5463e-03, 2.4052e-03,\n",
      "        8.6136e-03, 1.0025e-02, 7.6408e-03, 7.2746e-03, 1.6495e-02, 1.1787e-02,\n",
      "        8.1787e-03, 3.8815e-03, 4.8218e-03, 1.8072e-03, 2.0065e-02, 1.3153e-02,\n",
      "        1.5549e-02, 6.1073e-03, 8.0338e-03, 6.2370e-03, 2.1687e-03, 1.2192e-02,\n",
      "        9.3536e-03, 2.9469e-03, 2.2850e-03, 9.1248e-03, 1.6693e-02, 3.7289e-03,\n",
      "        1.2665e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [86] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [86] : torch.Size([1, 32, 1, 158])\n",
      "Last layer attentions for generated token [86] : tensor([1.2830e-01, 1.2805e-01, 1.5259e-04, 3.6955e-04, 1.5140e-04, 1.0509e-03,\n",
      "        2.3770e-04, 2.1684e-04, 1.8334e-04, 3.2377e-04, 1.3933e-03, 2.8062e-04,\n",
      "        6.6519e-04, 1.1501e-03, 7.9193e-03, 5.9814e-03, 1.2648e-04, 7.2956e-04,\n",
      "        1.6761e-04, 2.6417e-04, 6.4135e-05, 3.2997e-04, 7.1220e-03, 1.8864e-03,\n",
      "        2.7599e-03, 7.9575e-03, 5.6362e-04, 2.1434e-04, 3.6025e-04, 2.4962e-04,\n",
      "        5.9748e-04, 8.3113e-04, 7.8249e-04, 2.6274e-04, 1.6558e-04, 2.3818e-04,\n",
      "        2.1863e-04, 2.7299e-04, 2.0659e-04, 2.5058e-04, 1.8387e-03, 1.4601e-03,\n",
      "        4.3559e-04, 4.9543e-04, 3.2997e-04, 4.3988e-04, 1.0651e-04, 2.3675e-04,\n",
      "        4.3726e-04, 1.8177e-03, 1.1911e-03, 1.2600e-04, 1.7967e-03, 2.9163e-03,\n",
      "        1.6546e-03, 3.7289e-03, 4.3559e-04, 1.1606e-03, 2.8095e-03, 8.6975e-04,\n",
      "        1.4267e-03, 9.5129e-04, 1.9312e-03, 8.9550e-04, 5.9080e-04, 5.1832e-04,\n",
      "        1.7633e-03, 1.4648e-03, 1.0405e-03, 9.9869e-03, 6.1340e-03, 8.8120e-03,\n",
      "        2.8934e-03, 1.2188e-03, 4.5891e-03, 6.0310e-03, 5.1956e-03, 1.6403e-03,\n",
      "        7.4348e-03, 1.0216e-02, 3.9749e-03, 2.2812e-03, 2.4014e-03, 2.5501e-03,\n",
      "        2.5539e-03, 2.2144e-03, 4.1962e-03, 2.8744e-03, 1.4925e-03, 4.5280e-03,\n",
      "        4.2267e-03, 5.2567e-03, 7.3586e-03, 5.2719e-03, 3.7231e-03, 4.2763e-03,\n",
      "        5.6419e-03, 5.2071e-03, 6.6071e-03, 3.9787e-03, 3.2082e-03, 2.6321e-03,\n",
      "        3.3245e-03, 9.4748e-04, 3.4599e-03, 1.9989e-03, 8.3237e-03, 6.9695e-03,\n",
      "        3.4218e-03, 6.4240e-03, 2.1477e-03, 6.1989e-03, 4.2839e-03, 2.6569e-03,\n",
      "        7.7896e-03, 6.3591e-03, 9.4910e-03, 6.3667e-03, 7.3700e-03, 1.2108e-02,\n",
      "        4.0558e-02, 1.9989e-02, 1.1658e-02, 9.8038e-03, 7.2937e-03, 1.1055e-02,\n",
      "        8.2474e-03, 6.4468e-03, 1.2451e-02, 6.5231e-03, 3.2940e-03, 5.1842e-03,\n",
      "        1.0017e-02, 1.7624e-02, 1.6403e-02, 6.3171e-03, 1.8723e-02, 2.0737e-02,\n",
      "        9.7504e-03, 4.9019e-03, 5.0621e-03, 1.7128e-03, 8.8501e-03, 1.5915e-02,\n",
      "        2.8107e-02, 6.9580e-03, 1.3100e-02, 4.7760e-03, 2.2144e-03, 5.1613e-03,\n",
      "        5.9967e-03, 6.4430e-03, 1.8997e-03, 6.2790e-03, 2.8076e-02, 7.9727e-03,\n",
      "        8.2245e-03, 8.0948e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [87] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [87] : torch.Size([1, 32, 1, 159])\n",
      "Last layer attentions for generated token [87] : tensor([2.0923e-01, 2.0886e-01, 1.8489e-04, 2.2840e-04, 1.6713e-04, 9.9373e-04,\n",
      "        1.3268e-04, 2.2042e-04, 2.6393e-04, 2.5272e-04, 8.9979e-04, 3.0613e-04,\n",
      "        5.4693e-04, 1.1463e-03, 5.0392e-03, 4.3564e-03, 1.0705e-04, 4.1437e-04,\n",
      "        1.3530e-04, 3.1090e-04, 4.9591e-05, 3.1590e-04, 7.2670e-03, 1.5917e-03,\n",
      "        1.5392e-03, 5.7526e-03, 3.3498e-04, 1.6189e-04, 2.1708e-04, 2.4796e-04,\n",
      "        4.6134e-04, 6.5565e-04, 7.5626e-04, 3.2592e-04, 1.9455e-04, 1.3852e-04,\n",
      "        2.0790e-04, 3.0494e-04, 1.1259e-04, 1.4806e-04, 1.4954e-03, 6.2943e-04,\n",
      "        2.5868e-04, 2.7990e-04, 3.4213e-04, 2.1207e-04, 1.9991e-04, 1.4341e-04,\n",
      "        2.7990e-04, 9.8991e-04, 5.4455e-04, 1.8930e-04, 1.9684e-03, 2.7866e-03,\n",
      "        1.1892e-03, 3.0937e-03, 2.4891e-04, 5.2071e-04, 1.4048e-03, 5.7411e-04,\n",
      "        7.0047e-04, 7.8154e-04, 1.7271e-03, 6.5041e-04, 7.3576e-04, 5.0354e-04,\n",
      "        7.2861e-04, 1.0376e-03, 6.6614e-04, 7.9117e-03, 4.7989e-03, 6.1111e-03,\n",
      "        2.0504e-03, 1.0538e-03, 3.1776e-03, 5.5122e-03, 2.9793e-03, 1.7109e-03,\n",
      "        6.9580e-03, 4.7684e-03, 2.3918e-03, 1.1730e-03, 2.1553e-03, 3.0651e-03,\n",
      "        1.1148e-03, 1.3037e-03, 2.8954e-03, 1.6232e-03, 1.5602e-03, 3.4885e-03,\n",
      "        2.4509e-03, 3.1681e-03, 7.6332e-03, 2.5558e-03, 4.8637e-03, 7.9651e-03,\n",
      "        7.9498e-03, 3.4161e-03, 5.1575e-03, 2.5654e-03, 2.3193e-03, 2.2831e-03,\n",
      "        5.7297e-03, 9.7275e-04, 3.1242e-03, 2.5082e-03, 1.2665e-02, 1.0216e-02,\n",
      "        2.3479e-03, 7.4120e-03, 2.3270e-03, 7.4921e-03, 2.2163e-03, 2.2144e-03,\n",
      "        7.5226e-03, 2.8057e-03, 5.9547e-03, 6.6147e-03, 3.4657e-03, 1.1223e-02,\n",
      "        2.3895e-02, 1.3290e-02, 7.9269e-03, 3.8242e-03, 8.4229e-03, 8.2550e-03,\n",
      "        7.9727e-03, 2.9507e-03, 7.0915e-03, 4.2038e-03, 1.6232e-03, 1.9684e-03,\n",
      "        7.5569e-03, 1.1749e-02, 7.0381e-03, 3.0231e-03, 1.0368e-02, 1.3626e-02,\n",
      "        7.6981e-03, 2.7256e-03, 4.6959e-03, 1.3065e-03, 8.0872e-03, 1.2825e-02,\n",
      "        2.4490e-02, 7.9651e-03, 9.3689e-03, 3.4790e-03, 1.6384e-03, 4.0588e-03,\n",
      "        6.2675e-03, 6.7673e-03, 2.9697e-03, 1.0391e-02, 2.1622e-02, 3.8700e-03,\n",
      "        5.8823e-03, 6.8703e-03, 5.8250e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [88] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [88] : torch.Size([1, 32, 1, 160])\n",
      "Last layer attentions for generated token [88] : tensor([2.0764e-01, 2.0764e-01, 1.4722e-04, 3.8695e-04, 1.1784e-04, 1.2369e-03,\n",
      "        1.6415e-04, 3.0136e-04, 5.1975e-04, 3.1662e-04, 8.8406e-04, 4.8256e-04,\n",
      "        4.6968e-04, 1.0138e-03, 1.0277e-02, 4.4823e-03, 2.0754e-04, 3.6287e-04,\n",
      "        8.7202e-05, 2.0194e-04, 1.0854e-04, 3.1090e-04, 1.1566e-02, 2.3155e-03,\n",
      "        6.9733e-03, 7.9727e-03, 4.2272e-04, 1.4043e-04, 2.9850e-04, 2.1505e-04,\n",
      "        4.1604e-04, 7.6389e-04, 4.4298e-04, 1.2106e-04, 1.4377e-04, 2.4986e-04,\n",
      "        4.7421e-04, 3.1352e-04, 1.5795e-04, 2.0516e-04, 2.6741e-03, 8.7547e-04,\n",
      "        4.5872e-04, 2.7704e-04, 2.3067e-04, 3.9244e-04, 1.3769e-04, 3.1972e-04,\n",
      "        3.4571e-04, 2.0809e-03, 1.0920e-03, 9.1732e-05, 2.7905e-03, 2.5692e-03,\n",
      "        1.7176e-03, 2.2087e-03, 2.8920e-04, 8.9836e-04, 3.4428e-03, 7.4911e-04,\n",
      "        1.4238e-03, 8.8930e-04, 3.8185e-03, 1.5335e-03, 1.0481e-03, 1.4639e-03,\n",
      "        1.2865e-03, 7.0763e-04, 2.3670e-03, 1.0155e-02, 4.6997e-03, 4.6654e-03,\n",
      "        2.0123e-03, 9.1600e-04, 2.9507e-03, 3.6945e-03, 2.6531e-03, 1.8034e-03,\n",
      "        3.3703e-03, 2.4147e-03, 1.5039e-03, 1.1139e-03, 7.7868e-04, 9.9182e-04,\n",
      "        6.0558e-04, 9.2316e-04, 1.2255e-03, 6.5327e-04, 4.1938e-04, 1.4496e-03,\n",
      "        1.8253e-03, 2.6627e-03, 4.0970e-03, 2.4471e-03, 4.6921e-03, 3.2063e-03,\n",
      "        5.4436e-03, 1.8396e-03, 4.1199e-03, 1.1950e-03, 1.6613e-03, 1.0023e-03,\n",
      "        1.7672e-03, 4.0793e-04, 1.4296e-03, 9.3555e-04, 7.3128e-03, 3.6144e-03,\n",
      "        1.1854e-03, 3.8261e-03, 1.3828e-03, 3.4981e-03, 1.6584e-03, 1.0319e-03,\n",
      "        9.4528e-03, 1.8024e-03, 2.6455e-03, 6.7558e-03, 3.4370e-03, 2.0157e-02,\n",
      "        1.2726e-02, 9.3613e-03, 5.8975e-03, 1.7281e-03, 3.2444e-03, 3.9825e-03,\n",
      "        1.5945e-03, 1.3533e-03, 4.7531e-03, 4.2801e-03, 1.6775e-03, 3.1757e-03,\n",
      "        1.3695e-02, 5.8556e-03, 8.2169e-03, 7.3662e-03, 1.7792e-02, 1.2764e-02,\n",
      "        8.7051e-03, 5.3329e-03, 7.8430e-03, 3.3474e-03, 2.6474e-02, 1.1871e-02,\n",
      "        1.3870e-02, 6.2981e-03, 1.4633e-02, 7.9575e-03, 2.5311e-03, 1.3268e-02,\n",
      "        9.3842e-03, 7.4730e-03, 5.1155e-03, 8.8120e-03, 1.5274e-02, 6.8550e-03,\n",
      "        1.7883e-02, 1.2207e-02, 6.1073e-03, 5.0049e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [89] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [89] : torch.Size([1, 32, 1, 161])\n",
      "Last layer attentions for generated token [89] : tensor([2.6489e-01, 2.6489e-01, 2.0099e-04, 4.2796e-04, 1.2982e-04, 1.8883e-03,\n",
      "        1.2052e-04, 3.3665e-04, 9.0075e-04, 4.9257e-04, 7.3099e-04, 7.9823e-04,\n",
      "        2.6941e-04, 7.1526e-04, 1.1063e-02, 4.2229e-03, 3.9268e-04, 3.2377e-04,\n",
      "        1.0467e-04, 2.3961e-04, 2.5702e-04, 8.9359e-04, 1.2299e-02, 2.5063e-03,\n",
      "        7.4959e-03, 8.8425e-03, 5.2452e-04, 1.4997e-04, 2.7966e-04, 4.2558e-04,\n",
      "        5.7030e-04, 1.2579e-03, 5.7364e-04, 1.4472e-04, 1.9789e-04, 1.7262e-04,\n",
      "        2.8849e-04, 5.2977e-04, 1.6475e-04, 1.6022e-04, 3.0956e-03, 5.2738e-04,\n",
      "        3.5477e-04, 2.9659e-04, 3.0112e-04, 2.5821e-04, 3.2806e-04, 4.1723e-04,\n",
      "        2.8515e-04, 2.6455e-03, 9.7752e-04, 1.3816e-04, 2.7523e-03, 3.0270e-03,\n",
      "        1.1501e-03, 2.5311e-03, 2.0254e-04, 8.5640e-04, 3.1681e-03, 7.8726e-04,\n",
      "        9.7227e-04, 7.5102e-04, 4.6425e-03, 1.2388e-03, 6.1798e-04, 1.5383e-03,\n",
      "        7.9489e-04, 6.2656e-04, 2.1534e-03, 5.1842e-03, 3.2444e-03, 3.8490e-03,\n",
      "        8.4639e-04, 7.9012e-04, 2.5482e-03, 1.9178e-03, 2.4948e-03, 1.2312e-03,\n",
      "        3.9444e-03, 1.6632e-03, 1.3924e-03, 4.6372e-04, 5.6267e-04, 8.3494e-04,\n",
      "        2.5511e-04, 3.5763e-04, 6.0606e-04, 3.3855e-04, 1.5116e-04, 5.3692e-04,\n",
      "        7.4387e-04, 9.4938e-04, 1.9875e-03, 8.9025e-04, 5.1079e-03, 3.4943e-03,\n",
      "        5.7259e-03, 8.6117e-04, 2.3098e-03, 6.2990e-04, 5.3787e-04, 6.3896e-04,\n",
      "        1.8129e-03, 2.9349e-04, 9.1839e-04, 6.6566e-04, 6.6986e-03, 2.0103e-03,\n",
      "        5.6601e-04, 1.9760e-03, 7.8106e-04, 2.5272e-03, 6.2656e-04, 2.8849e-04,\n",
      "        6.3782e-03, 7.9823e-04, 9.2602e-04, 5.6572e-03, 1.3657e-03, 1.0490e-02,\n",
      "        4.5853e-03, 5.1689e-03, 2.5730e-03, 6.5756e-04, 2.1286e-03, 1.7090e-03,\n",
      "        8.9359e-04, 3.6836e-04, 2.4033e-03, 2.2640e-03, 7.1001e-04, 9.4223e-04,\n",
      "        9.1782e-03, 2.6913e-03, 3.0537e-03, 5.7945e-03, 1.0033e-02, 6.4163e-03,\n",
      "        4.8256e-03, 3.2406e-03, 6.4011e-03, 2.6035e-03, 3.7598e-02, 6.4545e-03,\n",
      "        6.4468e-03, 4.5319e-03, 1.4122e-02, 1.1734e-02, 5.0926e-03, 2.4292e-02,\n",
      "        1.2642e-02, 3.6259e-03, 6.4774e-03, 1.0010e-02, 8.8654e-03, 2.8934e-03,\n",
      "        1.5839e-02, 7.6332e-03, 3.5839e-03, 4.6005e-03, 4.8370e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [90] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [90] : torch.Size([1, 32, 1, 162])\n",
      "Last layer attentions for generated token [90] : tensor([3.5913e-01, 3.5840e-01, 4.6897e-04, 4.9162e-04, 2.3043e-04, 2.6417e-03,\n",
      "        1.2481e-04, 2.2590e-04, 5.2738e-04, 5.2118e-04, 6.8903e-04, 1.4138e-04,\n",
      "        1.5593e-04, 2.7347e-04, 2.1610e-03, 2.4719e-03, 1.1766e-04, 1.7059e-04,\n",
      "        9.4175e-05, 1.3232e-04, 5.1379e-05, 6.2847e-04, 2.0950e-02, 1.5049e-03,\n",
      "        1.1320e-03, 5.7640e-03, 1.8013e-04, 8.6427e-05, 1.6594e-04, 1.9324e-04,\n",
      "        3.5882e-04, 3.7169e-04, 1.5771e-04, 6.0320e-05, 5.9605e-05, 8.7082e-05,\n",
      "        3.8362e-04, 1.0550e-04, 8.5413e-05, 6.7830e-05, 3.5691e-04, 2.9111e-04,\n",
      "        9.6440e-05, 5.7578e-05, 4.0174e-05, 7.9930e-05, 7.9334e-05, 3.5191e-04,\n",
      "        1.2231e-04, 2.5511e-04, 4.8494e-04, 4.2140e-05, 2.1992e-03, 1.2999e-03,\n",
      "        1.0633e-03, 1.4877e-03, 5.3048e-05, 5.5695e-04, 1.8806e-03, 3.3593e-04,\n",
      "        4.5276e-04, 3.0875e-04, 1.6432e-03, 5.4216e-04, 1.5223e-04, 2.4438e-04,\n",
      "        5.0735e-04, 1.8370e-04, 1.7090e-03, 8.4925e-04, 1.1654e-03, 2.4338e-03,\n",
      "        3.0637e-04, 4.0197e-04, 4.4060e-04, 7.4816e-04, 8.1682e-04, 6.1893e-04,\n",
      "        8.8120e-04, 1.5650e-03, 9.9468e-04, 1.6272e-04, 2.2411e-04, 2.8110e-04,\n",
      "        1.8156e-04, 1.3697e-04, 2.1136e-04, 1.6534e-04, 1.0467e-04, 3.0994e-04,\n",
      "        4.6897e-04, 3.4189e-04, 5.3453e-04, 9.3842e-04, 1.4219e-03, 5.5361e-04,\n",
      "        1.8730e-03, 1.0042e-03, 7.8869e-04, 5.0306e-04, 1.5402e-04, 3.4928e-04,\n",
      "        6.2990e-04, 2.7156e-04, 4.9353e-04, 4.5276e-04, 1.1187e-03, 9.9087e-04,\n",
      "        7.1383e-04, 7.0286e-04, 3.2878e-04, 8.7261e-04, 5.8031e-04, 1.2767e-04,\n",
      "        1.5049e-03, 6.2370e-04, 7.0429e-04, 1.1539e-03, 1.4648e-03, 3.5477e-03,\n",
      "        2.1820e-03, 1.6432e-03, 1.1721e-03, 9.8133e-04, 1.0490e-03, 5.6458e-04,\n",
      "        7.3338e-04, 3.7098e-04, 9.8324e-04, 2.8267e-03, 1.5965e-03, 1.5898e-03,\n",
      "        5.0583e-03, 4.9286e-03, 1.3733e-03, 1.8444e-03, 6.0043e-03, 3.1071e-03,\n",
      "        1.7977e-03, 2.2469e-03, 2.5311e-03, 2.5978e-03, 1.5015e-02, 3.6888e-03,\n",
      "        2.5654e-03, 1.5049e-03, 7.1411e-03, 2.7828e-03, 2.8210e-03, 3.4790e-03,\n",
      "        5.6038e-03, 3.5858e-03, 2.1687e-03, 5.1918e-03, 1.5717e-02, 3.0861e-03,\n",
      "        7.6065e-03, 1.2085e-02, 8.3542e-03, 5.8823e-03, 9.7580e-03, 1.8219e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [91] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [91] : torch.Size([1, 32, 1, 163])\n",
      "Last layer attentions for generated token [91] : tensor([2.6245e-01, 2.6196e-01, 7.9536e-04, 1.0252e-03, 4.9782e-04, 4.6539e-03,\n",
      "        3.1161e-04, 6.7377e-04, 5.6744e-04, 3.9220e-04, 1.3456e-03, 3.6407e-04,\n",
      "        6.0415e-04, 5.4359e-04, 7.3586e-03, 5.3978e-03, 1.1867e-04, 2.3699e-04,\n",
      "        1.3828e-04, 2.1923e-04, 1.1414e-04, 1.1683e-03, 2.8854e-02, 3.1242e-03,\n",
      "        2.6970e-03, 6.9046e-03, 2.8253e-04, 6.5565e-05, 1.6999e-04, 1.7333e-04,\n",
      "        3.5024e-04, 6.9523e-04, 2.1660e-04, 8.6844e-05, 1.0073e-04, 7.2300e-05,\n",
      "        4.0007e-04, 2.7609e-04, 1.1373e-04, 7.9989e-05, 6.6090e-04, 6.2943e-04,\n",
      "        2.4462e-04, 1.1504e-04, 6.6340e-05, 1.0115e-04, 1.0723e-04, 4.2582e-04,\n",
      "        1.4210e-04, 6.3038e-04, 1.2245e-03, 2.8372e-04, 2.4509e-03, 2.4643e-03,\n",
      "        1.9197e-03, 2.7981e-03, 1.2153e-04, 5.2166e-04, 2.4261e-03, 5.4359e-04,\n",
      "        6.9809e-04, 5.2977e-04, 3.5820e-03, 7.3004e-04, 2.2352e-04, 6.3038e-04,\n",
      "        6.9809e-04, 3.5787e-04, 2.3518e-03, 4.0512e-03, 3.8548e-03, 3.3817e-03,\n",
      "        7.4291e-04, 6.0415e-04, 1.4343e-03, 1.5421e-03, 1.7338e-03, 1.1044e-03,\n",
      "        1.8826e-03, 3.8509e-03, 3.0918e-03, 1.7338e-03, 9.3555e-04, 1.7443e-03,\n",
      "        3.5238e-04, 1.0395e-03, 7.1144e-04, 4.2152e-04, 2.5535e-04, 9.9945e-04,\n",
      "        9.5367e-04, 8.6021e-04, 1.6737e-03, 1.6975e-03, 2.7981e-03, 1.7853e-03,\n",
      "        6.6566e-03, 2.3479e-03, 3.4142e-03, 1.6127e-03, 1.1892e-03, 1.2732e-03,\n",
      "        2.0828e-03, 4.6587e-04, 1.2054e-03, 6.8188e-04, 5.6763e-03, 2.5444e-03,\n",
      "        1.6613e-03, 2.1534e-03, 9.5224e-04, 1.7614e-03, 1.0252e-03, 2.1410e-04,\n",
      "        5.9357e-03, 1.1806e-03, 9.9754e-04, 1.7338e-03, 1.9836e-03, 7.3280e-03,\n",
      "        3.7994e-03, 3.1986e-03, 3.2463e-03, 1.0519e-03, 3.2749e-03, 1.5125e-03,\n",
      "        1.9913e-03, 4.5776e-04, 1.2007e-03, 4.4861e-03, 1.6317e-03, 1.7204e-03,\n",
      "        9.9869e-03, 5.6152e-03, 1.6165e-03, 1.2665e-03, 7.6942e-03, 4.3449e-03,\n",
      "        2.4223e-03, 2.4719e-03, 2.2316e-03, 1.8349e-03, 3.2196e-02, 6.2752e-03,\n",
      "        8.8196e-03, 5.0468e-03, 7.4081e-03, 2.8706e-03, 1.7614e-03, 5.2567e-03,\n",
      "        1.1108e-02, 4.0665e-03, 3.9864e-03, 1.1749e-02, 2.5360e-02, 4.6349e-03,\n",
      "        8.2245e-03, 2.4612e-02, 9.1934e-03, 5.2032e-03, 6.8550e-03, 1.0582e-02,\n",
      "        1.3191e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [92] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [92] : torch.Size([1, 32, 1, 164])\n",
      "Last layer attentions for generated token [92] : tensor([2.6074e-01, 2.6074e-01, 3.7336e-04, 6.8808e-04, 2.3675e-04, 3.2692e-03,\n",
      "        1.5712e-04, 2.7156e-04, 9.7942e-04, 9.1839e-04, 1.2798e-03, 7.8583e-04,\n",
      "        3.3712e-04, 7.2527e-04, 1.4397e-02, 7.1487e-03, 5.9414e-04, 4.8780e-04,\n",
      "        1.6022e-04, 3.3069e-04, 3.4189e-04, 1.3657e-03, 2.1362e-02, 3.0384e-03,\n",
      "        1.0803e-02, 1.1711e-02, 7.5245e-04, 2.4533e-04, 2.8968e-04, 3.2377e-04,\n",
      "        7.4100e-04, 1.3056e-03, 3.8290e-04, 1.0550e-04, 1.7595e-04, 2.3127e-04,\n",
      "        3.1805e-04, 6.3992e-04, 1.4472e-04, 9.3818e-05, 2.8152e-03, 5.8937e-04,\n",
      "        3.3450e-04, 2.4533e-04, 1.9550e-04, 1.8811e-04, 1.9634e-04, 4.5013e-04,\n",
      "        2.0170e-04, 2.7199e-03, 1.1883e-03, 1.9407e-04, 2.9755e-03, 2.6112e-03,\n",
      "        1.5593e-03, 2.4319e-03, 2.7156e-04, 9.1314e-04, 3.2272e-03, 7.4100e-04,\n",
      "        8.6784e-04, 5.3883e-04, 3.6755e-03, 1.0862e-03, 6.3610e-04, 1.4706e-03,\n",
      "        9.1839e-04, 5.8603e-04, 2.3365e-03, 3.8509e-03, 3.5515e-03, 4.7684e-03,\n",
      "        5.5170e-04, 4.9925e-04, 1.6060e-03, 1.7529e-03, 2.9736e-03, 1.0738e-03,\n",
      "        2.8706e-03, 2.0618e-03, 1.9588e-03, 4.8780e-04, 5.1022e-04, 9.4557e-04,\n",
      "        2.9135e-04, 3.2878e-04, 4.9257e-04, 2.8110e-04, 1.9860e-04, 4.9925e-04,\n",
      "        6.7472e-04, 9.8133e-04, 1.5354e-03, 1.2007e-03, 4.0092e-03, 2.3956e-03,\n",
      "        7.3128e-03, 1.0490e-03, 2.5463e-03, 7.4673e-04, 3.6311e-04, 4.3726e-04,\n",
      "        1.5774e-03, 2.4915e-04, 7.9632e-04, 4.7731e-04, 6.2408e-03, 1.7948e-03,\n",
      "        8.5115e-04, 1.3390e-03, 6.3229e-04, 2.6474e-03, 6.9475e-04, 1.6594e-04,\n",
      "        6.5041e-03, 9.3842e-04, 6.3848e-04, 4.6844e-03, 1.2531e-03, 8.4686e-03,\n",
      "        6.9923e-03, 6.8016e-03, 2.8801e-03, 8.9741e-04, 1.9627e-03, 1.5259e-03,\n",
      "        9.6083e-04, 2.4438e-04, 1.4143e-03, 2.7580e-03, 9.1505e-04, 8.5115e-04,\n",
      "        7.5302e-03, 3.7880e-03, 2.7771e-03, 4.1046e-03, 8.9798e-03, 7.5912e-03,\n",
      "        4.2229e-03, 2.8400e-03, 3.7441e-03, 1.8806e-03, 4.2053e-02, 6.2294e-03,\n",
      "        5.6610e-03, 2.7199e-03, 8.5678e-03, 6.1264e-03, 2.6112e-03, 1.1223e-02,\n",
      "        9.7656e-03, 1.9064e-03, 3.6964e-03, 8.6823e-03, 1.3489e-02, 3.0670e-03,\n",
      "        1.0353e-02, 9.6741e-03, 6.1569e-03, 5.7106e-03, 5.9128e-03, 5.8060e-03,\n",
      "        2.0981e-03, 1.2955e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [93] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [93] : torch.Size([1, 32, 1, 165])\n",
      "Last layer attentions for generated token [93] : tensor([3.0713e-01, 3.0713e-01, 2.5654e-04, 4.3368e-04, 1.8477e-04, 2.0065e-03,\n",
      "        7.4089e-05, 2.0528e-04, 5.6696e-04, 3.9887e-04, 7.0810e-04, 3.0231e-04,\n",
      "        5.2118e-04, 6.1750e-04, 3.9062e-03, 3.4256e-03, 1.3566e-04, 1.2648e-04,\n",
      "        6.3598e-05, 1.9133e-04, 1.1474e-04, 9.2173e-04, 2.0615e-02, 2.1439e-03,\n",
      "        2.4033e-03, 3.2520e-03, 1.8549e-04, 5.5492e-05, 1.8048e-04, 1.9586e-04,\n",
      "        3.8290e-04, 5.7602e-04, 1.6427e-04, 6.5386e-05, 8.0109e-05, 1.2600e-04,\n",
      "        2.0373e-04, 1.8406e-04, 5.6326e-05, 7.7903e-05, 8.1205e-04, 2.6774e-04,\n",
      "        9.7334e-05, 5.5492e-05, 5.2094e-05, 1.0443e-04, 1.2350e-04, 1.8048e-04,\n",
      "        1.2600e-04, 7.1669e-04, 5.5170e-04, 8.8632e-05, 2.4223e-03, 1.0147e-03,\n",
      "        9.8896e-04, 1.3390e-03, 1.4389e-04, 4.0841e-04, 1.7052e-03, 4.7088e-04,\n",
      "        9.4748e-04, 4.4417e-04, 2.5635e-03, 8.8835e-04, 4.0674e-04, 6.7997e-04,\n",
      "        5.1928e-04, 2.0289e-04, 2.0142e-03, 1.5869e-03, 1.6594e-03, 2.1782e-03,\n",
      "        2.1017e-04, 4.2367e-04, 1.0042e-03, 7.5102e-04, 1.1206e-03, 1.0738e-03,\n",
      "        1.3866e-03, 1.0147e-03, 9.9277e-04, 3.5763e-04, 3.8505e-04, 6.6519e-04,\n",
      "        1.6236e-04, 2.7418e-04, 3.5334e-04, 2.1935e-04, 1.7488e-04, 4.7636e-04,\n",
      "        6.6662e-04, 6.6137e-04, 1.0567e-03, 7.3481e-04, 2.0027e-03, 1.1978e-03,\n",
      "        4.2419e-03, 8.8501e-04, 2.1057e-03, 5.3692e-04, 2.7514e-04, 4.5466e-04,\n",
      "        1.4305e-03, 2.7943e-04, 9.1648e-04, 4.1223e-04, 3.2024e-03, 1.3790e-03,\n",
      "        8.8835e-04, 7.7772e-04, 4.7088e-04, 1.3075e-03, 5.1403e-04, 1.3566e-04,\n",
      "        4.7112e-03, 6.9857e-04, 5.2118e-04, 3.4924e-03, 1.0471e-03, 6.0577e-03,\n",
      "        2.7199e-03, 2.0580e-03, 1.1253e-03, 4.2629e-04, 7.4673e-04, 4.9067e-04,\n",
      "        5.4073e-04, 1.8120e-04, 1.1721e-03, 2.6550e-03, 6.5613e-04, 1.1835e-03,\n",
      "        5.9891e-03, 2.9068e-03, 2.1858e-03, 2.7523e-03, 6.0883e-03, 5.1689e-03,\n",
      "        2.6932e-03, 2.3499e-03, 3.1834e-03, 2.8648e-03, 3.4546e-02, 4.6539e-03,\n",
      "        4.8904e-03, 3.3760e-03, 8.8348e-03, 5.8365e-03, 2.6989e-03, 6.2599e-03,\n",
      "        5.6686e-03, 2.7580e-03, 4.1122e-03, 8.5449e-03, 1.4839e-02, 3.0804e-03,\n",
      "        1.0300e-02, 1.0345e-02, 1.0460e-02, 6.6185e-03, 7.4615e-03, 8.6365e-03,\n",
      "        4.6043e-03, 1.5106e-02, 2.5986e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [94] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [94] : torch.Size([1, 32, 1, 166])\n",
      "Last layer attentions for generated token [94] : tensor([3.4204e-01, 3.4058e-01, 1.7834e-04, 2.9516e-04, 1.1516e-04, 1.7357e-03,\n",
      "        4.8399e-05, 1.0204e-04, 4.1556e-04, 2.5058e-04, 4.7731e-04, 3.3593e-04,\n",
      "        3.8433e-04, 4.4322e-04, 3.7003e-03, 2.4223e-03, 1.5140e-04, 9.5844e-05,\n",
      "        5.9068e-05, 1.1426e-04, 9.8109e-05, 8.2016e-04, 1.3062e-02, 1.3237e-03,\n",
      "        2.4242e-03, 2.4681e-03, 1.1122e-04, 3.1114e-05, 1.2648e-04, 1.1033e-04,\n",
      "        2.6059e-04, 4.1246e-04, 1.1122e-04, 3.6120e-05, 4.4942e-05, 7.0691e-05,\n",
      "        1.0777e-04, 1.1384e-04, 4.2379e-05, 4.4763e-05, 5.2547e-04, 1.9896e-04,\n",
      "        6.3121e-05, 3.8564e-05, 3.1114e-05, 6.5088e-05, 6.4135e-05, 1.2648e-04,\n",
      "        8.9347e-05, 5.0545e-04, 4.0269e-04, 6.2108e-05, 1.8520e-03, 7.9155e-04,\n",
      "        7.0715e-04, 8.8310e-04, 9.4354e-05, 3.0756e-04, 1.3418e-03, 3.4046e-04,\n",
      "        7.0953e-04, 2.5964e-04, 1.9627e-03, 6.7568e-04, 3.1805e-04, 4.2129e-04,\n",
      "        3.9434e-04, 1.3947e-04, 1.8196e-03, 1.0548e-03, 8.6975e-04, 1.3733e-03,\n",
      "        9.3639e-05, 3.0947e-04, 5.7364e-04, 3.5477e-04, 6.9618e-04, 6.7568e-04,\n",
      "        7.5102e-04, 7.0810e-04, 6.8378e-04, 2.7847e-04, 2.4283e-04, 3.0828e-04,\n",
      "        1.0008e-04, 2.2292e-04, 2.9302e-04, 1.8692e-04, 9.1136e-05, 1.8764e-04,\n",
      "        4.6444e-04, 3.0351e-04, 5.1737e-04, 3.9959e-04, 1.2007e-03, 7.0953e-04,\n",
      "        3.1223e-03, 5.4646e-04, 1.5383e-03, 3.9339e-04, 2.1946e-04, 3.7336e-04,\n",
      "        8.4305e-04, 2.1434e-04, 6.0701e-04, 3.2949e-04, 1.9550e-03, 8.7643e-04,\n",
      "        6.4611e-04, 3.8147e-04, 3.1137e-04, 7.0000e-04, 3.1424e-04, 4.9710e-05,\n",
      "        2.4605e-03, 3.1185e-04, 2.7299e-04, 1.9484e-03, 5.9509e-04, 3.8700e-03,\n",
      "        1.4257e-03, 1.2264e-03, 5.9175e-04, 2.2638e-04, 4.7469e-04, 3.3784e-04,\n",
      "        2.4283e-04, 1.0484e-04, 6.7043e-04, 1.6499e-03, 4.0984e-04, 6.6805e-04,\n",
      "        4.1542e-03, 1.4114e-03, 1.2951e-03, 1.9026e-03, 2.8839e-03, 2.3460e-03,\n",
      "        1.4362e-03, 1.4420e-03, 1.9522e-03, 2.7828e-03, 3.6285e-02, 3.9406e-03,\n",
      "        3.8967e-03, 2.5692e-03, 8.8272e-03, 6.3896e-03, 3.1319e-03, 8.1635e-03,\n",
      "        3.9406e-03, 1.4906e-03, 3.7403e-03, 6.5804e-03, 1.0475e-02, 2.4014e-03,\n",
      "        8.7509e-03, 4.7188e-03, 4.4632e-03, 4.1237e-03, 5.6686e-03, 8.4534e-03,\n",
      "        4.8027e-03, 1.1162e-02, 2.9938e-02, 1.8906e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [95] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [95] : torch.Size([1, 32, 1, 167])\n",
      "Last layer attentions for generated token [95] : tensor([1.9434e-01, 1.9397e-01, 5.3310e-04, 8.2588e-04, 5.3930e-04, 1.7796e-03,\n",
      "        1.5330e-04, 2.6345e-04, 3.6931e-04, 5.6553e-04, 1.1358e-03, 3.2163e-04,\n",
      "        3.0684e-04, 6.7282e-04, 1.3046e-02, 7.4730e-03, 1.7726e-04, 7.4005e-04,\n",
      "        2.1887e-04, 2.6488e-04, 2.2137e-04, 1.5373e-03, 9.2773e-03, 3.6411e-03,\n",
      "        5.9586e-03, 1.4992e-02, 7.4482e-04, 3.3617e-04, 3.3426e-04, 3.9172e-04,\n",
      "        5.1880e-04, 1.3304e-03, 5.2166e-04, 2.1124e-04, 1.8430e-04, 1.9610e-04,\n",
      "        3.8552e-04, 3.4571e-04, 1.5163e-04, 1.4293e-04, 2.4986e-03, 1.1692e-03,\n",
      "        3.1972e-04, 5.6648e-04, 3.8171e-04, 3.8481e-04, 3.1352e-04, 5.6744e-04,\n",
      "        5.6314e-04, 1.7242e-03, 1.4725e-03, 5.8556e-04, 1.9388e-03, 8.3237e-03,\n",
      "        1.4439e-03, 4.9400e-03, 3.1781e-04, 1.8797e-03, 3.4027e-03, 1.1530e-03,\n",
      "        8.4066e-04, 6.5708e-04, 2.8400e-03, 1.4439e-03, 5.9843e-04, 1.1244e-03,\n",
      "        1.0262e-03, 1.0118e-03, 1.3590e-03, 6.4049e-03, 6.2599e-03, 7.0763e-03,\n",
      "        1.7109e-03, 6.9666e-04, 2.3174e-03, 2.6627e-03, 4.2114e-03, 1.2589e-03,\n",
      "        4.0321e-03, 7.3891e-03, 4.2992e-03, 6.1369e-04, 7.3433e-04, 1.4839e-03,\n",
      "        5.4073e-04, 4.5514e-04, 1.2541e-03, 5.3930e-04, 4.0555e-04, 1.9426e-03,\n",
      "        7.7581e-04, 1.6327e-03, 3.1624e-03, 1.8921e-03, 3.3894e-03, 2.5921e-03,\n",
      "        6.3705e-03, 2.8915e-03, 3.0270e-03, 1.4668e-03, 5.8222e-04, 1.2207e-03,\n",
      "        4.1580e-03, 4.0245e-04, 1.4181e-03, 6.9952e-04, 5.3635e-03, 5.1079e-03,\n",
      "        6.8188e-04, 1.5764e-03, 5.7411e-04, 2.2850e-03, 1.1597e-03, 3.9697e-04,\n",
      "        9.9945e-03, 1.3275e-03, 1.7109e-03, 3.3493e-03, 1.5373e-03, 5.1041e-03,\n",
      "        1.0170e-02, 9.9411e-03, 6.8359e-03, 1.3275e-03, 2.9202e-03, 2.6512e-03,\n",
      "        2.7618e-03, 1.0462e-03, 3.1929e-03, 4.9171e-03, 1.1578e-03, 8.3876e-04,\n",
      "        9.2087e-03, 1.2032e-02, 4.0817e-03, 3.5000e-03, 1.2093e-02, 7.5569e-03,\n",
      "        4.2191e-03, 1.6108e-03, 4.0512e-03, 2.3823e-03, 1.6556e-02, 9.9182e-03,\n",
      "        1.7944e-02, 9.4223e-03, 1.0818e-02, 4.9400e-03, 2.4529e-03, 7.3509e-03,\n",
      "        1.0033e-02, 5.2605e-03, 3.5000e-03, 1.0803e-02, 3.6194e-02, 5.0087e-03,\n",
      "        1.6205e-02, 1.2657e-02, 7.9651e-03, 6.1493e-03, 4.1809e-03, 9.5444e-03,\n",
      "        2.3174e-03, 7.8201e-03, 7.8812e-03, 3.5934e-03, 2.6917e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [96] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [96] : torch.Size([1, 32, 1, 168])\n",
      "Last layer attentions for generated token [96] : tensor([2.4670e-01, 2.4622e-01, 1.5616e-04, 1.9658e-04, 1.1742e-04, 5.4407e-04,\n",
      "        1.1116e-04, 2.3901e-04, 1.9062e-04, 2.5654e-04, 7.4053e-04, 1.3304e-04,\n",
      "        1.9360e-04, 2.9993e-04, 4.4518e-03, 2.9240e-03, 7.4029e-05, 3.1614e-04,\n",
      "        1.0198e-04, 1.7488e-04, 4.2498e-05, 2.7680e-04, 4.2267e-03, 1.0405e-03,\n",
      "        1.3943e-03, 4.8294e-03, 3.8338e-04, 1.7488e-04, 2.1100e-04, 2.0051e-04,\n",
      "        3.5739e-04, 4.4584e-04, 5.9509e-04, 3.9411e-04, 2.0134e-04, 1.5497e-04,\n",
      "        1.0526e-04, 1.0777e-04, 1.1337e-04, 1.8692e-04, 8.6260e-04, 5.4169e-04,\n",
      "        1.7977e-04, 2.7633e-04, 3.1424e-04, 2.3723e-04, 1.5736e-04, 2.2113e-04,\n",
      "        4.0746e-04, 7.8249e-04, 6.3992e-04, 1.4102e-04, 1.7252e-03, 2.0275e-03,\n",
      "        1.0633e-03, 2.3785e-03, 1.8620e-04, 9.6416e-04, 1.8082e-03, 7.5674e-04,\n",
      "        6.8760e-04, 5.5695e-04, 1.5678e-03, 6.8092e-04, 3.8576e-04, 3.5119e-04,\n",
      "        6.3610e-04, 6.4850e-04, 6.9857e-04, 4.0016e-03, 2.5368e-03, 4.0169e-03,\n",
      "        1.2646e-03, 7.0238e-04, 1.6890e-03, 2.1896e-03, 3.0155e-03, 1.5287e-03,\n",
      "        4.5090e-03, 4.0817e-03, 2.3785e-03, 7.6580e-04, 1.5020e-03, 1.6594e-03,\n",
      "        7.0810e-04, 8.5926e-04, 1.7872e-03, 1.1120e-03, 5.4502e-04, 4.3411e-03,\n",
      "        2.5272e-03, 3.1490e-03, 4.0550e-03, 3.4618e-03, 3.1281e-03, 3.1738e-03,\n",
      "        5.1575e-03, 2.4033e-03, 2.3232e-03, 2.1420e-03, 1.7529e-03, 2.4757e-03,\n",
      "        6.8130e-03, 1.4334e-03, 2.7924e-03, 2.6951e-03, 7.4883e-03, 4.9782e-03,\n",
      "        1.7662e-03, 4.6997e-03, 1.7595e-03, 3.7727e-03, 1.6241e-03, 1.0319e-03,\n",
      "        7.6294e-03, 2.7676e-03, 3.6411e-03, 5.9128e-03, 3.2902e-03, 6.3896e-03,\n",
      "        1.1963e-02, 8.2016e-03, 5.2681e-03, 2.5082e-03, 4.7073e-03, 6.8703e-03,\n",
      "        7.1144e-03, 2.1877e-03, 7.5150e-03, 3.6030e-03, 1.5497e-03, 1.4668e-03,\n",
      "        7.2479e-03, 9.2773e-03, 5.5695e-03, 2.5692e-03, 7.8354e-03, 7.1030e-03,\n",
      "        4.4441e-03, 1.6851e-03, 1.6527e-03, 5.4407e-04, 4.1847e-03, 9.5825e-03,\n",
      "        1.9821e-02, 9.0561e-03, 1.2955e-02, 5.9814e-03, 1.7023e-03, 6.0120e-03,\n",
      "        7.7324e-03, 6.8703e-03, 3.5152e-03, 1.2985e-02, 2.8473e-02, 6.3362e-03,\n",
      "        9.9869e-03, 8.0566e-03, 6.1951e-03, 5.8632e-03, 3.0193e-03, 4.4785e-03,\n",
      "        5.4073e-04, 2.5005e-03, 1.2302e-03, 7.1669e-04, 7.2861e-03, 1.1040e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [97] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [97] : torch.Size([1, 32, 1, 169])\n",
      "Last layer attentions for generated token [97] : tensor([2.2607e-01, 2.2571e-01, 1.9634e-04, 2.2519e-04, 1.5295e-04, 7.8869e-04,\n",
      "        1.8167e-04, 3.9744e-04, 3.0661e-04, 4.4608e-04, 1.2827e-03, 2.4247e-04,\n",
      "        2.0337e-04, 2.8849e-04, 5.9891e-03, 3.8643e-03, 1.5593e-04, 4.7016e-04,\n",
      "        1.4651e-04, 2.0742e-04, 6.7353e-05, 5.7268e-04, 5.8708e-03, 1.5383e-03,\n",
      "        2.3861e-03, 6.3591e-03, 4.8327e-04, 1.9944e-04, 2.1565e-04, 3.2949e-04,\n",
      "        5.1546e-04, 6.7854e-04, 5.1117e-04, 4.3154e-04, 2.9087e-04, 2.9469e-04,\n",
      "        1.4770e-04, 2.1565e-04, 1.3602e-04, 1.6212e-04, 1.0233e-03, 5.5933e-04,\n",
      "        1.6928e-04, 3.3855e-04, 3.8218e-04, 1.6999e-04, 3.1686e-04, 3.3855e-04,\n",
      "        4.2319e-04, 1.1520e-03, 7.1001e-04, 1.4651e-04, 1.8721e-03, 2.3193e-03,\n",
      "        8.3971e-04, 2.4872e-03, 1.7333e-04, 1.1168e-03, 2.2812e-03, 1.0872e-03,\n",
      "        6.9237e-04, 1.0576e-03, 2.4014e-03, 8.8692e-04, 5.6171e-04, 8.0919e-04,\n",
      "        6.0844e-04, 7.2527e-04, 8.8167e-04, 4.2191e-03, 2.9564e-03, 4.5090e-03,\n",
      "        1.1683e-03, 8.6498e-04, 2.4796e-03, 3.4714e-03, 2.8820e-03, 1.5268e-03,\n",
      "        6.2485e-03, 3.4904e-03, 2.2583e-03, 7.1573e-04, 1.4315e-03, 3.0994e-03,\n",
      "        9.9945e-04, 1.0233e-03, 1.9112e-03, 1.1549e-03, 6.6948e-04, 5.9052e-03,\n",
      "        2.8801e-03, 4.9057e-03, 5.8937e-03, 4.8714e-03, 4.6349e-03, 4.9515e-03,\n",
      "        6.9809e-03, 2.4567e-03, 2.2545e-03, 2.1610e-03, 1.2827e-03, 1.8091e-03,\n",
      "        6.8512e-03, 1.5383e-03, 2.9621e-03, 2.9221e-03, 1.0750e-02, 4.3945e-03,\n",
      "        2.3193e-03, 5.2109e-03, 2.0180e-03, 5.0049e-03, 2.1839e-03, 8.5974e-04,\n",
      "        7.7744e-03, 3.2997e-03, 3.6659e-03, 9.3994e-03, 4.4441e-03, 6.5651e-03,\n",
      "        9.9182e-03, 5.6305e-03, 5.1575e-03, 2.8458e-03, 2.8992e-03, 5.0774e-03,\n",
      "        7.2441e-03, 1.7815e-03, 9.5673e-03, 4.9706e-03, 1.9445e-03, 1.2856e-03,\n",
      "        8.5297e-03, 1.0269e-02, 4.7493e-03, 3.4103e-03, 9.2621e-03, 6.9504e-03,\n",
      "        3.6774e-03, 1.4229e-03, 1.6003e-03, 6.0272e-04, 4.3831e-03, 8.1253e-03,\n",
      "        1.4717e-02, 9.2239e-03, 9.4299e-03, 5.6915e-03, 1.6603e-03, 5.4092e-03,\n",
      "        1.0658e-02, 5.6801e-03, 3.3092e-03, 1.6876e-02, 2.4277e-02, 3.3340e-03,\n",
      "        8.7128e-03, 8.7738e-03, 5.7564e-03, 5.8632e-03, 2.4776e-03, 3.4981e-03,\n",
      "        3.8290e-04, 2.9430e-03, 1.0633e-03, 7.1144e-04, 7.3586e-03, 1.0262e-02,\n",
      "        1.5099e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [98] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [98] : torch.Size([1, 32, 1, 170])\n",
      "Last layer attentions for generated token [98] : tensor([1.4307e-01, 1.4307e-01, 9.9003e-05, 1.5700e-04, 7.3850e-05, 1.1988e-03,\n",
      "        1.5819e-04, 2.6298e-04, 4.6515e-04, 3.6931e-04, 1.4420e-03, 5.0688e-04,\n",
      "        3.4213e-04, 6.6376e-04, 8.0566e-03, 5.1651e-03, 1.0878e-04, 2.5129e-04,\n",
      "        9.3400e-05, 2.0480e-04, 8.8751e-05, 3.9768e-04, 7.7629e-03, 1.9293e-03,\n",
      "        2.7008e-03, 9.0790e-03, 3.5310e-04, 2.4128e-04, 2.5177e-04, 3.8338e-04,\n",
      "        3.3116e-04, 7.7438e-04, 6.5565e-04, 4.0555e-04, 1.9658e-04, 2.8944e-04,\n",
      "        2.0480e-04, 2.7394e-04, 1.4520e-04, 1.2231e-04, 1.1559e-03, 8.0681e-04,\n",
      "        2.1541e-04, 3.8552e-04, 4.2582e-04, 2.5177e-04, 2.7776e-04, 2.6298e-04,\n",
      "        3.2401e-04, 7.6818e-04, 6.4802e-04, 7.1585e-05, 2.0676e-03, 1.0605e-03,\n",
      "        1.1044e-03, 2.6951e-03, 1.8787e-04, 1.0118e-03, 2.4967e-03, 1.0300e-03,\n",
      "        8.3208e-04, 1.1024e-03, 1.7986e-03, 9.7322e-04, 4.2939e-04, 5.3930e-04,\n",
      "        9.7132e-04, 5.7650e-04, 1.2531e-03, 4.3678e-03, 3.4809e-03, 9.1553e-03,\n",
      "        1.9665e-03, 1.3952e-03, 3.1662e-03, 4.7073e-03, 4.2114e-03, 1.7233e-03,\n",
      "        6.4354e-03, 6.8016e-03, 2.3346e-03, 1.7090e-03, 2.9678e-03, 2.5959e-03,\n",
      "        1.2531e-03, 1.4982e-03, 2.3670e-03, 1.7719e-03, 8.7404e-04, 3.3340e-03,\n",
      "        3.4981e-03, 4.4136e-03, 5.7297e-03, 3.5534e-03, 4.7951e-03, 4.7188e-03,\n",
      "        6.2637e-03, 3.2558e-03, 3.0766e-03, 2.8343e-03, 1.9369e-03, 2.0332e-03,\n",
      "        3.5839e-03, 9.1076e-04, 1.5268e-03, 2.0218e-03, 6.0272e-03, 5.2032e-03,\n",
      "        2.2125e-03, 5.8479e-03, 2.6073e-03, 6.3248e-03, 1.8759e-03, 9.6369e-04,\n",
      "        5.7220e-03, 3.1567e-03, 5.0011e-03, 8.3923e-03, 3.4313e-03, 9.4376e-03,\n",
      "        2.1240e-02, 1.0368e-02, 5.1003e-03, 3.4142e-03, 3.8605e-03, 8.7204e-03,\n",
      "        5.0011e-03, 2.7313e-03, 9.0485e-03, 5.2338e-03, 2.8133e-03, 2.1000e-03,\n",
      "        9.0179e-03, 1.6418e-02, 7.7667e-03, 8.4457e-03, 1.2733e-02, 2.1652e-02,\n",
      "        1.1276e-02, 4.4365e-03, 5.1460e-03, 1.2751e-03, 9.7885e-03, 1.2619e-02,\n",
      "        1.5793e-02, 1.1925e-02, 1.4633e-02, 8.4534e-03, 3.1815e-03, 9.6741e-03,\n",
      "        1.2466e-02, 7.2823e-03, 4.5395e-03, 1.3031e-02, 3.4302e-02, 4.5738e-03,\n",
      "        1.4954e-02, 1.3191e-02, 1.2688e-02, 6.8283e-03, 4.0359e-03, 2.5368e-03,\n",
      "        4.0174e-04, 4.8218e-03, 1.2970e-03, 8.8406e-04, 9.7198e-03, 1.5274e-02,\n",
      "        1.6022e-02, 1.0834e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [99] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [99] : torch.Size([1, 32, 1, 171])\n",
      "Last layer attentions for generated token [99] : tensor([4.1968e-01, 4.1870e-01, 1.1224e-04, 9.8288e-05, 5.0783e-05, 7.4911e-04,\n",
      "        1.8954e-05, 3.8326e-05, 7.6830e-05, 9.9838e-05, 1.4079e-04, 1.0300e-04,\n",
      "        4.0269e-04, 9.7513e-04, 1.4305e-03, 2.1095e-03, 3.4213e-05, 6.3717e-05,\n",
      "        4.1783e-05, 1.3375e-04, 8.3745e-05, 1.9014e-04, 9.7961e-03, 3.0279e-04,\n",
      "        7.6103e-04, 1.4000e-03, 1.1808e-04, 6.5744e-05, 7.7426e-05, 3.8922e-05,\n",
      "        8.9467e-05, 9.5248e-05, 7.1049e-05, 2.4855e-05, 3.0696e-05, 9.2685e-05,\n",
      "        3.2353e-04, 8.9109e-05, 1.6332e-04, 9.0539e-05, 3.2997e-04, 4.3535e-04,\n",
      "        1.1623e-04, 4.2081e-05, 3.2902e-05, 9.9063e-05, 3.0577e-05, 9.0539e-05,\n",
      "        5.9843e-05, 1.8358e-04, 3.8934e-04, 6.2227e-05, 2.1801e-03, 7.5674e-04,\n",
      "        9.3079e-04, 8.9645e-04, 5.9605e-05, 2.4700e-04, 6.5470e-04, 1.2326e-04,\n",
      "        5.0402e-04, 1.5640e-04, 2.5988e-04, 3.1853e-04, 3.5453e-04, 1.1671e-04,\n",
      "        3.9172e-04, 1.3220e-04, 1.8187e-03, 1.1835e-03, 8.9502e-04, 7.9727e-04,\n",
      "        9.7513e-05, 6.2943e-05, 3.2997e-04, 1.2374e-04, 5.2404e-04, 2.2757e-04,\n",
      "        1.8942e-04, 4.1866e-04, 6.8092e-04, 2.0242e-04, 1.1224e-04, 1.1945e-04,\n",
      "        1.0753e-04, 1.5104e-04, 1.5759e-04, 1.3435e-04, 2.0969e-04, 1.7655e-04,\n",
      "        2.4509e-04, 2.6822e-04, 3.5119e-04, 1.9240e-04, 3.4714e-04, 1.1224e-04,\n",
      "        2.9802e-04, 2.8777e-04, 4.7445e-04, 1.7309e-04, 1.3328e-04, 1.0753e-04,\n",
      "        2.4986e-04, 1.0502e-04, 1.7381e-04, 1.5891e-04, 4.6253e-04, 7.1478e-04,\n",
      "        6.5088e-04, 4.2605e-04, 5.3310e-04, 4.7255e-04, 7.5674e-04, 4.1223e-04,\n",
      "        9.6369e-04, 8.4877e-04, 8.0395e-04, 4.7159e-04, 3.3903e-04, 1.0300e-03,\n",
      "        3.0937e-03, 1.1787e-03, 3.9172e-04, 4.2367e-04, 6.9571e-04, 3.8934e-04,\n",
      "        7.2193e-04, 4.0412e-04, 1.5802e-03, 1.0729e-03, 3.4165e-04, 5.6219e-04,\n",
      "        1.4553e-03, 1.9665e-03, 7.6675e-04, 1.3647e-03, 1.6241e-03, 1.3256e-03,\n",
      "        1.1406e-03, 1.1950e-03, 2.4052e-03, 8.8978e-04, 4.9248e-03, 1.5678e-03,\n",
      "        1.5802e-03, 1.8330e-03, 7.7782e-03, 1.6174e-03, 2.6608e-03, 1.5888e-03,\n",
      "        1.4353e-03, 2.6073e-03, 1.4725e-03, 2.8439e-03, 6.7368e-03, 9.1791e-04,\n",
      "        1.9169e-03, 1.7214e-03, 2.2717e-03, 1.7319e-03, 1.7586e-03, 1.5430e-03,\n",
      "        1.7834e-03, 3.9062e-03, 4.5967e-03, 4.1161e-03, 5.2567e-03, 2.7885e-03,\n",
      "        2.6817e-03, 2.4948e-03, 6.6261e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [100] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [100] : torch.Size([1, 32, 1, 172])\n",
      "Last layer attentions for generated token [100] : tensor([1.7163e-01, 1.7126e-01, 3.1614e-04, 2.3866e-04, 2.1100e-04, 1.6823e-03,\n",
      "        1.2094e-04, 1.6081e-04, 3.0875e-04, 3.3593e-04, 9.1457e-04, 2.6774e-04,\n",
      "        4.8876e-04, 8.2779e-04, 5.2261e-03, 7.2632e-03, 1.2094e-04, 3.0637e-04,\n",
      "        1.4699e-04, 2.5606e-04, 1.0145e-04, 4.1223e-04, 1.6602e-02, 2.6360e-03,\n",
      "        3.1776e-03, 1.0551e-02, 3.5620e-04, 1.5962e-04, 1.7667e-04, 2.4092e-04,\n",
      "        4.5371e-04, 5.8460e-04, 3.6383e-04, 1.2326e-04, 1.0341e-04, 2.7633e-04,\n",
      "        2.7895e-04, 2.1684e-04, 1.5652e-04, 1.2046e-04, 1.1864e-03, 7.4673e-04,\n",
      "        2.3174e-04, 2.6608e-04, 2.4045e-04, 2.5392e-04, 1.0383e-04, 2.9016e-04,\n",
      "        2.2411e-04, 6.5756e-04, 7.7343e-04, 1.3435e-04, 2.4414e-03, 2.2011e-03,\n",
      "        1.1139e-03, 3.3092e-03, 1.4305e-04, 8.4591e-04, 2.5101e-03, 8.3780e-04,\n",
      "        1.2312e-03, 1.0900e-03, 2.3479e-03, 1.2407e-03, 6.4611e-04, 7.6723e-04,\n",
      "        1.3494e-03, 9.1839e-04, 1.7738e-03, 3.4752e-03, 3.2558e-03, 6.5651e-03,\n",
      "        6.8521e-04, 7.5388e-04, 2.9850e-03, 2.4452e-03, 3.3131e-03, 1.7376e-03,\n",
      "        2.6131e-03, 4.9591e-03, 2.6321e-03, 7.7152e-04, 1.0653e-03, 1.1473e-03,\n",
      "        9.2745e-04, 6.7949e-04, 1.6594e-03, 7.5960e-04, 4.7731e-04, 1.0128e-03,\n",
      "        1.4505e-03, 1.7958e-03, 2.3537e-03, 2.4071e-03, 2.5177e-03, 1.8530e-03,\n",
      "        3.3894e-03, 2.6188e-03, 2.3251e-03, 1.8225e-03, 8.7261e-04, 1.2074e-03,\n",
      "        1.8349e-03, 5.4932e-04, 1.0777e-03, 7.4959e-04, 3.5038e-03, 2.8706e-03,\n",
      "        1.7138e-03, 2.2926e-03, 1.1950e-03, 2.8343e-03, 1.5745e-03, 4.3201e-04,\n",
      "        3.4828e-03, 1.8206e-03, 2.2831e-03, 3.9368e-03, 3.2673e-03, 6.9771e-03,\n",
      "        2.0782e-02, 6.4430e-03, 3.6716e-03, 2.7485e-03, 2.2087e-03, 3.2673e-03,\n",
      "        2.4738e-03, 1.2474e-03, 4.7989e-03, 3.6755e-03, 3.1242e-03, 2.7676e-03,\n",
      "        8.1406e-03, 1.4717e-02, 5.5847e-03, 6.1073e-03, 1.0376e-02, 1.1711e-02,\n",
      "        5.6953e-03, 3.2806e-03, 3.7880e-03, 1.4620e-03, 1.7960e-02, 1.1787e-02,\n",
      "        1.4099e-02, 9.7046e-03, 1.6663e-02, 1.0895e-02, 5.0659e-03, 1.2856e-02,\n",
      "        1.3359e-02, 8.9111e-03, 6.0425e-03, 1.4290e-02, 3.7079e-02, 4.8943e-03,\n",
      "        1.5404e-02, 1.0681e-02, 8.4381e-03, 4.7913e-03, 4.5090e-03, 5.5809e-03,\n",
      "        9.6798e-04, 5.1498e-03, 3.8166e-03, 2.7466e-03, 2.0035e-02, 1.6922e-02,\n",
      "        1.6235e-02, 1.3794e-02, 1.0742e-02, 8.5602e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [101] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [101] : torch.Size([1, 32, 1, 173])\n",
      "Last layer attentions for generated token [101] : tensor([2.7930e-01, 2.7930e-01, 1.4043e-04, 1.4710e-04, 1.0890e-04, 2.5330e-03,\n",
      "        5.3048e-05, 1.3816e-04, 4.9686e-04, 2.3782e-04, 5.3072e-04, 8.3351e-04,\n",
      "        2.8467e-04, 8.2064e-04, 1.1528e-02, 3.9482e-03, 2.4164e-04, 1.5295e-04,\n",
      "        1.0806e-04, 3.0065e-04, 4.7112e-04, 7.9823e-04, 1.2245e-02, 1.3218e-03,\n",
      "        7.3814e-03, 7.8049e-03, 2.5868e-04, 7.4863e-05, 8.2195e-05, 1.7810e-04,\n",
      "        3.8075e-04, 4.5586e-04, 1.1456e-04, 4.3988e-05, 5.7399e-05, 8.4460e-05,\n",
      "        1.5664e-04, 1.8740e-04, 7.1406e-05, 3.1054e-05, 1.3609e-03, 3.9220e-04,\n",
      "        1.7953e-04, 5.5611e-05, 6.4254e-05, 5.1022e-05, 5.6922e-05, 8.9228e-05,\n",
      "        2.4199e-05, 9.7084e-04, 5.1069e-04, 1.7476e-04, 1.2960e-03, 2.9469e-03,\n",
      "        6.7091e-04, 1.8349e-03, 1.3554e-04, 5.9080e-04, 5.0049e-03, 4.0293e-04,\n",
      "        8.7357e-04, 6.9523e-04, 4.9400e-03, 1.0157e-03, 1.1435e-03, 1.8263e-03,\n",
      "        6.3658e-04, 9.1696e-04, 4.5395e-03, 7.6218e-03, 5.2986e-03, 3.7174e-03,\n",
      "        3.8838e-04, 4.2248e-04, 1.2560e-03, 1.1168e-03, 8.3017e-04, 7.0047e-04,\n",
      "        1.3742e-03, 5.9795e-04, 5.6267e-04, 2.6488e-04, 2.4354e-04, 5.6410e-04,\n",
      "        8.4162e-05, 1.1963e-04, 2.0826e-04, 1.3709e-04, 9.4593e-05, 1.4603e-04,\n",
      "        2.2340e-04, 2.7919e-04, 8.7881e-04, 2.8467e-04, 1.3189e-03, 9.9754e-04,\n",
      "        2.1286e-03, 5.7745e-04, 2.1973e-03, 3.7932e-04, 2.4819e-04, 3.1686e-04,\n",
      "        6.1941e-04, 1.2684e-04, 2.4772e-04, 3.3736e-04, 3.2616e-03, 9.5558e-04,\n",
      "        4.9591e-04, 7.6914e-04, 2.6131e-04, 1.6050e-03, 7.8011e-04, 1.2684e-04,\n",
      "        5.1613e-03, 6.7091e-04, 3.2187e-04, 1.2007e-03, 9.9754e-04, 1.2550e-02,\n",
      "        2.6531e-03, 2.3327e-03, 9.5940e-04, 5.1546e-04, 4.7684e-04, 2.8348e-04,\n",
      "        5.4121e-04, 1.2529e-04, 7.9060e-04, 5.6744e-04, 3.9601e-04, 5.3310e-04,\n",
      "        5.4626e-03, 1.3800e-03, 9.1553e-04, 3.7861e-03, 3.0098e-03, 1.3847e-03,\n",
      "        1.3008e-03, 1.5469e-03, 2.0638e-03, 1.6365e-03, 3.1494e-02, 2.0218e-03,\n",
      "        2.0218e-03, 3.5820e-03, 1.1215e-02, 9.7733e-03, 9.2316e-03, 1.6754e-02,\n",
      "        1.0193e-02, 1.7290e-03, 3.5229e-03, 6.0577e-03, 6.2943e-03, 6.1464e-04,\n",
      "        6.0959e-03, 2.2297e-03, 1.2388e-03, 2.2411e-03, 1.5469e-03, 3.2082e-03,\n",
      "        7.8011e-04, 5.1498e-03, 9.8877e-03, 5.5389e-03, 3.9581e-02, 4.7340e-03,\n",
      "        2.6569e-03, 3.9711e-03, 8.3313e-03, 1.1093e-02, 3.1677e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [102] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [102] : torch.Size([1, 32, 1, 174])\n",
      "Last layer attentions for generated token [102] : tensor([2.6660e-01, 2.6562e-01, 1.5008e-04, 1.6224e-04, 1.5664e-04, 1.4439e-03,\n",
      "        2.5570e-05, 4.6849e-05, 1.6105e-04, 1.9503e-04, 4.5156e-04, 1.2684e-04,\n",
      "        2.2089e-04, 3.0017e-04, 3.0956e-03, 5.8937e-03, 1.0234e-04, 2.3699e-04,\n",
      "        1.0681e-04, 3.2592e-04, 5.8532e-05, 7.7248e-04, 1.3496e-02, 1.8902e-03,\n",
      "        2.2202e-03, 7.8011e-03, 1.2636e-04, 6.0380e-05, 5.2273e-05, 8.2850e-05,\n",
      "        3.3498e-04, 4.7970e-04, 1.3828e-04, 2.7359e-05, 3.4034e-05, 9.7275e-05,\n",
      "        1.8036e-04, 9.5785e-05, 7.3731e-05, 4.2021e-05, 7.7724e-04, 5.4026e-04,\n",
      "        1.4210e-04, 1.0723e-04, 4.6492e-05, 5.0664e-05, 5.4121e-05, 1.1283e-04,\n",
      "        4.8161e-05, 3.0494e-04, 4.0960e-04, 1.1152e-04, 1.2817e-03, 3.6545e-03,\n",
      "        8.4686e-04, 1.9255e-03, 5.3942e-05, 3.8242e-04, 1.4896e-03, 3.6573e-04,\n",
      "        5.5647e-04, 4.8161e-04, 1.1463e-03, 4.6778e-04, 3.7956e-04, 5.7745e-04,\n",
      "        7.7391e-04, 7.4291e-04, 1.7481e-03, 4.8103e-03, 5.0316e-03, 4.4594e-03,\n",
      "        2.5988e-04, 2.6965e-04, 1.1463e-03, 8.8739e-04, 1.8339e-03, 6.0987e-04,\n",
      "        1.3142e-03, 1.8301e-03, 1.0786e-03, 1.9801e-04, 2.4259e-04, 4.0245e-04,\n",
      "        2.1160e-04, 1.4102e-04, 5.3740e-04, 2.3794e-04, 2.4164e-04, 4.0317e-04,\n",
      "        2.7061e-04, 3.7003e-04, 9.4461e-04, 4.2582e-04, 6.7234e-04, 6.4659e-04,\n",
      "        2.2602e-03, 1.0748e-03, 1.7176e-03, 5.0449e-04, 2.1660e-04, 2.5272e-04,\n",
      "        5.2166e-04, 1.2636e-04, 4.1437e-04, 2.1493e-04, 3.4122e-03, 1.4019e-03,\n",
      "        6.6614e-04, 5.2691e-04, 2.9087e-04, 1.7138e-03, 1.0872e-03, 1.4377e-04,\n",
      "        2.9411e-03, 9.6703e-04, 6.6328e-04, 1.1578e-03, 1.3065e-03, 6.5536e-03,\n",
      "        8.2245e-03, 1.7004e-03, 1.8625e-03, 1.2417e-03, 9.1410e-04, 7.4863e-04,\n",
      "        1.1854e-03, 3.7813e-04, 1.2159e-03, 1.2064e-03, 5.3740e-04, 6.0654e-04,\n",
      "        4.4174e-03, 3.9291e-03, 1.3018e-03, 2.0580e-03, 5.3711e-03, 2.6855e-03,\n",
      "        1.5011e-03, 9.6130e-04, 2.3155e-03, 1.0586e-03, 1.8143e-02, 4.4327e-03,\n",
      "        6.8817e-03, 4.5662e-03, 9.4681e-03, 5.1842e-03, 5.8441e-03, 5.8632e-03,\n",
      "        1.4359e-02, 4.6425e-03, 3.5973e-03, 1.6830e-02, 1.6678e-02, 1.3323e-03,\n",
      "        3.3360e-03, 4.3983e-03, 2.6684e-03, 2.3003e-03, 1.5574e-03, 3.9101e-03,\n",
      "        1.3142e-03, 5.4512e-03, 3.8357e-03, 2.1019e-03, 1.5465e-02, 1.1208e-02,\n",
      "        6.5079e-03, 9.1095e-03, 1.1238e-02, 1.3374e-02, 5.3802e-02, 3.2562e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [103] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [103] : torch.Size([1, 32, 1, 175])\n",
      "Last layer attentions for generated token [103] : tensor([2.9858e-01, 2.9858e-01, 2.6131e-04, 1.8418e-04, 3.2711e-04, 1.9522e-03,\n",
      "        3.0458e-05, 8.6367e-05, 5.0879e-04, 2.6298e-04, 6.7902e-04, 5.1260e-04,\n",
      "        7.5626e-04, 8.2254e-04, 6.1226e-03, 3.5076e-03, 9.6679e-05, 2.1958e-04,\n",
      "        1.6975e-04, 7.0477e-04, 1.4806e-04, 6.1703e-04, 5.5237e-03, 8.5211e-04,\n",
      "        4.3564e-03, 3.2063e-03, 1.0097e-04, 1.0538e-04, 1.1849e-04, 1.3685e-04,\n",
      "        3.5572e-04, 4.8351e-04, 1.6510e-04, 6.5207e-05, 6.7770e-05, 1.5509e-04,\n",
      "        1.8716e-04, 1.2267e-04, 6.6698e-05, 4.2260e-05, 1.6193e-03, 5.1355e-04,\n",
      "        1.3912e-04, 9.0837e-05, 6.0022e-05, 7.4446e-05, 5.5969e-05, 1.3316e-04,\n",
      "        6.1452e-05, 7.4434e-04, 4.6134e-04, 1.9002e-04, 7.6532e-04, 1.3590e-03,\n",
      "        6.2943e-04, 1.4076e-03, 9.2983e-05, 3.5238e-04, 2.2945e-03, 4.2248e-04,\n",
      "        1.1091e-03, 5.4026e-04, 2.2469e-03, 8.0013e-04, 9.1934e-04, 1.2035e-03,\n",
      "        4.2176e-04, 7.0620e-04, 2.3689e-03, 4.5776e-03, 3.7727e-03, 2.6970e-03,\n",
      "        2.1458e-04, 4.5419e-04, 1.5421e-03, 7.9727e-04, 1.0042e-03, 4.6039e-04,\n",
      "        8.4543e-04, 4.0555e-04, 4.3678e-04, 2.2483e-04, 2.8253e-04, 6.1846e-04,\n",
      "        1.3530e-04, 1.8203e-04, 4.7874e-04, 2.1875e-04, 2.0635e-04, 1.7440e-04,\n",
      "        2.0313e-04, 2.6488e-04, 9.6703e-04, 2.5725e-04, 7.1573e-04, 8.1921e-04,\n",
      "        2.5578e-03, 5.2261e-04, 2.1648e-03, 3.3879e-04, 3.0732e-04, 3.8767e-04,\n",
      "        6.4802e-04, 2.4033e-04, 5.8651e-04, 2.4128e-04, 3.8109e-03, 1.6451e-03,\n",
      "        4.6039e-04, 3.8934e-04, 2.8872e-04, 2.1229e-03, 1.0176e-03, 1.9765e-04,\n",
      "        6.6528e-03, 6.9666e-04, 3.3498e-04, 1.2741e-03, 5.5456e-04, 1.2650e-02,\n",
      "        1.8644e-03, 1.0872e-03, 1.0157e-03, 2.9087e-04, 6.5184e-04, 3.8171e-04,\n",
      "        5.4789e-04, 9.6321e-05, 6.5708e-04, 6.7520e-04, 3.3045e-04, 3.2210e-04,\n",
      "        5.9662e-03, 1.3275e-03, 7.0620e-04, 2.7695e-03, 3.9635e-03, 1.5850e-03,\n",
      "        2.0428e-03, 1.2255e-03, 1.8387e-03, 1.1826e-03, 3.4332e-02, 3.3569e-03,\n",
      "        4.5509e-03, 3.1338e-03, 3.4332e-03, 2.0676e-03, 2.0065e-03, 5.7487e-03,\n",
      "        7.3204e-03, 1.8826e-03, 2.9125e-03, 8.3847e-03, 4.8103e-03, 5.7411e-04,\n",
      "        3.3455e-03, 2.6188e-03, 1.5917e-03, 3.3035e-03, 1.4381e-03, 1.4601e-03,\n",
      "        8.9121e-04, 5.1842e-03, 7.1754e-03, 5.7869e-03, 2.6138e-02, 6.4163e-03,\n",
      "        4.2953e-03, 5.5237e-03, 9.1858e-03, 1.1993e-02, 2.4017e-02, 2.6825e-02,\n",
      "        2.1683e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [104] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [104] : torch.Size([1, 32, 1, 176])\n",
      "Last layer attentions for generated token [104] : tensor([2.2205e-01, 2.2205e-01, 2.6369e-04, 3.3402e-04, 2.4009e-04, 9.3126e-04,\n",
      "        2.8551e-05, 5.7936e-05, 1.3101e-04, 2.2459e-04, 4.6539e-04, 7.6115e-05,\n",
      "        1.1075e-04, 1.8990e-04, 4.0359e-03, 4.3831e-03, 1.5986e-04, 3.3784e-04,\n",
      "        9.6977e-05, 2.5392e-04, 3.0160e-05, 4.3631e-04, 1.2344e-02, 1.3313e-03,\n",
      "        1.4734e-03, 7.2823e-03, 1.3733e-04, 7.7009e-05, 1.1742e-04, 8.1360e-05,\n",
      "        2.5058e-04, 4.4155e-04, 1.6177e-04, 6.5386e-05, 7.2062e-05, 1.8847e-04,\n",
      "        1.8692e-04, 7.4625e-05, 9.6202e-05, 8.8334e-05, 1.0147e-03, 4.5013e-04,\n",
      "        1.0365e-04, 1.7154e-04, 6.8247e-05, 1.0902e-04, 4.9531e-05, 1.0902e-04,\n",
      "        9.2924e-05, 3.0637e-04, 4.3201e-04, 1.3518e-04, 1.3151e-03, 5.1003e-03,\n",
      "        7.1239e-04, 1.7948e-03, 5.5075e-05, 6.4611e-04, 2.1858e-03, 5.8365e-04,\n",
      "        6.6805e-04, 4.6825e-04, 1.1168e-03, 5.1928e-04, 2.9826e-04, 2.9016e-04,\n",
      "        4.9067e-04, 8.0872e-04, 1.1959e-03, 5.6572e-03, 6.0234e-03, 4.6692e-03,\n",
      "        4.1556e-04, 3.2043e-04, 8.7118e-04, 4.8876e-04, 1.5945e-03, 3.2234e-04,\n",
      "        9.0218e-04, 1.7805e-03, 1.3285e-03, 1.6689e-04, 1.5378e-04, 2.2769e-04,\n",
      "        1.6892e-04, 1.5259e-04, 3.2544e-04, 2.3127e-04, 1.4555e-04, 4.8780e-04,\n",
      "        4.6444e-04, 5.1212e-04, 8.2016e-04, 5.2118e-04, 7.0000e-04, 4.8399e-04,\n",
      "        1.4734e-03, 9.2363e-04, 8.6784e-04, 3.5477e-04, 2.2864e-04, 2.6941e-04,\n",
      "        5.3692e-04, 1.4162e-04, 3.5954e-04, 2.4104e-04, 2.2087e-03, 1.2674e-03,\n",
      "        6.2370e-04, 8.9550e-04, 4.5371e-04, 1.3332e-03, 1.0824e-03, 1.7905e-04,\n",
      "        1.7567e-03, 9.2030e-04, 8.7118e-04, 1.3151e-03, 1.4391e-03, 4.2992e-03,\n",
      "        6.2675e-03, 2.1782e-03, 1.8988e-03, 7.1383e-04, 1.5841e-03, 9.2030e-04,\n",
      "        9.9659e-04, 3.2544e-04, 1.6565e-03, 7.8392e-04, 5.9080e-04, 4.2200e-04,\n",
      "        3.5858e-03, 3.0975e-03, 1.6909e-03, 2.3594e-03, 4.5776e-03, 1.9531e-03,\n",
      "        1.2827e-03, 7.8392e-04, 2.2488e-03, 7.7343e-04, 8.2321e-03, 4.0283e-03,\n",
      "        5.4703e-03, 5.1308e-03, 6.2904e-03, 3.5515e-03, 1.8320e-03, 4.0550e-03,\n",
      "        7.3624e-03, 4.2458e-03, 4.0092e-03, 1.5297e-02, 1.7395e-02, 2.1420e-03,\n",
      "        6.1531e-03, 5.9586e-03, 3.0918e-03, 3.5858e-03, 2.5425e-03, 3.8700e-03,\n",
      "        1.1606e-03, 7.7782e-03, 3.9177e-03, 3.0441e-03, 2.6581e-02, 1.2611e-02,\n",
      "        1.0818e-02, 1.0872e-02, 8.9188e-03, 1.3504e-02, 7.5623e-02, 6.5552e-02,\n",
      "        2.8992e-02, 1.8982e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [105] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [105] : torch.Size([1, 32, 1, 177])\n",
      "Last layer attentions for generated token [105] : tensor([1.6626e-01, 1.6663e-01, 4.2272e-04, 9.9468e-04, 5.5361e-04, 1.1930e-03,\n",
      "        5.7101e-05, 9.5248e-05, 1.7178e-04, 2.6512e-04, 4.6611e-04, 1.8001e-04,\n",
      "        4.1628e-04, 3.6883e-04, 5.7755e-03, 4.3678e-03, 1.7047e-04, 6.5231e-04,\n",
      "        1.1808e-04, 2.4700e-04, 6.1512e-05, 5.1403e-04, 1.2016e-02, 2.2049e-03,\n",
      "        2.5387e-03, 6.3095e-03, 5.6314e-04, 1.3483e-04, 1.9777e-04, 2.0003e-04,\n",
      "        6.7949e-04, 1.1005e-03, 4.6158e-04, 1.6081e-04, 1.6081e-04, 1.6201e-04,\n",
      "        2.4188e-04, 2.3711e-04, 1.5223e-04, 1.4305e-04, 2.3766e-03, 8.2922e-04,\n",
      "        2.7514e-04, 5.2738e-04, 2.3437e-04, 3.7241e-04, 8.6725e-05, 9.9421e-05,\n",
      "        1.7726e-04, 7.7295e-04, 5.9366e-04, 4.2868e-04, 1.3809e-03, 9.9487e-03,\n",
      "        9.0218e-04, 2.9526e-03, 1.9312e-04, 1.0357e-03, 2.1000e-03, 5.2023e-04,\n",
      "        9.8705e-04, 4.2939e-04, 1.5612e-03, 5.8365e-04, 6.0415e-04, 3.3498e-04,\n",
      "        8.2779e-04, 1.1406e-03, 8.4877e-04, 1.1383e-02, 7.6027e-03, 8.5983e-03,\n",
      "        1.0796e-03, 4.8089e-04, 1.5020e-03, 1.6270e-03, 3.0212e-03, 7.9584e-04,\n",
      "        2.7981e-03, 6.7596e-03, 3.7174e-03, 1.0338e-03, 5.4264e-04, 6.4325e-04,\n",
      "        4.5800e-04, 5.1117e-04, 6.7139e-04, 3.0684e-04, 2.5630e-04, 7.9298e-04,\n",
      "        7.8201e-04, 9.1982e-04, 1.8854e-03, 7.1907e-04, 7.8058e-04, 8.1348e-04,\n",
      "        2.0504e-03, 2.2564e-03, 1.9264e-03, 1.1625e-03, 7.7152e-04, 7.9298e-04,\n",
      "        1.3151e-03, 2.2590e-04, 8.1015e-04, 6.7568e-04, 3.1853e-03, 2.3689e-03,\n",
      "        1.1177e-03, 1.5459e-03, 9.6941e-04, 2.3918e-03, 2.0447e-03, 6.7425e-04,\n",
      "        4.1351e-03, 1.5316e-03, 2.2221e-03, 2.5463e-03, 1.3971e-03, 4.5891e-03,\n",
      "        1.2100e-02, 5.8517e-03, 3.6125e-03, 1.5497e-03, 2.3899e-03, 1.5554e-03,\n",
      "        2.3098e-03, 1.2741e-03, 4.2229e-03, 1.9455e-03, 8.2779e-04, 1.0529e-03,\n",
      "        6.1264e-03, 6.6452e-03, 3.0746e-03, 3.0746e-03, 4.7760e-03, 3.8395e-03,\n",
      "        1.6346e-03, 1.0204e-03, 2.6798e-03, 8.4066e-04, 8.8272e-03, 4.7112e-03,\n",
      "        1.1520e-02, 8.5220e-03, 1.2558e-02, 1.1055e-02, 3.7422e-03, 8.2397e-03,\n",
      "        1.3519e-02, 6.1035e-03, 8.8577e-03, 2.7679e-02, 2.5818e-02, 3.7975e-03,\n",
      "        8.2245e-03, 6.2599e-03, 3.5191e-03, 2.1706e-03, 2.1420e-03, 9.9792e-03,\n",
      "        2.3975e-03, 4.3030e-03, 3.7899e-03, 2.3975e-03, 1.5518e-02, 8.4610e-03,\n",
      "        6.3934e-03, 1.2123e-02, 8.8654e-03, 6.3210e-03, 7.0801e-02, 3.7018e-02,\n",
      "        1.6357e-02, 1.9623e-02, 1.2001e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [106] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [106] : torch.Size([1, 32, 1, 178])\n",
      "Last layer attentions for generated token [106] : tensor([2.2278e-01, 2.2278e-01, 2.0313e-04, 5.4455e-04, 1.7238e-04, 1.0662e-03,\n",
      "        6.1452e-05, 1.0252e-04, 2.0313e-04, 4.4274e-04, 5.6839e-04, 5.3501e-04,\n",
      "        4.6754e-04, 9.4652e-04, 6.1722e-03, 3.8853e-03, 2.0790e-04, 3.7360e-04,\n",
      "        1.3685e-04, 4.0317e-04, 1.7309e-04, 1.2493e-03, 9.7733e-03, 2.1992e-03,\n",
      "        3.4313e-03, 6.1226e-03, 2.9731e-04, 1.0914e-04, 1.2612e-04, 2.4211e-04,\n",
      "        5.2357e-04, 4.3845e-04, 2.5272e-04, 2.2399e-04, 9.4116e-05, 2.2662e-04,\n",
      "        2.0707e-04, 2.7990e-04, 1.5509e-04, 9.4116e-05, 7.0333e-04, 2.2566e-04,\n",
      "        1.2219e-04, 1.3268e-04, 1.5938e-04, 1.3793e-04, 1.7369e-04, 1.6904e-04,\n",
      "        1.3685e-04, 5.9462e-04, 2.7823e-04, 2.2399e-04, 6.8998e-04, 1.7824e-03,\n",
      "        3.5858e-04, 1.8492e-03, 7.0751e-05, 1.6041e-03, 2.0275e-03, 5.1975e-04,\n",
      "        4.3249e-04, 5.4026e-04, 2.2736e-03, 7.4148e-04, 1.2617e-03, 6.7902e-04,\n",
      "        3.3689e-04, 5.0974e-04, 6.7902e-04, 3.8624e-03, 2.1229e-03, 3.1700e-03,\n",
      "        1.2884e-03, 5.4121e-04, 1.2417e-03, 1.3933e-03, 1.9312e-03, 6.8712e-04,\n",
      "        2.2106e-03, 2.2793e-03, 1.1330e-03, 4.9877e-04, 7.1287e-04, 8.6164e-04,\n",
      "        2.8133e-04, 2.5630e-04, 6.1131e-04, 5.9128e-04, 4.5776e-04, 7.8154e-04,\n",
      "        6.6710e-04, 9.6512e-04, 1.6327e-03, 7.3004e-04, 1.6747e-03, 1.0080e-03,\n",
      "        3.0937e-03, 8.0824e-04, 2.1038e-03, 4.2915e-04, 4.6682e-04, 4.6754e-04,\n",
      "        7.1287e-04, 1.6904e-04, 2.3746e-04, 6.8712e-04, 2.5520e-03, 2.0428e-03,\n",
      "        7.1192e-04, 2.8667e-03, 1.0033e-03, 2.9392e-03, 1.0214e-03, 2.4402e-04,\n",
      "        3.5725e-03, 8.0347e-04, 8.9979e-04, 3.6011e-03, 1.8349e-03, 6.3667e-03,\n",
      "        5.8517e-03, 4.2610e-03, 3.2024e-03, 1.0414e-03, 1.6289e-03, 1.0052e-03,\n",
      "        1.7338e-03, 3.1948e-04, 1.9140e-03, 1.1024e-03, 6.6090e-04, 3.8457e-04,\n",
      "        4.3640e-03, 3.0766e-03, 9.6512e-04, 2.4529e-03, 1.9798e-03, 1.5450e-03,\n",
      "        1.7509e-03, 4.9114e-04, 2.1725e-03, 1.1463e-03, 9.3918e-03, 2.5864e-03,\n",
      "        4.6921e-03, 8.9340e-03, 9.4223e-03, 4.6425e-03, 3.9673e-03, 9.8419e-03,\n",
      "        2.1011e-02, 7.9880e-03, 1.5091e-02, 2.7374e-02, 1.2169e-02, 6.4564e-04,\n",
      "        3.5229e-03, 2.5711e-03, 1.7004e-03, 1.3266e-03, 1.0748e-03, 1.8349e-03,\n",
      "        7.0333e-04, 3.2654e-03, 3.3302e-03, 2.2945e-03, 1.0178e-02, 4.6883e-03,\n",
      "        4.1847e-03, 4.5776e-03, 5.2032e-03, 8.3694e-03, 3.0441e-02, 2.4918e-02,\n",
      "        2.6459e-02, 3.3325e-02, 3.0136e-02, 3.4271e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [107] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [107] : torch.Size([1, 32, 1, 179])\n",
      "Last layer attentions for generated token [107] : tensor([2.2314e-01, 2.2266e-01, 1.0271e-03, 1.0614e-03, 8.5831e-04, 2.7981e-03,\n",
      "        7.3135e-05, 9.8765e-05, 2.6751e-04, 3.5572e-04, 7.4863e-04, 2.9135e-04,\n",
      "        9.9945e-04, 1.2007e-03, 5.9471e-03, 9.3079e-03, 1.3340e-04, 5.6839e-04,\n",
      "        1.9801e-04, 5.8889e-04, 1.9646e-04, 8.3160e-04, 2.2324e-02, 3.5362e-03,\n",
      "        4.1428e-03, 1.6846e-02, 5.1737e-04, 1.4591e-04, 1.6606e-04, 2.0027e-04,\n",
      "        4.4346e-04, 5.6076e-04, 3.1376e-04, 1.3089e-04, 1.5843e-04, 1.6415e-04,\n",
      "        3.1257e-04, 3.0017e-04, 9.0659e-05, 8.6844e-05, 9.6512e-04, 5.8079e-04,\n",
      "        3.7050e-04, 1.8597e-04, 1.5235e-04, 1.2386e-04, 1.4651e-04, 2.1827e-04,\n",
      "        1.7071e-04, 6.6328e-04, 6.5804e-04, 8.3017e-04, 2.2526e-03, 4.4022e-03,\n",
      "        7.2145e-04, 3.6030e-03, 6.1095e-05, 3.6120e-04, 9.1171e-04, 2.3973e-04,\n",
      "        3.8314e-04, 2.9778e-04, 1.0910e-03, 2.7657e-04, 3.3283e-04, 3.7861e-04,\n",
      "        3.5214e-04, 1.0147e-03, 6.1464e-04, 6.1722e-03, 6.1684e-03, 6.3553e-03,\n",
      "        1.3323e-03, 6.5422e-04, 2.0657e-03, 2.1935e-03, 2.4014e-03, 1.2512e-03,\n",
      "        2.5673e-03, 3.0537e-03, 2.7275e-03, 7.8917e-04, 7.0715e-04, 1.0786e-03,\n",
      "        4.9019e-04, 3.8671e-04, 7.2670e-04, 3.2258e-04, 4.9496e-04, 9.9564e-04,\n",
      "        3.8314e-04, 9.8038e-04, 2.3575e-03, 7.1716e-04, 1.3218e-03, 1.8415e-03,\n",
      "        3.1948e-03, 2.0809e-03, 3.3703e-03, 8.3160e-04, 7.9823e-04, 4.8804e-04,\n",
      "        9.3889e-04, 1.8597e-04, 3.8147e-04, 3.9053e-04, 3.8166e-03, 2.1248e-03,\n",
      "        5.1546e-04, 1.1063e-03, 8.1587e-04, 3.2806e-03, 6.7616e-04, 2.4068e-04,\n",
      "        4.4403e-03, 8.9931e-04, 9.2602e-04, 2.6913e-03, 1.2417e-03, 7.9041e-03,\n",
      "        1.4069e-02, 4.0550e-03, 3.9024e-03, 1.5879e-03, 2.5215e-03, 1.2655e-03,\n",
      "        1.8969e-03, 4.2081e-04, 1.5631e-03, 2.1133e-03, 7.5722e-04, 5.6171e-04,\n",
      "        4.8637e-03, 5.3177e-03, 1.5030e-03, 1.2054e-03, 3.4046e-03, 2.8248e-03,\n",
      "        1.4601e-03, 5.0640e-04, 1.5545e-03, 9.3508e-04, 1.3771e-02, 4.2648e-03,\n",
      "        7.2899e-03, 5.3749e-03, 7.4730e-03, 3.8567e-03, 2.0580e-03, 6.7177e-03,\n",
      "        1.7776e-02, 1.0262e-02, 9.5978e-03, 2.0981e-02, 2.2537e-02, 1.7681e-03,\n",
      "        4.1809e-03, 6.5002e-03, 3.9482e-03, 1.1635e-03, 1.0176e-03, 2.1420e-03,\n",
      "        9.4461e-04, 1.6708e-03, 1.4915e-03, 1.1549e-03, 9.2621e-03, 6.4049e-03,\n",
      "        4.7569e-03, 5.7411e-03, 6.8283e-03, 4.9248e-03, 1.4954e-02, 1.3092e-02,\n",
      "        1.5114e-02, 1.3451e-02, 1.7944e-02, 2.3331e-02, 7.9498e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [108] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [108] : torch.Size([1, 32, 1, 180])\n",
      "Last layer attentions for generated token [108] : tensor([3.1909e-01, 3.1763e-01, 3.4142e-04, 4.1676e-04, 3.8910e-04, 3.2845e-03,\n",
      "        8.4341e-05, 1.4341e-04, 3.1829e-04, 3.1567e-04, 9.1362e-04, 4.9877e-04,\n",
      "        9.1362e-04, 8.0776e-04, 3.5324e-03, 4.1275e-03, 8.6308e-05, 3.8457e-04,\n",
      "        2.8968e-04, 6.8283e-04, 1.4067e-04, 4.8923e-04, 1.3084e-02, 1.3638e-03,\n",
      "        3.2520e-03, 9.7427e-03, 2.3556e-04, 5.5075e-05, 9.2983e-05, 1.2362e-04,\n",
      "        4.2319e-04, 1.6069e-04, 2.6083e-04, 8.9765e-05, 1.0133e-04, 5.9307e-05,\n",
      "        1.2034e-04, 2.0146e-04, 4.6015e-05, 6.9618e-05, 2.4486e-04, 2.2125e-04,\n",
      "        1.4067e-04, 6.3658e-05, 7.9811e-05, 7.4983e-05, 1.2612e-04, 1.0914e-04,\n",
      "        1.2219e-04, 4.5061e-04, 3.8385e-04, 4.9210e-04, 9.3508e-04, 9.3508e-04,\n",
      "        2.7442e-04, 1.7920e-03, 2.7895e-05, 1.1480e-04, 6.7377e-04, 1.3638e-04,\n",
      "        3.2830e-04, 1.2505e-04, 1.0195e-03, 2.0230e-04, 4.1986e-04, 3.3212e-04,\n",
      "        1.5271e-04, 4.9686e-04, 4.5061e-04, 4.0092e-03, 3.5133e-03, 3.6240e-03,\n",
      "        3.1710e-04, 3.3998e-04, 6.7377e-04, 9.0313e-04, 9.0456e-04, 6.3419e-04,\n",
      "        9.7847e-04, 1.2465e-03, 6.7234e-04, 3.2330e-04, 3.4404e-04, 5.0831e-04,\n",
      "        1.3471e-04, 1.7035e-04, 3.4952e-04, 2.1195e-04, 2.4307e-04, 3.0971e-04,\n",
      "        1.5879e-04, 4.0770e-04, 9.5367e-04, 2.2566e-04, 7.6056e-04, 7.8440e-04,\n",
      "        2.0828e-03, 7.3433e-04, 1.7958e-03, 2.7657e-04, 4.3654e-04, 2.5463e-04,\n",
      "        3.1090e-04, 1.5509e-04, 1.9372e-04, 3.1829e-04, 2.1706e-03, 1.0958e-03,\n",
      "        2.4891e-04, 3.5357e-04, 3.6764e-04, 1.4973e-03, 2.8086e-04, 7.5281e-05,\n",
      "        2.1248e-03, 2.8181e-04, 2.3007e-04, 1.5755e-03, 6.4135e-04, 6.7825e-03,\n",
      "        3.3913e-03, 9.1028e-04, 8.4352e-04, 4.6945e-04, 7.6056e-04, 3.0613e-04,\n",
      "        5.0974e-04, 6.2168e-05, 3.9983e-04, 6.9237e-04, 2.0075e-04, 2.5868e-04,\n",
      "        2.1286e-03, 1.1663e-03, 3.2187e-04, 4.6039e-04, 9.5749e-04, 6.8140e-04,\n",
      "        5.5075e-04, 2.5368e-04, 5.8079e-04, 3.3617e-04, 9.8343e-03, 1.5068e-03,\n",
      "        3.6564e-03, 3.0289e-03, 3.1891e-03, 1.8272e-03, 9.1743e-04, 6.5498e-03,\n",
      "        1.3588e-02, 3.9406e-03, 1.0338e-02, 1.0193e-02, 5.2795e-03, 3.2711e-04,\n",
      "        8.8024e-04, 1.1082e-03, 6.2084e-04, 3.7718e-04, 3.3474e-04, 1.4744e-03,\n",
      "        7.4291e-04, 1.0033e-03, 1.0595e-03, 6.9094e-04, 6.4621e-03, 3.1204e-03,\n",
      "        2.7542e-03, 3.4275e-03, 3.2177e-03, 5.6572e-03, 6.3171e-03, 1.0773e-02,\n",
      "        7.9880e-03, 2.4612e-02, 2.7313e-02, 2.6184e-02, 1.0216e-02, 1.9958e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [109] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [109] : torch.Size([1, 32, 1, 181])\n",
      "Last layer attentions for generated token [109] : tensor([2.0361e-01, 2.0325e-01, 6.3801e-04, 7.5483e-04, 7.3576e-04, 8.5211e-04,\n",
      "        4.8041e-05, 8.4996e-05, 2.0874e-04, 3.3426e-04, 5.0783e-04, 7.5936e-05,\n",
      "        1.7309e-04, 4.9305e-04, 4.7417e-03, 5.7526e-03, 5.6863e-05, 5.0259e-04,\n",
      "        1.4186e-04, 2.7180e-04, 7.9870e-05, 4.4727e-04, 1.8173e-02, 3.9368e-03,\n",
      "        2.7046e-03, 1.0231e-02, 4.8828e-04, 1.8358e-04, 1.2422e-04, 1.1897e-04,\n",
      "        5.9032e-04, 4.9305e-04, 3.8028e-04, 1.1945e-04, 1.1176e-04, 1.8644e-04,\n",
      "        2.0957e-04, 1.6582e-04, 1.0872e-04, 1.1665e-04, 1.0300e-03, 8.5688e-04,\n",
      "        4.0865e-04, 2.6798e-04, 1.4293e-04, 1.5330e-04, 9.6321e-05, 2.2221e-04,\n",
      "        1.6010e-04, 4.3082e-04, 8.4877e-04, 1.1330e-03, 1.4610e-03, 9.7275e-03,\n",
      "        1.3800e-03, 2.3994e-03, 4.9949e-05, 3.9005e-04, 8.1110e-04, 3.1209e-04,\n",
      "        4.5776e-04, 2.5892e-04, 9.2840e-04, 4.7708e-04, 1.9538e-04, 3.0851e-04,\n",
      "        4.4727e-04, 8.6880e-04, 6.4564e-04, 6.8016e-03, 7.8354e-03, 4.3297e-03,\n",
      "        1.0920e-03, 3.6931e-04, 1.3666e-03, 1.3914e-03, 2.8458e-03, 7.3576e-04,\n",
      "        1.6842e-03, 2.5043e-03, 2.3804e-03, 3.6645e-04, 3.0804e-04, 5.2691e-04,\n",
      "        3.1710e-04, 1.7381e-04, 3.2330e-04, 1.6642e-04, 2.3663e-04, 8.0204e-04,\n",
      "        3.6287e-04, 7.9727e-04, 1.3065e-03, 6.6328e-04, 6.6090e-04, 5.7745e-04,\n",
      "        1.8721e-03, 1.0338e-03, 1.2865e-03, 4.3941e-04, 4.3082e-04, 4.0078e-04,\n",
      "        6.8998e-04, 1.4985e-04, 3.5715e-04, 2.3568e-04, 1.3618e-03, 1.1740e-03,\n",
      "        5.2261e-04, 1.8644e-03, 8.2111e-04, 1.4238e-03, 8.9121e-04, 2.0957e-04,\n",
      "        3.0975e-03, 1.1902e-03, 9.2649e-04, 1.2369e-03, 1.6165e-03, 3.6640e-03,\n",
      "        9.3918e-03, 2.8744e-03, 1.8539e-03, 1.0462e-03, 2.5616e-03, 8.9264e-04,\n",
      "        1.6813e-03, 4.5323e-04, 1.1415e-03, 1.0061e-03, 6.0654e-04, 6.1607e-04,\n",
      "        2.7885e-03, 4.0779e-03, 1.6975e-03, 7.8344e-04, 2.9678e-03, 1.9970e-03,\n",
      "        1.2064e-03, 6.2943e-04, 1.2178e-03, 5.1594e-04, 6.2752e-03, 2.5539e-03,\n",
      "        5.5122e-03, 4.6921e-03, 5.3596e-03, 3.7289e-03, 1.6203e-03, 3.4790e-03,\n",
      "        8.9417e-03, 7.6447e-03, 8.3313e-03, 2.0950e-02, 1.9455e-02, 2.8439e-03,\n",
      "        3.9253e-03, 6.6643e-03, 4.1733e-03, 2.3842e-03, 2.1496e-03, 2.8534e-03,\n",
      "        7.6056e-04, 3.7079e-03, 2.3041e-03, 1.5726e-03, 1.2184e-02, 9.4147e-03,\n",
      "        7.7705e-03, 1.2939e-02, 1.0841e-02, 1.0857e-02, 2.2552e-02, 2.3758e-02,\n",
      "        8.2245e-03, 2.2949e-02, 2.1759e-02, 2.5726e-02, 2.1118e-02, 1.4961e-02,\n",
      "        4.4159e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [110] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [110] : torch.Size([1, 32, 1, 182])\n",
      "Last layer attentions for generated token [110] : tensor([1.0175e-01, 1.0150e-01, 1.8263e-04, 4.3392e-04, 2.0385e-04, 7.8869e-04,\n",
      "        2.6345e-05, 5.0902e-05, 8.2672e-05, 1.5140e-04, 3.1757e-04, 1.3137e-04,\n",
      "        1.3137e-04, 2.8181e-04, 1.2070e-02, 4.1771e-03, 8.7976e-05, 5.6362e-04,\n",
      "        6.0499e-05, 1.8513e-04, 5.4002e-05, 1.9145e-04, 3.2024e-03, 8.3303e-04,\n",
      "        2.2011e-03, 5.3024e-03, 9.0981e-04, 1.5080e-04, 1.1569e-04, 1.7464e-04,\n",
      "        4.5395e-04, 9.7227e-04, 2.0218e-04, 9.7394e-05, 1.5056e-04, 1.4710e-04,\n",
      "        9.6262e-05, 1.8632e-04, 9.3639e-05, 7.5281e-05, 3.3760e-03, 7.4530e-04,\n",
      "        3.7122e-04, 3.1137e-04, 1.5998e-04, 1.8883e-04, 8.4281e-05, 7.5579e-05,\n",
      "        1.3387e-04, 1.7920e-03, 9.7322e-04, 1.8919e-04, 9.5987e-04, 9.0332e-03,\n",
      "        7.3671e-04, 3.3264e-03, 2.1482e-04, 9.7609e-04, 2.3079e-03, 2.8849e-04,\n",
      "        9.7609e-04, 4.1008e-04, 1.2608e-03, 3.6836e-04, 3.3593e-04, 4.6659e-04,\n",
      "        4.9162e-04, 8.2207e-04, 7.0047e-04, 7.4615e-03, 8.9493e-03, 1.3000e-02,\n",
      "        1.7776e-03, 9.6464e-04, 4.5204e-03, 5.3253e-03, 6.3248e-03, 1.3332e-03,\n",
      "        4.8676e-03, 6.3019e-03, 6.8283e-03, 1.4954e-03, 9.0981e-04, 1.7519e-03,\n",
      "        8.9407e-04, 6.3992e-04, 7.8440e-04, 3.5906e-04, 2.1362e-04, 7.1001e-04,\n",
      "        5.5075e-04, 1.8415e-03, 3.0518e-03, 1.3332e-03, 9.2030e-04, 1.9054e-03,\n",
      "        2.7561e-03, 3.1128e-03, 3.2330e-03, 2.1629e-03, 1.1520e-03, 1.1950e-03,\n",
      "        2.7637e-03, 3.0351e-04, 8.6308e-04, 9.0790e-04, 9.9869e-03, 3.1376e-03,\n",
      "        1.3790e-03, 1.8272e-03, 1.5144e-03, 4.8561e-03, 1.8616e-03, 7.4387e-04,\n",
      "        1.2611e-02, 1.5640e-03, 1.9016e-03, 6.0616e-03, 1.8377e-03, 1.0628e-02,\n",
      "        2.2415e-02, 2.0386e-02, 1.2840e-02, 2.6035e-03, 4.1542e-03, 2.6169e-03,\n",
      "        5.6305e-03, 1.3514e-03, 6.1722e-03, 5.7030e-03, 1.2074e-03, 9.4032e-04,\n",
      "        1.2878e-02, 1.5762e-02, 6.9962e-03, 6.7482e-03, 1.2680e-02, 5.5237e-03,\n",
      "        3.4561e-03, 1.3380e-03, 3.4962e-03, 7.6771e-04, 7.3051e-03, 6.4888e-03,\n",
      "        1.1337e-02, 7.5378e-03, 8.0566e-03, 4.6501e-03, 2.8725e-03, 4.8027e-03,\n",
      "        1.1757e-02, 2.1915e-03, 2.7790e-03, 1.7044e-02, 4.5105e-02, 7.0648e-03,\n",
      "        1.7075e-02, 1.8951e-02, 5.3864e-03, 3.5000e-03, 2.1420e-03, 7.6942e-03,\n",
      "        6.2418e-04, 3.6049e-03, 3.7346e-03, 1.6899e-03, 1.7563e-02, 1.2428e-02,\n",
      "        1.0857e-02, 1.4618e-02, 8.9874e-03, 5.2872e-03, 2.8214e-02, 3.4576e-02,\n",
      "        1.3046e-02, 1.5701e-02, 1.2276e-02, 5.8708e-03, 3.8338e-03, 4.4746e-03,\n",
      "        5.7564e-03, 1.3268e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [111] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [111] : torch.Size([1, 32, 1, 183])\n",
      "Last layer attentions for generated token [111] : tensor([1.3696e-01, 1.3696e-01, 1.2130e-04, 3.0732e-04, 1.3530e-04, 1.1778e-03,\n",
      "        1.7369e-04, 1.9538e-04, 2.0707e-04, 2.7537e-04, 7.7868e-04, 3.1519e-04,\n",
      "        2.5415e-04, 3.0851e-04, 5.8594e-03, 4.4899e-03, 2.0993e-04, 6.5947e-04,\n",
      "        1.0175e-04, 1.5390e-04, 2.6643e-05, 1.2267e-04, 2.4719e-03, 7.5006e-04,\n",
      "        2.5787e-03, 8.4000e-03, 1.5306e-03, 2.1124e-04, 2.1875e-04, 2.1708e-04,\n",
      "        2.5821e-04, 9.5606e-04, 6.7377e-04, 4.3416e-04, 3.7789e-04, 2.5129e-04,\n",
      "        1.7750e-04, 3.5715e-04, 1.3745e-04, 1.3685e-04, 2.0580e-03, 7.6056e-04,\n",
      "        5.5981e-04, 6.4278e-04, 5.6171e-04, 2.8634e-04, 2.3329e-04, 8.3685e-05,\n",
      "        2.2876e-04, 1.3809e-03, 7.0047e-04, 1.7643e-04, 1.6232e-03, 5.2376e-03,\n",
      "        1.0977e-03, 3.6392e-03, 3.6216e-04, 7.9250e-04, 1.2951e-03, 5.6982e-04,\n",
      "        7.2145e-04, 6.7234e-04, 2.0370e-03, 5.5742e-04, 3.6502e-04, 4.0865e-04,\n",
      "        9.3555e-04, 1.5154e-03, 8.8739e-04, 9.7504e-03, 7.1907e-03, 1.3123e-02,\n",
      "        5.0659e-03, 2.2163e-03, 4.9782e-03, 6.8855e-03, 7.3471e-03, 5.0659e-03,\n",
      "        8.3847e-03, 8.3847e-03, 6.5308e-03, 7.2021e-03, 4.0932e-03, 3.5801e-03,\n",
      "        8.9598e-04, 4.9744e-03, 3.3894e-03, 1.5211e-03, 4.0078e-04, 1.4086e-03,\n",
      "        2.6627e-03, 2.8648e-03, 6.6185e-03, 2.2316e-03, 4.7493e-03, 4.5280e-03,\n",
      "        5.9128e-03, 4.1466e-03, 8.4534e-03, 3.9558e-03, 6.2370e-03, 5.3825e-03,\n",
      "        1.1459e-02, 1.5182e-03, 5.1422e-03, 4.1389e-03, 1.6525e-02, 1.0193e-02,\n",
      "        2.5902e-03, 4.1962e-03, 3.5858e-03, 8.9645e-03, 2.5291e-03, 1.5516e-03,\n",
      "        5.8746e-03, 2.9068e-03, 5.8594e-03, 5.5313e-03, 1.7195e-03, 7.3776e-03,\n",
      "        2.5604e-02, 2.8564e-02, 1.6312e-02, 3.5667e-03, 6.8398e-03, 9.4833e-03,\n",
      "        5.2032e-03, 2.7447e-03, 5.9547e-03, 3.5553e-03, 1.4172e-03, 1.1158e-03,\n",
      "        6.8207e-03, 1.4893e-02, 7.4844e-03, 3.2940e-03, 8.4305e-03, 1.3870e-02,\n",
      "        5.6725e-03, 1.4620e-03, 1.5965e-03, 4.7231e-04, 5.7030e-03, 1.1436e-02,\n",
      "        1.5099e-02, 4.9934e-03, 6.4888e-03, 3.6354e-03, 1.3428e-03, 3.0384e-03,\n",
      "        4.8981e-03, 2.8934e-03, 1.0414e-03, 1.0674e-02, 1.9775e-02, 3.5400e-03,\n",
      "        3.6583e-03, 5.9929e-03, 3.9062e-03, 2.0981e-03, 8.9264e-04, 1.8940e-03,\n",
      "        1.4400e-04, 1.1969e-03, 6.0272e-04, 4.0960e-04, 4.2877e-03, 9.7122e-03,\n",
      "        1.0468e-02, 8.9188e-03, 6.6071e-03, 2.9697e-03, 6.4545e-03, 2.1019e-03,\n",
      "        2.3422e-03, 1.5306e-03, 1.5717e-03, 2.1000e-03, 2.4891e-04, 5.1546e-04,\n",
      "        6.0034e-04, 9.0103e-03, 2.8732e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [112] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [112] : torch.Size([1, 32, 1, 184])\n",
      "Last layer attentions for generated token [112] : tensor([1.3647e-01, 1.3647e-01, 6.0380e-05, 1.6356e-04, 5.7399e-05, 1.2407e-03,\n",
      "        2.1207e-04, 2.5725e-04, 4.8923e-04, 4.4441e-04, 1.0252e-03, 4.0722e-04,\n",
      "        2.8086e-04, 7.5006e-04, 5.5389e-03, 6.1493e-03, 3.4881e-04, 6.1846e-04,\n",
      "        1.0514e-04, 1.3351e-04, 5.7638e-05, 2.0671e-04, 4.2648e-03, 7.3433e-04,\n",
      "        2.8648e-03, 7.6714e-03, 4.0555e-04, 3.4952e-04, 3.1829e-04, 3.9625e-04,\n",
      "        6.2561e-04, 7.9727e-04, 8.8739e-04, 8.0347e-04, 6.8712e-04, 6.5422e-04,\n",
      "        3.1590e-04, 4.3011e-04, 2.2089e-04, 2.6798e-04, 2.0161e-03, 8.9073e-04,\n",
      "        4.1604e-04, 6.0892e-04, 5.2977e-04, 3.5381e-04, 2.1625e-04, 2.1625e-04,\n",
      "        4.5419e-04, 1.4801e-03, 9.5415e-04, 1.0437e-04, 2.1648e-03, 2.6016e-03,\n",
      "        1.9407e-03, 2.6989e-03, 2.9564e-04, 1.0681e-03, 1.4982e-03, 8.6689e-04,\n",
      "        8.2541e-04, 1.1024e-03, 1.8082e-03, 7.2718e-04, 8.9788e-04, 6.7663e-04,\n",
      "        1.0414e-03, 1.7338e-03, 1.2064e-03, 1.1528e-02, 5.2185e-03, 1.6602e-02,\n",
      "        2.7599e-03, 3.3455e-03, 8.1406e-03, 6.6872e-03, 4.8561e-03, 2.6722e-03,\n",
      "        6.6643e-03, 5.5313e-03, 3.2101e-03, 3.0174e-03, 3.4943e-03, 4.2305e-03,\n",
      "        1.3285e-03, 3.0060e-03, 2.8992e-03, 2.3956e-03, 1.8015e-03, 3.8090e-03,\n",
      "        4.6310e-03, 7.2746e-03, 8.0185e-03, 4.2000e-03, 1.0422e-02, 6.0387e-03,\n",
      "        5.7983e-03, 4.7569e-03, 6.0387e-03, 4.0207e-03, 5.8098e-03, 3.5267e-03,\n",
      "        6.3858e-03, 2.0599e-03, 4.2648e-03, 6.1607e-03, 2.0950e-02, 8.7662e-03,\n",
      "        3.8681e-03, 1.0933e-02, 5.4131e-03, 1.1635e-02, 4.1924e-03, 4.0016e-03,\n",
      "        1.0979e-02, 4.6387e-03, 6.8092e-03, 7.4692e-03, 4.0817e-03, 8.6899e-03,\n",
      "        3.7598e-02, 2.2903e-02, 1.1963e-02, 3.7498e-03, 8.9035e-03, 1.1604e-02,\n",
      "        7.4081e-03, 3.1643e-03, 8.0032e-03, 2.6188e-03, 1.3399e-03, 1.5678e-03,\n",
      "        4.6539e-03, 7.3357e-03, 5.2986e-03, 2.7714e-03, 4.9019e-03, 6.5384e-03,\n",
      "        3.8452e-03, 1.5526e-03, 2.2221e-03, 4.5586e-04, 2.9678e-03, 5.7869e-03,\n",
      "        7.6332e-03, 3.3417e-03, 4.4975e-03, 2.1400e-03, 6.2323e-04, 2.6436e-03,\n",
      "        2.8362e-03, 2.5482e-03, 1.1110e-03, 5.5504e-03, 1.5236e-02, 4.2267e-03,\n",
      "        6.9618e-03, 5.9166e-03, 4.0550e-03, 2.4281e-03, 1.7023e-03, 1.3494e-03,\n",
      "        2.5868e-04, 1.7109e-03, 5.5218e-04, 3.4285e-04, 3.3455e-03, 6.8588e-03,\n",
      "        8.9798e-03, 6.4087e-03, 6.4888e-03, 2.8934e-03, 5.3635e-03, 2.4586e-03,\n",
      "        2.6569e-03, 3.1719e-03, 2.1095e-03, 2.3251e-03, 5.6076e-04, 1.1005e-03,\n",
      "        1.2579e-03, 6.9122e-03, 3.0594e-02, 9.1553e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [113] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [113] : torch.Size([1, 32, 1, 185])\n",
      "Last layer attentions for generated token [113] : tensor([2.2314e-01, 2.2400e-01, 5.9128e-05, 9.3699e-05, 3.3677e-05, 7.9250e-04,\n",
      "        8.1420e-05, 1.2863e-04, 1.7369e-04, 1.9526e-04, 4.0555e-04, 1.6642e-04,\n",
      "        1.5092e-04, 4.1676e-04, 3.4676e-03, 2.8114e-03, 1.5688e-04, 2.9039e-04,\n",
      "        6.7472e-05, 9.5546e-05, 3.6120e-05, 1.5998e-04, 7.1411e-03, 6.6853e-04,\n",
      "        1.2884e-03, 3.5648e-03, 2.0504e-04, 1.5211e-04, 1.6129e-04, 2.0111e-04,\n",
      "        2.0504e-04, 3.4738e-04, 4.7398e-04, 3.4332e-04, 3.4142e-04, 3.2210e-04,\n",
      "        1.3006e-04, 3.6550e-04, 2.6631e-04, 1.5390e-04, 8.7023e-04, 6.5279e-04,\n",
      "        2.2256e-04, 4.9591e-04, 4.9591e-04, 2.6965e-04, 2.3150e-04, 2.3508e-04,\n",
      "        3.7289e-04, 7.6628e-04, 8.9598e-04, 1.1843e-04, 1.4830e-03, 2.9144e-03,\n",
      "        9.9373e-04, 3.3817e-03, 1.8632e-04, 1.3037e-03, 8.6355e-04, 8.0776e-04,\n",
      "        7.4577e-04, 7.8154e-04, 1.7071e-03, 6.3038e-04, 5.3072e-04, 6.3658e-04,\n",
      "        9.7466e-04, 1.0538e-03, 8.2397e-04, 6.9618e-03, 4.2915e-03, 1.2077e-02,\n",
      "        1.2465e-03, 1.8635e-03, 6.2180e-03, 3.4142e-03, 5.0926e-03, 1.1282e-03,\n",
      "        5.4398e-03, 6.8855e-03, 2.8973e-03, 9.6130e-04, 1.7891e-03, 3.9711e-03,\n",
      "        1.5240e-03, 9.9373e-04, 1.4601e-03, 1.1845e-03, 1.1711e-03, 5.0774e-03,\n",
      "        2.1133e-03, 3.0365e-03, 3.3131e-03, 3.7842e-03, 7.7782e-03, 5.7983e-03,\n",
      "        5.0354e-03, 4.2191e-03, 2.1343e-03, 2.9926e-03, 1.7958e-03, 2.2621e-03,\n",
      "        5.9547e-03, 1.6079e-03, 2.5959e-03, 2.8725e-03, 1.1726e-02, 4.7646e-03,\n",
      "        4.0092e-03, 6.3286e-03, 2.5845e-03, 9.2468e-03, 3.5000e-03, 1.6670e-03,\n",
      "        4.8828e-03, 3.3417e-03, 2.8858e-03, 4.3526e-03, 3.8624e-03, 4.2305e-03,\n",
      "        3.1403e-02, 9.1705e-03, 4.9782e-03, 3.6449e-03, 4.0054e-03, 4.7722e-03,\n",
      "        6.6948e-03, 1.5440e-03, 4.9400e-03, 1.7662e-03, 1.1530e-03, 8.8739e-04,\n",
      "        2.4910e-03, 7.0610e-03, 2.1133e-03, 1.2541e-03, 2.5501e-03, 4.6158e-03,\n",
      "        2.1019e-03, 7.4291e-04, 9.1553e-04, 2.1791e-04, 2.4910e-03, 4.9477e-03,\n",
      "        5.1689e-03, 3.0670e-03, 2.9984e-03, 1.0576e-03, 4.6396e-04, 1.6022e-03,\n",
      "        2.8973e-03, 2.5311e-03, 1.1148e-03, 6.0425e-03, 2.1957e-02, 2.5158e-03,\n",
      "        4.9934e-03, 5.0507e-03, 3.7804e-03, 1.8244e-03, 1.2884e-03, 6.1464e-04,\n",
      "        1.4293e-04, 1.3342e-03, 4.8518e-04, 3.3951e-04, 3.5229e-03, 6.3591e-03,\n",
      "        6.2485e-03, 7.0000e-03, 6.1874e-03, 2.8286e-03, 3.9520e-03, 1.6851e-03,\n",
      "        2.2278e-03, 2.4147e-03, 1.9989e-03, 2.9030e-03, 5.8651e-04, 1.2436e-03,\n",
      "        1.5240e-03, 6.9656e-03, 5.0812e-02, 8.8577e-03, 5.8937e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [114] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [114] : torch.Size([1, 32, 1, 186])\n",
      "Last layer attentions for generated token [114] : tensor([1.7908e-01, 1.7908e-01, 1.0294e-04, 2.8706e-04, 8.9109e-05, 1.2226e-03,\n",
      "        1.3423e-04, 1.9503e-04, 1.5891e-04, 5.0592e-04, 9.1791e-04, 2.1291e-04,\n",
      "        3.0088e-04, 7.4625e-04, 4.4365e-03, 5.5008e-03, 2.2316e-04, 1.0376e-03,\n",
      "        1.4412e-04, 1.8430e-04, 6.7770e-05, 5.3740e-04, 7.2479e-03, 9.4891e-04,\n",
      "        1.8024e-03, 5.0087e-03, 3.5238e-04, 3.4571e-04, 3.3164e-04, 2.7013e-04,\n",
      "        4.0793e-04, 5.3406e-04, 7.1907e-04, 5.5218e-04, 4.0793e-04, 5.5218e-04,\n",
      "        3.7003e-04, 3.8695e-04, 4.2176e-04, 4.3774e-04, 1.3046e-03, 8.4686e-04,\n",
      "        3.6216e-04, 7.2861e-04, 5.3644e-04, 5.6219e-04, 1.9884e-04, 3.2520e-04,\n",
      "        5.0497e-04, 1.2207e-03, 1.1044e-03, 1.3268e-04, 1.7824e-03, 3.4161e-03,\n",
      "        1.2865e-03, 2.6474e-03, 1.9765e-04, 1.0767e-03, 1.3323e-03, 7.7868e-04,\n",
      "        9.3555e-04, 8.1158e-04, 1.5869e-03, 7.9250e-04, 1.1511e-03, 6.3086e-04,\n",
      "        1.1091e-03, 7.4339e-04, 5.7077e-04, 1.1055e-02, 7.1411e-03, 1.3306e-02,\n",
      "        3.6583e-03, 2.1877e-03, 6.0539e-03, 4.9667e-03, 7.0953e-03, 2.3270e-03,\n",
      "        4.4365e-03, 8.6288e-03, 3.0441e-03, 1.6041e-03, 1.9131e-03, 1.7738e-03,\n",
      "        1.4181e-03, 1.1158e-03, 1.5697e-03, 1.1740e-03, 1.5440e-03, 4.5624e-03,\n",
      "        1.8740e-03, 3.5667e-03, 2.9507e-03, 3.8872e-03, 3.9253e-03, 2.5997e-03,\n",
      "        4.5471e-03, 5.7487e-03, 3.6259e-03, 2.8973e-03, 3.0155e-03, 2.2774e-03,\n",
      "        3.6831e-03, 1.2302e-03, 2.6112e-03, 1.8740e-03, 7.3547e-03, 5.8670e-03,\n",
      "        4.2610e-03, 6.6223e-03, 2.9945e-03, 6.0310e-03, 5.0163e-03, 3.3207e-03,\n",
      "        7.5989e-03, 8.9722e-03, 9.9640e-03, 3.8223e-03, 7.4387e-03, 5.1689e-03,\n",
      "        3.5980e-02, 1.6235e-02, 7.7209e-03, 6.2981e-03, 6.1073e-03, 4.6616e-03,\n",
      "        6.2141e-03, 4.2610e-03, 5.5885e-03, 1.9522e-03, 1.6680e-03, 1.8148e-03,\n",
      "        2.5692e-03, 6.7215e-03, 2.7962e-03, 1.7366e-03, 2.8934e-03, 5.5008e-03,\n",
      "        2.4223e-03, 1.1225e-03, 1.3962e-03, 5.2166e-04, 3.0231e-03, 4.0665e-03,\n",
      "        8.8577e-03, 3.5839e-03, 4.2572e-03, 2.3594e-03, 3.4618e-04, 1.7796e-03,\n",
      "        3.3989e-03, 4.7493e-03, 1.1463e-03, 4.1885e-03, 1.8845e-02, 2.3937e-03,\n",
      "        3.7441e-03, 4.2534e-03, 4.3030e-03, 1.3638e-03, 1.2178e-03, 1.2016e-03,\n",
      "        3.2854e-04, 1.3914e-03, 8.2445e-04, 2.9731e-04, 1.6909e-03, 3.8948e-03,\n",
      "        4.9934e-03, 5.8060e-03, 5.3215e-03, 2.0943e-03, 4.7226e-03, 2.0828e-03,\n",
      "        1.5011e-03, 2.2755e-03, 2.3708e-03, 4.4212e-03, 7.1478e-04, 1.3514e-03,\n",
      "        1.8005e-03, 6.4621e-03, 5.4535e-02, 1.2100e-02, 2.1240e-02, 2.3327e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [115] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [115] : torch.Size([1, 32, 1, 187])\n",
      "Last layer attentions for generated token [115] : tensor([1.2744e-01, 1.2744e-01, 8.8394e-05, 2.4366e-04, 8.9467e-05, 1.2579e-03,\n",
      "        2.1970e-04, 3.8767e-04, 5.0783e-04, 3.6573e-04, 8.5878e-04, 4.4203e-04,\n",
      "        2.0278e-04, 5.0783e-04, 4.6425e-03, 4.8370e-03, 2.6560e-04, 4.5872e-04,\n",
      "        9.7871e-05, 1.3173e-04, 5.1141e-05, 2.8920e-04, 2.6970e-03, 6.1846e-04,\n",
      "        3.5057e-03, 6.6452e-03, 3.7956e-04, 2.7275e-04, 2.5678e-04, 6.7282e-04,\n",
      "        5.4455e-04, 7.2432e-04, 6.9284e-04, 5.9032e-04, 6.2323e-04, 3.0494e-04,\n",
      "        1.4830e-04, 4.5872e-04, 1.9348e-04, 2.7275e-04, 3.1681e-03, 6.6233e-04,\n",
      "        3.0494e-04, 6.0320e-04, 6.3324e-04, 4.4370e-04, 3.1972e-04, 2.0432e-04,\n",
      "        6.1464e-04, 2.6302e-03, 8.6546e-04, 1.5879e-04, 1.2913e-03, 3.2463e-03,\n",
      "        1.2169e-03, 1.8940e-03, 1.4436e-04, 5.9366e-04, 1.1854e-03, 6.5565e-04,\n",
      "        5.8889e-04, 1.1969e-03, 2.3842e-03, 5.7554e-04, 5.7983e-04, 1.0862e-03,\n",
      "        6.9666e-04, 1.3685e-03, 7.9727e-04, 1.9135e-02, 8.5144e-03, 1.8951e-02,\n",
      "        2.9430e-03, 3.7079e-03, 1.2146e-02, 9.9258e-03, 7.3280e-03, 3.6106e-03,\n",
      "        1.1620e-02, 5.7869e-03, 2.5444e-03, 2.3746e-03, 3.7003e-03, 5.3940e-03,\n",
      "        1.1377e-03, 2.0256e-03, 2.8191e-03, 2.1381e-03, 1.4410e-03, 3.0746e-03,\n",
      "        3.4161e-03, 6.5575e-03, 7.8354e-03, 3.6583e-03, 9.8648e-03, 1.0231e-02,\n",
      "        9.4070e-03, 4.8866e-03, 7.3853e-03, 3.8185e-03, 4.5280e-03, 3.8433e-03,\n",
      "        7.8735e-03, 1.7672e-03, 4.4479e-03, 5.3253e-03, 3.2593e-02, 1.0635e-02,\n",
      "        3.6831e-03, 9.3307e-03, 3.1109e-03, 1.2726e-02, 3.0346e-03, 2.6474e-03,\n",
      "        1.7105e-02, 4.4670e-03, 6.5422e-03, 7.9269e-03, 3.3798e-03, 9.2316e-03,\n",
      "        3.5034e-02, 2.5620e-02, 1.4603e-02, 2.6169e-03, 8.3771e-03, 7.8087e-03,\n",
      "        5.6190e-03, 1.8358e-03, 5.5580e-03, 2.9297e-03, 8.5354e-04, 7.7105e-04,\n",
      "        5.3864e-03, 6.3362e-03, 1.9293e-03, 2.2373e-03, 4.8332e-03, 3.9215e-03,\n",
      "        2.0199e-03, 6.6471e-04, 1.1873e-03, 2.4319e-04, 2.9926e-03, 4.7836e-03,\n",
      "        9.9792e-03, 3.5896e-03, 3.2978e-03, 1.4648e-03, 2.5272e-04, 1.7281e-03,\n",
      "        3.6240e-03, 1.5306e-03, 9.7322e-04, 5.6877e-03, 1.4671e-02, 2.2602e-03,\n",
      "        5.6114e-03, 6.2790e-03, 2.3918e-03, 1.1349e-03, 7.7105e-04, 1.6041e-03,\n",
      "        1.6510e-04, 1.0691e-03, 4.3511e-04, 2.2185e-04, 3.3588e-03, 5.9547e-03,\n",
      "        8.2932e-03, 6.2981e-03, 4.4594e-03, 2.7218e-03, 2.6836e-03, 2.8229e-03,\n",
      "        2.0752e-03, 2.8858e-03, 2.9583e-03, 3.3703e-03, 7.7248e-04, 9.6178e-04,\n",
      "        1.5821e-03, 9.0561e-03, 3.5889e-02, 7.0572e-03, 3.6373e-03, 1.3590e-03,\n",
      "        2.7809e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [116] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [116] : torch.Size([1, 32, 1, 188])\n",
      "Last layer attentions for generated token [116] : tensor([1.3953e-01, 1.3953e-01, 7.3671e-05, 2.3770e-04, 6.3956e-05, 1.6699e-03,\n",
      "        3.0756e-04, 4.6825e-04, 7.5245e-04, 4.1151e-04, 7.7629e-04, 3.5143e-04,\n",
      "        1.3387e-04, 3.9649e-04, 6.3591e-03, 5.7182e-03, 3.4523e-04, 4.2295e-04,\n",
      "        1.0967e-04, 8.8513e-05, 5.1796e-05, 3.4857e-04, 3.8834e-03, 1.0204e-03,\n",
      "        4.5776e-03, 9.7656e-03, 6.1417e-04, 3.1376e-04, 2.4247e-04, 1.0405e-03,\n",
      "        8.6975e-04, 9.3842e-04, 6.0606e-04, 4.5657e-04, 6.0940e-04, 2.4867e-04,\n",
      "        9.4593e-05, 6.0844e-04, 1.8191e-04, 1.7023e-04, 3.3207e-03, 9.1314e-04,\n",
      "        2.9302e-04, 5.2023e-04, 5.1498e-04, 2.0254e-04, 4.4584e-04, 1.9443e-04,\n",
      "        4.4680e-04, 2.5578e-03, 1.0653e-03, 2.0778e-04, 1.1272e-03, 5.1651e-03,\n",
      "        9.3842e-04, 3.2063e-03, 1.7118e-04, 5.3453e-04, 9.1648e-04, 5.0211e-04,\n",
      "        3.6526e-04, 8.7976e-04, 2.4872e-03, 5.5599e-04, 4.5919e-04, 1.1511e-03,\n",
      "        6.5279e-04, 1.1978e-03, 6.5756e-04, 2.2095e-02, 1.0544e-02, 1.7700e-02,\n",
      "        3.8071e-03, 3.4046e-03, 1.1696e-02, 9.7809e-03, 5.7182e-03, 2.3232e-03,\n",
      "        1.3947e-02, 8.0719e-03, 2.9640e-03, 2.4033e-03, 2.8133e-03, 4.2534e-03,\n",
      "        8.2636e-04, 1.9016e-03, 2.1648e-03, 1.5383e-03, 9.3651e-04, 1.9064e-03,\n",
      "        2.0962e-03, 5.8975e-03, 8.1406e-03, 2.7561e-03, 1.3100e-02, 1.3733e-02,\n",
      "        1.0956e-02, 4.8676e-03, 4.9858e-03, 2.4815e-03, 3.8338e-03, 4.0474e-03,\n",
      "        8.8272e-03, 1.5564e-03, 2.9240e-03, 3.9825e-03, 3.0182e-02, 8.4763e-03,\n",
      "        2.0218e-03, 9.6207e-03, 3.3302e-03, 1.3405e-02, 2.3022e-03, 1.3084e-03,\n",
      "        1.0910e-02, 2.1000e-03, 3.5381e-03, 6.4659e-03, 2.5635e-03, 6.1531e-03,\n",
      "        2.6535e-02, 1.5419e-02, 7.7095e-03, 1.7433e-03, 6.6414e-03, 3.8643e-03,\n",
      "        3.0594e-03, 8.3447e-04, 3.8185e-03, 2.2221e-03, 5.0116e-04, 4.8018e-04,\n",
      "        3.2368e-03, 5.7526e-03, 1.7080e-03, 2.0580e-03, 3.5076e-03, 3.1986e-03,\n",
      "        1.4954e-03, 5.7364e-04, 1.4553e-03, 4.1723e-04, 3.1910e-03, 3.9330e-03,\n",
      "        5.5161e-03, 2.7599e-03, 2.8133e-03, 1.2484e-03, 2.6369e-04, 1.3561e-03,\n",
      "        3.5362e-03, 9.8896e-04, 9.6464e-04, 6.0158e-03, 1.8661e-02, 1.9503e-03,\n",
      "        5.5809e-03, 4.9210e-03, 2.0294e-03, 1.1826e-03, 8.9550e-04, 1.8482e-03,\n",
      "        1.4818e-04, 1.3800e-03, 4.6015e-04, 2.6011e-04, 5.3062e-03, 6.0501e-03,\n",
      "        5.9280e-03, 5.6190e-03, 4.3411e-03, 2.3136e-03, 2.8992e-03, 2.1973e-03,\n",
      "        1.6022e-03, 2.1648e-03, 2.5940e-03, 3.1738e-03, 8.8692e-04, 1.2445e-03,\n",
      "        1.7462e-03, 9.3231e-03, 5.4077e-02, 7.1602e-03, 2.8744e-03, 2.5826e-03,\n",
      "        3.8452e-03, 2.0538e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [117] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [117] : torch.Size([1, 32, 1, 189])\n",
      "Last layer attentions for generated token [117] : tensor([2.1594e-01, 2.1558e-01, 2.4462e-04, 6.3801e-04, 1.4889e-04, 1.9970e-03,\n",
      "        2.7609e-04, 2.5797e-04, 4.5156e-04, 5.3310e-04, 8.0824e-04, 2.0361e-04,\n",
      "        2.5845e-04, 4.3607e-04, 5.4817e-03, 6.0463e-03, 3.7217e-04, 5.1403e-04,\n",
      "        1.9348e-04, 1.6046e-04, 5.2273e-05, 6.6090e-04, 1.2749e-02, 1.7099e-03,\n",
      "        3.2597e-03, 1.2970e-02, 5.0974e-04, 4.9591e-04, 3.8266e-04, 4.5705e-04,\n",
      "        7.8630e-04, 7.4625e-04, 5.3644e-04, 3.5524e-04, 3.4428e-04, 2.7776e-04,\n",
      "        2.3711e-04, 5.8889e-04, 3.3498e-04, 3.3498e-04, 2.5806e-03, 1.2817e-03,\n",
      "        2.8491e-04, 4.5776e-04, 3.1543e-04, 3.1710e-04, 2.0683e-04, 2.3293e-04,\n",
      "        3.4904e-04, 1.3695e-03, 1.2207e-03, 1.3399e-04, 1.8721e-03, 4.7798e-03,\n",
      "        1.6909e-03, 4.5319e-03, 3.4571e-04, 1.5278e-03, 1.9588e-03, 7.4339e-04,\n",
      "        1.0080e-03, 9.1791e-04, 3.2787e-03, 9.3031e-04, 1.0920e-03, 9.7513e-04,\n",
      "        1.1311e-03, 9.6560e-04, 1.1091e-03, 9.6054e-03, 9.8038e-03, 1.4412e-02,\n",
      "        2.5349e-03, 2.5997e-03, 4.9477e-03, 5.4436e-03, 6.9504e-03, 1.9407e-03,\n",
      "        6.2332e-03, 9.0942e-03, 5.8784e-03, 1.4925e-03, 1.6966e-03, 2.2621e-03,\n",
      "        9.4128e-04, 9.4700e-04, 1.1559e-03, 8.6737e-04, 7.7868e-04, 1.6813e-03,\n",
      "        1.2321e-03, 1.9913e-03, 1.7624e-03, 2.7962e-03, 4.6234e-03, 3.1376e-03,\n",
      "        6.1302e-03, 3.0804e-03, 2.9621e-03, 1.1511e-03, 7.6962e-04, 1.1358e-03,\n",
      "        2.0714e-03, 5.2309e-04, 8.3542e-04, 1.1787e-03, 9.5673e-03, 3.4504e-03,\n",
      "        2.1458e-03, 5.3558e-03, 1.6079e-03, 5.8937e-03, 3.2921e-03, 1.1024e-03,\n",
      "        8.1406e-03, 2.6741e-03, 3.9482e-03, 2.7771e-03, 2.7828e-03, 6.4926e-03,\n",
      "        2.1179e-02, 6.6757e-03, 2.9945e-03, 1.9798e-03, 2.9945e-03, 1.7948e-03,\n",
      "        2.5864e-03, 9.6750e-04, 2.9716e-03, 1.7605e-03, 7.3910e-04, 9.7322e-04,\n",
      "        3.2978e-03, 6.8054e-03, 2.5024e-03, 1.7166e-03, 4.1237e-03, 5.1880e-03,\n",
      "        2.0885e-03, 1.2131e-03, 1.4105e-03, 5.2309e-04, 3.1204e-03, 2.4281e-03,\n",
      "        4.8866e-03, 2.9316e-03, 3.8757e-03, 1.0242e-03, 3.0446e-04, 6.1035e-04,\n",
      "        3.3302e-03, 1.7414e-03, 9.4509e-04, 5.4970e-03, 1.5732e-02, 2.8381e-03,\n",
      "        4.5090e-03, 5.2261e-03, 3.1567e-03, 1.4105e-03, 1.5278e-03, 1.3514e-03,\n",
      "        2.2757e-04, 1.7128e-03, 8.9455e-04, 4.1771e-04, 3.3836e-03, 3.4122e-03,\n",
      "        3.5801e-03, 4.6158e-03, 4.4327e-03, 2.4014e-03, 9.3307e-03, 2.8458e-03,\n",
      "        1.6422e-03, 1.6851e-03, 2.8019e-03, 3.7994e-03, 9.8801e-04, 1.6489e-03,\n",
      "        2.4509e-03, 8.1635e-03, 5.2856e-02, 7.8354e-03, 6.0997e-03, 1.4782e-03,\n",
      "        2.2717e-03, 1.1322e-02, 2.9659e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [118] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [118] : torch.Size([1, 32, 1, 190])\n",
      "Last layer attentions for generated token [118] : tensor([8.1360e-02, 8.1177e-02, 3.7611e-05, 1.7321e-04, 3.7432e-05, 7.5674e-04,\n",
      "        7.9274e-05, 8.5413e-05, 1.2231e-04, 2.8896e-04, 8.2779e-04, 1.1766e-04,\n",
      "        3.0637e-04, 4.1866e-04, 5.1765e-03, 6.1378e-03, 2.3997e-04, 1.1377e-03,\n",
      "        3.0923e-04, 3.1114e-04, 8.5413e-05, 6.2132e-04, 5.2605e-03, 2.3136e-03,\n",
      "        3.3417e-03, 7.5951e-03, 1.0090e-03, 5.4169e-04, 2.4462e-04, 2.5797e-04,\n",
      "        5.1880e-04, 7.8964e-04, 5.5456e-04, 4.3273e-04, 2.8944e-04, 4.7255e-04,\n",
      "        2.0444e-04, 3.4380e-04, 1.6296e-04, 1.6296e-04, 2.3575e-03, 1.7214e-03,\n",
      "        4.3035e-04, 8.4829e-04, 3.4165e-04, 2.7823e-04, 1.5771e-04, 1.7154e-04,\n",
      "        3.3975e-04, 1.4935e-03, 1.5736e-03, 1.0967e-04, 1.2197e-03, 3.6106e-03,\n",
      "        1.7748e-03, 4.8676e-03, 3.8052e-04, 1.0147e-03, 1.0614e-03, 4.9639e-04,\n",
      "        8.0252e-04, 5.3024e-04, 1.3866e-03, 9.6846e-04, 7.3767e-04, 7.2479e-04,\n",
      "        1.9360e-03, 8.2111e-04, 6.3944e-04, 8.1863e-03, 7.0457e-03, 2.0157e-02,\n",
      "        2.6016e-03, 2.9469e-03, 5.7716e-03, 5.7869e-03, 1.2634e-02, 2.3613e-03,\n",
      "        1.0323e-02, 1.4015e-02, 6.5842e-03, 2.3270e-03, 2.6302e-03, 2.5654e-03,\n",
      "        1.6966e-03, 1.0643e-03, 1.6966e-03, 1.0862e-03, 6.2990e-04, 2.8172e-03,\n",
      "        1.1969e-03, 3.2349e-03, 2.0256e-03, 2.7905e-03, 2.3689e-03, 1.6756e-03,\n",
      "        3.7880e-03, 5.5389e-03, 6.0196e-03, 4.6234e-03, 2.1763e-03, 2.7122e-03,\n",
      "        3.2787e-03, 7.8821e-04, 1.8511e-03, 1.1578e-03, 6.0730e-03, 3.9711e-03,\n",
      "        1.6499e-03, 2.0790e-03, 1.4811e-03, 4.9973e-03, 3.3569e-03, 1.4668e-03,\n",
      "        8.7051e-03, 4.3716e-03, 3.7403e-03, 2.8610e-03, 3.6373e-03, 4.3068e-03,\n",
      "        4.4678e-02, 2.4124e-02, 1.2825e-02, 6.8703e-03, 7.3700e-03, 6.8092e-03,\n",
      "        5.1498e-03, 2.9068e-03, 4.4289e-03, 2.3155e-03, 1.0471e-03, 1.2970e-03,\n",
      "        3.3855e-03, 9.3002e-03, 5.7220e-03, 1.7748e-03, 4.2686e-03, 9.2163e-03,\n",
      "        2.4338e-03, 1.2741e-03, 1.5831e-03, 5.6648e-04, 3.3398e-03, 5.7526e-03,\n",
      "        7.9803e-03, 2.3575e-03, 6.4316e-03, 1.9045e-03, 1.1005e-03, 1.9741e-03,\n",
      "        3.7632e-03, 2.9526e-03, 8.5068e-04, 5.7526e-03, 4.9072e-02, 8.3847e-03,\n",
      "        6.3705e-03, 7.6828e-03, 8.6060e-03, 2.3155e-03, 1.7958e-03, 2.0847e-03,\n",
      "        2.9063e-04, 1.4915e-03, 8.0681e-04, 4.1461e-04, 3.3703e-03, 1.0971e-02,\n",
      "        7.3814e-03, 7.5684e-03, 7.3128e-03, 1.6851e-03, 1.6739e-02, 2.6779e-03,\n",
      "        2.5806e-03, 2.7676e-03, 2.3575e-03, 4.5509e-03, 6.6662e-04, 2.1629e-03,\n",
      "        1.7729e-03, 5.1384e-03, 9.0576e-02, 2.7924e-02, 1.4931e-02, 5.9586e-03,\n",
      "        5.9052e-03, 1.7426e-02, 4.1618e-03, 1.0651e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [119] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [119] : torch.Size([1, 32, 1, 191])\n",
      "Last layer attentions for generated token [119] : tensor([7.8735e-02, 7.8552e-02, 7.1228e-05, 2.2686e-04, 8.6963e-05, 9.6798e-04,\n",
      "        1.6081e-04, 1.6928e-04, 1.3864e-04, 2.4104e-04, 6.9332e-04, 1.5616e-04,\n",
      "        3.5000e-04, 5.7793e-04, 4.6997e-03, 5.4626e-03, 2.8849e-04, 1.0777e-03,\n",
      "        2.1684e-04, 3.7479e-04, 4.9353e-05, 2.9588e-04, 6.3934e-03, 1.8225e-03,\n",
      "        2.0542e-03, 8.2626e-03, 1.2722e-03, 2.6631e-04, 2.0850e-04, 1.8990e-04,\n",
      "        3.5834e-04, 6.7711e-04, 7.0429e-04, 5.5790e-04, 2.5153e-04, 1.6630e-04,\n",
      "        1.2803e-04, 1.8883e-04, 9.6619e-05, 1.0365e-04, 1.2226e-03, 9.1505e-04,\n",
      "        5.9414e-04, 5.7030e-04, 4.1556e-04, 3.1066e-04, 1.7524e-04, 1.2231e-04,\n",
      "        2.5105e-04, 8.2731e-04, 1.0061e-03, 2.7680e-04, 1.4877e-03, 5.1079e-03,\n",
      "        1.7424e-03, 6.9580e-03, 8.1539e-04, 1.4114e-03, 1.6661e-03, 9.6512e-04,\n",
      "        1.6232e-03, 6.9714e-04, 2.0370e-03, 8.7023e-04, 1.0738e-03, 5.1498e-04,\n",
      "        1.7672e-03, 1.6890e-03, 7.4530e-04, 1.4839e-02, 8.8882e-03, 1.7105e-02,\n",
      "        6.0081e-03, 3.0060e-03, 5.2986e-03, 6.6528e-03, 1.0513e-02, 3.4332e-03,\n",
      "        1.1620e-02, 1.3382e-02, 1.0185e-02, 5.0125e-03, 5.5618e-03, 2.2373e-03,\n",
      "        1.0891e-03, 2.7256e-03, 3.2902e-03, 2.2202e-03, 1.6060e-03, 3.8052e-03,\n",
      "        3.1242e-03, 2.5806e-03, 3.0575e-03, 2.2850e-03, 4.8943e-03, 2.7828e-03,\n",
      "        4.4022e-03, 4.0817e-03, 6.1836e-03, 3.9711e-03, 5.6190e-03, 5.0850e-03,\n",
      "        6.5346e-03, 1.4305e-03, 2.4281e-03, 3.1891e-03, 8.5449e-03, 5.5809e-03,\n",
      "        2.2984e-03, 4.5547e-03, 2.1973e-03, 5.3482e-03, 2.9697e-03, 2.0142e-03,\n",
      "        6.8779e-03, 4.2076e-03, 4.8981e-03, 3.4504e-03, 2.7142e-03, 5.9967e-03,\n",
      "        4.7394e-02, 2.5589e-02, 1.0132e-02, 4.2648e-03, 1.0406e-02, 1.2383e-02,\n",
      "        4.8027e-03, 2.2392e-03, 4.7531e-03, 2.0657e-03, 7.7200e-04, 1.5202e-03,\n",
      "        3.7670e-03, 9.2392e-03, 5.7564e-03, 1.6041e-03, 5.3787e-03, 1.0689e-02,\n",
      "        3.3073e-03, 1.0605e-03, 1.4629e-03, 4.1962e-04, 4.1542e-03, 9.8114e-03,\n",
      "        1.6617e-02, 4.5853e-03, 6.7596e-03, 2.1534e-03, 8.1778e-04, 1.6861e-03,\n",
      "        3.2368e-03, 2.9278e-03, 7.0810e-04, 5.0926e-03, 2.5406e-02, 6.6414e-03,\n",
      "        3.5820e-03, 5.1460e-03, 5.3864e-03, 1.8826e-03, 1.2913e-03, 1.2617e-03,\n",
      "        2.5558e-04, 1.0738e-03, 5.5790e-04, 2.9135e-04, 2.4071e-03, 6.2103e-03,\n",
      "        7.1983e-03, 7.7934e-03, 8.5220e-03, 2.2545e-03, 8.4076e-03, 2.4853e-03,\n",
      "        2.5387e-03, 2.3270e-03, 1.7509e-03, 3.5324e-03, 5.2214e-04, 1.2283e-03,\n",
      "        9.9754e-04, 7.2174e-03, 6.2042e-02, 3.2562e-02, 8.9188e-03, 3.4599e-03,\n",
      "        5.4550e-03, 2.0157e-02, 3.7098e-03, 1.3351e-02, 7.6485e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [120] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [120] : torch.Size([1, 32, 1, 192])\n",
      "Last layer attentions for generated token [120] : tensor([2.0447e-01, 2.0447e-01, 1.3280e-04, 4.3607e-04, 6.6996e-05, 1.4277e-03,\n",
      "        6.0081e-05, 8.9467e-05, 1.1092e-04, 1.3018e-04, 2.4462e-04, 9.7513e-05,\n",
      "        2.6035e-04, 5.5218e-04, 3.0441e-03, 3.7155e-03, 1.2970e-04, 2.2757e-04,\n",
      "        5.8889e-05, 1.6582e-04, 5.0008e-05, 2.6655e-04, 1.3336e-02, 1.4105e-03,\n",
      "        2.3270e-03, 8.3466e-03, 1.0986e-03, 3.8719e-04, 1.8430e-04, 2.2149e-04,\n",
      "        4.0245e-04, 9.7132e-04, 3.2592e-04, 2.8539e-04, 1.7250e-04, 1.8799e-04,\n",
      "        3.5048e-04, 2.4748e-04, 1.6201e-04, 1.1760e-04, 2.0180e-03, 1.4381e-03,\n",
      "        4.2939e-04, 3.8481e-04, 2.4128e-04, 1.5461e-04, 1.1092e-04, 1.5283e-04,\n",
      "        1.6785e-04, 1.2064e-03, 1.0777e-03, 2.3949e-04, 2.3956e-03, 4.4861e-03,\n",
      "        1.7452e-03, 3.3607e-03, 2.6703e-04, 1.1854e-03, 2.2945e-03, 7.3195e-04,\n",
      "        1.1787e-03, 7.7152e-04, 1.2350e-03, 4.3702e-04, 7.1049e-04, 6.2084e-04,\n",
      "        1.2112e-03, 1.3380e-03, 1.2236e-03, 8.6746e-03, 6.8130e-03, 8.8272e-03,\n",
      "        1.4324e-03, 1.2331e-03, 3.6488e-03, 3.4275e-03, 6.0310e-03, 2.0123e-03,\n",
      "        5.0011e-03, 6.9656e-03, 4.2267e-03, 2.2717e-03, 3.1414e-03, 2.0485e-03,\n",
      "        1.6222e-03, 1.1759e-03, 2.2526e-03, 1.0605e-03, 9.5654e-04, 3.0193e-03,\n",
      "        1.0300e-03, 2.2793e-03, 1.9970e-03, 1.5297e-03, 1.7433e-03, 1.0099e-03,\n",
      "        1.6994e-03, 2.6436e-03, 2.6817e-03, 2.8248e-03, 1.4410e-03, 1.8997e-03,\n",
      "        1.8044e-03, 7.5054e-04, 8.7738e-04, 9.3985e-04, 4.7913e-03, 1.9474e-03,\n",
      "        1.7672e-03, 2.1172e-03, 1.7500e-03, 2.4681e-03, 2.0618e-03, 1.1377e-03,\n",
      "        4.2000e-03, 2.1019e-03, 2.6722e-03, 1.8597e-03, 1.6375e-03, 4.3030e-03,\n",
      "        2.7267e-02, 9.5749e-03, 5.0201e-03, 4.3335e-03, 3.3474e-03, 6.4545e-03,\n",
      "        4.7188e-03, 1.5955e-03, 4.0474e-03, 1.3695e-03, 8.1778e-04, 1.1396e-03,\n",
      "        3.5057e-03, 7.9193e-03, 3.5458e-03, 1.7538e-03, 4.8752e-03, 6.3324e-03,\n",
      "        1.2922e-03, 6.3467e-04, 1.1425e-03, 3.2854e-04, 3.3321e-03, 4.5242e-03,\n",
      "        5.3482e-03, 2.6684e-03, 4.8714e-03, 1.2617e-03, 7.1478e-04, 1.2941e-03,\n",
      "        3.5076e-03, 2.0447e-03, 1.1072e-03, 5.4665e-03, 1.8723e-02, 2.6245e-03,\n",
      "        4.0283e-03, 6.2714e-03, 3.7670e-03, 8.9312e-04, 8.7738e-04, 1.0204e-03,\n",
      "        5.1689e-04, 1.0834e-03, 4.2772e-04, 2.7776e-04, 2.3785e-03, 5.3329e-03,\n",
      "        4.0665e-03, 5.1537e-03, 6.8092e-03, 1.7490e-03, 5.0392e-03, 2.8629e-03,\n",
      "        2.4681e-03, 2.0199e-03, 2.5635e-03, 2.9106e-03, 5.1880e-04, 1.5049e-03,\n",
      "        1.7986e-03, 8.0338e-03, 5.9570e-02, 2.0111e-02, 1.0941e-02, 5.3902e-03,\n",
      "        4.5662e-03, 9.9411e-03, 2.7676e-03, 1.4694e-02, 1.1864e-02, 5.8899e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [121] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [121] : torch.Size([1, 32, 1, 193])\n",
      "Last layer attentions for generated token [121] : tensor([1.3770e-01, 1.3745e-01, 3.0816e-05, 1.0973e-04, 2.3365e-05, 5.8651e-04,\n",
      "        1.4830e-04, 1.7059e-04, 2.9778e-04, 3.2759e-04, 1.0271e-03, 2.0623e-04,\n",
      "        2.0742e-04, 6.1321e-04, 1.1131e-02, 7.1945e-03, 1.8060e-04, 7.8297e-04,\n",
      "        1.7226e-04, 2.9492e-04, 1.0109e-04, 3.0947e-04, 3.4904e-03, 9.6273e-04,\n",
      "        4.3602e-03, 1.0033e-02, 9.5749e-04, 4.1413e-04, 1.7333e-04, 1.6606e-04,\n",
      "        2.0456e-04, 5.4026e-04, 5.0926e-04, 5.7030e-04, 3.8600e-04, 2.3317e-04,\n",
      "        1.2529e-04, 2.9302e-04, 8.5473e-05, 9.1672e-05, 1.1387e-03, 5.4455e-04,\n",
      "        3.2377e-04, 3.6693e-04, 4.8423e-04, 1.7166e-04, 2.2781e-04, 1.6093e-04,\n",
      "        2.1565e-04, 1.3046e-03, 9.5940e-04, 1.1408e-04, 1.0738e-03, 2.3308e-03,\n",
      "        8.7690e-04, 3.2806e-03, 3.6335e-04, 6.1321e-04, 1.0090e-03, 5.1451e-04,\n",
      "        6.1321e-04, 4.1819e-04, 1.5278e-03, 8.5497e-04, 5.3787e-04, 7.0715e-04,\n",
      "        6.4039e-04, 6.5804e-04, 6.8951e-04, 7.5836e-03, 4.7150e-03, 1.0857e-02,\n",
      "        2.9564e-03, 2.2411e-03, 5.3482e-03, 6.2981e-03, 6.6757e-03, 2.4948e-03,\n",
      "        8.6060e-03, 4.2953e-03, 3.2997e-03, 3.2253e-03, 3.9406e-03, 6.3438e-03,\n",
      "        1.1501e-03, 2.2888e-03, 2.6093e-03, 1.3618e-03, 8.6164e-04, 3.8853e-03,\n",
      "        1.9817e-03, 3.2253e-03, 3.2043e-03, 2.3079e-03, 5.2109e-03, 4.1771e-03,\n",
      "        6.3591e-03, 2.4948e-03, 6.2828e-03, 2.7981e-03, 4.7569e-03, 3.7003e-03,\n",
      "        8.6594e-03, 1.1663e-03, 2.9812e-03, 4.1046e-03, 1.1986e-02, 5.9204e-03,\n",
      "        2.1381e-03, 6.0616e-03, 2.5501e-03, 9.6054e-03, 1.6203e-03, 6.9761e-04,\n",
      "        7.6790e-03, 3.2101e-03, 4.4327e-03, 4.6043e-03, 2.4834e-03, 5.5580e-03,\n",
      "        2.0950e-02, 1.7075e-02, 8.8959e-03, 2.6436e-03, 6.8207e-03, 8.1787e-03,\n",
      "        5.7068e-03, 1.8234e-03, 4.7836e-03, 1.9360e-03, 6.9094e-04, 7.1287e-04,\n",
      "        3.5973e-03, 7.2556e-03, 4.3297e-03, 1.6289e-03, 3.9368e-03, 7.5493e-03,\n",
      "        3.2482e-03, 9.6846e-04, 1.0252e-03, 3.2902e-04, 4.9400e-03, 5.5733e-03,\n",
      "        1.0185e-02, 4.0016e-03, 5.2528e-03, 2.0828e-03, 5.9462e-04, 1.9093e-03,\n",
      "        2.6150e-03, 1.7691e-03, 6.3038e-04, 6.7177e-03, 1.9775e-02, 4.2839e-03,\n",
      "        5.1689e-03, 6.4011e-03, 5.4398e-03, 2.1400e-03, 1.2150e-03, 1.0014e-03,\n",
      "        1.0109e-04, 9.5749e-04, 5.9080e-04, 3.1495e-04, 4.8256e-03, 7.9346e-03,\n",
      "        9.6970e-03, 7.3090e-03, 8.0261e-03, 4.0703e-03, 5.5466e-03, 2.2354e-03,\n",
      "        1.7815e-03, 2.1019e-03, 1.7099e-03, 3.6640e-03, 4.6372e-04, 8.9407e-04,\n",
      "        1.1683e-03, 1.1848e-02, 5.3497e-02, 2.6779e-02, 6.4659e-03, 4.3297e-03,\n",
      "        5.6839e-03, 1.5160e-02, 2.9030e-03, 2.0355e-02, 7.4196e-03, 1.5907e-03,\n",
      "        6.4354e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [122] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [122] : torch.Size([1, 32, 1, 194])\n",
      "Last layer attentions for generated token [122] : tensor([1.7773e-01, 1.7773e-01, 3.6657e-05, 2.0599e-04, 2.6166e-05, 8.2302e-04,\n",
      "        7.1764e-05, 8.3566e-05, 2.4700e-04, 1.8978e-04, 5.1594e-04, 2.1970e-04,\n",
      "        2.2066e-04, 8.8120e-04, 7.0000e-03, 4.6844e-03, 1.3256e-04, 4.2367e-04,\n",
      "        1.1069e-04, 2.2233e-04, 1.2839e-04, 3.3641e-04, 3.7022e-03, 9.8515e-04,\n",
      "        4.2458e-03, 8.6899e-03, 7.6675e-04, 3.5048e-04, 2.5249e-04, 2.2805e-04,\n",
      "        3.3236e-04, 6.8092e-04, 5.0211e-04, 3.1424e-04, 2.3305e-04, 2.3806e-04,\n",
      "        2.6107e-04, 3.0160e-04, 1.4961e-04, 1.5306e-04, 2.3422e-03, 1.1158e-03,\n",
      "        7.9918e-04, 3.5667e-04, 2.9683e-04, 2.9683e-04, 1.4389e-04, 1.7560e-04,\n",
      "        2.3580e-04, 1.3809e-03, 1.2503e-03, 1.0079e-04, 1.7576e-03, 3.0727e-03,\n",
      "        1.8311e-03, 2.6093e-03, 2.8062e-04, 9.8896e-04, 2.2812e-03, 7.9584e-04,\n",
      "        1.6127e-03, 8.0204e-04, 1.6060e-03, 1.0834e-03, 6.8760e-04, 8.1158e-04,\n",
      "        1.7643e-03, 1.1072e-03, 1.8454e-03, 9.9869e-03, 6.1493e-03, 1.3161e-02,\n",
      "        2.6665e-03, 1.9112e-03, 4.2877e-03, 6.2447e-03, 8.3542e-03, 3.1147e-03,\n",
      "        6.0158e-03, 6.1073e-03, 3.3188e-03, 5.4779e-03, 2.7943e-03, 2.3956e-03,\n",
      "        2.1973e-03, 3.1567e-03, 3.0689e-03, 1.2255e-03, 7.9155e-04, 2.6817e-03,\n",
      "        1.6603e-03, 4.5357e-03, 3.2139e-03, 3.6373e-03, 2.4147e-03, 2.2964e-03,\n",
      "        3.8490e-03, 3.6163e-03, 3.9787e-03, 2.7485e-03, 5.4703e-03, 2.3708e-03,\n",
      "        2.2774e-03, 8.9836e-04, 2.0618e-03, 1.8930e-03, 5.7678e-03, 4.0054e-03,\n",
      "        2.5845e-03, 4.0894e-03, 3.2806e-03, 3.9291e-03, 2.4872e-03, 1.6203e-03,\n",
      "        5.9547e-03, 4.1809e-03, 6.2370e-03, 4.6043e-03, 4.5013e-03, 9.4223e-03,\n",
      "        2.8214e-02, 1.5579e-02, 6.1188e-03, 4.6959e-03, 5.8861e-03, 6.6147e-03,\n",
      "        4.4327e-03, 4.0550e-03, 6.2981e-03, 2.6379e-03, 1.2445e-03, 2.7008e-03,\n",
      "        5.9586e-03, 6.2294e-03, 7.2632e-03, 2.8687e-03, 3.5572e-03, 4.9477e-03,\n",
      "        2.8954e-03, 1.9684e-03, 2.2545e-03, 7.7438e-04, 5.8861e-03, 4.6997e-03,\n",
      "        6.7024e-03, 3.3817e-03, 6.7062e-03, 2.3174e-03, 1.0338e-03, 2.5711e-03,\n",
      "        2.9278e-03, 2.5673e-03, 1.3943e-03, 5.1193e-03, 1.5915e-02, 5.6381e-03,\n",
      "        6.2408e-03, 5.6572e-03, 2.7809e-03, 1.2569e-03, 1.5049e-03, 1.2999e-03,\n",
      "        1.7905e-04, 1.1740e-03, 5.8031e-04, 2.6917e-04, 2.6455e-03, 4.3373e-03,\n",
      "        3.7651e-03, 3.4142e-03, 5.5771e-03, 2.5444e-03, 5.4779e-03, 3.4199e-03,\n",
      "        2.0504e-03, 2.7370e-03, 2.2430e-03, 3.7174e-03, 4.5967e-04, 8.0204e-04,\n",
      "        1.1721e-03, 8.1863e-03, 3.1769e-02, 1.3168e-02, 5.9662e-03, 2.5272e-03,\n",
      "        3.2101e-03, 8.4305e-03, 3.3741e-03, 1.0506e-02, 5.2338e-03, 3.2730e-03,\n",
      "        6.9351e-03, 5.6190e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [123] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [123] : torch.Size([1, 32, 1, 195])\n",
      "Last layer attentions for generated token [123] : tensor([1.2744e-01, 1.2744e-01, 3.8087e-05, 1.4293e-04, 3.3259e-05, 7.2336e-04,\n",
      "        8.1956e-05, 1.0359e-04, 1.7548e-04, 1.7178e-04, 5.5027e-04, 1.4496e-04,\n",
      "        1.2589e-04, 3.9411e-04, 1.1307e-02, 6.5193e-03, 1.3721e-04, 4.5347e-04,\n",
      "        8.0347e-05, 1.1921e-04, 4.0412e-05, 2.0516e-04, 5.3940e-03, 1.4248e-03,\n",
      "        4.7913e-03, 9.4452e-03, 6.4468e-04, 3.1424e-04, 1.5128e-04, 2.0647e-04,\n",
      "        3.4976e-04, 1.0052e-03, 3.1424e-04, 2.7442e-04, 2.3532e-04, 1.7929e-04,\n",
      "        1.2493e-04, 2.2447e-04, 6.8724e-05, 6.7651e-05, 2.1057e-03, 8.5402e-04,\n",
      "        3.8862e-04, 3.7313e-04, 2.8706e-04, 9.0361e-05, 1.2791e-04, 9.2506e-05,\n",
      "        1.3041e-04, 1.2941e-03, 8.0538e-04, 9.2149e-05, 1.0099e-03, 3.3092e-03,\n",
      "        9.9564e-04, 3.6221e-03, 3.7599e-04, 7.4196e-04, 1.9093e-03, 7.8344e-04,\n",
      "        1.0958e-03, 1.1787e-03, 1.8721e-03, 8.4543e-04, 5.3978e-04, 8.4066e-04,\n",
      "        9.2888e-04, 9.8228e-04, 8.8787e-04, 1.2924e-02, 7.2861e-03, 1.4236e-02,\n",
      "        2.8648e-03, 1.4458e-03, 4.1008e-03, 9.5825e-03, 8.1482e-03, 2.2125e-03,\n",
      "        1.0910e-02, 8.6975e-03, 3.6049e-03, 2.9449e-03, 2.3556e-03, 4.7684e-03,\n",
      "        1.6384e-03, 2.3441e-03, 3.3150e-03, 1.1883e-03, 5.9509e-04, 3.8776e-03,\n",
      "        1.5192e-03, 3.8986e-03, 4.1695e-03, 3.1719e-03, 3.7098e-03, 2.7351e-03,\n",
      "        4.9019e-03, 3.4466e-03, 4.8141e-03, 2.7637e-03, 3.1071e-03, 2.2354e-03,\n",
      "        3.5553e-03, 5.5552e-04, 1.3742e-03, 1.1339e-03, 7.7248e-03, 2.9716e-03,\n",
      "        1.5078e-03, 5.2185e-03, 1.9398e-03, 5.7678e-03, 1.9073e-03, 8.6403e-04,\n",
      "        6.6032e-03, 2.4643e-03, 3.6793e-03, 3.5419e-03, 4.2000e-03, 9.8114e-03,\n",
      "        3.2837e-02, 1.2657e-02, 8.9645e-03, 3.9978e-03, 4.9896e-03, 7.3242e-03,\n",
      "        4.5509e-03, 1.6174e-03, 4.3678e-03, 2.1725e-03, 1.1148e-03, 1.5135e-03,\n",
      "        7.0686e-03, 1.0208e-02, 6.4316e-03, 2.2678e-03, 8.9264e-03, 7.1602e-03,\n",
      "        2.5997e-03, 9.4891e-04, 1.2255e-03, 3.4356e-04, 5.5809e-03, 6.7558e-03,\n",
      "        8.7891e-03, 3.9978e-03, 5.9624e-03, 1.6031e-03, 5.8699e-04, 1.4982e-03,\n",
      "        3.8662e-03, 1.4896e-03, 4.9639e-04, 5.6305e-03, 2.5345e-02, 3.9024e-03,\n",
      "        5.9090e-03, 1.1414e-02, 3.9101e-03, 1.3342e-03, 7.9441e-04, 6.6519e-04,\n",
      "        1.0812e-04, 7.7295e-04, 2.3389e-04, 1.3828e-04, 3.5553e-03, 7.8812e-03,\n",
      "        7.1602e-03, 5.7220e-03, 5.4054e-03, 2.6169e-03, 3.8795e-03, 1.2722e-03,\n",
      "        1.8091e-03, 1.4830e-03, 2.5845e-03, 2.7637e-03, 2.2149e-04, 7.5197e-04,\n",
      "        9.0170e-04, 9.3765e-03, 6.6895e-02, 1.4557e-02, 4.8256e-03, 3.3989e-03,\n",
      "        4.7951e-03, 1.0941e-02, 2.2087e-03, 1.8021e-02, 9.8572e-03, 2.4509e-03,\n",
      "        1.1139e-02, 9.3842e-03, 5.4283e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [124] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [124] : torch.Size([1, 32, 1, 196])\n",
      "Last layer attentions for generated token [124] : tensor([2.5464e-01, 2.5464e-01, 4.2200e-05, 1.2851e-04, 3.5942e-05, 6.9714e-04,\n",
      "        1.2994e-04, 1.5807e-04, 3.6383e-04, 2.1434e-04, 6.2990e-04, 2.0051e-04,\n",
      "        1.5020e-04, 4.5466e-04, 5.5695e-03, 4.1084e-03, 5.9724e-05, 4.0340e-04,\n",
      "        9.7752e-05, 1.2505e-04, 4.6372e-05, 2.5201e-04, 5.2757e-03, 1.0090e-03,\n",
      "        4.1885e-03, 8.0490e-03, 3.5405e-04, 1.6117e-04, 1.3626e-04, 2.1768e-04,\n",
      "        4.7350e-04, 8.1491e-04, 5.3883e-04, 2.6774e-04, 2.9993e-04, 1.3733e-04,\n",
      "        1.6427e-04, 2.9182e-04, 6.6400e-05, 6.1631e-05, 1.3628e-03, 6.1274e-04,\n",
      "        1.8764e-04, 1.7703e-04, 2.2900e-04, 7.2360e-05, 1.7428e-04, 1.2600e-04,\n",
      "        1.8263e-04, 1.1883e-03, 7.3051e-04, 7.1764e-05, 1.1072e-03, 3.7136e-03,\n",
      "        9.9277e-04, 3.2806e-03, 1.7703e-04, 4.8971e-04, 1.0128e-03, 5.2834e-04,\n",
      "        4.8971e-04, 5.9175e-04, 1.6050e-03, 7.0143e-04, 5.2214e-04, 6.3372e-04,\n",
      "        6.7186e-04, 6.2609e-04, 8.6594e-04, 9.1171e-03, 3.4695e-03, 6.5460e-03,\n",
      "        1.2045e-03, 7.3910e-04, 2.7237e-03, 6.6223e-03, 3.5057e-03, 9.9277e-04,\n",
      "        6.7673e-03, 4.5547e-03, 1.8549e-03, 1.0986e-03, 1.3256e-03, 3.1395e-03,\n",
      "        9.5320e-04, 1.3628e-03, 2.4757e-03, 1.0166e-03, 6.5994e-04, 3.0861e-03,\n",
      "        1.2426e-03, 2.9564e-03, 3.6392e-03, 2.4529e-03, 3.6144e-03, 3.4962e-03,\n",
      "        5.0163e-03, 1.9817e-03, 2.4529e-03, 1.5049e-03, 1.4591e-03, 1.4620e-03,\n",
      "        3.9711e-03, 6.4993e-04, 2.0657e-03, 1.6308e-03, 8.5907e-03, 3.7689e-03,\n",
      "        9.4557e-04, 3.3321e-03, 1.3895e-03, 5.1613e-03, 1.2531e-03, 5.4407e-04,\n",
      "        4.9858e-03, 1.3285e-03, 2.4700e-03, 4.9591e-03, 2.6970e-03, 5.7793e-03,\n",
      "        1.5312e-02, 5.5923e-03, 3.7098e-03, 1.8492e-03, 3.3817e-03, 3.7880e-03,\n",
      "        2.6779e-03, 8.0395e-04, 2.2144e-03, 1.9798e-03, 6.4087e-04, 6.2609e-04,\n",
      "        4.2229e-03, 5.7907e-03, 1.6594e-03, 1.0166e-03, 3.9043e-03, 3.3722e-03,\n",
      "        1.3628e-03, 3.2353e-04, 6.8235e-04, 2.5439e-04, 3.2272e-03, 3.7537e-03,\n",
      "        8.1711e-03, 2.9678e-03, 3.4485e-03, 1.2169e-03, 5.5933e-04, 1.5993e-03,\n",
      "        3.3417e-03, 1.2283e-03, 5.4407e-04, 5.4550e-03, 1.5167e-02, 1.1950e-03,\n",
      "        3.6526e-03, 4.7379e-03, 2.1267e-03, 1.0242e-03, 5.2118e-04, 9.5320e-04,\n",
      "        1.2946e-04, 6.8235e-04, 2.5940e-04, 1.6427e-04, 3.2806e-03, 4.7836e-03,\n",
      "        5.0011e-03, 5.5923e-03, 4.5967e-03, 2.1763e-03, 3.0460e-03, 1.5411e-03,\n",
      "        1.8787e-03, 2.3365e-03, 2.6417e-03, 2.6417e-03, 3.0231e-04, 7.3910e-04,\n",
      "        1.5802e-03, 8.5526e-03, 3.0014e-02, 6.4430e-03, 1.7490e-03, 2.7409e-03,\n",
      "        3.1376e-03, 7.2670e-03, 2.4719e-03, 1.5526e-02, 7.1678e-03, 1.4763e-03,\n",
      "        4.5357e-03, 5.9967e-03, 3.4351e-03, 7.3280e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [125] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [125] : torch.Size([1, 32, 1, 197])\n",
      "Last layer attentions for generated token [125] : tensor([2.5781e-01, 2.5781e-01, 9.5546e-05, 3.3212e-04, 7.2658e-05, 8.8358e-04,\n",
      "        1.0371e-04, 1.3316e-04, 2.3556e-04, 1.2803e-04, 2.7418e-04, 6.0737e-05,\n",
      "        1.3626e-04, 3.9363e-04, 4.4365e-03, 2.6722e-03, 5.0724e-05, 2.0456e-04,\n",
      "        7.4089e-05, 9.6262e-05, 3.7432e-05, 2.5368e-04, 1.1665e-02, 1.6737e-03,\n",
      "        3.5820e-03, 8.8577e-03, 4.4084e-04, 3.0661e-04, 2.0456e-04, 1.9217e-04,\n",
      "        3.9530e-04, 7.2813e-04, 3.7861e-04, 3.4738e-04, 2.3091e-04, 2.3007e-04,\n",
      "        2.4867e-04, 2.5964e-04, 1.6308e-04, 1.5879e-04, 1.4801e-03, 1.1892e-03,\n",
      "        2.3830e-04, 4.2820e-04, 3.3212e-04, 1.8275e-04, 1.5938e-04, 1.6379e-04,\n",
      "        3.8671e-04, 1.1797e-03, 1.1320e-03, 1.0532e-04, 1.4095e-03, 4.3030e-03,\n",
      "        1.4458e-03, 3.6125e-03, 2.3830e-04, 1.0090e-03, 1.5726e-03, 7.7963e-04,\n",
      "        9.8801e-04, 6.7377e-04, 1.7271e-03, 1.2779e-03, 3.9983e-04, 4.6372e-04,\n",
      "        1.3876e-03, 6.4278e-04, 1.1101e-03, 7.4005e-03, 5.7602e-03, 8.2703e-03,\n",
      "        8.6832e-04, 1.0996e-03, 2.4929e-03, 3.6812e-03, 6.0692e-03, 8.3351e-04,\n",
      "        3.8528e-03, 7.8278e-03, 3.1357e-03, 1.6193e-03, 1.3952e-03, 2.5539e-03,\n",
      "        2.1820e-03, 1.8892e-03, 2.6131e-03, 1.1749e-03, 7.6151e-04, 6.4278e-03,\n",
      "        1.1988e-03, 3.2349e-03, 1.8997e-03, 4.6425e-03, 2.1877e-03, 1.1320e-03,\n",
      "        2.9831e-03, 2.1610e-03, 1.1168e-03, 1.4544e-03, 1.0891e-03, 1.4124e-03,\n",
      "        1.7405e-03, 6.6328e-04, 1.0452e-03, 7.3671e-04, 3.3836e-03, 2.4014e-03,\n",
      "        1.8063e-03, 2.0618e-03, 2.0218e-03, 2.9278e-03, 2.6455e-03, 6.2799e-04,\n",
      "        3.0632e-03, 1.8454e-03, 4.0512e-03, 2.2125e-03, 4.7722e-03, 3.7937e-03,\n",
      "        1.6632e-02, 4.0169e-03, 2.8076e-03, 2.8782e-03, 2.1362e-03, 3.0880e-03,\n",
      "        3.5496e-03, 1.7576e-03, 2.6474e-03, 1.3905e-03, 1.5535e-03, 1.5783e-03,\n",
      "        2.6226e-03, 7.1030e-03, 3.6812e-03, 1.2074e-03, 2.1324e-03, 3.5496e-03,\n",
      "        1.3399e-03, 7.8297e-04, 7.0047e-04, 4.4179e-04, 2.1763e-03, 2.6932e-03,\n",
      "        4.4174e-03, 2.1114e-03, 3.8605e-03, 9.4986e-04, 4.9448e-04, 9.4795e-04,\n",
      "        2.0027e-03, 1.4458e-03, 7.7963e-04, 4.6539e-03, 1.4900e-02, 2.1286e-03,\n",
      "        4.8676e-03, 3.8872e-03, 3.0060e-03, 1.2264e-03, 1.0052e-03, 9.2793e-04,\n",
      "        2.8086e-04, 1.0290e-03, 3.2759e-04, 1.9908e-04, 2.4261e-03, 4.2038e-03,\n",
      "        4.9248e-03, 4.0741e-03, 4.1313e-03, 1.6603e-03, 3.5286e-03, 1.6346e-03,\n",
      "        1.6060e-03, 1.5383e-03, 1.6384e-03, 2.5501e-03, 4.5133e-04, 1.0014e-03,\n",
      "        1.7643e-03, 4.9706e-03, 3.3325e-02, 7.2784e-03, 3.3875e-03, 2.5444e-03,\n",
      "        3.3226e-03, 7.2937e-03, 2.0752e-03, 7.3624e-03, 5.9547e-03, 3.2578e-03,\n",
      "        4.5242e-03, 5.9547e-03, 5.1956e-03, 8.3313e-03, 5.6610e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [126] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [126] : torch.Size([1, 32, 1, 198])\n",
      "Last layer attentions for generated token [126] : tensor([2.0544e-01, 2.0544e-01, 8.1658e-05, 2.8133e-04, 8.3625e-05, 1.0653e-03,\n",
      "        1.4961e-04, 1.5557e-04, 2.7847e-04, 1.4675e-04, 5.0831e-04, 2.2423e-04,\n",
      "        1.3733e-04, 4.3297e-04, 8.0338e-03, 3.8376e-03, 1.2755e-04, 3.3665e-04,\n",
      "        9.9719e-05, 1.1611e-04, 1.0949e-04, 3.2115e-04, 6.5308e-03, 1.4763e-03,\n",
      "        7.6561e-03, 1.3985e-02, 6.6662e-04, 2.2554e-04, 2.5749e-04, 2.2960e-04,\n",
      "        4.4084e-04, 9.0265e-04, 3.1686e-04, 2.4295e-04, 2.1815e-04, 1.8919e-04,\n",
      "        2.4581e-04, 2.4724e-04, 1.1081e-04, 1.2898e-04, 1.7567e-03, 7.9489e-04,\n",
      "        3.3784e-04, 2.9588e-04, 3.2496e-04, 2.2209e-04, 2.0063e-04, 1.1295e-04,\n",
      "        1.9145e-04, 1.0366e-03, 7.9966e-04, 1.5557e-04, 1.6308e-03, 4.0359e-03,\n",
      "        1.1654e-03, 2.3327e-03, 2.1183e-04, 9.8324e-04, 2.2125e-03, 6.9904e-04,\n",
      "        1.2484e-03, 8.0585e-04, 1.7843e-03, 1.0595e-03, 4.7183e-04, 6.1655e-04,\n",
      "        9.0456e-04, 9.6846e-04, 1.7109e-03, 1.3725e-02, 8.6441e-03, 9.5520e-03,\n",
      "        1.5812e-03, 9.9754e-04, 2.3499e-03, 4.6043e-03, 4.8332e-03, 1.7986e-03,\n",
      "        5.1804e-03, 4.3297e-03, 2.7523e-03, 2.3613e-03, 1.5841e-03, 2.4986e-03,\n",
      "        1.0204e-03, 1.9236e-03, 2.1553e-03, 1.0090e-03, 4.8780e-04, 2.5330e-03,\n",
      "        1.7262e-03, 3.6221e-03, 2.7351e-03, 2.5539e-03, 2.6150e-03, 1.7691e-03,\n",
      "        3.5114e-03, 1.8597e-03, 2.3785e-03, 1.3552e-03, 1.7719e-03, 1.2026e-03,\n",
      "        1.6785e-03, 4.5037e-04, 1.0052e-03, 1.4448e-03, 4.6883e-03, 2.8915e-03,\n",
      "        1.5087e-03, 3.0384e-03, 1.7967e-03, 2.7027e-03, 1.7796e-03, 6.4611e-04,\n",
      "        3.0975e-03, 1.4591e-03, 2.9087e-03, 3.0632e-03, 2.8000e-03, 7.5722e-03,\n",
      "        1.8295e-02, 8.0948e-03, 4.3793e-03, 2.5158e-03, 4.2534e-03, 5.0774e-03,\n",
      "        2.8629e-03, 1.7672e-03, 5.3825e-03, 2.3327e-03, 1.2856e-03, 2.1152e-03,\n",
      "        6.9618e-03, 6.5804e-03, 6.7291e-03, 4.3144e-03, 5.6229e-03, 4.7493e-03,\n",
      "        3.0022e-03, 1.8272e-03, 1.4315e-03, 4.8113e-04, 4.5433e-03, 3.8834e-03,\n",
      "        8.2092e-03, 6.7825e-03, 6.9466e-03, 2.9144e-03, 8.5115e-04, 3.4981e-03,\n",
      "        3.9444e-03, 1.4505e-03, 1.3161e-03, 5.7983e-03, 1.3817e-02, 3.7632e-03,\n",
      "        7.8735e-03, 7.8583e-03, 2.6989e-03, 1.7843e-03, 1.3847e-03, 1.1368e-03,\n",
      "        1.6963e-04, 1.4563e-03, 4.8876e-04, 2.9254e-04, 4.1428e-03, 4.9896e-03,\n",
      "        6.2943e-03, 5.4398e-03, 5.4321e-03, 4.9286e-03, 6.2752e-03, 4.3488e-03,\n",
      "        2.9449e-03, 3.9673e-03, 3.2845e-03, 2.6264e-03, 5.5838e-04, 7.1526e-04,\n",
      "        1.6832e-03, 8.6594e-03, 2.8458e-02, 7.7820e-03, 2.1324e-03, 1.3084e-03,\n",
      "        1.8215e-03, 9.0027e-03, 1.9894e-03, 8.1482e-03, 3.7403e-03, 3.3379e-03,\n",
      "        5.7564e-03, 5.9853e-03, 6.0387e-03, 6.5689e-03, 6.3591e-03, 4.2191e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [127] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [127] : torch.Size([1, 32, 1, 199])\n",
      "Last layer attentions for generated token [127] : tensor([1.6736e-01, 1.6736e-01, 9.1136e-05, 1.7083e-04, 7.8559e-05, 1.4086e-03,\n",
      "        1.8442e-04, 2.7728e-04, 3.6454e-04, 1.5259e-04, 4.2796e-04, 1.8549e-04,\n",
      "        1.1790e-04, 4.0746e-04, 6.7711e-03, 3.7746e-03, 1.0949e-04, 2.5392e-04,\n",
      "        5.3585e-05, 9.8526e-05, 9.2566e-05, 3.6168e-04, 6.0768e-03, 1.0691e-03,\n",
      "        4.9858e-03, 1.5381e-02, 5.8031e-04, 1.6308e-04, 2.0933e-04, 4.0841e-04,\n",
      "        3.9196e-04, 8.4925e-04, 3.2735e-04, 3.2926e-04, 2.6989e-04, 1.1653e-04,\n",
      "        1.8334e-04, 3.2043e-04, 1.0693e-04, 1.2255e-04, 1.1206e-03, 5.0116e-04,\n",
      "        1.6499e-04, 2.3174e-04, 3.9649e-04, 1.7226e-04, 4.3297e-04, 1.4555e-04,\n",
      "        2.6464e-04, 1.1787e-03, 9.9850e-04, 2.5344e-04, 1.4734e-03, 4.2381e-03,\n",
      "        7.0143e-04, 3.2177e-03, 2.1386e-04, 9.6798e-04, 2.0008e-03, 7.6866e-04,\n",
      "        1.1187e-03, 8.6451e-04, 1.5945e-03, 7.7009e-04, 3.8433e-04, 6.0225e-04,\n",
      "        6.4230e-04, 9.0075e-04, 1.5364e-03, 1.3336e-02, 9.4910e-03, 9.1934e-03,\n",
      "        1.2655e-03, 1.3399e-03, 4.2496e-03, 3.8261e-03, 4.6883e-03, 1.6546e-03,\n",
      "        6.6490e-03, 4.7150e-03, 3.5095e-03, 1.9007e-03, 2.1458e-03, 3.5210e-03,\n",
      "        9.6416e-04, 1.6642e-03, 2.5101e-03, 1.5802e-03, 6.7854e-04, 3.3817e-03,\n",
      "        1.9970e-03, 4.5280e-03, 3.5248e-03, 2.2984e-03, 6.9237e-03, 3.4294e-03,\n",
      "        4.2496e-03, 2.1877e-03, 2.8152e-03, 1.7500e-03, 2.2182e-03, 2.5940e-03,\n",
      "        4.2686e-03, 1.0147e-03, 1.6165e-03, 2.6913e-03, 1.0963e-02, 4.5891e-03,\n",
      "        2.0199e-03, 4.2152e-03, 2.3766e-03, 3.7155e-03, 1.8644e-03, 6.1035e-04,\n",
      "        3.7231e-03, 1.5516e-03, 4.2915e-03, 7.0763e-03, 3.7937e-03, 6.4392e-03,\n",
      "        2.0096e-02, 9.6207e-03, 5.1994e-03, 2.7523e-03, 4.3564e-03, 4.8714e-03,\n",
      "        3.3951e-03, 1.9407e-03, 7.0457e-03, 2.9869e-03, 1.0281e-03, 1.4830e-03,\n",
      "        6.9771e-03, 8.4229e-03, 5.8937e-03, 5.9280e-03, 6.8207e-03, 5.4131e-03,\n",
      "        3.4447e-03, 1.2960e-03, 1.8339e-03, 4.9543e-04, 5.1117e-03, 4.3564e-03,\n",
      "        1.0696e-02, 8.5144e-03, 4.0817e-03, 3.7365e-03, 5.6791e-04, 5.2376e-03,\n",
      "        5.7297e-03, 1.1187e-03, 1.4706e-03, 6.1035e-03, 1.6129e-02, 2.4986e-03,\n",
      "        1.1047e-02, 7.0496e-03, 2.7657e-03, 2.3346e-03, 1.4858e-03, 1.6661e-03,\n",
      "        1.7834e-04, 2.1839e-03, 6.3753e-04, 3.5000e-04, 5.7449e-03, 4.8523e-03,\n",
      "        7.6714e-03, 6.3553e-03, 4.7073e-03, 6.0463e-03, 3.7823e-03, 4.8370e-03,\n",
      "        2.4891e-03, 5.2719e-03, 3.6697e-03, 2.2411e-03, 6.7997e-04, 6.3848e-04,\n",
      "        2.1725e-03, 8.9951e-03, 3.0685e-02, 8.3084e-03, 2.2049e-03, 1.5182e-03,\n",
      "        2.5826e-03, 1.1520e-02, 2.1133e-03, 8.4457e-03, 4.8866e-03, 2.2259e-03,\n",
      "        4.8409e-03, 9.0027e-03, 3.8185e-03, 5.6496e-03, 7.4463e-03, 5.8479e-03,\n",
      "        5.0354e-03], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "i = -1\n",
    "for j in range(len(outputs.hidden_states)):\n",
    "    print(f\" Number of elements : {len(outputs.hidden_states[j][1:])}\")\n",
    "    print(f\"Hidden states shape for generated token [{j}] : {outputs.hidden_states[j][i].shape}\")\n",
    "    #print(f\"Some values: {outputs.hidden_states[j][i][:5]}\")\n",
    "    \n",
    "    print(f\"Attention weights shape for generated token [{j}] : {outputs.attentions[j][i].shape}\")\n",
    "    print(f\"Last layer attentions for generated token [{j}] : {outputs.attentions[j][i][0,-1,-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.3057,  1.5254, -0.2416,  ...,  0.3003, -1.8262, -2.1016],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.1152, -1.7266,  0.7705,  ...,  2.9336,  0.3928, -4.2734],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.4785,  0.6235,  4.4258,  ...,  1.9551, -1.7490, -4.8086],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.4873, -1.3467,  0.0574,  ...,  3.5391,  0.3665, -2.8145],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-5.3906,  5.2148,  0.3464,  ...,  0.4092, -2.9355,  0.1616],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-5.7734,  3.5352,  1.3125,  ...,  1.8682, -1.0205,  0.5771],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 2.1035, -2.1172, -0.3342,  ...,  4.5430, -3.7988, -0.2561],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.1992, -0.0919,  1.8438,  ..., -1.1729, -4.7109,  1.4404],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.0938,  0.3894,  1.7725,  ..., -1.6318, -2.1484, -2.3438],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.7031, -0.4368,  1.4619,  ..., -1.6377, -2.6367, -2.8086],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.6016,  0.9702,  0.7393,  ...,  0.1337, -0.3062,  1.2285],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.2266, -0.8047,  0.8188,  ..., -0.4741, -1.8682, -0.3289],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.7656,  1.8398, -2.1016,  ..., -5.3398, -3.8086, -2.3828],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.5801,  0.2301,  2.3047,  ..., -2.2129, -0.1447, -2.7617],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.5234,  1.7021,  0.4060,  ..., -0.8906, -0.3479,  0.9893],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.7207, -1.2334, -1.1289,  ..., -0.3555, -0.9409,  0.3906],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.1758,  0.9790, -1.8838,  ..., -1.2246, -3.4629, -0.8838],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.9072, -0.5220, -0.4604,  ..., -0.9971, -0.7520,  1.0098],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.3347, -1.1758, -0.2861,  ..., -0.5659, -4.5000,  0.4348],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.6270, -2.1211,  0.2629,  ..., -0.2264, -4.8672,  0.1509],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.2451, -0.2107,  0.7188,  ...,  1.1631, -1.1914, -2.1309],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.7500,  1.8232, -0.4539,  ...,  4.2734, -4.5273, -0.1267],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.3074, -1.5791,  0.8486,  ...,  0.8252,  0.9087, -1.7686],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.7744, -0.7505, -0.8442,  ...,  2.2539, -1.5137, -3.4277],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.5664,  0.4028,  0.6606,  ...,  1.6289, -4.6523, -1.5947],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.8691,  0.8071,  2.1660,  ..., -2.4980, -2.0820, -0.7593],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.7930, -1.7148,  3.5840,  ..., -0.6353, -2.3457, -1.4893],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.2129,  1.3281,  1.2939,  ..., -2.4004, -2.5645,  0.0469],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.1875,  1.5674, -0.3276,  ..., -2.5195, -2.2852, -0.4387],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.2188,  1.0840,  2.8086,  ..., -0.9243, -0.4932, -0.3943],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-5.7773,  0.8901,  0.4724,  ..., -2.2812, -0.0234,  0.0502],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-5.8789,  1.7129, -3.2070,  ..., -0.5708, -2.4316, -0.2030],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-7.9141, -0.2969, -1.0850,  ..., -0.9170, -0.9351, -2.1055],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.9102,  0.1654,  0.4424,  ..., -2.9160, -1.3887, -0.3855],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.6758, -0.1866,  1.3916,  ..., -1.2891, -3.0859, -2.1953],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.5488,  0.0863,  1.3164,  ..., -1.7705, -3.4629, -0.1967],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.0742,  0.2900,  0.2017,  ..., -1.4971, -1.4277, -0.7920],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.7158, -4.0547,  1.5176,  ...,  0.7612, -0.1835, -0.2993],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-5.5977, -2.8809,  0.9160,  ...,  1.0029,  1.3164, -2.4141],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.0312,  1.2246,  1.6621,  ...,  2.1367, -1.6289, -2.0273],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.6895, -2.3301,  1.9092,  ..., -0.4785,  0.2517, -1.4395],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 2.9414, -4.1562,  1.9648,  ...,  0.2703,  1.2129, -0.2554],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.5796, -5.8633,  2.1289,  ...,  1.0967,  1.1084, -0.6670],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.1797,  0.7354,  0.3267,  ..., -3.2012, -1.4277,  0.4653],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 2.2480, -5.3711, -0.5439,  ...,  0.0338,  0.5264,  0.3982],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.5010, -3.6309, -0.7314,  ..., -1.9961,  0.6060,  1.2236],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.2211, -1.2627,  4.1602,  ..., -3.0957,  0.8677,  1.9883],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.7222, -4.4375,  1.6055,  ..., -1.1953,  1.7578, -2.5723],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.9727, -0.4075,  2.1699,  ..., -2.7969,  0.5356, -0.0938],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.0234,  0.4531,  1.6807,  ..., -3.4277, -1.6396, -0.7451],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 3.1543,  2.8672,  3.7148,  ..., -3.1738,  0.7017, -2.4395],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.2559,  1.3379,  3.5293,  ..., -2.2266, -3.6777, -3.0957],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.3176, -1.7334,  4.6602,  ..., -3.1777,  1.2930, -0.3542],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.0137, -0.7871,  3.2754,  ..., -1.5625,  0.7051, -0.4036],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.3066, -2.0684,  3.1172,  ..., -1.0967, -1.1426,  0.0767],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.0684, -1.6924, -0.2852,  ..., -1.9941,  2.0488,  1.3076],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.3672, -3.7617, -0.4805,  ..., -1.1289,  1.3164,  1.6250],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.6816, -5.8125, -0.0089,  ..., -1.4912,  2.7754,  0.5767],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.2734, -0.1417,  3.8711,  ..., -2.5254,  0.2236,  1.4619],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.4414, -3.3594,  2.8633,  ..., -3.4199,  2.5664, -2.4844],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.5732, -2.3809,  2.1641,  ..., -1.9219,  3.0879, -1.1934],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.1680, -2.6621, -1.2061,  ..., -2.9277, -0.5776,  0.2537],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.8301, -1.0703,  1.3252,  ..., -4.2188,  1.9795, -0.9922],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 2.0195, -1.8535,  1.3809,  ..., -4.1445,  3.2598, -0.9370],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.2278, -0.6226,  0.5693,  ..., -3.9355, -0.3706, -0.6826],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.8047, -2.8281,  0.6226,  ..., -1.5547, -2.2949, -1.1572],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.7871, -4.2500,  3.4141,  ..., -0.5200,  1.1504, -0.4756],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.5010,  0.2346, -3.5703,  ..., -2.9043,  3.2754, -0.7261],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.1010, -1.8428, -2.0176,  ..., -3.9727,  2.0391,  1.5703],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-7.6328,  0.7690, -0.8184,  ..., -2.7500, -0.1487, -2.0762],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.4414, -2.8320, -0.7485,  ..., -0.0237,  0.3394, -1.1191],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.9902,  1.8389,  1.5820,  ..., -0.9448,  2.3672, -3.2344],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.8262, -3.2891,  5.3633,  ..., -0.6392, -0.5225, -2.8516],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.2620, -4.2578,  1.7529,  ..., -2.6562,  2.2500, -2.6152],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.6562, -3.4863,  1.7861,  ..., -1.9912, -1.9531,  1.9736],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.7246,  0.9028,  0.6587,  ..., -0.4443, -0.4119,  0.0947],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 3.0977, -3.2188,  0.2695,  ..., -2.8125, -2.8438, -3.4336],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.5625,  0.4470, -0.9346,  ..., -1.6504,  1.0703,  1.6738],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.7812, -4.3945,  1.0664,  ..., -4.0859, -1.4980, -2.1191],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-5.3177e-03, -2.0293e+00, -2.6680e+00,  ..., -5.5391e+00,\n",
      "        -3.0410e+00, -1.5771e+00], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.4746, -1.7988, -3.4180,  ..., -1.0283,  0.8101, -1.7354],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.3750, -2.5977,  2.5176,  ..., -2.5762,  0.5215, -0.8369],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-5.9180, -4.1953,  0.4614,  ..., -5.6484, -2.5859,  1.0342],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.0622, -1.7646,  1.4316,  ..., -1.8057,  0.6914, -3.4492],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 3.6914, -1.3828, -0.0857,  ..., -3.7461,  5.5977, -1.3398],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.8203,  2.0449, -1.2607,  ..., -2.7695,  0.0255, -1.0879],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.1016,  0.2412, -2.6445,  ...,  0.0150, -0.3857, -1.0234],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.1088, -1.4775, -0.0224,  ...,  0.0076,  0.9126, -1.2227],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.5615,  1.8730, -3.8281,  ..., -2.8906,  3.0020, -4.0781],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 2.7637, -0.1220, -2.9004,  ..., -5.0703,  3.2480, -1.6494],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.7891,  4.9805, -0.0300,  ...,  1.2588,  0.0303,  3.8789],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.7202,  2.4551,  1.4785,  ..., -0.9976,  2.4082,  1.4355],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.3872,  2.1719, -1.4707,  ..., -2.7402, -0.0431, -0.3757],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 4.9727,  2.0664,  2.0000,  ..., -0.5557, -0.0222,  4.4609],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 6.3398,  1.3711,  1.5459,  ..., -0.8608,  0.4858,  5.5312],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.6504,  1.2617, -0.7705,  ..., -2.7344,  1.4336, -4.9453],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.2101, -3.3789,  4.5078,  ...,  0.6372, -0.8687, -3.2988],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 3.0176, -4.4922, -0.3857,  ..., -2.4277, -3.4551, -2.5098],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 2.8066, -2.8652,  2.8691,  ..., -0.6265, -0.5947, -3.8691],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.3594,  2.1445, -0.6406,  ...,  0.9282, -2.3008, -0.0895],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.5186, -3.8145,  2.2598,  ..., -0.5259, -0.7451, -2.4980],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.8311, -3.6484,  0.6230,  ..., -3.4004, -1.5293, -0.6724],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.6294, -0.9590,  1.1221,  ..., -2.8555, -3.1309, -2.7344],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.3264,  0.8110, -2.0996,  ..., -1.9482,  1.2334,  0.4192],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.2432, -2.9844,  1.7686,  ..., -5.2695, -2.2852, -1.6680],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.0273, -2.6660,  0.0330,  ..., -2.4434, -4.2578, -2.7012],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 2.1211, -0.2411,  0.9146,  ..., -0.5444,  0.1476,  0.0934],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.8247,  1.3320,  1.2842,  ..., -3.5371, -1.9463, -4.8906],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.3564,  2.1914,  1.9443,  ...,  1.7275,  1.1533, -4.5234],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.9238, -4.1016,  2.8789,  ..., -2.4258,  0.8525, -3.4004],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-7.1992, -5.0000,  1.1641,  ..., -3.9219, -3.4844, -1.0459],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.4421, -1.3125,  2.6816,  ..., -0.5332, -0.9336, -2.8203],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.3303, -5.3477,  0.0633,  ...,  0.8906, -0.3284, -1.8740],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.1340, -6.0664, -0.2866,  ...,  3.3965, -1.8711, -1.4551],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.7622, -1.0830,  2.1777,  ...,  1.3018, -1.7686,  2.9883],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.3306, -1.9795,  1.3232,  ...,  2.1250, -1.6982, -0.3625],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.7832, -2.5742,  1.0488,  ...,  1.7412, -0.4558, -0.2454],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.0410, -0.5220, -2.2930,  ..., -0.6113, -0.0981, -1.5801],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 2.1016,  1.9111,  0.2607,  ..., -1.8750, -2.2676, -1.8311],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.7852,  1.4033,  0.6021,  ..., -0.6904,  1.4912,  1.4043],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.6133, -1.0176,  1.8555,  ..., -0.1541,  0.7812, -0.6240],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.1870,  0.6636,  3.7793,  ...,  1.3359, -1.7988, -2.1973],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.0166, -0.7466,  0.7583,  ..., -0.4854,  3.0488, -2.9453],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.2285,  2.6172,  3.8027,  ...,  0.8442, -1.9727,  1.4502],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.0084,  0.7236,  1.9033,  ...,  2.7793, -2.8027, -1.6396],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.6606, -2.5586,  3.4375,  ..., -0.6050,  1.8994, -1.4297],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.1533, -1.4043,  3.2070,  ..., -1.8154,  4.7383, -3.9805],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 2.3242, -1.2422,  1.9834,  ..., -1.6465,  5.7891, -3.4121],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "j = 1\n",
    "i = -1\n",
    "for j in range(len(outputs.hidden_states)):\n",
    "    print(f\" Number of elements : {len(outputs.hidden_states[j])}\")\n",
    "    #print(f\"Hidden states shape for generated token [{j}] : {outputs.hidden_states[j][i].shape}\")\n",
    "    print(f\"Last hidden state : {outputs.hidden_states[j][i][0,-1,:4096]}\")\n",
    "    #print(f\"Some values: {outputs.hidden_states[j][i][:5]}\")\n",
    "    \n",
    "    #print(f\"Attention weights shape for generated token [{j}] : {outputs.attentions[j][i].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moutputs\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequences\u001b[39m\u001b[38;5;124m'\u001b[39m][:,nb_tokens_in:]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "outputs['sequences'][:,nb_tokens_in:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (1): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (2): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (3): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (4): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (5): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (6): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (7): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (8): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (9): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (10): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (11): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (12): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (13): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (14): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (15): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (16): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (17): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (18): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (19): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (20): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (21): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (22): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (23): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (24): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (25): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (26): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (27): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (28): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (29): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (30): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (31): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A question that gets to the heart of the absurd. I must admit, I've never come across any research on this topic. In fact, I'm not even sure it's possible for a human to eat a helicopter in one sitting. Helicopters are large, complex machines made of metal, plastic, and other materials, not exactly something you'd find on the menu at your local diner.\n",
      "\n",
      "However, if we assume a hypothetical scenario where a human could somehow consume a helicopter, I'd estimate the number of helicopters a person could eat in one sitting would be... zero. That's right, folks, a big fat zero. Not\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs['sequences'][0][nb_tokens_in:], skip_special_tokens=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_inf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
