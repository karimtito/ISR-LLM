{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from luna.utils.llama import LLaMATokenizer, LLaMAForCausalLM\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "luna_models= {  'tokenizer': LLaMATokenizer, 'model': LLaMAForCausalLM}\n",
    "hf_llama_models = { 'tokenizer': LlamaTokenizer, 'model': LlamaForCausalLM}\n",
    "hf_auto_models = { 'tokenizer': AutoTokenizer, 'model': AutoModelForCausalLM}\n",
    "\n",
    "backends = {'luna': luna_models, 'hf_llama': hf_llama_models, 'hf_auto': hf_auto_models}\n",
    "\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of chat templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f596d4640834ac1ae33d0ac8cadfb70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "backend_name = 'hf_auto'\n",
    "checkpoint = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "backend = backends[backend_name]\n",
    "device='cuda:0'\n",
    "tokenizer = backend['tokenizer'].from_pretrained(checkpoint,device_map=\"auto\")\n",
    "model = backend['model'].from_pretrained(checkpoint,low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a researcher. You are also a bit of a comedian.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_tokens_in: 72\n"
     ]
    }
   ],
   "source": [
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=False,add_generation_prompt=True, )\n",
    "inputs = tokenizer(tokenized_chat, return_tensors=\"pt\", padding=False, truncation=True, max_length=2500).to(device)\n",
    "del tokenized_chat\n",
    "nb_tokens_in = len(inputs[0])\n",
    "print(f\"nb_tokens_in: {nb_tokens_in}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(inputs.input_ids, top_k=32, max_new_tokens=128, pad_token_id=tokenizer.eos_token_id, output_scores=True,return_dict_in_generate=True,\n",
    "                         output_hidden_states=True, output_attentions=True, attention_mask=inputs['attention_mask'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.generation.utils.GenerateDecoderOnlyOutput"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a friendly chatbot who always responds in the style of a researcher. You are also a bit of a comedian.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How many helicopters can a human eat in one sitting?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "(chuckling) Ahah, I think we may have a bit of a \"flying\" question on our hands here! (pausing for comedic effect) As a researcher, I must inform you that it's not possible for a human to eat helicopters in one sitting. In fact, it's not possible for a human to eat a helicopter at all, as they are complex machines made of metal, plastic, and other materials that aren't exactly digestible.\n",
      "\n",
      "But, if we're looking for a more theoretical answer, let's consider the nutritional value of a helicopter. (smiling) Unfortunately, there isn't much to go\n",
      "nb of new tokens: 128 \n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs['sequences'][0],skip_special_tokens=False))\n",
    "outputs.__dict__.keys()\n",
    "nb_tokens_out = len(outputs.sequences[0])\n",
    "print(f\"nb of new tokens: { nb_tokens_out-nb_tokens_in} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0047,  0.0104, -0.0247,  ..., -0.0127,  0.0014, -0.0083],\n",
      "         [ 0.0047,  0.0104, -0.0247,  ..., -0.0127,  0.0014, -0.0083],\n",
      "         [ 0.0043,  0.0107, -0.0240,  ..., -0.0166,  0.0016, -0.0087],\n",
      "         ...,\n",
      "         [-0.0053,  0.0075, -0.0223,  ..., -0.0236, -0.0030, -0.0009],\n",
      "         [ 0.0040,  0.0105, -0.0242,  ..., -0.0171,  0.0008, -0.0088],\n",
      "         [ 0.0016,  0.0148, -0.0243,  ..., -0.0157,  0.0018, -0.0072]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.hidden_states[0][0][:5]-outputs.hidden_states[8][0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of elements : 32\n",
      "Hidden states shape for generated token [0] : torch.Size([1, 72, 4096])\n",
      "Attention weights shape for generated token [0] : torch.Size([1, 32, 72, 72])\n",
      "Last layer attentions for generated token [0] : tensor([0.2471, 0.2476, 0.0012, 0.0014, 0.0008, 0.0104, 0.0011, 0.0009, 0.0013,\n",
      "        0.0019, 0.0077, 0.0013, 0.0009, 0.0012, 0.0284, 0.0297, 0.0022, 0.0029,\n",
      "        0.0009, 0.0016, 0.0009, 0.0014, 0.0110, 0.0027, 0.0119, 0.0503, 0.0099,\n",
      "        0.0026, 0.0011, 0.0026, 0.0029, 0.0064, 0.0024, 0.0027, 0.0018, 0.0025,\n",
      "        0.0010, 0.0018, 0.0012, 0.0005, 0.0066, 0.0024, 0.0024, 0.0025, 0.0025,\n",
      "        0.0009, 0.0016, 0.0009, 0.0010, 0.0058, 0.0037, 0.0023, 0.0047, 0.0137,\n",
      "        0.0028, 0.0212, 0.0036, 0.0039, 0.0104, 0.0047, 0.0037, 0.0049, 0.0091,\n",
      "        0.0052, 0.0026, 0.0030, 0.0032, 0.0150, 0.0027, 0.0438, 0.0311, 0.0701],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [1] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [1] : torch.Size([1, 32, 1, 73])\n",
      "Last layer attentions for generated token [1] : tensor([3.6035e-01, 3.6035e-01, 4.4036e-04, 9.5797e-04, 3.0088e-04, 3.0174e-03,\n",
      "        2.6822e-04, 3.4356e-04, 1.0500e-03, 1.5488e-03, 6.4888e-03, 5.3310e-04,\n",
      "        2.8324e-04, 5.2691e-04, 1.0513e-02, 1.1841e-02, 5.7411e-04, 9.1791e-04,\n",
      "        5.3644e-04, 5.0020e-04, 3.3903e-04, 1.0443e-03, 9.8877e-03, 1.5860e-03,\n",
      "        1.1642e-02, 1.3870e-02, 2.1553e-03, 5.6744e-04, 3.2401e-04, 1.3828e-03,\n",
      "        1.5306e-03, 2.4738e-03, 4.0579e-04, 4.2272e-04, 4.5800e-04, 7.4911e-04,\n",
      "        5.5218e-04, 6.4707e-04, 4.1604e-04, 2.4509e-04, 6.9542e-03, 2.8000e-03,\n",
      "        9.8896e-04, 7.4625e-04, 6.6471e-04, 1.6522e-04, 5.4932e-04, 8.3399e-04,\n",
      "        4.7612e-04, 4.2152e-03, 2.5043e-03, 3.9482e-04, 3.1452e-03, 6.6147e-03,\n",
      "        2.2678e-03, 8.3542e-03, 7.6103e-04, 1.3590e-03, 5.4550e-03, 2.2583e-03,\n",
      "        1.1644e-03, 2.1954e-03, 4.0207e-03, 2.3060e-03, 9.8038e-04, 1.7729e-03,\n",
      "        2.3689e-03, 6.1188e-03, 1.9350e-03, 3.6102e-02, 1.5594e-02, 4.1168e-02,\n",
      "        2.0859e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [2] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [2] : torch.Size([1, 32, 1, 74])\n",
      "Last layer attentions for generated token [2] : tensor([2.9907e-01, 2.9907e-01, 9.8038e-04, 1.1082e-03, 5.7268e-04, 5.7030e-03,\n",
      "        2.9039e-04, 4.6301e-04, 1.7509e-03, 2.5654e-03, 5.6229e-03, 2.3055e-04,\n",
      "        4.0078e-04, 5.6601e-04, 6.1760e-03, 1.4008e-02, 6.5041e-04, 2.9411e-03,\n",
      "        1.0214e-03, 1.4544e-03, 2.6631e-04, 1.7748e-03, 2.3865e-02, 4.4479e-03,\n",
      "        1.2459e-02, 1.5671e-02, 1.3323e-03, 1.0414e-03, 4.9210e-04, 1.0452e-03,\n",
      "        1.2684e-03, 2.7294e-03, 7.1144e-04, 6.2561e-04, 8.9073e-04, 2.2926e-03,\n",
      "        1.1749e-03, 2.1152e-03, 7.7105e-04, 3.1447e-04, 1.0231e-02, 4.7073e-03,\n",
      "        1.0157e-03, 1.7920e-03, 9.2793e-04, 3.2187e-04, 4.9973e-04, 8.4639e-04,\n",
      "        4.8232e-04, 3.6488e-03, 5.0507e-03, 7.8917e-04, 3.9330e-03, 1.4618e-02,\n",
      "        3.8433e-03, 1.6235e-02, 7.0715e-04, 2.3746e-03, 6.4812e-03, 2.0390e-03,\n",
      "        1.7748e-03, 2.0924e-03, 8.4000e-03, 4.9515e-03, 1.7815e-03, 2.9202e-03,\n",
      "        4.4365e-03, 9.3002e-03, 1.9684e-03, 3.4607e-02, 2.3743e-02, 7.4158e-02,\n",
      "        2.2629e-02, 6.7863e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [3] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [3] : torch.Size([1, 32, 1, 75])\n",
      "Last layer attentions for generated token [3] : tensor([0.2211, 0.2211, 0.0007, 0.0010, 0.0004, 0.0080, 0.0006, 0.0007, 0.0013,\n",
      "        0.0022, 0.0065, 0.0010, 0.0011, 0.0017, 0.0189, 0.0241, 0.0013, 0.0022,\n",
      "        0.0011, 0.0013, 0.0008, 0.0025, 0.0122, 0.0030, 0.0138, 0.0334, 0.0060,\n",
      "        0.0034, 0.0012, 0.0034, 0.0024, 0.0053, 0.0022, 0.0032, 0.0020, 0.0041,\n",
      "        0.0014, 0.0032, 0.0012, 0.0006, 0.0102, 0.0037, 0.0020, 0.0039, 0.0040,\n",
      "        0.0009, 0.0021, 0.0008, 0.0015, 0.0081, 0.0052, 0.0011, 0.0031, 0.0102,\n",
      "        0.0025, 0.0267, 0.0028, 0.0033, 0.0061, 0.0027, 0.0026, 0.0036, 0.0048,\n",
      "        0.0040, 0.0035, 0.0016, 0.0041, 0.0072, 0.0018, 0.0439, 0.0225, 0.0742,\n",
      "        0.0345, 0.0066, 0.0627], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [4] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [4] : torch.Size([1, 32, 1, 76])\n",
      "Last layer attentions for generated token [4] : tensor([0.2727, 0.2727, 0.0009, 0.0009, 0.0006, 0.0058, 0.0005, 0.0005, 0.0008,\n",
      "        0.0015, 0.0046, 0.0006, 0.0004, 0.0007, 0.0161, 0.0183, 0.0010, 0.0014,\n",
      "        0.0006, 0.0011, 0.0004, 0.0017, 0.0101, 0.0024, 0.0072, 0.0224, 0.0054,\n",
      "        0.0010, 0.0005, 0.0014, 0.0016, 0.0030, 0.0018, 0.0018, 0.0016, 0.0017,\n",
      "        0.0005, 0.0010, 0.0007, 0.0004, 0.0051, 0.0014, 0.0017, 0.0021, 0.0018,\n",
      "        0.0006, 0.0011, 0.0006, 0.0011, 0.0045, 0.0032, 0.0016, 0.0028, 0.0137,\n",
      "        0.0018, 0.0165, 0.0020, 0.0029, 0.0055, 0.0043, 0.0026, 0.0034, 0.0060,\n",
      "        0.0029, 0.0015, 0.0014, 0.0031, 0.0117, 0.0016, 0.0321, 0.0208, 0.0625,\n",
      "        0.0252, 0.0025, 0.0560, 0.0244], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [5] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [5] : torch.Size([1, 32, 1, 77])\n",
      "Last layer attentions for generated token [5] : tensor([2.4902e-01, 2.4902e-01, 1.1158e-03, 1.4839e-03, 5.3835e-04, 6.7863e-03,\n",
      "        5.0879e-04, 4.7517e-04, 9.2506e-04, 1.8177e-03, 4.2992e-03, 1.3304e-04,\n",
      "        4.3344e-04, 5.6553e-04, 9.4986e-03, 1.5930e-02, 8.9121e-04, 1.7719e-03,\n",
      "        8.0681e-04, 1.2064e-03, 3.5048e-04, 1.4019e-03, 1.2505e-02, 2.0027e-03,\n",
      "        9.7961e-03, 2.0493e-02, 4.5509e-03, 1.7824e-03, 8.1921e-04, 1.2665e-03,\n",
      "        1.6356e-03, 2.6627e-03, 1.0061e-03, 7.4768e-04, 6.5327e-04, 1.6489e-03,\n",
      "        8.3256e-04, 7.0763e-04, 7.0381e-04, 4.5705e-04, 4.5433e-03, 2.8858e-03,\n",
      "        1.4210e-03, 1.3247e-03, 9.3222e-04, 6.9523e-04, 6.4707e-04, 8.3065e-04,\n",
      "        1.0424e-03, 4.4479e-03, 5.7259e-03, 7.6818e-04, 2.9736e-03, 9.5901e-03,\n",
      "        2.3479e-03, 1.7456e-02, 3.7556e-03, 3.6373e-03, 8.5373e-03, 5.1537e-03,\n",
      "        3.6697e-03, 2.2392e-03, 4.7112e-03, 3.9215e-03, 2.0218e-03, 1.6232e-03,\n",
      "        8.2397e-03, 7.2861e-03, 2.8687e-03, 3.6255e-02, 2.2171e-02, 7.6172e-02,\n",
      "        2.9587e-02, 5.7793e-03, 2.8580e-02, 3.3142e-02, 3.9703e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [6] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [6] : torch.Size([1, 32, 1, 78])\n",
      "Last layer attentions for generated token [6] : tensor([0.2306, 0.2306, 0.0005, 0.0007, 0.0003, 0.0060, 0.0009, 0.0006, 0.0010,\n",
      "        0.0018, 0.0063, 0.0007, 0.0006, 0.0008, 0.0160, 0.0216, 0.0020, 0.0019,\n",
      "        0.0010, 0.0010, 0.0004, 0.0013, 0.0099, 0.0022, 0.0119, 0.0307, 0.0055,\n",
      "        0.0012, 0.0006, 0.0014, 0.0011, 0.0020, 0.0020, 0.0018, 0.0012, 0.0026,\n",
      "        0.0008, 0.0016, 0.0009, 0.0005, 0.0042, 0.0024, 0.0023, 0.0024, 0.0024,\n",
      "        0.0007, 0.0011, 0.0005, 0.0010, 0.0037, 0.0038, 0.0012, 0.0039, 0.0099,\n",
      "        0.0030, 0.0172, 0.0037, 0.0031, 0.0054, 0.0046, 0.0030, 0.0034, 0.0055,\n",
      "        0.0057, 0.0025, 0.0015, 0.0049, 0.0141, 0.0026, 0.0375, 0.0227, 0.0660,\n",
      "        0.0376, 0.0043, 0.0416, 0.0241, 0.0271, 0.0153], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [7] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [7] : torch.Size([1, 32, 1, 79])\n",
      "Last layer attentions for generated token [7] : tensor([2.7686e-01, 2.7686e-01, 3.1972e-04, 4.2200e-04, 1.8251e-04, 4.4632e-03,\n",
      "        3.2043e-04, 3.8791e-04, 8.7404e-04, 8.7786e-04, 3.9711e-03, 3.4428e-04,\n",
      "        2.4283e-04, 5.3549e-04, 9.9792e-03, 9.6817e-03, 7.3624e-04, 8.2302e-04,\n",
      "        3.1233e-04, 3.5882e-04, 1.3781e-04, 5.2738e-04, 8.2932e-03, 1.0500e-03,\n",
      "        7.0038e-03, 1.5373e-02, 9.8133e-04, 5.1785e-04, 2.7084e-04, 7.9584e-04,\n",
      "        6.4087e-04, 1.6718e-03, 6.5756e-04, 1.1787e-03, 1.0986e-03, 8.0538e-04,\n",
      "        3.2854e-04, 7.8964e-04, 3.9172e-04, 2.0123e-04, 2.9354e-03, 1.9455e-03,\n",
      "        5.1880e-04, 1.0004e-03, 1.1158e-03, 4.2200e-04, 9.8705e-04, 5.7220e-04,\n",
      "        1.3103e-03, 4.6883e-03, 3.8204e-03, 6.8235e-04, 3.7060e-03, 7.5226e-03,\n",
      "        2.0409e-03, 1.0445e-02, 8.7929e-04, 1.8930e-03, 3.4275e-03, 2.1343e-03,\n",
      "        1.3437e-03, 3.0746e-03, 4.3793e-03, 2.1000e-03, 9.7132e-04, 1.1091e-03,\n",
      "        3.6087e-03, 1.1566e-02, 2.0447e-03, 4.7180e-02, 2.3666e-02, 6.1707e-02,\n",
      "        1.1505e-02, 1.8206e-03, 4.1718e-02, 3.1128e-02, 2.6047e-02, 1.7990e-02,\n",
      "        2.3773e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [8] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [8] : torch.Size([1, 32, 1, 80])\n",
      "Last layer attentions for generated token [8] : tensor([0.2167, 0.2167, 0.0010, 0.0005, 0.0006, 0.0081, 0.0008, 0.0008, 0.0011,\n",
      "        0.0011, 0.0060, 0.0005, 0.0007, 0.0014, 0.0127, 0.0176, 0.0011, 0.0013,\n",
      "        0.0007, 0.0010, 0.0004, 0.0012, 0.0147, 0.0043, 0.0071, 0.0255, 0.0040,\n",
      "        0.0009, 0.0006, 0.0013, 0.0014, 0.0032, 0.0024, 0.0019, 0.0016, 0.0016,\n",
      "        0.0008, 0.0016, 0.0009, 0.0005, 0.0050, 0.0032, 0.0017, 0.0020, 0.0020,\n",
      "        0.0012, 0.0013, 0.0010, 0.0017, 0.0040, 0.0039, 0.0013, 0.0043, 0.0107,\n",
      "        0.0038, 0.0149, 0.0019, 0.0036, 0.0048, 0.0035, 0.0027, 0.0036, 0.0068,\n",
      "        0.0048, 0.0031, 0.0034, 0.0062, 0.0104, 0.0022, 0.0369, 0.0223, 0.0617,\n",
      "        0.0220, 0.0024, 0.0551, 0.0379, 0.0318, 0.0133, 0.0195, 0.0119],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [9] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [9] : torch.Size([1, 32, 1, 81])\n",
      "Last layer attentions for generated token [9] : tensor([0.2257, 0.2257, 0.0004, 0.0008, 0.0003, 0.0041, 0.0008, 0.0007, 0.0016,\n",
      "        0.0019, 0.0066, 0.0007, 0.0008, 0.0012, 0.0190, 0.0178, 0.0011, 0.0011,\n",
      "        0.0007, 0.0010, 0.0004, 0.0012, 0.0094, 0.0022, 0.0072, 0.0173, 0.0081,\n",
      "        0.0012, 0.0008, 0.0013, 0.0011, 0.0031, 0.0030, 0.0019, 0.0012, 0.0024,\n",
      "        0.0011, 0.0017, 0.0015, 0.0009, 0.0071, 0.0044, 0.0028, 0.0034, 0.0028,\n",
      "        0.0016, 0.0010, 0.0008, 0.0014, 0.0049, 0.0048, 0.0009, 0.0044, 0.0072,\n",
      "        0.0044, 0.0130, 0.0053, 0.0049, 0.0089, 0.0067, 0.0051, 0.0037, 0.0055,\n",
      "        0.0077, 0.0035, 0.0021, 0.0100, 0.0071, 0.0033, 0.0393, 0.0180, 0.0474,\n",
      "        0.0260, 0.0027, 0.0289, 0.0249, 0.0221, 0.0154, 0.0199, 0.0142, 0.0236],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [10] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [10] : torch.Size([1, 32, 1, 82])\n",
      "Last layer attentions for generated token [10] : tensor([0.1648, 0.1648, 0.0005, 0.0007, 0.0004, 0.0060, 0.0006, 0.0010, 0.0020,\n",
      "        0.0025, 0.0063, 0.0010, 0.0004, 0.0009, 0.0119, 0.0197, 0.0014, 0.0011,\n",
      "        0.0007, 0.0010, 0.0008, 0.0018, 0.0102, 0.0024, 0.0076, 0.0261, 0.0060,\n",
      "        0.0018, 0.0011, 0.0024, 0.0017, 0.0036, 0.0032, 0.0022, 0.0019, 0.0025,\n",
      "        0.0010, 0.0021, 0.0013, 0.0008, 0.0070, 0.0038, 0.0017, 0.0020, 0.0025,\n",
      "        0.0010, 0.0020, 0.0008, 0.0011, 0.0057, 0.0053, 0.0014, 0.0066, 0.0069,\n",
      "        0.0044, 0.0156, 0.0035, 0.0034, 0.0067, 0.0047, 0.0036, 0.0039, 0.0061,\n",
      "        0.0074, 0.0034, 0.0030, 0.0084, 0.0129, 0.0032, 0.0472, 0.0321, 0.0750,\n",
      "        0.0145, 0.0030, 0.0418, 0.0417, 0.0262, 0.0243, 0.0175, 0.0122, 0.0297,\n",
      "        0.0287], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [11] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [11] : torch.Size([1, 32, 1, 83])\n",
      "Last layer attentions for generated token [11] : tensor([0.2043, 0.2043, 0.0006, 0.0006, 0.0003, 0.0059, 0.0005, 0.0011, 0.0018,\n",
      "        0.0013, 0.0042, 0.0006, 0.0005, 0.0010, 0.0060, 0.0106, 0.0010, 0.0005,\n",
      "        0.0003, 0.0004, 0.0002, 0.0007, 0.0107, 0.0014, 0.0060, 0.0190, 0.0020,\n",
      "        0.0009, 0.0010, 0.0025, 0.0025, 0.0024, 0.0015, 0.0017, 0.0013, 0.0012,\n",
      "        0.0007, 0.0014, 0.0010, 0.0008, 0.0063, 0.0033, 0.0009, 0.0013, 0.0015,\n",
      "        0.0008, 0.0017, 0.0009, 0.0019, 0.0059, 0.0053, 0.0015, 0.0072, 0.0060,\n",
      "        0.0037, 0.0111, 0.0016, 0.0025, 0.0060, 0.0036, 0.0024, 0.0060, 0.0045,\n",
      "        0.0023, 0.0020, 0.0020, 0.0045, 0.0133, 0.0033, 0.0710, 0.0331, 0.0496,\n",
      "        0.0135, 0.0027, 0.0394, 0.0269, 0.0227, 0.0190, 0.0190, 0.0223, 0.0253,\n",
      "        0.0229, 0.0146], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [12] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [12] : torch.Size([1, 32, 1, 84])\n",
      "Last layer attentions for generated token [12] : tensor([0.2549, 0.2549, 0.0007, 0.0011, 0.0004, 0.0045, 0.0004, 0.0004, 0.0011,\n",
      "        0.0027, 0.0041, 0.0006, 0.0008, 0.0009, 0.0104, 0.0182, 0.0012, 0.0015,\n",
      "        0.0009, 0.0014, 0.0003, 0.0014, 0.0103, 0.0017, 0.0067, 0.0135, 0.0070,\n",
      "        0.0011, 0.0007, 0.0011, 0.0011, 0.0019, 0.0014, 0.0009, 0.0008, 0.0010,\n",
      "        0.0006, 0.0011, 0.0007, 0.0004, 0.0048, 0.0034, 0.0019, 0.0016, 0.0014,\n",
      "        0.0007, 0.0005, 0.0006, 0.0006, 0.0043, 0.0043, 0.0008, 0.0040, 0.0107,\n",
      "        0.0029, 0.0124, 0.0036, 0.0034, 0.0070, 0.0040, 0.0033, 0.0034, 0.0047,\n",
      "        0.0039, 0.0028, 0.0019, 0.0088, 0.0085, 0.0029, 0.0388, 0.0193, 0.0360,\n",
      "        0.0154, 0.0032, 0.0181, 0.0184, 0.0198, 0.0110, 0.0128, 0.0125, 0.0154,\n",
      "        0.0222, 0.0146, 0.0146], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [13] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [13] : torch.Size([1, 32, 1, 85])\n",
      "Last layer attentions for generated token [13] : tensor([2.0605e-01, 2.0520e-01, 4.2415e-04, 5.9605e-04, 3.3951e-04, 2.7523e-03,\n",
      "        3.4475e-04, 3.7289e-04, 7.0190e-04, 1.0605e-03, 3.5076e-03, 6.4802e-04,\n",
      "        3.5310e-04, 6.0987e-04, 1.3100e-02, 1.3969e-02, 5.0068e-04, 7.9727e-04,\n",
      "        3.9148e-04, 5.9843e-04, 1.9193e-04, 4.1842e-04, 5.3596e-03, 1.5869e-03,\n",
      "        5.3673e-03, 1.6373e-02, 4.4250e-03, 6.9952e-04, 4.7684e-04, 9.3174e-04,\n",
      "        9.4461e-04, 2.1191e-03, 1.1578e-03, 9.9754e-04, 7.7105e-04, 1.0118e-03,\n",
      "        4.5967e-04, 9.4128e-04, 5.8985e-04, 4.0627e-04, 5.2681e-03, 2.1954e-03,\n",
      "        1.7481e-03, 1.9550e-03, 1.7967e-03, 7.2432e-04, 9.5034e-04, 5.2166e-04,\n",
      "        8.3065e-04, 2.8458e-03, 2.3899e-03, 7.8917e-04, 3.2253e-03, 7.4654e-03,\n",
      "        2.5578e-03, 9.8648e-03, 1.7929e-03, 2.6245e-03, 5.3596e-03, 2.9755e-03,\n",
      "        2.3289e-03, 2.3022e-03, 3.6182e-03, 2.7962e-03, 1.4658e-03, 1.1511e-03,\n",
      "        4.2191e-03, 1.0147e-02, 2.1477e-03, 3.6713e-02, 2.3529e-02, 5.5328e-02,\n",
      "        2.4231e-02, 1.5516e-03, 4.4525e-02, 3.2501e-02, 3.2532e-02, 1.7517e-02,\n",
      "        1.7166e-02, 8.4152e-03, 3.0899e-02, 2.6337e-02, 1.0612e-02, 2.1042e-02,\n",
      "        3.1525e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [14] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [14] : torch.Size([1, 32, 1, 86])\n",
      "Last layer attentions for generated token [14] : tensor([2.4023e-01, 2.3975e-01, 3.5357e-04, 6.0987e-04, 1.8668e-04, 2.5539e-03,\n",
      "        2.5320e-04, 2.7800e-04, 3.7193e-04, 8.2350e-04, 1.6508e-03, 2.9302e-04,\n",
      "        3.5644e-04, 7.9203e-04, 4.3907e-03, 5.8174e-03, 3.2449e-04, 3.7336e-04,\n",
      "        2.4915e-04, 3.1638e-04, 1.7333e-04, 5.0354e-04, 7.8430e-03, 9.1887e-04,\n",
      "        4.4861e-03, 9.0408e-03, 2.0885e-03, 7.0572e-04, 3.0351e-04, 6.1560e-04,\n",
      "        5.4455e-04, 1.1730e-03, 7.1573e-04, 8.0442e-04, 4.8327e-04, 5.3787e-04,\n",
      "        8.4162e-04, 9.2220e-04, 9.8610e-04, 4.2319e-04, 4.1428e-03, 4.0550e-03,\n",
      "        1.1368e-03, 1.1959e-03, 1.3208e-03, 7.7200e-04, 6.0606e-04, 7.9060e-04,\n",
      "        8.2541e-04, 5.2567e-03, 5.0392e-03, 5.8651e-04, 4.1504e-03, 7.8583e-03,\n",
      "        3.3741e-03, 7.6485e-03, 2.4700e-03, 3.6240e-03, 8.1940e-03, 3.0670e-03,\n",
      "        2.5120e-03, 3.1910e-03, 4.1924e-03, 4.0932e-03, 1.7776e-03, 1.4658e-03,\n",
      "        1.1955e-02, 8.2703e-03, 5.4588e-03, 4.0192e-02, 2.7634e-02, 4.5013e-02,\n",
      "        1.9226e-02, 2.3956e-03, 1.7715e-02, 3.6255e-02, 2.2018e-02, 2.2507e-02,\n",
      "        1.6678e-02, 6.6490e-03, 1.4420e-02, 2.4246e-02, 8.6594e-03, 9.9716e-03,\n",
      "        3.5492e-02, 7.7324e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [15] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [15] : torch.Size([1, 32, 1, 87])\n",
      "Last layer attentions for generated token [15] : tensor([0.1759, 0.1755, 0.0005, 0.0006, 0.0003, 0.0042, 0.0004, 0.0004, 0.0005,\n",
      "        0.0010, 0.0038, 0.0006, 0.0010, 0.0010, 0.0117, 0.0164, 0.0006, 0.0005,\n",
      "        0.0004, 0.0007, 0.0002, 0.0003, 0.0050, 0.0011, 0.0044, 0.0185, 0.0036,\n",
      "        0.0008, 0.0006, 0.0005, 0.0005, 0.0013, 0.0017, 0.0014, 0.0007, 0.0010,\n",
      "        0.0003, 0.0008, 0.0007, 0.0005, 0.0031, 0.0031, 0.0021, 0.0019, 0.0020,\n",
      "        0.0009, 0.0008, 0.0005, 0.0008, 0.0025, 0.0034, 0.0009, 0.0049, 0.0052,\n",
      "        0.0032, 0.0145, 0.0034, 0.0037, 0.0072, 0.0040, 0.0053, 0.0026, 0.0043,\n",
      "        0.0044, 0.0029, 0.0013, 0.0085, 0.0119, 0.0048, 0.0304, 0.0299, 0.0719,\n",
      "        0.0260, 0.0021, 0.0356, 0.0333, 0.0261, 0.0201, 0.0231, 0.0077, 0.0210,\n",
      "        0.0260, 0.0107, 0.0183, 0.0395, 0.0098, 0.0144], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [16] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [16] : torch.Size([1, 32, 1, 88])\n",
      "Last layer attentions for generated token [16] : tensor([0.1620, 0.1620, 0.0007, 0.0007, 0.0005, 0.0050, 0.0007, 0.0009, 0.0017,\n",
      "        0.0009, 0.0062, 0.0005, 0.0005, 0.0012, 0.0158, 0.0206, 0.0009, 0.0006,\n",
      "        0.0004, 0.0006, 0.0002, 0.0006, 0.0090, 0.0022, 0.0067, 0.0213, 0.0034,\n",
      "        0.0015, 0.0010, 0.0018, 0.0021, 0.0035, 0.0012, 0.0015, 0.0008, 0.0008,\n",
      "        0.0004, 0.0009, 0.0006, 0.0005, 0.0070, 0.0040, 0.0017, 0.0019, 0.0018,\n",
      "        0.0007, 0.0014, 0.0009, 0.0012, 0.0033, 0.0028, 0.0007, 0.0038, 0.0047,\n",
      "        0.0043, 0.0112, 0.0028, 0.0027, 0.0071, 0.0042, 0.0037, 0.0068, 0.0041,\n",
      "        0.0026, 0.0016, 0.0018, 0.0041, 0.0067, 0.0027, 0.0450, 0.0263, 0.0531,\n",
      "        0.0137, 0.0016, 0.0317, 0.0246, 0.0263, 0.0201, 0.0192, 0.0164, 0.0353,\n",
      "        0.0333, 0.0089, 0.0297, 0.0354, 0.0106, 0.0146, 0.0104],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [17] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [17] : torch.Size([1, 32, 1, 89])\n",
      "Last layer attentions for generated token [17] : tensor([2.0972e-01, 2.0923e-01, 2.4748e-04, 4.3774e-04, 1.6940e-04, 3.0060e-03,\n",
      "        7.6056e-04, 9.6893e-04, 1.5097e-03, 1.6489e-03, 4.5547e-03, 1.0338e-03,\n",
      "        5.0688e-04, 1.0853e-03, 1.1406e-02, 9.4986e-03, 1.1005e-03, 9.5034e-04,\n",
      "        4.4990e-04, 4.6229e-04, 2.3425e-04, 7.0333e-04, 6.0463e-03, 1.4839e-03,\n",
      "        7.6752e-03, 9.2010e-03, 2.3022e-03, 9.5415e-04, 5.1069e-04, 1.6584e-03,\n",
      "        1.7481e-03, 2.1706e-03, 1.5221e-03, 2.3270e-03, 1.1158e-03, 8.1301e-04,\n",
      "        2.8539e-04, 1.2589e-03, 5.7077e-04, 3.1829e-04, 3.6259e-03, 2.1629e-03,\n",
      "        9.4652e-04, 1.4238e-03, 2.4242e-03, 6.8331e-04, 2.0599e-03, 1.0042e-03,\n",
      "        1.5278e-03, 4.6692e-03, 2.6779e-03, 3.3951e-04, 3.4122e-03, 2.4242e-03,\n",
      "        2.5806e-03, 7.3738e-03, 1.7242e-03, 2.5215e-03, 4.4556e-03, 3.8261e-03,\n",
      "        1.7014e-03, 3.8815e-03, 5.1193e-03, 2.5063e-03, 1.5764e-03, 1.6518e-03,\n",
      "        3.2253e-03, 5.1270e-03, 2.1763e-03, 2.5345e-02, 1.3779e-02, 3.1982e-02,\n",
      "        1.6998e-02, 3.1109e-03, 4.6234e-02, 1.9638e-02, 2.1011e-02, 1.6083e-02,\n",
      "        2.0065e-02, 1.7075e-02, 2.9572e-02, 2.8748e-02, 1.1116e-02, 4.2816e-02,\n",
      "        2.3453e-02, 1.0147e-02, 1.3420e-02, 7.7362e-03, 2.1210e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [18] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [18] : torch.Size([1, 32, 1, 90])\n",
      "Last layer attentions for generated token [18] : tensor([2.3254e-01, 2.3254e-01, 2.7013e-04, 4.4370e-04, 2.1040e-04, 3.2349e-03,\n",
      "        2.8419e-04, 2.7561e-04, 5.8079e-04, 8.0013e-04, 1.8072e-03, 3.9315e-04,\n",
      "        6.4325e-04, 8.5020e-04, 6.8283e-03, 1.0452e-02, 5.4884e-04, 3.7217e-04,\n",
      "        2.6608e-04, 4.3416e-04, 8.8394e-05, 2.7227e-04, 9.6054e-03, 1.6289e-03,\n",
      "        4.9057e-03, 1.4587e-02, 2.0638e-03, 6.0034e-04, 7.4911e-04, 4.9973e-04,\n",
      "        4.7970e-04, 1.1177e-03, 1.2369e-03, 8.8263e-04, 4.5609e-04, 6.1703e-04,\n",
      "        4.5776e-04, 6.1131e-04, 4.7684e-04, 3.6502e-04, 2.8687e-03, 3.6888e-03,\n",
      "        1.4267e-03, 1.2817e-03, 1.2493e-03, 7.0095e-04, 4.1771e-04, 3.9077e-04,\n",
      "        6.4802e-04, 2.3060e-03, 2.4891e-03, 3.6287e-04, 4.4250e-03, 3.5210e-03,\n",
      "        4.1122e-03, 8.9798e-03, 2.4071e-03, 3.8662e-03, 5.7411e-03, 2.3689e-03,\n",
      "        3.2024e-03, 2.4586e-03, 3.0136e-03, 2.5749e-03, 2.6340e-03, 8.3065e-04,\n",
      "        9.7275e-03, 5.2032e-03, 3.7289e-03, 2.3544e-02, 2.0950e-02, 4.3518e-02,\n",
      "        7.5340e-03, 1.2465e-03, 1.7075e-02, 2.4506e-02, 1.1078e-02, 1.3977e-02,\n",
      "        1.4069e-02, 8.9493e-03, 2.0706e-02, 2.7908e-02, 1.2604e-02, 1.3634e-02,\n",
      "        4.5441e-02, 1.0895e-02, 1.7715e-02, 1.2009e-02, 1.4320e-02, 1.6174e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [19] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [19] : torch.Size([1, 32, 1, 91])\n",
      "Last layer attentions for generated token [19] : tensor([2.1375e-01, 2.1375e-01, 2.9659e-04, 5.9462e-04, 2.0504e-04, 3.2139e-03,\n",
      "        3.5214e-04, 5.1260e-04, 6.8140e-04, 5.7745e-04, 2.1286e-03, 6.6566e-04,\n",
      "        5.0831e-04, 5.2357e-04, 9.9106e-03, 1.4084e-02, 8.3494e-04, 5.4121e-04,\n",
      "        5.0640e-04, 5.1165e-04, 1.3554e-04, 5.6601e-04, 5.6152e-03, 1.6193e-03,\n",
      "        7.9651e-03, 1.5442e-02, 2.9469e-03, 4.3321e-04, 4.3321e-04, 7.7105e-04,\n",
      "        1.0128e-03, 3.4084e-03, 1.2836e-03, 7.9679e-04, 6.9523e-04, 7.1859e-04,\n",
      "        1.9419e-04, 5.9223e-04, 3.5572e-04, 2.3603e-04, 4.6272e-03, 2.3937e-03,\n",
      "        1.2341e-03, 1.0939e-03, 1.2178e-03, 3.9530e-04, 8.7166e-04, 4.2820e-04,\n",
      "        4.7207e-04, 3.0785e-03, 2.5482e-03, 7.5436e-04, 3.6983e-03, 7.1907e-03,\n",
      "        1.7910e-03, 8.3389e-03, 2.4910e-03, 4.2534e-03, 1.0765e-02, 5.0507e-03,\n",
      "        3.6812e-03, 4.4937e-03, 6.9580e-03, 3.4237e-03, 2.5330e-03, 1.7939e-03,\n",
      "        6.9809e-03, 1.0551e-02, 6.5765e-03, 4.0924e-02, 1.9730e-02, 3.2593e-02,\n",
      "        1.1398e-02, 7.1859e-04, 2.2400e-02, 2.0172e-02, 2.0966e-02, 1.0506e-02,\n",
      "        1.4137e-02, 9.9030e-03, 1.9714e-02, 1.5747e-02, 1.0376e-02, 2.0416e-02,\n",
      "        3.5126e-02, 1.0574e-02, 1.6083e-02, 6.7406e-03, 1.0628e-02, 1.4648e-02,\n",
      "        1.7456e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [20] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [20] : torch.Size([1, 32, 1, 92])\n",
      "Last layer attentions for generated token [20] : tensor([2.9297e-01, 2.9297e-01, 2.5201e-04, 6.7711e-04, 1.7583e-04, 2.4242e-03,\n",
      "        2.0647e-04, 4.5800e-04, 5.6553e-04, 4.4489e-04, 1.1883e-03, 2.4319e-04,\n",
      "        3.8791e-04, 3.9029e-04, 5.4016e-03, 7.8659e-03, 4.3964e-04, 3.0160e-04,\n",
      "        2.1970e-04, 2.6608e-04, 4.5180e-05, 2.8229e-04, 1.3863e-02, 1.4696e-03,\n",
      "        3.9444e-03, 9.4147e-03, 1.4057e-03, 3.4308e-04, 5.5885e-04, 7.1192e-04,\n",
      "        7.1096e-04, 2.8591e-03, 1.1883e-03, 8.2111e-04, 6.7711e-04, 3.5596e-04,\n",
      "        1.4019e-04, 5.6648e-04, 3.9482e-04, 2.7037e-04, 3.9024e-03, 2.1935e-03,\n",
      "        5.6553e-04, 9.7752e-04, 1.1244e-03, 5.2404e-04, 5.7459e-04, 3.7670e-04,\n",
      "        3.6812e-04, 2.2411e-03, 2.0485e-03, 5.4169e-04, 3.1052e-03, 5.5237e-03,\n",
      "        1.9588e-03, 5.0392e-03, 1.2236e-03, 2.9755e-03, 1.0902e-02, 3.8834e-03,\n",
      "        2.9125e-03, 2.6875e-03, 5.0011e-03, 2.3346e-03, 1.3590e-03, 1.0281e-03,\n",
      "        5.7907e-03, 4.6577e-03, 5.4626e-03, 3.4332e-02, 2.1576e-02, 2.2568e-02,\n",
      "        3.0632e-03, 3.2806e-04, 8.9035e-03, 1.3596e-02, 7.0953e-03, 6.9733e-03,\n",
      "        7.0000e-03, 6.4430e-03, 9.4147e-03, 9.7046e-03, 6.3248e-03, 9.7122e-03,\n",
      "        2.6611e-02, 8.1100e-03, 8.8272e-03, 5.0850e-03, 7.6866e-03, 1.2756e-02,\n",
      "        1.4610e-02, 2.0111e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [21] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [21] : torch.Size([1, 32, 1, 93])\n",
      "Last layer attentions for generated token [21] : tensor([2.9688e-01, 2.9810e-01, 3.5810e-04, 5.9032e-04, 1.8907e-04, 4.8714e-03,\n",
      "        2.6965e-04, 6.0177e-04, 7.0524e-04, 7.0238e-04, 1.6623e-03, 3.1662e-04,\n",
      "        3.0208e-04, 4.3344e-04, 7.5302e-03, 1.0437e-02, 5.8699e-04, 3.3689e-04,\n",
      "        2.6250e-04, 2.2626e-04, 8.6904e-05, 6.8855e-04, 1.3893e-02, 1.8797e-03,\n",
      "        4.4098e-03, 1.3489e-02, 1.3533e-03, 3.3832e-04, 4.5514e-04, 7.6532e-04,\n",
      "        5.0306e-04, 1.9550e-03, 1.4725e-03, 7.8487e-04, 7.6962e-04, 3.5310e-04,\n",
      "        1.8609e-04, 8.2922e-04, 3.6716e-04, 2.5845e-04, 2.5711e-03, 1.6489e-03,\n",
      "        4.0555e-04, 6.2943e-04, 7.8058e-04, 3.1519e-04, 6.0892e-04, 3.8171e-04,\n",
      "        3.4094e-04, 1.6232e-03, 2.0008e-03, 4.4036e-04, 2.7676e-03, 9.0332e-03,\n",
      "        1.5516e-03, 8.2932e-03, 1.1072e-03, 3.6659e-03, 8.3160e-03, 4.3106e-03,\n",
      "        3.0766e-03, 2.4319e-03, 6.2790e-03, 4.0131e-03, 1.6518e-03, 1.4582e-03,\n",
      "        4.7836e-03, 4.4098e-03, 4.6692e-03, 2.0554e-02, 1.6983e-02, 2.2629e-02,\n",
      "        2.9030e-03, 2.8038e-04, 8.2855e-03, 1.3870e-02, 7.6218e-03, 6.3705e-03,\n",
      "        7.5302e-03, 6.0654e-03, 6.9275e-03, 6.7978e-03, 3.6755e-03, 1.0216e-02,\n",
      "        2.4094e-02, 6.5079e-03, 6.0387e-03, 2.8057e-03, 5.1804e-03, 1.3252e-02,\n",
      "        1.4999e-02, 2.1530e-02, 1.1108e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [22] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [22] : torch.Size([1, 32, 1, 94])\n",
      "Last layer attentions for generated token [22] : tensor([2.0020e-01, 2.0020e-01, 1.9062e-04, 3.5810e-04, 1.4389e-04, 3.5744e-03,\n",
      "        3.0756e-04, 4.7636e-04, 8.5592e-04, 8.6069e-04, 2.5578e-03, 5.3120e-04,\n",
      "        3.2735e-04, 5.4169e-04, 1.2947e-02, 1.2962e-02, 5.0497e-04, 5.8603e-04,\n",
      "        3.2854e-04, 5.1594e-04, 1.9813e-04, 6.5994e-04, 7.5912e-03, 1.9798e-03,\n",
      "        1.3145e-02, 1.4442e-02, 1.5621e-03, 4.4656e-04, 4.8876e-04, 8.9359e-04,\n",
      "        6.4230e-04, 2.4261e-03, 1.2255e-03, 8.5402e-04, 7.8392e-04, 5.7364e-04,\n",
      "        2.4772e-04, 8.1205e-04, 4.0340e-04, 2.4605e-04, 4.3373e-03, 3.2711e-03,\n",
      "        5.8603e-04, 8.5258e-04, 1.1768e-03, 6.1035e-04, 6.0081e-04, 4.0340e-04,\n",
      "        6.0081e-04, 4.6692e-03, 4.0016e-03, 4.0817e-04, 5.7449e-03, 5.3024e-03,\n",
      "        3.4122e-03, 8.5602e-03, 2.5692e-03, 3.4180e-03, 1.2611e-02, 4.5586e-03,\n",
      "        4.8065e-03, 3.5648e-03, 4.6349e-03, 4.0970e-03, 1.8167e-03, 1.6403e-03,\n",
      "        1.2039e-02, 1.1894e-02, 7.8812e-03, 3.3264e-02, 2.4643e-02, 4.5288e-02,\n",
      "        5.0621e-03, 7.0667e-04, 1.3557e-02, 2.2629e-02, 1.0323e-02, 1.3824e-02,\n",
      "        1.9623e-02, 9.7122e-03, 1.5701e-02, 1.2833e-02, 6.1111e-03, 1.5251e-02,\n",
      "        3.6499e-02, 1.2390e-02, 1.2688e-02, 5.0774e-03, 7.3509e-03, 1.5480e-02,\n",
      "        1.5472e-02, 1.8112e-02, 8.4763e-03, 1.1223e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [23] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [23] : torch.Size([1, 32, 1, 95])\n",
      "Last layer attentions for generated token [23] : tensor([2.0312e-01, 2.0276e-01, 1.4400e-04, 2.9206e-04, 9.9361e-05, 5.2948e-03,\n",
      "        2.4986e-04, 4.9591e-04, 7.1573e-04, 4.5681e-04, 3.5515e-03, 6.9523e-04,\n",
      "        4.4441e-04, 5.4359e-04, 8.2016e-03, 1.0597e-02, 5.3310e-04, 3.2401e-04,\n",
      "        2.0790e-04, 3.6478e-04, 1.5211e-04, 4.5776e-04, 8.2932e-03, 1.1282e-03,\n",
      "        5.7564e-03, 1.4740e-02, 9.4461e-04, 2.0075e-04, 2.8253e-04, 6.6710e-04,\n",
      "        4.1103e-04, 1.2665e-03, 6.7616e-04, 5.4979e-04, 4.5943e-04, 3.9005e-04,\n",
      "        1.8632e-04, 6.4659e-04, 3.1281e-04, 1.2314e-04, 3.1033e-03, 2.1687e-03,\n",
      "        3.7146e-04, 5.4979e-04, 8.8930e-04, 4.2152e-04, 7.7534e-04, 3.1829e-04,\n",
      "        6.5041e-04, 3.5686e-03, 2.1534e-03, 2.8539e-04, 4.3221e-03, 3.1796e-03,\n",
      "        2.2259e-03, 6.4468e-03, 6.5422e-04, 1.9417e-03, 7.3891e-03, 2.0351e-03,\n",
      "        1.5697e-03, 2.8763e-03, 3.8013e-03, 1.3742e-03, 9.0837e-04, 9.2793e-04,\n",
      "        5.0354e-03, 5.9814e-03, 3.5343e-03, 2.9068e-02, 1.6632e-02, 3.6072e-02,\n",
      "        8.4915e-03, 1.0433e-03, 2.4933e-02, 2.5787e-02, 1.3039e-02, 1.2115e-02,\n",
      "        1.3252e-02, 1.5549e-02, 2.1561e-02, 1.2497e-02, 4.8790e-03, 2.1790e-02,\n",
      "        4.4586e-02, 1.2825e-02, 1.6129e-02, 8.3389e-03, 1.7624e-02, 3.3203e-02,\n",
      "        1.4572e-02, 1.6220e-02, 4.0169e-03, 1.3748e-02, 1.5732e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [24] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [24] : torch.Size([1, 32, 1, 96])\n",
      "Last layer attentions for generated token [24] : tensor([2.1985e-01, 2.1899e-01, 1.2302e-04, 2.3437e-04, 8.8632e-05, 2.6531e-03,\n",
      "        4.0889e-04, 5.5122e-04, 5.0211e-04, 3.6240e-04, 3.5400e-03, 6.8235e-04,\n",
      "        3.1042e-04, 5.3024e-04, 1.0017e-02, 1.1017e-02, 4.4560e-04, 4.2200e-04,\n",
      "        4.2367e-04, 2.8706e-04, 1.7488e-04, 5.8365e-04, 8.2092e-03, 1.5497e-03,\n",
      "        6.4545e-03, 1.5732e-02, 9.6750e-04, 3.1471e-04, 2.5797e-04, 5.7793e-04,\n",
      "        4.7350e-04, 1.5831e-03, 8.4400e-04, 5.2404e-04, 5.7316e-04, 4.4298e-04,\n",
      "        2.5988e-04, 6.3229e-04, 3.2783e-04, 1.6689e-04, 2.9659e-03, 1.8110e-03,\n",
      "        4.8661e-04, 7.9775e-04, 1.0386e-03, 3.3760e-04, 9.3985e-04, 5.4932e-04,\n",
      "        6.4230e-04, 2.7981e-03, 2.7561e-03, 4.5538e-04, 3.3779e-03, 2.8858e-03,\n",
      "        1.8291e-03, 5.9853e-03, 9.3222e-04, 1.8816e-03, 4.8141e-03, 2.5940e-03,\n",
      "        1.5926e-03, 2.7428e-03, 3.3455e-03, 2.3441e-03, 1.1740e-03, 7.6389e-04,\n",
      "        3.5706e-03, 5.4741e-03, 3.7136e-03, 2.5543e-02, 1.4114e-02, 2.9266e-02,\n",
      "        6.5384e-03, 6.1369e-04, 2.0660e-02, 2.0966e-02, 1.3184e-02, 1.1902e-02,\n",
      "        1.3504e-02, 1.1436e-02, 2.2125e-02, 1.2466e-02, 3.9177e-03, 2.1683e-02,\n",
      "        3.4973e-02, 1.2993e-02, 1.6403e-02, 7.2823e-03, 1.2650e-02, 3.1097e-02,\n",
      "        1.6830e-02, 1.4534e-02, 3.9139e-03, 1.4458e-02, 2.0477e-02, 9.7809e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [25] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [25] : torch.Size([1, 32, 1, 97])\n",
      "Last layer attentions for generated token [25] : tensor([1.9629e-01, 1.9629e-01, 1.6725e-04, 2.5058e-04, 1.1539e-04, 2.6913e-03,\n",
      "        6.1750e-04, 9.9468e-04, 1.1253e-03, 6.1274e-04, 3.4256e-03, 4.3964e-04,\n",
      "        2.3210e-04, 4.8375e-04, 7.5722e-03, 8.9569e-03, 5.2500e-04, 4.6349e-04,\n",
      "        2.5439e-04, 2.2233e-04, 1.3387e-04, 3.9172e-04, 6.8512e-03, 1.1120e-03,\n",
      "        6.3438e-03, 1.3039e-02, 1.0929e-03, 2.6941e-04, 2.7084e-04, 7.1239e-04,\n",
      "        6.9857e-04, 1.4219e-03, 9.4175e-04, 7.1526e-04, 8.9359e-04, 4.3201e-04,\n",
      "        3.0327e-04, 1.1654e-03, 2.9755e-04, 2.2495e-04, 3.7212e-03, 1.7996e-03,\n",
      "        3.6454e-04, 7.9775e-04, 1.4248e-03, 3.5882e-04, 1.5202e-03, 5.7030e-04,\n",
      "        9.8515e-04, 4.6844e-03, 3.0842e-03, 5.6028e-04, 3.3989e-03, 3.7098e-03,\n",
      "        1.8730e-03, 5.3291e-03, 1.0061e-03, 1.7805e-03, 4.2763e-03, 2.0752e-03,\n",
      "        1.2188e-03, 3.0308e-03, 4.1466e-03, 2.8172e-03, 1.5955e-03, 1.2646e-03,\n",
      "        2.8114e-03, 5.3596e-03, 3.6068e-03, 2.9984e-02, 1.4481e-02, 2.2690e-02,\n",
      "        6.8016e-03, 8.4400e-04, 1.8295e-02, 1.8784e-02, 1.8356e-02, 9.0408e-03,\n",
      "        1.5419e-02, 1.4977e-02, 2.2568e-02, 1.6006e-02, 4.6310e-03, 3.0319e-02,\n",
      "        3.1616e-02, 1.1726e-02, 1.2611e-02, 5.4741e-03, 1.0963e-02, 4.7455e-02,\n",
      "        2.0065e-02, 2.3941e-02, 7.4654e-03, 1.6571e-02, 1.8707e-02, 1.0437e-02,\n",
      "        1.1482e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [26] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [26] : torch.Size([1, 32, 1, 98])\n",
      "Last layer attentions for generated token [26] : tensor([0.1639, 0.1639, 0.0003, 0.0003, 0.0002, 0.0031, 0.0004, 0.0005, 0.0008,\n",
      "        0.0007, 0.0029, 0.0003, 0.0005, 0.0011, 0.0083, 0.0119, 0.0003, 0.0005,\n",
      "        0.0003, 0.0004, 0.0003, 0.0005, 0.0069, 0.0023, 0.0104, 0.0156, 0.0015,\n",
      "        0.0004, 0.0004, 0.0005, 0.0007, 0.0020, 0.0012, 0.0006, 0.0006, 0.0006,\n",
      "        0.0004, 0.0008, 0.0003, 0.0002, 0.0039, 0.0025, 0.0006, 0.0008, 0.0012,\n",
      "        0.0004, 0.0009, 0.0004, 0.0006, 0.0033, 0.0029, 0.0007, 0.0039, 0.0059,\n",
      "        0.0033, 0.0071, 0.0014, 0.0016, 0.0043, 0.0023, 0.0018, 0.0029, 0.0033,\n",
      "        0.0024, 0.0024, 0.0017, 0.0054, 0.0061, 0.0042, 0.0334, 0.0255, 0.0265,\n",
      "        0.0099, 0.0009, 0.0235, 0.0228, 0.0182, 0.0084, 0.0086, 0.0100, 0.0240,\n",
      "        0.0172, 0.0074, 0.0332, 0.0468, 0.0157, 0.0178, 0.0126, 0.0134, 0.0305,\n",
      "        0.0208, 0.0185, 0.0069, 0.0153, 0.0200, 0.0105, 0.0062, 0.0107],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [27] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [27] : torch.Size([1, 32, 1, 99])\n",
      "Last layer attentions for generated token [27] : tensor([2.7734e-01, 2.7734e-01, 1.8823e-04, 3.6216e-04, 1.3149e-04, 2.3861e-03,\n",
      "        6.1607e-04, 1.2379e-03, 1.2941e-03, 7.5340e-04, 2.7618e-03, 4.8828e-04,\n",
      "        4.1771e-04, 5.1498e-04, 4.7760e-03, 6.9466e-03, 5.3120e-04, 3.7289e-04,\n",
      "        3.0041e-04, 3.2210e-04, 1.6809e-04, 4.2343e-04, 8.5831e-03, 9.0551e-04,\n",
      "        6.7062e-03, 1.4099e-02, 1.2064e-03, 3.1853e-04, 2.9802e-04, 5.3835e-04,\n",
      "        4.1604e-04, 9.1219e-04, 9.7513e-04, 5.5647e-04, 6.5994e-04, 4.4370e-04,\n",
      "        3.1781e-04, 6.2609e-04, 3.6216e-04, 2.2709e-04, 4.0016e-03, 1.9312e-03,\n",
      "        2.6870e-04, 6.4945e-04, 1.1854e-03, 3.5930e-04, 6.6900e-04, 4.3941e-04,\n",
      "        6.9809e-04, 3.7975e-03, 2.2316e-03, 4.4727e-04, 4.1580e-03, 3.5591e-03,\n",
      "        2.2087e-03, 4.3526e-03, 1.0080e-03, 1.6975e-03, 7.2556e-03, 2.7294e-03,\n",
      "        1.4925e-03, 2.9030e-03, 4.1275e-03, 1.9970e-03, 1.6174e-03, 1.1177e-03,\n",
      "        3.6469e-03, 4.3030e-03, 4.1122e-03, 2.2079e-02, 1.2421e-02, 1.7609e-02,\n",
      "        4.5166e-03, 5.4693e-04, 1.0391e-02, 1.1703e-02, 6.8130e-03, 6.5842e-03,\n",
      "        6.5498e-03, 7.3624e-03, 1.3931e-02, 9.9640e-03, 3.2063e-03, 1.4816e-02,\n",
      "        2.3026e-02, 8.6365e-03, 8.2932e-03, 2.7714e-03, 5.0545e-03, 1.8219e-02,\n",
      "        1.3321e-02, 1.7807e-02, 4.8637e-03, 1.7090e-02, 1.3802e-02, 5.9776e-03,\n",
      "        5.3635e-03, 1.1833e-02, 8.7280e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [28] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [28] : torch.Size([1, 32, 1, 100])\n",
      "Last layer attentions for generated token [28] : tensor([2.1106e-01, 2.1106e-01, 1.7869e-04, 2.6202e-04, 1.4365e-04, 2.2068e-03,\n",
      "        4.2868e-04, 6.6662e-04, 1.2550e-03, 1.1683e-03, 4.4250e-03, 5.5933e-04,\n",
      "        2.9135e-04, 7.6723e-04, 6.7978e-03, 9.9335e-03, 5.3358e-04, 4.6825e-04,\n",
      "        2.2769e-04, 1.9860e-04, 8.0228e-05, 2.6155e-04, 5.3749e-03, 1.1473e-03,\n",
      "        8.5068e-03, 1.1497e-02, 1.1816e-03, 3.0994e-04, 4.3988e-04, 7.9346e-04,\n",
      "        6.7568e-04, 1.8158e-03, 1.1539e-03, 5.4836e-04, 7.3099e-04, 4.4584e-04,\n",
      "        3.1543e-04, 7.8535e-04, 3.4046e-04, 2.0015e-04, 4.6310e-03, 2.9068e-03,\n",
      "        6.0701e-04, 6.1417e-04, 8.9884e-04, 2.7633e-04, 7.0429e-04, 4.7827e-04,\n",
      "        5.9748e-04, 4.7226e-03, 2.5177e-03, 4.0674e-04, 3.3436e-03, 4.2152e-03,\n",
      "        3.1414e-03, 4.2992e-03, 9.9468e-04, 1.3332e-03, 3.8700e-03, 2.0962e-03,\n",
      "        1.3208e-03, 3.3569e-03, 3.3817e-03, 1.9970e-03, 1.6890e-03, 1.8482e-03,\n",
      "        4.0512e-03, 4.4632e-03, 2.9449e-03, 2.6215e-02, 1.5976e-02, 1.5961e-02,\n",
      "        6.7978e-03, 8.2493e-04, 2.0279e-02, 1.5900e-02, 1.1353e-02, 7.2594e-03,\n",
      "        9.6664e-03, 8.2855e-03, 2.6794e-02, 1.6617e-02, 6.1989e-03, 2.6978e-02,\n",
      "        2.9419e-02, 1.4565e-02, 1.2253e-02, 5.6076e-03, 1.1391e-02, 2.3941e-02,\n",
      "        1.3382e-02, 1.6968e-02, 5.7106e-03, 1.4099e-02, 2.7649e-02, 1.1162e-02,\n",
      "        7.4654e-03, 1.3237e-02, 1.0002e-02, 1.0963e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [29] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [29] : torch.Size([1, 32, 1, 101])\n",
      "Last layer attentions for generated token [29] : tensor([1.8567e-01, 1.8530e-01, 2.0981e-04, 4.7565e-04, 1.6797e-04, 2.7637e-03,\n",
      "        3.3021e-04, 4.1556e-04, 9.3508e-04, 1.2684e-03, 3.0956e-03, 6.4516e-04,\n",
      "        4.5109e-04, 8.0442e-04, 1.3336e-02, 1.5457e-02, 9.6655e-04, 6.9904e-04,\n",
      "        2.9469e-04, 3.6764e-04, 1.9109e-04, 7.5722e-04, 9.1705e-03, 2.6951e-03,\n",
      "        1.2283e-02, 1.7944e-02, 3.8872e-03, 5.9319e-04, 6.8951e-04, 1.2903e-03,\n",
      "        1.0719e-03, 3.3073e-03, 1.7567e-03, 6.9094e-04, 8.1205e-04, 1.2121e-03,\n",
      "        4.6563e-04, 1.0557e-03, 5.1451e-04, 3.9673e-04, 6.2981e-03, 3.5706e-03,\n",
      "        1.4172e-03, 6.9094e-04, 5.9557e-04, 2.9588e-04, 3.0303e-04, 2.9755e-04,\n",
      "        2.9826e-04, 3.1376e-03, 2.7771e-03, 2.6584e-04, 2.7180e-03, 8.0032e-03,\n",
      "        2.6302e-03, 6.1989e-03, 1.8997e-03, 2.4776e-03, 8.1406e-03, 2.9945e-03,\n",
      "        2.9945e-03, 3.1605e-03, 4.7188e-03, 3.2883e-03, 2.6760e-03, 1.4849e-03,\n",
      "        9.1476e-03, 5.1422e-03, 4.6272e-03, 3.4576e-02, 2.0554e-02, 2.2202e-02,\n",
      "        7.6065e-03, 7.1955e-04, 1.0796e-02, 1.6357e-02, 1.0757e-02, 6.9046e-03,\n",
      "        1.2268e-02, 1.1559e-02, 2.2919e-02, 1.3374e-02, 8.4610e-03, 1.9714e-02,\n",
      "        2.9495e-02, 1.2642e-02, 1.0880e-02, 9.1324e-03, 6.2485e-03, 1.1925e-02,\n",
      "        1.4053e-02, 1.7715e-02, 9.0485e-03, 1.1795e-02, 1.6922e-02, 9.3689e-03,\n",
      "        4.5242e-03, 1.1757e-02, 8.2092e-03, 8.4305e-03, 2.2491e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [30] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [30] : torch.Size([1, 32, 1, 102])\n",
      "Last layer attentions for generated token [30] : tensor([2.1741e-01, 2.1741e-01, 2.2638e-04, 3.4189e-04, 1.6439e-04, 2.4643e-03,\n",
      "        1.6439e-04, 3.2187e-04, 7.4100e-04, 5.2214e-04, 1.8988e-03, 3.9434e-04,\n",
      "        2.4581e-04, 4.6444e-04, 8.1329e-03, 1.1765e-02, 6.5517e-04, 3.0279e-04,\n",
      "        2.0456e-04, 2.1017e-04, 7.4923e-05, 3.4857e-04, 8.4534e-03, 1.9369e-03,\n",
      "        5.2795e-03, 1.3191e-02, 1.2140e-03, 3.5954e-04, 4.0841e-04, 9.6607e-04,\n",
      "        7.6580e-04, 2.2736e-03, 1.1635e-03, 6.1655e-04, 5.9080e-04, 3.9434e-04,\n",
      "        2.0218e-04, 6.2275e-04, 3.2496e-04, 2.7418e-04, 4.8637e-03, 2.5883e-03,\n",
      "        1.0347e-03, 6.2275e-04, 7.8392e-04, 2.9135e-04, 4.8876e-04, 3.0708e-04,\n",
      "        3.3188e-04, 2.1439e-03, 1.5020e-03, 3.1853e-04, 2.5978e-03, 5.2910e-03,\n",
      "        2.0599e-03, 4.1084e-03, 6.4135e-04, 1.5841e-03, 5.0507e-03, 1.8301e-03,\n",
      "        1.4334e-03, 2.9583e-03, 3.5934e-03, 1.5745e-03, 2.1763e-03, 1.1587e-03,\n",
      "        4.3221e-03, 6.8054e-03, 2.7962e-03, 4.4159e-02, 2.1439e-02, 2.1454e-02,\n",
      "        3.9482e-03, 4.3392e-04, 1.2154e-02, 1.3870e-02, 9.2239e-03, 5.5962e-03,\n",
      "        1.0788e-02, 6.7291e-03, 1.4130e-02, 8.8654e-03, 6.1378e-03, 1.9272e-02,\n",
      "        2.5940e-02, 1.0460e-02, 9.1095e-03, 8.9569e-03, 8.4534e-03, 1.2100e-02,\n",
      "        1.1879e-02, 1.5221e-02, 6.9885e-03, 1.2817e-02, 1.7120e-02, 9.9792e-03,\n",
      "        6.3820e-03, 1.3832e-02, 9.5901e-03, 9.5291e-03, 2.5360e-02, 1.9363e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [31] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [31] : torch.Size([1, 32, 1, 103])\n",
      "Last layer attentions for generated token [31] : tensor([0.1642, 0.1638, 0.0003, 0.0005, 0.0002, 0.0040, 0.0005, 0.0005, 0.0008,\n",
      "        0.0010, 0.0026, 0.0005, 0.0006, 0.0007, 0.0154, 0.0214, 0.0009, 0.0009,\n",
      "        0.0004, 0.0004, 0.0003, 0.0010, 0.0089, 0.0024, 0.0152, 0.0221, 0.0048,\n",
      "        0.0009, 0.0006, 0.0011, 0.0013, 0.0048, 0.0014, 0.0006, 0.0006, 0.0010,\n",
      "        0.0003, 0.0009, 0.0007, 0.0004, 0.0066, 0.0033, 0.0011, 0.0012, 0.0010,\n",
      "        0.0004, 0.0003, 0.0003, 0.0003, 0.0034, 0.0036, 0.0004, 0.0023, 0.0106,\n",
      "        0.0022, 0.0078, 0.0016, 0.0026, 0.0077, 0.0027, 0.0021, 0.0032, 0.0041,\n",
      "        0.0027, 0.0021, 0.0012, 0.0095, 0.0051, 0.0043, 0.0502, 0.0279, 0.0309,\n",
      "        0.0085, 0.0009, 0.0170, 0.0250, 0.0175, 0.0103, 0.0110, 0.0119, 0.0164,\n",
      "        0.0162, 0.0083, 0.0159, 0.0296, 0.0110, 0.0115, 0.0078, 0.0076, 0.0107,\n",
      "        0.0101, 0.0132, 0.0051, 0.0106, 0.0116, 0.0060, 0.0024, 0.0056, 0.0051,\n",
      "        0.0058, 0.0140, 0.0121, 0.0096], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [32] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [32] : torch.Size([1, 32, 1, 104])\n",
      "Last layer attentions for generated token [32] : tensor([1.8225e-01, 1.8225e-01, 2.9469e-04, 4.6539e-04, 1.9622e-04, 2.0924e-03,\n",
      "        2.7037e-04, 4.2701e-04, 4.9829e-04, 4.7636e-04, 1.4629e-03, 4.4227e-04,\n",
      "        2.8443e-04, 3.2806e-04, 7.7209e-03, 8.2703e-03, 4.0340e-04, 3.0994e-04,\n",
      "        2.6917e-04, 2.7132e-04, 1.1033e-04, 4.7159e-04, 4.1962e-03, 1.6127e-03,\n",
      "        5.0621e-03, 1.4671e-02, 2.5253e-03, 5.4407e-04, 3.1185e-04, 7.9775e-04,\n",
      "        6.7425e-04, 2.8286e-03, 9.4175e-04, 5.9986e-04, 7.0810e-04, 4.0030e-04,\n",
      "        1.6427e-04, 5.3024e-04, 2.1219e-04, 2.0850e-04, 4.6539e-03, 2.1152e-03,\n",
      "        7.7152e-04, 7.5960e-04, 7.4196e-04, 2.0933e-04, 4.1962e-04, 2.4557e-04,\n",
      "        2.6417e-04, 2.5978e-03, 1.7319e-03, 4.0197e-04, 3.0556e-03, 6.5117e-03,\n",
      "        1.8148e-03, 5.4741e-03, 1.0166e-03, 1.5268e-03, 5.7526e-03, 2.2430e-03,\n",
      "        1.3180e-03, 2.1477e-03, 4.3907e-03, 2.3937e-03, 1.2770e-03, 1.0777e-03,\n",
      "        3.2482e-03, 6.2027e-03, 2.6588e-03, 3.1342e-02, 2.1027e-02, 3.0319e-02,\n",
      "        1.2581e-02, 7.8821e-04, 2.7634e-02, 2.4628e-02, 2.5711e-02, 1.1879e-02,\n",
      "        1.2306e-02, 5.8556e-03, 2.1576e-02, 1.5945e-02, 7.5493e-03, 2.2858e-02,\n",
      "        3.6255e-02, 1.3168e-02, 1.3031e-02, 4.6272e-03, 8.4305e-03, 2.1179e-02,\n",
      "        1.1269e-02, 1.2436e-02, 4.0245e-03, 1.1986e-02, 1.3985e-02, 7.5188e-03,\n",
      "        3.7346e-03, 6.4278e-03, 4.2381e-03, 5.7144e-03, 1.8570e-02, 1.0376e-02,\n",
      "        1.4549e-02, 1.2840e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [33] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [33] : torch.Size([1, 32, 1, 105])\n",
      "Last layer attentions for generated token [33] : tensor([3.3276e-01, 3.3276e-01, 1.8370e-04, 4.6539e-04, 7.3910e-05, 2.7618e-03,\n",
      "        9.4891e-05, 2.8658e-04, 4.7827e-04, 3.2425e-04, 1.0023e-03, 1.4877e-04,\n",
      "        1.8084e-04, 2.5892e-04, 8.1482e-03, 8.2932e-03, 5.0592e-04, 3.2616e-04,\n",
      "        3.9411e-04, 5.4932e-04, 2.0254e-04, 1.1320e-03, 1.5030e-02, 9.5654e-04,\n",
      "        3.6201e-03, 7.2517e-03, 6.0320e-04, 2.1219e-04, 4.3464e-04, 4.0674e-04,\n",
      "        5.4073e-04, 1.2503e-03, 2.8896e-04, 2.6941e-04, 2.9016e-04, 1.9395e-04,\n",
      "        1.4818e-04, 3.6025e-04, 2.0409e-04, 1.1182e-04, 3.9043e-03, 1.5078e-03,\n",
      "        4.1461e-04, 5.5170e-04, 5.0592e-04, 3.6383e-04, 3.7599e-04, 6.5231e-04,\n",
      "        3.7313e-04, 3.0251e-03, 1.9932e-03, 2.0814e-04, 2.3422e-03, 3.0289e-03,\n",
      "        1.3075e-03, 3.6316e-03, 1.1120e-03, 1.5259e-03, 1.3306e-02, 2.6379e-03,\n",
      "        1.7252e-03, 1.2875e-03, 5.4054e-03, 2.2488e-03, 9.6798e-04, 1.4086e-03,\n",
      "        3.7727e-03, 1.1950e-03, 4.0016e-03, 8.0566e-03, 6.2637e-03, 1.3901e-02,\n",
      "        2.2774e-03, 5.0688e-04, 7.2365e-03, 7.2594e-03, 5.5542e-03, 5.9814e-03,\n",
      "        5.8746e-03, 3.6888e-03, 3.7498e-03, 4.8256e-03, 2.2106e-03, 5.2109e-03,\n",
      "        1.0178e-02, 3.9940e-03, 2.6340e-03, 1.5650e-03, 2.1019e-03, 5.4207e-03,\n",
      "        3.1700e-03, 7.6942e-03, 4.1504e-03, 6.9160e-03, 5.9547e-03, 2.7313e-03,\n",
      "        2.5311e-03, 4.5853e-03, 3.7460e-03, 3.1185e-03, 1.7166e-02, 6.3744e-03,\n",
      "        1.2054e-02, 1.3641e-02, 9.5062e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [34] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [34] : torch.Size([1, 32, 1, 106])\n",
      "Last layer attentions for generated token [34] : tensor([3.7109e-01, 3.7109e-01, 4.2701e-04, 5.6696e-04, 1.7869e-04, 1.7910e-03,\n",
      "        2.8729e-05, 1.1581e-04, 1.5831e-04, 1.0103e-04, 5.9175e-04, 1.2279e-04,\n",
      "        1.4186e-04, 3.4165e-04, 8.6670e-03, 3.5706e-03, 1.0180e-04, 1.3697e-04,\n",
      "        1.6725e-04, 1.9097e-04, 1.9467e-04, 7.2193e-04, 2.3178e-02, 1.4811e-03,\n",
      "        4.4289e-03, 4.2305e-03, 2.2674e-04, 5.9128e-05, 1.3864e-04, 1.2970e-04,\n",
      "        2.6822e-04, 6.5756e-04, 1.3328e-04, 7.4208e-05, 8.5056e-05, 3.9995e-05,\n",
      "        1.6272e-04, 4.1795e-04, 1.7250e-04, 4.5002e-05, 3.1395e-03, 1.5345e-03,\n",
      "        2.4700e-04, 2.4700e-04, 2.1982e-04, 1.1009e-04, 1.7250e-04, 2.9182e-04,\n",
      "        1.4818e-04, 2.0180e-03, 1.3571e-03, 3.0327e-04, 2.3079e-03, 7.6294e-03,\n",
      "        1.6022e-03, 2.0256e-03, 2.9993e-04, 5.9414e-04, 8.6441e-03, 7.8058e-04,\n",
      "        4.4560e-04, 8.0729e-04, 4.3068e-03, 5.5885e-04, 5.8699e-04, 7.2193e-04,\n",
      "        1.6661e-03, 1.8511e-03, 4.4441e-03, 1.4893e-02, 8.3618e-03, 4.7722e-03,\n",
      "        9.6416e-04, 5.1117e-04, 6.7062e-03, 6.9046e-03, 3.0079e-03, 2.0485e-03,\n",
      "        1.1492e-03, 1.4334e-03, 2.1534e-03, 2.4548e-03, 1.1740e-03, 2.4376e-03,\n",
      "        6.0501e-03, 3.0994e-03, 1.2550e-03, 4.1962e-04, 1.1339e-03, 3.7861e-03,\n",
      "        1.6785e-03, 3.6602e-03, 1.4057e-03, 4.6616e-03, 3.8548e-03, 1.6718e-03,\n",
      "        6.9714e-04, 1.9207e-03, 1.2283e-03, 1.9512e-03, 1.0765e-02, 2.2507e-03,\n",
      "        7.1449e-03, 1.3512e-02, 1.2489e-02, 1.0864e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [35] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [35] : torch.Size([1, 32, 1, 107])\n",
      "Last layer attentions for generated token [35] : tensor([1.8848e-01, 1.8848e-01, 2.3770e-04, 3.9721e-04, 1.0180e-04, 2.3289e-03,\n",
      "        1.6522e-04, 3.4571e-04, 4.9162e-04, 6.0320e-04, 2.0313e-03, 2.0564e-04,\n",
      "        3.4857e-04, 3.1924e-04, 1.2329e-02, 1.2291e-02, 8.6594e-04, 6.5517e-04,\n",
      "        3.6168e-04, 3.5739e-04, 6.3479e-05, 3.5548e-04, 9.1095e-03, 1.2283e-03,\n",
      "        5.0049e-03, 1.7654e-02, 1.7576e-03, 5.9652e-04, 6.9332e-04, 4.8661e-04,\n",
      "        5.6887e-04, 1.8787e-03, 7.7152e-04, 5.8460e-04, 4.4584e-04, 6.2609e-04,\n",
      "        1.7452e-04, 5.0497e-04, 3.9268e-04, 2.7347e-04, 4.0359e-03, 2.6665e-03,\n",
      "        7.0524e-04, 6.2609e-04, 6.3229e-04, 4.4918e-04, 2.5749e-04, 2.2590e-04,\n",
      "        3.0637e-04, 1.8187e-03, 2.0638e-03, 1.5342e-04, 4.9706e-03, 4.5586e-03,\n",
      "        1.7290e-03, 5.7182e-03, 1.1120e-03, 3.8967e-03, 1.0384e-02, 2.1534e-03,\n",
      "        2.0943e-03, 2.8553e-03, 4.0703e-03, 1.5974e-03, 1.3542e-03, 6.0797e-04,\n",
      "        1.1002e-02, 5.1079e-03, 4.9629e-03, 2.0813e-02, 2.0279e-02, 3.7811e-02,\n",
      "        4.2191e-03, 5.6124e-04, 1.0246e-02, 1.7975e-02, 8.4991e-03, 1.1681e-02,\n",
      "        1.3725e-02, 8.1558e-03, 9.4986e-03, 1.4236e-02, 5.0697e-03, 1.2077e-02,\n",
      "        3.1464e-02, 1.1604e-02, 1.2413e-02, 3.1033e-03, 6.6452e-03, 1.2802e-02,\n",
      "        1.6327e-02, 2.4963e-02, 5.7678e-03, 1.1200e-02, 1.2718e-02, 6.8054e-03,\n",
      "        5.0926e-03, 6.0081e-03, 6.7139e-03, 4.4556e-03, 1.4526e-02, 9.9258e-03,\n",
      "        1.3969e-02, 2.6993e-02, 7.0953e-03, 2.3937e-03, 1.5533e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [36] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [36] : torch.Size([1, 32, 1, 108])\n",
      "Last layer attentions for generated token [36] : tensor([1.5063e-01, 1.5027e-01, 2.1517e-04, 5.2214e-04, 1.6499e-04, 5.2261e-03,\n",
      "        2.4188e-04, 7.4387e-04, 1.2455e-03, 6.2752e-04, 2.1915e-03, 3.6168e-04,\n",
      "        2.3401e-04, 4.3559e-04, 1.1574e-02, 1.2718e-02, 7.1096e-04, 3.7241e-04,\n",
      "        1.7977e-04, 1.6499e-04, 8.5592e-05, 3.5143e-04, 8.4305e-03, 1.1864e-03,\n",
      "        4.8981e-03, 2.5909e-02, 9.4938e-04, 3.2306e-04, 4.5991e-04, 7.2384e-04,\n",
      "        9.2220e-04, 1.7843e-03, 1.2903e-03, 9.3842e-04, 7.1955e-04, 3.9887e-04,\n",
      "        1.4615e-04, 5.9748e-04, 1.7774e-04, 1.9705e-04, 3.0193e-03, 1.4153e-03,\n",
      "        3.5834e-04, 4.0126e-04, 6.3753e-04, 2.4724e-04, 5.5075e-04, 1.8513e-04,\n",
      "        3.9887e-04, 2.1286e-03, 1.3609e-03, 3.4308e-04, 4.8256e-03, 6.9656e-03,\n",
      "        1.7023e-03, 5.8861e-03, 4.6897e-04, 1.5259e-03, 6.2943e-03, 1.7195e-03,\n",
      "        1.3466e-03, 2.7256e-03, 3.9711e-03, 1.5011e-03, 9.3842e-04, 5.3263e-04,\n",
      "        3.3627e-03, 5.4436e-03, 3.1319e-03, 2.8397e-02, 2.2751e-02, 3.4027e-02,\n",
      "        5.2681e-03, 8.1062e-04, 1.8417e-02, 1.9836e-02, 1.9913e-02, 8.6899e-03,\n",
      "        1.2421e-02, 9.4452e-03, 1.1162e-02, 7.8125e-03, 3.1185e-03, 1.8311e-02,\n",
      "        3.1250e-02, 1.1406e-02, 8.7357e-03, 2.9449e-03, 8.8196e-03, 2.0966e-02,\n",
      "        1.3351e-02, 2.2888e-02, 6.3324e-03, 1.4931e-02, 1.6083e-02, 8.7814e-03,\n",
      "        9.8877e-03, 1.2146e-02, 7.4959e-03, 1.1024e-02, 2.6993e-02, 1.2543e-02,\n",
      "        1.9943e-02, 2.0691e-02, 4.3182e-03, 2.5749e-03, 1.8951e-02, 1.3870e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [37] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [37] : torch.Size([1, 32, 1, 109])\n",
      "Last layer attentions for generated token [37] : tensor([1.7603e-01, 1.7603e-01, 6.8760e-04, 8.7452e-04, 5.2834e-04, 7.0610e-03,\n",
      "        2.8610e-04, 7.4244e-04, 1.1997e-03, 9.9087e-04, 2.5349e-03, 4.0364e-04,\n",
      "        6.3133e-04, 1.2407e-03, 1.0498e-02, 1.5190e-02, 6.4754e-04, 5.0116e-04,\n",
      "        2.6464e-04, 3.7980e-04, 1.5438e-04, 4.2200e-04, 1.6846e-02, 2.1496e-03,\n",
      "        5.2834e-03, 1.7685e-02, 7.7009e-04, 3.4857e-04, 5.7030e-04, 6.0940e-04,\n",
      "        9.9850e-04, 1.2331e-03, 1.1539e-03, 5.8937e-04, 5.4407e-04, 4.5466e-04,\n",
      "        2.5296e-04, 6.0129e-04, 2.5296e-04, 2.5463e-04, 3.6526e-03, 2.4147e-03,\n",
      "        5.2023e-04, 5.2309e-04, 6.1750e-04, 3.7622e-04, 3.1304e-04, 2.9993e-04,\n",
      "        4.5109e-04, 2.0161e-03, 1.5287e-03, 6.2513e-04, 6.1378e-03, 7.4234e-03,\n",
      "        2.9354e-03, 5.4092e-03, 4.5991e-04, 1.3676e-03, 7.2746e-03, 1.7395e-03,\n",
      "        1.8568e-03, 2.2793e-03, 3.8967e-03, 1.4505e-03, 1.6088e-03, 7.9632e-04,\n",
      "        4.0665e-03, 8.8043e-03, 3.7842e-03, 3.3081e-02, 2.3300e-02, 2.8854e-02,\n",
      "        6.7596e-03, 1.2026e-03, 1.3824e-02, 1.5976e-02, 1.1887e-02, 5.7335e-03,\n",
      "        7.6332e-03, 7.9575e-03, 9.2392e-03, 7.2021e-03, 2.6646e-03, 1.0056e-02,\n",
      "        2.6093e-02, 7.9422e-03, 6.7253e-03, 2.2659e-03, 5.7487e-03, 1.8112e-02,\n",
      "        1.4084e-02, 1.6647e-02, 8.4686e-03, 1.2451e-02, 1.1787e-02, 5.7182e-03,\n",
      "        6.9313e-03, 1.0834e-02, 8.0414e-03, 8.7891e-03, 2.5009e-02, 1.3855e-02,\n",
      "        1.4679e-02, 2.4323e-02, 6.2447e-03, 4.2458e-03, 1.2405e-02, 1.2955e-02,\n",
      "        7.7438e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [38] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [38] : torch.Size([1, 32, 1, 110])\n",
      "Last layer attentions for generated token [38] : tensor([1.6602e-01, 1.6602e-01, 1.7977e-04, 5.4741e-04, 1.2022e-04, 3.8223e-03,\n",
      "        4.9639e-04, 9.6273e-04, 1.2856e-03, 1.2140e-03, 2.0275e-03, 3.7479e-04,\n",
      "        3.4380e-04, 6.8903e-04, 8.3084e-03, 1.1642e-02, 6.7711e-04, 7.9012e-04,\n",
      "        2.5463e-04, 3.6955e-04, 7.9453e-05, 2.7728e-04, 8.0338e-03, 1.5430e-03,\n",
      "        6.3972e-03, 1.8982e-02, 1.7357e-03, 3.9053e-04, 5.6791e-04, 5.5504e-04,\n",
      "        7.4244e-04, 1.1978e-03, 2.2469e-03, 8.1682e-04, 5.8508e-04, 6.3610e-04,\n",
      "        2.4629e-04, 8.2827e-04, 3.9816e-04, 2.2554e-04, 4.4937e-03, 2.8648e-03,\n",
      "        7.1526e-04, 1.0033e-03, 1.0309e-03, 4.6825e-04, 3.5977e-04, 2.5845e-04,\n",
      "        4.8876e-04, 2.6035e-03, 1.9913e-03, 2.9469e-04, 3.8605e-03, 4.0436e-03,\n",
      "        2.3804e-03, 6.3553e-03, 7.9489e-04, 1.8339e-03, 6.1684e-03, 1.7500e-03,\n",
      "        1.4324e-03, 2.5311e-03, 4.0665e-03, 1.3208e-03, 1.2388e-03, 7.2670e-04,\n",
      "        5.7716e-03, 4.8256e-03, 2.5139e-03, 1.8906e-02, 1.5358e-02, 2.8488e-02,\n",
      "        5.1613e-03, 9.3269e-04, 1.3359e-02, 1.9394e-02, 1.1314e-02, 8.4000e-03,\n",
      "        1.1459e-02, 1.1841e-02, 1.3939e-02, 1.4748e-02, 3.2845e-03, 1.4107e-02,\n",
      "        3.1235e-02, 7.6942e-03, 8.5144e-03, 3.0136e-03, 6.3095e-03, 2.0187e-02,\n",
      "        1.5869e-02, 1.9531e-02, 4.8141e-03, 1.1864e-02, 1.5526e-02, 6.7024e-03,\n",
      "        6.9160e-03, 1.2794e-02, 9.0561e-03, 8.8959e-03, 3.5980e-02, 1.2489e-02,\n",
      "        1.5152e-02, 2.2614e-02, 7.7858e-03, 2.7962e-03, 1.4160e-02, 1.2955e-02,\n",
      "        6.1760e-03, 9.4376e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [39] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [39] : torch.Size([1, 32, 1, 111])\n",
      "Last layer attentions for generated token [39] : tensor([1.9263e-01, 1.9263e-01, 2.5558e-04, 4.3464e-04, 1.3673e-04, 3.4027e-03,\n",
      "        4.6277e-04, 9.0933e-04, 1.2703e-03, 6.5899e-04, 1.4620e-03, 1.8764e-04,\n",
      "        5.0402e-04, 1.0862e-03, 6.5079e-03, 7.5340e-03, 2.5153e-04, 2.7633e-04,\n",
      "        1.5867e-04, 2.5749e-04, 6.3837e-05, 2.4188e-04, 2.1561e-02, 2.2278e-03,\n",
      "        3.7766e-03, 1.4984e-02, 7.7629e-04, 3.2687e-04, 5.7793e-04, 5.4216e-04,\n",
      "        8.5449e-04, 6.4373e-04, 1.2236e-03, 6.7043e-04, 4.1723e-04, 5.9414e-04,\n",
      "        6.0368e-04, 6.6519e-04, 4.5562e-04, 4.2295e-04, 2.2144e-03, 2.1973e-03,\n",
      "        5.1641e-04, 8.1348e-04, 9.2936e-04, 7.3814e-04, 3.7026e-04, 4.8971e-04,\n",
      "        8.0872e-04, 1.2045e-03, 1.5717e-03, 2.1851e-04, 5.0545e-03, 2.2430e-03,\n",
      "        2.3918e-03, 3.9444e-03, 3.9434e-04, 1.6060e-03, 5.2910e-03, 1.4563e-03,\n",
      "        1.1835e-03, 1.5717e-03, 2.8954e-03, 1.0862e-03, 1.1301e-03, 4.9353e-04,\n",
      "        4.6730e-03, 2.2869e-03, 2.7924e-03, 1.4153e-02, 1.0544e-02, 1.8829e-02,\n",
      "        3.2024e-03, 6.5899e-04, 9.5215e-03, 1.4801e-02, 8.8654e-03, 6.3019e-03,\n",
      "        7.7934e-03, 7.1068e-03, 8.3008e-03, 9.0942e-03, 2.6035e-03, 8.2245e-03,\n",
      "        2.6474e-02, 4.7379e-03, 6.1607e-03, 3.8471e-03, 6.5575e-03, 1.7960e-02,\n",
      "        1.2657e-02, 1.0338e-02, 7.4959e-03, 8.4457e-03, 1.5396e-02, 9.5215e-03,\n",
      "        1.1406e-02, 1.4702e-02, 1.2894e-02, 1.1711e-02, 2.3300e-02, 1.6846e-02,\n",
      "        1.4603e-02, 3.2471e-02, 7.0877e-03, 4.5204e-03, 1.3763e-02, 2.1454e-02,\n",
      "        1.2123e-02, 1.3321e-02, 1.5076e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [40] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [40] : torch.Size([1, 32, 1, 112])\n",
      "Last layer attentions for generated token [40] : tensor([0.1823, 0.1819, 0.0006, 0.0007, 0.0002, 0.0052, 0.0002, 0.0008, 0.0015,\n",
      "        0.0007, 0.0018, 0.0003, 0.0003, 0.0007, 0.0107, 0.0109, 0.0006, 0.0003,\n",
      "        0.0002, 0.0003, 0.0002, 0.0007, 0.0130, 0.0017, 0.0058, 0.0302, 0.0014,\n",
      "        0.0004, 0.0006, 0.0009, 0.0008, 0.0013, 0.0012, 0.0007, 0.0005, 0.0004,\n",
      "        0.0002, 0.0010, 0.0005, 0.0003, 0.0051, 0.0022, 0.0004, 0.0006, 0.0011,\n",
      "        0.0005, 0.0007, 0.0005, 0.0007, 0.0029, 0.0024, 0.0006, 0.0050, 0.0063,\n",
      "        0.0018, 0.0047, 0.0004, 0.0017, 0.0097, 0.0014, 0.0012, 0.0018, 0.0034,\n",
      "        0.0013, 0.0013, 0.0006, 0.0031, 0.0056, 0.0043, 0.0197, 0.0181, 0.0202,\n",
      "        0.0046, 0.0007, 0.0120, 0.0229, 0.0092, 0.0065, 0.0074, 0.0060, 0.0069,\n",
      "        0.0073, 0.0022, 0.0083, 0.0214, 0.0064, 0.0043, 0.0012, 0.0052, 0.0146,\n",
      "        0.0107, 0.0140, 0.0069, 0.0132, 0.0131, 0.0084, 0.0070, 0.0119, 0.0062,\n",
      "        0.0102, 0.0211, 0.0109, 0.0137, 0.0234, 0.0063, 0.0043, 0.0160, 0.0181,\n",
      "        0.0104, 0.0123, 0.0106, 0.0124], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [41] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [41] : torch.Size([1, 32, 1, 113])\n",
      "Last layer attentions for generated token [41] : tensor([0.1791, 0.1791, 0.0005, 0.0009, 0.0002, 0.0031, 0.0003, 0.0005, 0.0009,\n",
      "        0.0007, 0.0015, 0.0003, 0.0004, 0.0010, 0.0105, 0.0106, 0.0006, 0.0004,\n",
      "        0.0003, 0.0003, 0.0003, 0.0008, 0.0116, 0.0037, 0.0110, 0.0189, 0.0028,\n",
      "        0.0004, 0.0005, 0.0007, 0.0008, 0.0016, 0.0012, 0.0005, 0.0005, 0.0009,\n",
      "        0.0004, 0.0012, 0.0005, 0.0003, 0.0054, 0.0030, 0.0008, 0.0010, 0.0010,\n",
      "        0.0005, 0.0005, 0.0004, 0.0006, 0.0032, 0.0032, 0.0006, 0.0043, 0.0064,\n",
      "        0.0018, 0.0050, 0.0010, 0.0025, 0.0066, 0.0016, 0.0014, 0.0015, 0.0030,\n",
      "        0.0016, 0.0016, 0.0007, 0.0040, 0.0053, 0.0040, 0.0251, 0.0159, 0.0199,\n",
      "        0.0084, 0.0015, 0.0106, 0.0207, 0.0138, 0.0062, 0.0099, 0.0087, 0.0121,\n",
      "        0.0079, 0.0034, 0.0106, 0.0262, 0.0104, 0.0096, 0.0036, 0.0052, 0.0168,\n",
      "        0.0139, 0.0106, 0.0059, 0.0126, 0.0129, 0.0075, 0.0047, 0.0095, 0.0063,\n",
      "        0.0065, 0.0268, 0.0085, 0.0103, 0.0214, 0.0117, 0.0045, 0.0132, 0.0114,\n",
      "        0.0078, 0.0079, 0.0085, 0.0065, 0.0023], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [42] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [42] : torch.Size([1, 32, 1, 114])\n",
      "Last layer attentions for generated token [42] : tensor([2.2021e-01, 2.2021e-01, 5.7793e-04, 7.4339e-04, 2.5988e-04, 3.7823e-03,\n",
      "        4.0340e-04, 9.1982e-04, 1.5888e-03, 8.5068e-04, 1.5135e-03, 4.1461e-04,\n",
      "        3.0923e-04, 6.7949e-04, 1.0582e-02, 7.1411e-03, 4.2367e-04, 2.5582e-04,\n",
      "        1.6916e-04, 2.4223e-04, 2.7394e-04, 6.4468e-04, 1.1627e-02, 2.4834e-03,\n",
      "        1.2650e-02, 2.6413e-02, 1.5163e-03, 5.2691e-04, 6.5231e-04, 6.8235e-04,\n",
      "        8.7595e-04, 1.6556e-03, 1.6174e-03, 7.7915e-04, 6.2227e-04, 7.4625e-04,\n",
      "        3.6669e-04, 8.8978e-04, 5.2404e-04, 3.4165e-04, 4.0092e-03, 1.9970e-03,\n",
      "        3.9792e-04, 9.1457e-04, 1.1473e-03, 4.3035e-04, 5.5122e-04, 3.7980e-04,\n",
      "        7.1192e-04, 3.2387e-03, 2.5711e-03, 6.4325e-04, 4.4365e-03, 7.7934e-03,\n",
      "        1.6336e-03, 4.8065e-03, 5.9366e-04, 1.7185e-03, 7.3891e-03, 1.6756e-03,\n",
      "        1.2379e-03, 1.4524e-03, 4.0245e-03, 1.6012e-03, 1.1091e-03, 6.6519e-04,\n",
      "        2.5826e-03, 5.5695e-03, 4.6997e-03, 2.1469e-02, 1.6495e-02, 1.8768e-02,\n",
      "        4.4098e-03, 1.0920e-03, 1.2779e-02, 1.6815e-02, 7.5951e-03, 5.5885e-03,\n",
      "        4.7226e-03, 3.4504e-03, 6.6566e-03, 6.4507e-03, 2.2087e-03, 8.1482e-03,\n",
      "        1.8402e-02, 6.2218e-03, 4.0627e-03, 1.4725e-03, 3.7975e-03, 1.2093e-02,\n",
      "        7.4043e-03, 8.2321e-03, 4.3564e-03, 1.1803e-02, 1.1017e-02, 5.3368e-03,\n",
      "        4.5700e-03, 7.1373e-03, 3.4771e-03, 6.5079e-03, 1.9028e-02, 5.3635e-03,\n",
      "        1.1337e-02, 2.1149e-02, 5.8174e-03, 3.4809e-03, 1.4671e-02, 1.2360e-02,\n",
      "        6.6147e-03, 9.5673e-03, 9.9792e-03, 1.0544e-02, 4.7302e-03, 5.5504e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [43] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [43] : torch.Size([1, 32, 1, 115])\n",
      "Last layer attentions for generated token [43] : tensor([2.2595e-01, 2.2644e-01, 8.0395e-04, 9.9659e-04, 4.6611e-04, 3.0499e-03,\n",
      "        3.2163e-04, 5.2023e-04, 6.4850e-04, 8.9359e-04, 1.1768e-03, 1.5867e-04,\n",
      "        6.6805e-04, 1.0929e-03, 4.4327e-03, 6.9504e-03, 2.8157e-04, 3.3975e-04,\n",
      "        1.9062e-04, 3.7527e-04, 1.4162e-04, 5.2118e-04, 1.2245e-02, 3.4580e-03,\n",
      "        7.7591e-03, 1.4061e-02, 2.6550e-03, 4.9067e-04, 5.6887e-04, 5.5790e-04,\n",
      "        7.7772e-04, 1.2751e-03, 1.2379e-03, 4.8876e-04, 3.9029e-04, 7.4625e-04,\n",
      "        5.0116e-04, 6.0320e-04, 4.5371e-04, 3.0947e-04, 3.3131e-03, 2.3518e-03,\n",
      "        4.2367e-04, 9.9659e-04, 1.0204e-03, 5.5361e-04, 2.3079e-04, 3.9101e-04,\n",
      "        5.0783e-04, 2.3918e-03, 2.4338e-03, 7.7152e-04, 3.0727e-03, 5.5161e-03,\n",
      "        1.8654e-03, 4.8790e-03, 6.9284e-04, 2.4796e-03, 5.9929e-03, 1.4391e-03,\n",
      "        1.2722e-03, 1.1053e-03, 2.6417e-03, 1.7090e-03, 1.6050e-03, 7.4053e-04,\n",
      "        3.8815e-03, 3.6144e-03, 2.7905e-03, 2.0660e-02, 1.3275e-02, 1.7181e-02,\n",
      "        4.8294e-03, 1.5373e-03, 6.9389e-03, 1.6800e-02, 1.3290e-02, 4.8866e-03,\n",
      "        5.5313e-03, 5.9700e-03, 8.4686e-03, 1.1238e-02, 3.4714e-03, 8.4686e-03,\n",
      "        2.6184e-02, 8.0490e-03, 7.7972e-03, 1.3599e-03, 3.1605e-03, 1.4061e-02,\n",
      "        1.1505e-02, 1.0727e-02, 6.6032e-03, 8.8806e-03, 1.0193e-02, 4.4289e-03,\n",
      "        3.4046e-03, 6.2752e-03, 7.1335e-03, 4.5547e-03, 1.7166e-02, 1.3832e-02,\n",
      "        7.1754e-03, 1.9684e-02, 9.4223e-03, 5.3635e-03, 8.9340e-03, 1.1826e-02,\n",
      "        5.3444e-03, 7.5150e-03, 1.1703e-02, 8.4686e-03, 2.5406e-03, 3.8929e-03,\n",
      "        4.5586e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [44] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [44] : torch.Size([1, 32, 1, 116])\n",
      "Last layer attentions for generated token [44] : tensor([2.6660e-01, 2.6660e-01, 1.1921e-03, 1.4372e-03, 6.7520e-04, 3.4351e-03,\n",
      "        5.3835e-04, 6.7902e-04, 8.5163e-04, 8.9264e-04, 1.6193e-03, 3.3426e-04,\n",
      "        6.7759e-04, 9.9945e-04, 9.2087e-03, 8.4915e-03, 3.2401e-04, 3.5858e-04,\n",
      "        2.3150e-04, 3.4070e-04, 2.2614e-04, 6.3562e-04, 1.5991e-02, 3.1185e-03,\n",
      "        9.9640e-03, 2.7161e-02, 2.0828e-03, 6.5327e-04, 7.9250e-04, 6.8855e-04,\n",
      "        7.1049e-04, 1.1482e-03, 1.5392e-03, 6.7139e-04, 5.3740e-04, 9.0504e-04,\n",
      "        9.6893e-04, 5.8556e-04, 6.9380e-04, 3.9840e-04, 2.7199e-03, 2.3479e-03,\n",
      "        5.9462e-04, 9.6130e-04, 9.8038e-04, 5.8556e-04, 3.2020e-04, 6.1846e-04,\n",
      "        6.3658e-04, 1.7548e-03, 2.8038e-03, 7.6056e-04, 5.1079e-03, 1.0201e-02,\n",
      "        2.0828e-03, 5.7030e-03, 6.3419e-04, 2.0924e-03, 7.2136e-03, 1.5392e-03,\n",
      "        1.8930e-03, 9.9945e-04, 2.5635e-03, 1.5039e-03, 1.1396e-03, 4.9877e-04,\n",
      "        2.8610e-03, 4.0321e-03, 5.5962e-03, 1.2314e-02, 1.1383e-02, 1.1543e-02,\n",
      "        4.6310e-03, 1.3266e-03, 8.5602e-03, 1.1047e-02, 5.7068e-03, 3.7479e-03,\n",
      "        3.2558e-03, 2.3022e-03, 3.3627e-03, 4.4403e-03, 2.5425e-03, 3.9062e-03,\n",
      "        1.3245e-02, 6.2027e-03, 3.5229e-03, 1.6937e-03, 2.4548e-03, 5.8708e-03,\n",
      "        5.3368e-03, 6.2637e-03, 4.0588e-03, 6.6948e-03, 6.4812e-03, 2.6798e-03,\n",
      "        1.7824e-03, 5.5389e-03, 2.7256e-03, 3.5725e-03, 7.7324e-03, 5.1956e-03,\n",
      "        6.1378e-03, 2.2507e-02, 8.3618e-03, 4.7646e-03, 9.0637e-03, 1.3519e-02,\n",
      "        5.2071e-03, 6.5079e-03, 7.9727e-03, 9.6359e-03, 4.7646e-03, 7.8201e-03,\n",
      "        5.4741e-03, 1.4687e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [45] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [45] : torch.Size([1, 32, 1, 117])\n",
      "Last layer attentions for generated token [45] : tensor([2.1326e-01, 2.1277e-01, 5.7459e-04, 1.0223e-03, 4.2295e-04, 3.8528e-03,\n",
      "        6.8140e-04, 1.2341e-03, 1.8091e-03, 1.1587e-03, 2.6875e-03, 5.7697e-04,\n",
      "        4.6015e-04, 9.1314e-04, 1.2772e-02, 9.3307e-03, 4.3392e-04, 3.2949e-04,\n",
      "        2.2161e-04, 2.2948e-04, 2.0182e-04, 4.5466e-04, 1.3611e-02, 2.5959e-03,\n",
      "        1.5320e-02, 3.6896e-02, 2.4033e-03, 7.0143e-04, 7.4387e-04, 9.0790e-04,\n",
      "        8.9884e-04, 1.5440e-03, 1.4849e-03, 1.0576e-03, 8.0585e-04, 5.9652e-04,\n",
      "        7.7963e-04, 1.1959e-03, 5.6267e-04, 4.0770e-04, 3.5686e-03, 1.9445e-03,\n",
      "        4.8876e-04, 1.1368e-03, 1.6890e-03, 5.3692e-04, 6.9475e-04, 6.3515e-04,\n",
      "        9.6083e-04, 4.3526e-03, 2.7580e-03, 8.3160e-04, 4.9477e-03, 7.2632e-03,\n",
      "        2.3117e-03, 6.2332e-03, 6.3753e-04, 1.8778e-03, 8.0414e-03, 1.3256e-03,\n",
      "        1.4009e-03, 1.1654e-03, 3.1414e-03, 1.7061e-03, 1.0014e-03, 5.9414e-04,\n",
      "        1.8158e-03, 3.7441e-03, 4.0550e-03, 1.5564e-02, 1.3580e-02, 1.2962e-02,\n",
      "        3.1128e-03, 1.1034e-03, 1.3924e-02, 1.6861e-02, 8.1940e-03, 5.1346e-03,\n",
      "        4.6196e-03, 2.9392e-03, 6.6605e-03, 6.2408e-03, 2.0924e-03, 8.1711e-03,\n",
      "        1.6556e-02, 8.4000e-03, 4.2610e-03, 1.3628e-03, 4.4670e-03, 1.0620e-02,\n",
      "        4.4518e-03, 6.1073e-03, 3.4218e-03, 8.8882e-03, 1.0605e-02, 4.5280e-03,\n",
      "        3.3722e-03, 8.9493e-03, 3.4218e-03, 7.2289e-03, 1.4732e-02, 5.9204e-03,\n",
      "        1.0323e-02, 1.7746e-02, 4.3831e-03, 3.3493e-03, 1.4008e-02, 1.3313e-02,\n",
      "        6.4583e-03, 9.3536e-03, 1.1780e-02, 1.3535e-02, 6.9313e-03, 7.8888e-03,\n",
      "        7.4844e-03, 8.7118e-04, 5.3864e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [46] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [46] : torch.Size([1, 32, 1, 118])\n",
      "Last layer attentions for generated token [46] : tensor([1.8335e-01, 1.8335e-01, 5.3644e-04, 5.9175e-04, 3.4642e-04, 4.5013e-03,\n",
      "        3.0923e-04, 7.0810e-04, 9.7561e-04, 6.7711e-04, 1.2894e-03, 1.9133e-04,\n",
      "        3.3522e-04, 8.7452e-04, 5.8212e-03, 9.7504e-03, 3.6812e-04, 2.3901e-04,\n",
      "        1.0777e-04, 1.2064e-04, 5.4181e-05, 1.7011e-04, 8.8120e-03, 8.9836e-04,\n",
      "        4.0741e-03, 2.0264e-02, 1.2922e-03, 3.6311e-04, 5.4264e-04, 7.7915e-04,\n",
      "        1.1625e-03, 1.1692e-03, 1.4219e-03, 8.8120e-04, 5.3978e-04, 3.0804e-04,\n",
      "        3.4165e-04, 5.7125e-04, 3.0756e-04, 2.5153e-04, 3.5057e-03, 1.8692e-03,\n",
      "        3.8052e-04, 4.7445e-04, 7.4625e-04, 3.5596e-04, 3.4976e-04, 3.2854e-04,\n",
      "        5.3120e-04, 2.5063e-03, 2.1954e-03, 6.0463e-04, 4.3221e-03, 4.0283e-03,\n",
      "        1.9932e-03, 4.5319e-03, 3.6597e-04, 1.6336e-03, 6.4621e-03, 9.1076e-04,\n",
      "        1.0242e-03, 1.5898e-03, 2.1591e-03, 7.9918e-04, 8.8310e-04, 4.0650e-04,\n",
      "        2.1935e-03, 2.8000e-03, 2.9144e-03, 1.4008e-02, 1.3618e-02, 1.4183e-02,\n",
      "        2.5902e-03, 7.8392e-04, 1.0849e-02, 1.4221e-02, 8.7967e-03, 4.0627e-03,\n",
      "        4.9133e-03, 4.3297e-03, 7.9498e-03, 5.8670e-03, 2.6798e-03, 9.2163e-03,\n",
      "        2.0920e-02, 6.1455e-03, 5.3864e-03, 2.2545e-03, 6.1874e-03, 1.0986e-02,\n",
      "        7.1564e-03, 9.4910e-03, 4.1313e-03, 8.6288e-03, 1.0925e-02, 5.6915e-03,\n",
      "        6.9962e-03, 1.1620e-02, 8.6823e-03, 1.2711e-02, 2.6291e-02, 1.1780e-02,\n",
      "        1.5961e-02, 2.4612e-02, 5.8098e-03, 4.2419e-03, 1.5541e-02, 2.2247e-02,\n",
      "        1.5221e-02, 2.3575e-02, 1.8585e-02, 2.1057e-02, 9.2163e-03, 1.2009e-02,\n",
      "        8.7128e-03, 1.3075e-03, 8.6517e-03, 7.6904e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [47] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [47] : torch.Size([1, 32, 1, 119])\n",
      "Last layer attentions for generated token [47] : tensor([1.6650e-01, 1.6650e-01, 2.8205e-04, 9.5034e-04, 2.1970e-04, 5.0125e-03,\n",
      "        5.2404e-04, 5.6505e-04, 6.8331e-04, 9.6369e-04, 2.4319e-03, 2.7823e-04,\n",
      "        5.3930e-04, 1.0920e-03, 1.2672e-02, 1.8036e-02, 4.6420e-04, 5.8222e-04,\n",
      "        2.1207e-04, 2.7347e-04, 9.1910e-05, 3.2854e-04, 6.6376e-03, 1.4181e-03,\n",
      "        8.0490e-03, 2.4673e-02, 3.6430e-03, 5.9605e-04, 5.4598e-04, 6.0034e-04,\n",
      "        7.9393e-04, 1.5488e-03, 1.5793e-03, 7.0333e-04, 3.6788e-04, 5.1355e-04,\n",
      "        4.7779e-04, 5.2786e-04, 3.1781e-04, 2.1458e-04, 4.0321e-03, 2.7847e-03,\n",
      "        7.4291e-04, 8.0967e-04, 6.9237e-04, 4.1604e-04, 1.6391e-04, 1.9228e-04,\n",
      "        2.7657e-04, 2.0294e-03, 2.1114e-03, 3.3164e-04, 2.8343e-03, 4.4785e-03,\n",
      "        2.4014e-03, 5.4283e-03, 9.7084e-04, 2.2945e-03, 8.9569e-03, 1.4467e-03,\n",
      "        1.4324e-03, 1.5278e-03, 1.6012e-03, 9.0504e-04, 7.4005e-04, 3.1638e-04,\n",
      "        4.4212e-03, 2.7885e-03, 2.4700e-03, 1.6403e-02, 1.3077e-02, 2.2324e-02,\n",
      "        6.1226e-03, 1.1463e-03, 8.4534e-03, 1.5671e-02, 1.4893e-02, 7.0419e-03,\n",
      "        9.3002e-03, 8.8882e-03, 1.1711e-02, 9.7046e-03, 3.4866e-03, 9.4376e-03,\n",
      "        2.9190e-02, 1.0826e-02, 1.1795e-02, 7.4196e-03, 7.5836e-03, 1.1551e-02,\n",
      "        9.8877e-03, 8.0948e-03, 3.3340e-03, 8.2703e-03, 1.3763e-02, 7.6561e-03,\n",
      "        4.4556e-03, 1.7761e-02, 9.2239e-03, 9.6817e-03, 1.7990e-02, 1.3672e-02,\n",
      "        1.1887e-02, 2.5970e-02, 1.1772e-02, 5.3139e-03, 2.0081e-02, 1.7639e-02,\n",
      "        7.1220e-03, 8.5678e-03, 8.9722e-03, 8.5602e-03, 4.0054e-03, 4.2191e-03,\n",
      "        2.9564e-03, 5.0068e-04, 2.7523e-03, 2.3289e-03, 5.4741e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [48] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [48] : torch.Size([1, 32, 1, 120])\n",
      "Last layer attentions for generated token [48] : tensor([1.7603e-01, 1.7603e-01, 1.6367e-04, 5.2118e-04, 1.3149e-04, 2.9984e-03,\n",
      "        4.9925e-04, 5.8603e-04, 6.0940e-04, 6.7806e-04, 2.1553e-03, 4.4227e-04,\n",
      "        2.2948e-04, 5.1785e-04, 9.2392e-03, 1.0773e-02, 5.2834e-04, 5.6458e-04,\n",
      "        1.8907e-04, 1.7905e-04, 5.1677e-05, 2.8610e-04, 3.3188e-03, 8.2111e-04,\n",
      "        5.1994e-03, 1.8875e-02, 2.9831e-03, 5.2834e-04, 3.5453e-04, 5.4693e-04,\n",
      "        7.2050e-04, 1.8120e-03, 1.9798e-03, 8.0395e-04, 4.1533e-04, 2.6774e-04,\n",
      "        3.2163e-04, 4.8971e-04, 3.0279e-04, 1.5080e-04, 4.2152e-03, 2.2850e-03,\n",
      "        5.8365e-04, 8.1825e-04, 7.4053e-04, 2.7180e-04, 2.7514e-04, 1.7357e-04,\n",
      "        2.8110e-04, 2.7294e-03, 1.8311e-03, 2.5439e-04, 3.0346e-03, 5.2032e-03,\n",
      "        2.2926e-03, 4.6501e-03, 1.2283e-03, 1.9760e-03, 7.4234e-03, 1.6270e-03,\n",
      "        1.1034e-03, 2.5177e-03, 1.7891e-03, 8.4591e-04, 7.3481e-04, 4.2534e-04,\n",
      "        3.5954e-03, 3.9139e-03, 2.6703e-03, 1.7776e-02, 1.1406e-02, 2.0447e-02,\n",
      "        5.7220e-03, 1.0366e-03, 1.3802e-02, 1.4442e-02, 1.4908e-02, 9.2239e-03,\n",
      "        1.0490e-02, 8.2703e-03, 1.8051e-02, 1.0902e-02, 3.3817e-03, 1.1856e-02,\n",
      "        2.6367e-02, 1.3054e-02, 1.4000e-02, 6.1722e-03, 9.2392e-03, 1.4229e-02,\n",
      "        8.8882e-03, 7.0419e-03, 2.0027e-03, 1.0384e-02, 1.7136e-02, 7.7362e-03,\n",
      "        3.6697e-03, 1.2299e-02, 7.7820e-03, 8.0032e-03, 2.2949e-02, 1.1002e-02,\n",
      "        1.7258e-02, 2.0081e-02, 7.0686e-03, 3.2482e-03, 2.1652e-02, 1.5007e-02,\n",
      "        4.7646e-03, 7.0877e-03, 6.9733e-03, 7.6332e-03, 3.6697e-03, 3.6888e-03,\n",
      "        2.5806e-03, 3.4785e-04, 2.3174e-03, 1.8835e-03, 7.3280e-03, 1.1063e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [49] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [49] : torch.Size([1, 32, 1, 121])\n",
      "Last layer attentions for generated token [49] : tensor([0.1385, 0.1383, 0.0002, 0.0002, 0.0002, 0.0023, 0.0003, 0.0006, 0.0006,\n",
      "        0.0003, 0.0020, 0.0003, 0.0002, 0.0003, 0.0042, 0.0075, 0.0003, 0.0005,\n",
      "        0.0003, 0.0002, 0.0001, 0.0004, 0.0072, 0.0017, 0.0039, 0.0186, 0.0009,\n",
      "        0.0004, 0.0005, 0.0009, 0.0008, 0.0014, 0.0014, 0.0008, 0.0008, 0.0004,\n",
      "        0.0003, 0.0006, 0.0003, 0.0003, 0.0023, 0.0013, 0.0003, 0.0006, 0.0009,\n",
      "        0.0003, 0.0011, 0.0004, 0.0009, 0.0033, 0.0020, 0.0005, 0.0035, 0.0061,\n",
      "        0.0019, 0.0060, 0.0003, 0.0009, 0.0033, 0.0013, 0.0013, 0.0019, 0.0032,\n",
      "        0.0011, 0.0009, 0.0009, 0.0026, 0.0084, 0.0025, 0.0271, 0.0244, 0.0341,\n",
      "        0.0028, 0.0006, 0.0186, 0.0224, 0.0175, 0.0107, 0.0084, 0.0101, 0.0131,\n",
      "        0.0133, 0.0039, 0.0189, 0.0301, 0.0075, 0.0082, 0.0040, 0.0068, 0.0176,\n",
      "        0.0082, 0.0077, 0.0043, 0.0105, 0.0133, 0.0067, 0.0058, 0.0111, 0.0082,\n",
      "        0.0137, 0.0308, 0.0115, 0.0196, 0.0217, 0.0028, 0.0014, 0.0187, 0.0127,\n",
      "        0.0068, 0.0096, 0.0065, 0.0084, 0.0033, 0.0040, 0.0046, 0.0006, 0.0059,\n",
      "        0.0044, 0.0109, 0.0158, 0.0109], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [50] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [50] : torch.Size([1, 32, 1, 122])\n",
      "Last layer attentions for generated token [50] : tensor([2.2437e-01, 2.2437e-01, 2.9778e-04, 3.5071e-04, 2.1231e-04, 2.7885e-03,\n",
      "        2.7204e-04, 4.8876e-04, 8.4639e-04, 7.5293e-04, 1.3657e-03, 3.1757e-04,\n",
      "        3.1328e-04, 6.1321e-04, 5.9776e-03, 1.0742e-02, 4.0293e-04, 3.5572e-04,\n",
      "        1.7464e-04, 2.2435e-04, 1.1683e-04, 3.9840e-04, 5.2299e-03, 8.7166e-04,\n",
      "        4.4212e-03, 1.1177e-02, 1.0891e-03, 3.0184e-04, 3.8767e-04, 6.4516e-04,\n",
      "        7.1001e-04, 1.4429e-03, 1.6346e-03, 8.1587e-04, 8.2684e-04, 4.5133e-04,\n",
      "        2.1994e-04, 9.2983e-04, 3.8004e-04, 2.4152e-04, 3.6869e-03, 1.6098e-03,\n",
      "        3.1948e-04, 5.3406e-04, 7.7534e-04, 1.8668e-04, 5.9795e-04, 2.3139e-04,\n",
      "        3.3474e-04, 4.9591e-03, 3.4142e-03, 6.5947e-04, 4.2648e-03, 6.7062e-03,\n",
      "        1.9321e-03, 6.5498e-03, 7.1001e-04, 9.6655e-04, 4.4441e-03, 1.2808e-03,\n",
      "        1.3266e-03, 1.6870e-03, 2.6226e-03, 1.4372e-03, 1.2341e-03, 9.9754e-04,\n",
      "        2.4471e-03, 5.2910e-03, 3.0956e-03, 2.5909e-02, 2.1545e-02, 1.8692e-02,\n",
      "        2.7065e-03, 6.6042e-04, 1.0780e-02, 1.2321e-02, 9.1934e-03, 5.3978e-03,\n",
      "        6.6032e-03, 7.9346e-03, 1.2054e-02, 9.0637e-03, 3.5610e-03, 1.1574e-02,\n",
      "        1.8539e-02, 6.3667e-03, 4.0741e-03, 1.2884e-03, 3.2234e-03, 9.5444e-03,\n",
      "        4.3831e-03, 5.9013e-03, 2.6608e-03, 7.2899e-03, 6.3286e-03, 2.7599e-03,\n",
      "        2.9812e-03, 5.7640e-03, 4.5662e-03, 6.5155e-03, 3.3234e-02, 7.1449e-03,\n",
      "        1.3832e-02, 1.4084e-02, 3.2578e-03, 1.3266e-03, 8.7433e-03, 9.0637e-03,\n",
      "        6.7101e-03, 1.0506e-02, 6.4850e-03, 5.8517e-03, 3.3779e-03, 4.2038e-03,\n",
      "        3.9444e-03, 6.8665e-04, 4.0703e-03, 2.8801e-03, 8.5907e-03, 1.0796e-02,\n",
      "        1.0963e-02, 8.1863e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [51] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [51] : torch.Size([1, 32, 1, 123])\n",
      "Last layer attentions for generated token [51] : tensor([0.1812, 0.1812, 0.0006, 0.0013, 0.0004, 0.0044, 0.0004, 0.0005, 0.0007,\n",
      "        0.0012, 0.0018, 0.0007, 0.0008, 0.0014, 0.0118, 0.0201, 0.0007, 0.0009,\n",
      "        0.0004, 0.0005, 0.0003, 0.0010, 0.0069, 0.0020, 0.0109, 0.0184, 0.0043,\n",
      "        0.0009, 0.0008, 0.0009, 0.0012, 0.0031, 0.0015, 0.0006, 0.0007, 0.0012,\n",
      "        0.0004, 0.0014, 0.0007, 0.0004, 0.0078, 0.0035, 0.0009, 0.0013, 0.0011,\n",
      "        0.0005, 0.0004, 0.0002, 0.0004, 0.0071, 0.0038, 0.0010, 0.0036, 0.0105,\n",
      "        0.0026, 0.0068, 0.0012, 0.0017, 0.0067, 0.0014, 0.0016, 0.0018, 0.0027,\n",
      "        0.0019, 0.0018, 0.0010, 0.0057, 0.0067, 0.0040, 0.0346, 0.0183, 0.0208,\n",
      "        0.0073, 0.0015, 0.0131, 0.0175, 0.0121, 0.0066, 0.0064, 0.0109, 0.0094,\n",
      "        0.0106, 0.0047, 0.0100, 0.0244, 0.0115, 0.0084, 0.0040, 0.0043, 0.0066,\n",
      "        0.0051, 0.0058, 0.0028, 0.0080, 0.0073, 0.0032, 0.0017, 0.0090, 0.0047,\n",
      "        0.0052, 0.0126, 0.0083, 0.0087, 0.0160, 0.0081, 0.0027, 0.0123, 0.0076,\n",
      "        0.0054, 0.0066, 0.0041, 0.0033, 0.0025, 0.0026, 0.0026, 0.0005, 0.0023,\n",
      "        0.0020, 0.0058, 0.0100, 0.0070, 0.0068, 0.0084], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [52] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [52] : torch.Size([1, 32, 1, 124])\n",
      "Last layer attentions for generated token [52] : tensor([1.4050e-01, 1.4014e-01, 1.2147e-04, 3.0494e-04, 1.0151e-04, 1.5984e-03,\n",
      "        2.7227e-04, 3.3617e-04, 4.4274e-04, 4.3750e-04, 1.4658e-03, 4.2152e-04,\n",
      "        2.0182e-04, 5.1546e-04, 7.3738e-03, 1.0231e-02, 2.6321e-04, 3.9530e-04,\n",
      "        1.5175e-04, 2.7919e-04, 1.0806e-04, 2.2089e-04, 1.9989e-03, 5.6410e-04,\n",
      "        3.7346e-03, 1.1620e-02, 2.2373e-03, 4.2748e-04, 3.1757e-04, 5.2166e-04,\n",
      "        5.5838e-04, 1.2798e-03, 9.4652e-04, 9.4461e-04, 8.0013e-04, 3.5357e-04,\n",
      "        2.9254e-04, 6.9094e-04, 2.3007e-04, 2.4259e-04, 4.9324e-03, 1.4668e-03,\n",
      "        7.0190e-04, 7.5006e-04, 1.0433e-03, 4.0531e-04, 6.3276e-04, 2.3782e-04,\n",
      "        3.7050e-04, 3.8490e-03, 1.7767e-03, 2.5630e-04, 3.1586e-03, 6.0883e-03,\n",
      "        1.9932e-03, 4.7836e-03, 9.6321e-04, 1.2484e-03, 5.3558e-03, 1.4219e-03,\n",
      "        1.4658e-03, 1.4992e-03, 2.4338e-03, 9.5177e-04, 8.0776e-04, 6.5422e-04,\n",
      "        2.6951e-03, 3.8757e-03, 2.5673e-03, 1.9012e-02, 1.3748e-02, 2.4475e-02,\n",
      "        1.1642e-02, 9.9754e-04, 2.8610e-02, 2.0935e-02, 2.0111e-02, 1.4824e-02,\n",
      "        1.5434e-02, 9.7427e-03, 2.7328e-02, 1.6174e-02, 8.1406e-03, 2.1469e-02,\n",
      "        3.3691e-02, 1.3008e-02, 1.2024e-02, 9.3002e-03, 8.6823e-03, 1.1871e-02,\n",
      "        8.1406e-03, 8.6288e-03, 4.8943e-03, 6.7673e-03, 8.9035e-03, 4.7836e-03,\n",
      "        3.0479e-03, 5.5504e-03, 4.7607e-03, 3.9444e-03, 1.7410e-02, 6.0310e-03,\n",
      "        1.5190e-02, 2.1744e-02, 5.6686e-03, 9.3889e-04, 1.2779e-02, 1.1444e-02,\n",
      "        4.6425e-03, 7.8354e-03, 7.6561e-03, 4.6005e-03, 4.3983e-03, 2.7866e-03,\n",
      "        3.2730e-03, 3.6263e-04, 2.5883e-03, 3.6411e-03, 8.6365e-03, 1.2817e-02,\n",
      "        9.8267e-03, 8.4839e-03, 1.8021e-02, 1.6541e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [53] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [53] : torch.Size([1, 32, 1, 125])\n",
      "Last layer attentions for generated token [53] : tensor([2.6807e-01, 2.6807e-01, 1.8525e-04, 2.3413e-04, 7.7844e-05, 1.7710e-03,\n",
      "        1.7750e-04, 3.3164e-04, 5.4455e-04, 3.7718e-04, 1.5974e-03, 1.6546e-04,\n",
      "        2.2089e-04, 6.3419e-04, 5.6267e-03, 8.6517e-03, 3.1638e-04, 2.5225e-04,\n",
      "        1.4770e-04, 1.6797e-04, 5.6744e-05, 2.4939e-04, 8.3542e-03, 5.3406e-04,\n",
      "        1.4009e-03, 9.2621e-03, 6.2895e-04, 3.0255e-04, 3.3617e-04, 5.3406e-04,\n",
      "        5.5647e-04, 6.4135e-04, 4.9877e-04, 6.9761e-04, 5.3072e-04, 2.9421e-04,\n",
      "        2.7752e-04, 5.3692e-04, 2.1243e-04, 1.9801e-04, 1.9836e-03, 1.2369e-03,\n",
      "        3.5501e-04, 2.9612e-04, 4.9210e-04, 2.9731e-04, 4.6039e-04, 3.0732e-04,\n",
      "        4.1103e-04, 1.3742e-03, 1.0910e-03, 1.4150e-04, 3.8700e-03, 2.6798e-03,\n",
      "        2.5902e-03, 3.4523e-03, 7.9060e-04, 1.1234e-03, 3.6411e-03, 9.8419e-04,\n",
      "        7.1430e-04, 1.3962e-03, 1.8597e-03, 6.4278e-04, 7.3433e-04, 5.7507e-04,\n",
      "        2.1286e-03, 1.7576e-03, 2.0847e-03, 8.9188e-03, 5.9624e-03, 1.5541e-02,\n",
      "        3.1338e-03, 7.2575e-04, 1.1871e-02, 1.0765e-02, 1.0567e-02, 5.5809e-03,\n",
      "        6.3324e-03, 9.2087e-03, 8.9645e-03, 5.9128e-03, 2.6855e-03, 1.2718e-02,\n",
      "        1.8127e-02, 6.7787e-03, 6.7711e-03, 2.9907e-03, 8.4686e-03, 1.6815e-02,\n",
      "        4.7684e-03, 5.5199e-03, 2.1858e-03, 2.7294e-03, 6.5117e-03, 4.5395e-03,\n",
      "        3.2196e-03, 9.4376e-03, 5.3101e-03, 7.5188e-03, 1.0445e-02, 4.6539e-03,\n",
      "        8.8272e-03, 1.3206e-02, 2.9716e-03, 1.3142e-03, 5.9242e-03, 5.1117e-03,\n",
      "        4.6310e-03, 4.3373e-03, 3.7937e-03, 2.6493e-03, 1.2712e-03, 1.1663e-03,\n",
      "        1.7071e-03, 2.8133e-04, 2.1725e-03, 3.1757e-03, 5.5580e-03, 8.2703e-03,\n",
      "        7.1564e-03, 9.2239e-03, 1.3077e-02, 1.7456e-02, 7.8354e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [54] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [54] : torch.Size([1, 32, 1, 126])\n",
      "Last layer attentions for generated token [54] : tensor([1.8726e-01, 1.8726e-01, 4.2272e-04, 5.5361e-04, 2.7728e-04, 2.6226e-03,\n",
      "        2.1672e-04, 2.9445e-04, 2.5034e-04, 7.6675e-04, 1.5306e-03, 1.8835e-04,\n",
      "        6.6900e-04, 1.1997e-03, 5.6458e-03, 8.7051e-03, 2.7347e-04, 5.4598e-04,\n",
      "        2.8706e-04, 4.5085e-04, 1.4782e-04, 5.1069e-04, 6.2485e-03, 1.1759e-03,\n",
      "        2.9984e-03, 7.3547e-03, 1.5402e-03, 4.2677e-04, 4.0579e-04, 3.7885e-04,\n",
      "        5.8556e-04, 8.2588e-04, 4.7517e-04, 3.0994e-04, 3.4237e-04, 5.4073e-04,\n",
      "        3.7742e-04, 4.3011e-04, 5.5361e-04, 3.6216e-04, 3.4714e-03, 2.7943e-03,\n",
      "        7.8344e-04, 7.6246e-04, 6.0892e-04, 5.5885e-04, 1.8835e-04, 2.8205e-04,\n",
      "        4.4560e-04, 2.3556e-03, 2.0542e-03, 4.3011e-04, 2.7943e-03, 7.8506e-03,\n",
      "        2.1801e-03, 5.9967e-03, 7.9584e-04, 2.1038e-03, 4.3602e-03, 1.0881e-03,\n",
      "        1.0920e-03, 8.7214e-04, 2.0065e-03, 1.1625e-03, 1.0777e-03, 5.1165e-04,\n",
      "        6.9656e-03, 3.8528e-03, 2.2907e-03, 1.7685e-02, 1.1581e-02, 1.8845e-02,\n",
      "        1.0521e-02, 1.8311e-03, 1.1627e-02, 1.8265e-02, 1.3855e-02, 8.9645e-03,\n",
      "        5.7373e-03, 6.5918e-03, 8.6517e-03, 9.9869e-03, 2.9945e-03, 8.2397e-03,\n",
      "        3.6987e-02, 6.9656e-03, 1.2817e-02, 5.9433e-03, 4.9133e-03, 1.3634e-02,\n",
      "        1.4214e-02, 1.3481e-02, 5.8174e-03, 5.7755e-03, 9.0790e-03, 4.0169e-03,\n",
      "        2.5368e-03, 5.7030e-03, 6.7902e-03, 4.8599e-03, 1.0345e-02, 8.1863e-03,\n",
      "        6.1569e-03, 3.3936e-02, 7.5455e-03, 2.0638e-03, 8.0948e-03, 1.0117e-02,\n",
      "        3.8948e-03, 4.6844e-03, 4.0054e-03, 3.9444e-03, 1.8654e-03, 3.3207e-03,\n",
      "        2.0161e-03, 9.5415e-04, 3.3092e-03, 2.0370e-03, 3.5706e-03, 1.3176e-02,\n",
      "        7.4692e-03, 4.1389e-03, 6.1913e-03, 3.1067e-02, 1.8005e-02, 3.8052e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [55] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [55] : torch.Size([1, 32, 1, 127])\n",
      "Last layer attentions for generated token [55] : tensor([1.7957e-01, 1.7920e-01, 1.5020e-04, 2.7251e-04, 9.3281e-05, 1.8501e-03,\n",
      "        4.5466e-04, 6.0606e-04, 5.0640e-04, 5.2834e-04, 1.7881e-03, 5.9080e-04,\n",
      "        4.4084e-04, 7.8583e-04, 8.3847e-03, 9.5825e-03, 5.2547e-04, 4.5657e-04,\n",
      "        3.1567e-04, 3.5405e-04, 9.2208e-05, 2.5058e-04, 3.7956e-03, 6.6185e-04,\n",
      "        4.0894e-03, 1.3504e-02, 1.3599e-03, 2.4390e-04, 3.4857e-04, 2.9755e-04,\n",
      "        3.8290e-04, 5.9557e-04, 7.6294e-04, 4.6730e-04, 4.2295e-04, 2.5058e-04,\n",
      "        1.7500e-04, 4.3821e-04, 2.2292e-04, 2.2912e-04, 2.1782e-03, 1.4877e-03,\n",
      "        7.6771e-04, 4.6182e-04, 7.1001e-04, 3.3474e-04, 3.7336e-04, 2.5201e-04,\n",
      "        4.1008e-04, 2.1133e-03, 1.5411e-03, 2.6536e-04, 3.7346e-03, 2.8152e-03,\n",
      "        1.8377e-03, 4.8523e-03, 7.5436e-04, 1.5907e-03, 5.0201e-03, 1.3685e-03,\n",
      "        1.3628e-03, 1.1749e-03, 3.0746e-03, 1.1406e-03, 1.1797e-03, 6.1083e-04,\n",
      "        3.0212e-03, 4.6921e-03, 4.0321e-03, 1.2779e-02, 8.6517e-03, 1.3245e-02,\n",
      "        6.0616e-03, 7.1239e-04, 1.3756e-02, 8.6746e-03, 8.7204e-03, 5.8861e-03,\n",
      "        7.2250e-03, 6.1378e-03, 1.0300e-02, 7.0801e-03, 3.1204e-03, 1.3657e-02,\n",
      "        2.0172e-02, 9.0027e-03, 9.7885e-03, 5.3558e-03, 6.7215e-03, 1.3077e-02,\n",
      "        7.8278e-03, 1.1375e-02, 4.3602e-03, 1.3031e-02, 1.4191e-02, 5.7793e-03,\n",
      "        4.8714e-03, 6.9160e-03, 6.9313e-03, 5.8556e-03, 1.9821e-02, 5.0011e-03,\n",
      "        1.3000e-02, 1.1871e-02, 5.4131e-03, 1.1768e-03, 1.2741e-02, 1.0063e-02,\n",
      "        5.9128e-03, 8.5297e-03, 7.2670e-03, 8.2474e-03, 4.6883e-03, 4.2763e-03,\n",
      "        3.8490e-03, 5.3883e-04, 4.0359e-03, 5.4131e-03, 1.0674e-02, 1.8295e-02,\n",
      "        1.4938e-02, 1.2482e-02, 2.2797e-02, 2.1164e-02, 1.0147e-02, 8.0261e-03,\n",
      "        1.0185e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [56] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [56] : torch.Size([1, 32, 1, 128])\n",
      "Last layer attentions for generated token [56] : tensor([1.7554e-01, 1.7517e-01, 1.4126e-04, 3.3355e-04, 8.4043e-05, 3.1033e-03,\n",
      "        4.8733e-04, 5.3406e-04, 4.5776e-04, 4.9400e-04, 1.8864e-03, 2.3520e-04,\n",
      "        5.1785e-04, 9.0694e-04, 1.1322e-02, 9.1858e-03, 4.2915e-04, 4.7588e-04,\n",
      "        1.9538e-04, 3.5167e-04, 8.4043e-05, 2.1839e-04, 8.2626e-03, 1.2112e-03,\n",
      "        7.6141e-03, 1.8753e-02, 1.7586e-03, 4.3249e-04, 5.0545e-04, 3.9315e-04,\n",
      "        5.3215e-04, 8.9598e-04, 5.9366e-04, 5.3501e-04, 2.1124e-04, 3.2210e-04,\n",
      "        3.1281e-04, 5.0783e-04, 2.9159e-04, 2.2745e-04, 2.3270e-03, 2.2030e-03,\n",
      "        6.2323e-04, 7.9107e-04, 7.3004e-04, 3.3951e-04, 2.5773e-04, 1.7929e-04,\n",
      "        4.1103e-04, 2.6646e-03, 2.1400e-03, 1.5092e-04, 3.3817e-03, 2.5940e-03,\n",
      "        2.5597e-03, 6.6452e-03, 7.5340e-04, 1.8244e-03, 5.0240e-03, 1.0061e-03,\n",
      "        1.0624e-03, 1.3428e-03, 2.3270e-03, 9.2125e-04, 7.5912e-04, 5.0068e-04,\n",
      "        4.1466e-03, 2.1629e-03, 2.5597e-03, 1.4763e-02, 8.7128e-03, 1.8600e-02,\n",
      "        4.2877e-03, 7.2718e-04, 9.3307e-03, 1.1711e-02, 9.9869e-03, 8.1711e-03,\n",
      "        5.0049e-03, 7.5874e-03, 7.4310e-03, 1.0307e-02, 2.6474e-03, 8.7891e-03,\n",
      "        2.1408e-02, 9.3231e-03, 8.4686e-03, 5.5885e-03, 8.7128e-03, 1.4572e-02,\n",
      "        6.1264e-03, 5.6496e-03, 2.1992e-03, 6.7940e-03, 9.8953e-03, 4.2496e-03,\n",
      "        3.3169e-03, 5.1384e-03, 6.5994e-03, 6.0043e-03, 1.4862e-02, 4.3488e-03,\n",
      "        9.6436e-03, 1.7166e-02, 3.1433e-03, 1.8177e-03, 1.1978e-02, 9.7580e-03,\n",
      "        4.4327e-03, 5.8632e-03, 4.3564e-03, 5.9547e-03, 3.6926e-03, 2.8400e-03,\n",
      "        3.1433e-03, 3.7003e-04, 2.5425e-03, 3.1624e-03, 8.0948e-03, 2.1835e-02,\n",
      "        1.3351e-02, 5.7487e-03, 1.0239e-02, 3.2959e-02, 1.9409e-02, 1.3924e-02,\n",
      "        1.5671e-02, 1.9684e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [57] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [57] : torch.Size([1, 32, 1, 129])\n",
      "Last layer attentions for generated token [57] : tensor([2.8223e-01, 2.8223e-01, 1.8692e-04, 3.5882e-04, 1.0002e-04, 3.9940e-03,\n",
      "        3.9482e-04, 6.7043e-04, 8.1968e-04, 4.7445e-04, 1.6403e-03, 3.0923e-04,\n",
      "        3.9411e-04, 6.7282e-04, 6.8626e-03, 8.0566e-03, 3.6955e-04, 2.2900e-04,\n",
      "        1.2302e-04, 3.1853e-04, 8.5890e-05, 2.2459e-04, 7.9269e-03, 7.5817e-04,\n",
      "        3.2845e-03, 1.8295e-02, 9.6798e-04, 2.1255e-04, 2.3806e-04, 4.0412e-04,\n",
      "        3.9411e-04, 5.4598e-04, 4.9734e-04, 5.7030e-04, 2.6250e-04, 1.7560e-04,\n",
      "        1.8322e-04, 5.8937e-04, 2.0766e-04, 1.1975e-04, 2.0294e-03, 1.3618e-03,\n",
      "        3.2735e-04, 3.4237e-04, 6.2370e-04, 2.1505e-04, 5.1498e-04, 1.5986e-04,\n",
      "        3.2997e-04, 1.6527e-03, 1.0185e-03, 1.8120e-04, 3.1033e-03, 2.5902e-03,\n",
      "        2.0847e-03, 4.9553e-03, 4.2033e-04, 9.7752e-04, 3.1147e-03, 5.8031e-04,\n",
      "        4.1533e-04, 7.1526e-04, 1.7452e-03, 5.4598e-04, 4.2367e-04, 3.2663e-04,\n",
      "        1.5955e-03, 2.0447e-03, 1.3809e-03, 1.3008e-02, 7.4844e-03, 1.3023e-02,\n",
      "        4.4632e-03, 7.1764e-04, 1.0712e-02, 9.7351e-03, 8.4915e-03, 5.0545e-03,\n",
      "        4.6501e-03, 6.7101e-03, 6.1035e-03, 6.4240e-03, 2.3842e-03, 9.5749e-03,\n",
      "        1.2543e-02, 5.4359e-03, 4.0245e-03, 2.5749e-03, 6.3705e-03, 1.3863e-02,\n",
      "        2.8362e-03, 5.4131e-03, 1.3466e-03, 3.2654e-03, 5.8136e-03, 3.9177e-03,\n",
      "        2.8954e-03, 4.9286e-03, 4.2686e-03, 4.4022e-03, 9.5749e-03, 3.2101e-03,\n",
      "        5.8823e-03, 7.3853e-03, 2.1687e-03, 1.3151e-03, 5.4398e-03, 5.0430e-03,\n",
      "        4.4479e-03, 3.1452e-03, 4.2152e-03, 3.1452e-03, 1.4000e-03, 9.7942e-04,\n",
      "        1.5230e-03, 1.3411e-04, 1.1206e-03, 1.9703e-03, 4.7073e-03, 7.1335e-03,\n",
      "        8.8425e-03, 7.0000e-03, 7.3891e-03, 9.5978e-03, 5.6419e-03, 7.1068e-03,\n",
      "        4.2992e-03, 9.2926e-03, 1.2123e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [58] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [58] : torch.Size([1, 32, 1, 130])\n",
      "Last layer attentions for generated token [58] : tensor([2.4634e-01, 2.4634e-01, 1.1384e-04, 3.2115e-04, 5.5015e-05, 1.3542e-03,\n",
      "        3.6883e-04, 5.2118e-04, 4.8876e-04, 3.3784e-04, 1.2407e-03, 2.6989e-04,\n",
      "        2.4676e-04, 5.3644e-04, 5.5122e-03, 5.4893e-03, 2.7204e-04, 3.0279e-04,\n",
      "        1.4281e-04, 1.8406e-04, 6.1631e-05, 1.7560e-04, 5.7602e-03, 5.0592e-04,\n",
      "        2.8019e-03, 1.5656e-02, 7.6294e-04, 3.0231e-04, 2.2995e-04, 2.6560e-04,\n",
      "        2.6560e-04, 4.4847e-04, 6.4373e-04, 4.6349e-04, 2.9182e-04, 1.5497e-04,\n",
      "        2.6870e-04, 4.3035e-04, 2.1851e-04, 1.5318e-04, 2.0638e-03, 9.7942e-04,\n",
      "        3.4571e-04, 4.6706e-04, 7.0429e-04, 1.8120e-04, 5.0116e-04, 2.3532e-04,\n",
      "        3.0279e-04, 1.3924e-03, 1.0653e-03, 2.0444e-04, 2.2182e-03, 2.5597e-03,\n",
      "        1.1034e-03, 3.1433e-03, 3.7837e-04, 9.3460e-04, 2.2030e-03, 7.4196e-04,\n",
      "        3.9721e-04, 7.6723e-04, 1.3762e-03, 5.8460e-04, 5.6124e-04, 3.4857e-04,\n",
      "        1.3361e-03, 1.3809e-03, 1.5926e-03, 1.0559e-02, 6.6719e-03, 1.1528e-02,\n",
      "        3.6469e-03, 6.1750e-04, 8.1253e-03, 7.1449e-03, 6.2332e-03, 5.3253e-03,\n",
      "        3.5210e-03, 6.6185e-03, 6.1798e-03, 6.0692e-03, 1.7462e-03, 7.5722e-03,\n",
      "        9.0332e-03, 5.1918e-03, 5.1308e-03, 2.2297e-03, 5.1308e-03, 1.4549e-02,\n",
      "        5.1308e-03, 7.4730e-03, 1.7490e-03, 5.6763e-03, 7.9880e-03, 4.9400e-03,\n",
      "        3.9253e-03, 7.3280e-03, 7.8049e-03, 4.6120e-03, 1.0971e-02, 4.2305e-03,\n",
      "        7.9269e-03, 7.7744e-03, 1.9064e-03, 7.6723e-04, 7.6332e-03, 8.1329e-03,\n",
      "        5.3558e-03, 7.6523e-03, 8.4076e-03, 7.9269e-03, 2.8820e-03, 2.8534e-03,\n",
      "        3.0880e-03, 4.3631e-04, 3.4046e-03, 3.9062e-03, 1.3451e-02, 1.5259e-02,\n",
      "        1.3214e-02, 9.2926e-03, 1.2032e-02, 1.3550e-02, 7.5684e-03, 1.3412e-02,\n",
      "        9.3231e-03, 1.1261e-02, 1.7563e-02, 9.0714e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [59] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [59] : torch.Size([1, 32, 1, 131])\n",
      "Last layer attentions for generated token [59] : tensor([2.5317e-01, 2.5317e-01, 1.2720e-04, 3.0923e-04, 6.6757e-05, 1.2398e-03,\n",
      "        2.7347e-04, 4.5991e-04, 5.8699e-04, 4.3631e-04, 9.8896e-04, 3.4499e-04,\n",
      "        1.9324e-04, 4.2605e-04, 5.4817e-03, 5.2376e-03, 3.9339e-04, 3.4571e-04,\n",
      "        1.6987e-04, 1.5402e-04, 6.0081e-05, 2.4235e-04, 5.7755e-03, 5.6982e-04,\n",
      "        3.2749e-03, 1.0811e-02, 8.4925e-04, 2.9683e-04, 2.6202e-04, 3.4976e-04,\n",
      "        2.5892e-04, 3.3450e-04, 6.9714e-04, 4.2200e-04, 4.4823e-04, 2.1887e-04,\n",
      "        1.9169e-04, 4.7994e-04, 2.9230e-04, 1.9622e-04, 2.1553e-03, 1.1339e-03,\n",
      "        3.5119e-04, 4.9829e-04, 7.4911e-04, 2.3949e-04, 4.0579e-04, 3.1543e-04,\n",
      "        3.7169e-04, 1.8654e-03, 1.2503e-03, 2.2674e-04, 2.5806e-03, 3.5896e-03,\n",
      "        1.1377e-03, 3.1452e-03, 4.3631e-04, 1.2455e-03, 2.3079e-03, 8.6069e-04,\n",
      "        5.1594e-04, 7.4768e-04, 1.7977e-03, 7.7152e-04, 6.0558e-04, 5.2595e-04,\n",
      "        1.6661e-03, 1.7052e-03, 1.6146e-03, 1.1940e-02, 6.8932e-03, 1.2077e-02,\n",
      "        2.3212e-03, 6.1750e-04, 7.1869e-03, 8.0032e-03, 4.9591e-03, 6.0310e-03,\n",
      "        4.1580e-03, 6.6872e-03, 8.4763e-03, 5.8899e-03, 1.6594e-03, 7.9269e-03,\n",
      "        1.0437e-02, 4.6577e-03, 4.2839e-03, 1.2302e-03, 3.3150e-03, 1.1383e-02,\n",
      "        5.6648e-03, 7.4997e-03, 2.0676e-03, 5.0850e-03, 7.1068e-03, 3.9215e-03,\n",
      "        3.6564e-03, 7.9346e-03, 8.0719e-03, 6.0310e-03, 1.3596e-02, 5.8403e-03,\n",
      "        8.2245e-03, 7.9727e-03, 1.9064e-03, 7.1621e-04, 6.0349e-03, 5.7678e-03,\n",
      "        3.6659e-03, 5.2223e-03, 6.4926e-03, 7.7629e-03, 2.5902e-03, 2.3117e-03,\n",
      "        2.3804e-03, 3.7670e-04, 3.6659e-03, 3.8795e-03, 9.7046e-03, 1.1833e-02,\n",
      "        1.1948e-02, 1.2039e-02, 1.4778e-02, 1.5442e-02, 6.8474e-03, 1.0277e-02,\n",
      "        7.6256e-03, 9.5901e-03, 1.1017e-02, 7.9346e-03, 1.1154e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [60] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [60] : torch.Size([1, 32, 1, 132])\n",
      "Last layer attentions for generated token [60] : tensor([1.9873e-01, 1.9873e-01, 2.4247e-04, 7.4530e-04, 1.5175e-04, 1.9426e-03,\n",
      "        2.3961e-04, 2.2686e-04, 4.1485e-04, 5.6362e-04, 1.4677e-03, 3.0351e-04,\n",
      "        3.5143e-04, 8.5783e-04, 1.4526e-02, 1.2550e-02, 6.2752e-04, 7.7629e-04,\n",
      "        3.1137e-04, 3.7336e-04, 1.5962e-04, 3.9887e-04, 6.9084e-03, 1.1339e-03,\n",
      "        7.2632e-03, 1.1009e-02, 2.6455e-03, 5.1022e-04, 6.4611e-04, 3.6168e-04,\n",
      "        4.4322e-04, 7.2813e-04, 6.1560e-04, 2.6774e-04, 2.4915e-04, 5.1403e-04,\n",
      "        2.7680e-04, 3.6597e-04, 3.4928e-04, 3.3522e-04, 3.3703e-03, 2.9182e-03,\n",
      "        1.1816e-03, 5.2309e-04, 4.6086e-04, 5.3263e-04, 1.3602e-04, 2.3770e-04,\n",
      "        2.4629e-04, 2.1839e-03, 1.9178e-03, 1.6463e-04, 4.4518e-03, 5.5580e-03,\n",
      "        2.0752e-03, 4.0932e-03, 1.0471e-03, 3.0384e-03, 6.7978e-03, 1.8120e-03,\n",
      "        1.7986e-03, 1.5469e-03, 3.4904e-03, 2.0790e-03, 1.8663e-03, 8.3303e-04,\n",
      "        8.7662e-03, 3.0270e-03, 3.8147e-03, 1.9882e-02, 1.0338e-02, 1.4336e-02,\n",
      "        2.9888e-03, 6.9046e-04, 4.4250e-03, 6.3286e-03, 3.8223e-03, 4.1428e-03,\n",
      "        3.9043e-03, 4.4327e-03, 7.3204e-03, 5.5122e-03, 3.1967e-03, 6.3400e-03,\n",
      "        1.4618e-02, 5.9891e-03, 5.9319e-03, 4.4861e-03, 3.0003e-03, 4.5166e-03,\n",
      "        7.9956e-03, 9.4223e-03, 5.6305e-03, 4.9667e-03, 6.9504e-03, 2.8992e-03,\n",
      "        2.5635e-03, 5.7487e-03, 7.0686e-03, 4.0245e-03, 1.0254e-02, 1.0406e-02,\n",
      "        9.4910e-03, 1.6281e-02, 6.2752e-03, 1.7633e-03, 6.7978e-03, 7.4081e-03,\n",
      "        2.5063e-03, 3.4866e-03, 3.3798e-03, 5.4893e-03, 2.7523e-03, 2.5234e-03,\n",
      "        1.6336e-03, 4.4155e-04, 2.7199e-03, 3.5763e-03, 5.9013e-03, 1.0826e-02,\n",
      "        1.2062e-02, 9.1629e-03, 1.2032e-02, 3.3356e-02, 1.6693e-02, 5.5161e-03,\n",
      "        1.4153e-02, 9.1476e-03, 1.0956e-02, 8.7204e-03, 7.8201e-03, 1.3756e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [61] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [61] : torch.Size([1, 32, 1, 133])\n",
      "Last layer attentions for generated token [61] : tensor([2.5488e-01, 2.5488e-01, 1.5664e-04, 3.1042e-04, 1.2791e-04, 1.7443e-03,\n",
      "        1.4377e-04, 2.5439e-04, 4.4012e-04, 3.3951e-04, 1.5392e-03, 2.7609e-04,\n",
      "        2.1327e-04, 5.0259e-04, 9.7504e-03, 1.1627e-02, 4.2582e-04, 5.4359e-04,\n",
      "        3.7718e-04, 2.5630e-04, 9.6917e-05, 2.6083e-04, 7.8812e-03, 1.2083e-03,\n",
      "        4.5776e-03, 1.1749e-02, 1.4658e-03, 4.5967e-04, 5.7888e-04, 4.8923e-04,\n",
      "        6.0177e-04, 9.8991e-04, 4.8351e-04, 2.4891e-04, 2.0194e-04, 2.1923e-04,\n",
      "        1.2010e-04, 2.5582e-04, 1.5604e-04, 1.7273e-04, 2.1420e-03, 1.4496e-03,\n",
      "        5.1546e-04, 3.1638e-04, 3.2401e-04, 3.2973e-04, 2.0587e-04, 1.5306e-04,\n",
      "        2.0754e-04, 1.2035e-03, 9.3555e-04, 1.5664e-04, 2.8038e-03, 4.0207e-03,\n",
      "        1.5392e-03, 3.3398e-03, 7.2432e-04, 2.0714e-03, 5.6992e-03, 1.1921e-03,\n",
      "        1.5459e-03, 2.0504e-03, 3.2711e-03, 1.5574e-03, 2.0733e-03, 8.0347e-04,\n",
      "        4.1046e-03, 2.4624e-03, 2.7237e-03, 1.8951e-02, 8.3771e-03, 1.1642e-02,\n",
      "        1.5945e-03, 4.1604e-04, 5.3558e-03, 6.0730e-03, 4.9591e-03, 2.5215e-03,\n",
      "        3.1567e-03, 3.5686e-03, 5.1231e-03, 3.0403e-03, 2.7962e-03, 7.3814e-03,\n",
      "        9.4910e-03, 4.6730e-03, 3.1109e-03, 2.4891e-03, 3.0460e-03, 3.8033e-03,\n",
      "        3.3016e-03, 7.2441e-03, 3.7403e-03, 4.9706e-03, 5.4016e-03, 2.9793e-03,\n",
      "        2.2678e-03, 3.8300e-03, 4.1237e-03, 4.2839e-03, 1.2024e-02, 8.4152e-03,\n",
      "        1.4702e-02, 1.1154e-02, 3.9005e-03, 1.6289e-03, 7.9956e-03, 6.5117e-03,\n",
      "        2.8820e-03, 3.9597e-03, 2.9526e-03, 3.9864e-03, 1.8806e-03, 1.4238e-03,\n",
      "        1.5612e-03, 2.0671e-04, 1.4744e-03, 3.1109e-03, 5.4245e-03, 7.3547e-03,\n",
      "        9.8953e-03, 7.0267e-03, 1.1353e-02, 1.9867e-02, 9.9182e-03, 7.3242e-03,\n",
      "        7.2479e-03, 6.3705e-03, 8.0261e-03, 6.9237e-03, 8.3313e-03, 1.3741e-02,\n",
      "        6.7329e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [62] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [62] : torch.Size([1, 32, 1, 134])\n",
      "Last layer attentions for generated token [62] : tensor([2.4792e-01, 2.4744e-01, 3.0541e-04, 4.0150e-04, 1.5056e-04, 2.4643e-03,\n",
      "        2.0421e-04, 3.8147e-04, 6.0129e-04, 4.0293e-04, 1.5697e-03, 3.0899e-04,\n",
      "        1.7810e-04, 6.3181e-04, 1.0223e-02, 1.0384e-02, 5.5075e-04, 5.9795e-04,\n",
      "        3.6192e-04, 2.0909e-04, 1.5724e-04, 5.3406e-04, 8.6288e-03, 1.8024e-03,\n",
      "        4.6844e-03, 1.2894e-02, 1.5631e-03, 5.4026e-04, 7.3862e-04, 7.2813e-04,\n",
      "        7.2956e-04, 1.5326e-03, 5.1641e-04, 3.3665e-04, 2.4986e-04, 2.0504e-04,\n",
      "        1.1021e-04, 4.2152e-04, 1.8239e-04, 1.9038e-04, 2.0332e-03, 1.1063e-03,\n",
      "        3.1567e-04, 2.9659e-04, 3.2640e-04, 2.7370e-04, 2.8181e-04, 1.4937e-04,\n",
      "        2.7704e-04, 1.2341e-03, 1.0090e-03, 2.4819e-04, 2.6798e-03, 6.3438e-03,\n",
      "        1.0805e-03, 3.8872e-03, 5.8985e-04, 2.1591e-03, 4.6997e-03, 1.0910e-03,\n",
      "        1.2054e-03, 2.5673e-03, 3.6526e-03, 1.5211e-03, 1.2913e-03, 7.5293e-04,\n",
      "        2.4261e-03, 3.3035e-03, 2.1858e-03, 1.8967e-02, 6.8588e-03, 1.0712e-02,\n",
      "        1.6222e-03, 3.7050e-04, 6.4240e-03, 7.6904e-03, 7.3891e-03, 2.9030e-03,\n",
      "        4.1313e-03, 3.5572e-03, 5.7182e-03, 3.6697e-03, 2.7523e-03, 1.2108e-02,\n",
      "        1.0590e-02, 4.4136e-03, 2.6512e-03, 1.5507e-03, 2.6703e-03, 4.8637e-03,\n",
      "        3.5706e-03, 8.6365e-03, 4.2839e-03, 6.2447e-03, 5.3940e-03, 3.2997e-03,\n",
      "        2.6093e-03, 3.8395e-03, 2.6913e-03, 4.3030e-03, 1.3336e-02, 7.8506e-03,\n",
      "        1.6464e-02, 1.2543e-02, 6.0043e-03, 2.5902e-03, 9.6664e-03, 6.3553e-03,\n",
      "        3.7498e-03, 3.6354e-03, 3.0689e-03, 3.9062e-03, 2.3041e-03, 1.6069e-03,\n",
      "        2.4490e-03, 2.7061e-04, 1.8816e-03, 3.0537e-03, 4.2000e-03, 5.1498e-03,\n",
      "        6.0844e-03, 6.9199e-03, 9.0103e-03, 1.4801e-02, 7.2708e-03, 6.2561e-03,\n",
      "        6.8550e-03, 8.4839e-03, 6.0349e-03, 6.0730e-03, 7.9880e-03, 1.2093e-02,\n",
      "        7.9346e-03, 6.5956e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [63] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [63] : torch.Size([1, 32, 1, 135])\n",
      "Last layer attentions for generated token [63] : tensor([2.0630e-01, 2.0593e-01, 3.8457e-04, 4.9448e-04, 1.9836e-04, 3.4199e-03,\n",
      "        2.7537e-04, 3.7122e-04, 6.5136e-04, 5.6171e-04, 2.0084e-03, 3.2949e-04,\n",
      "        3.3402e-04, 8.7833e-04, 8.2397e-03, 1.3290e-02, 4.4179e-04, 5.8413e-04,\n",
      "        4.5657e-04, 4.0841e-04, 2.5058e-04, 5.4026e-04, 8.5907e-03, 1.5240e-03,\n",
      "        4.6730e-03, 1.4610e-02, 1.4229e-03, 9.2077e-04, 9.1887e-04, 6.3896e-04,\n",
      "        9.9754e-04, 1.5173e-03, 8.2207e-04, 3.2258e-04, 2.6727e-04, 5.2643e-04,\n",
      "        2.4772e-04, 4.2653e-04, 3.6693e-04, 3.3140e-04, 3.0365e-03, 1.8597e-03,\n",
      "        6.9094e-04, 5.1928e-04, 4.6659e-04, 5.0735e-04, 2.6584e-04, 3.1376e-04,\n",
      "        5.3692e-04, 1.7748e-03, 1.6155e-03, 2.1112e-04, 4.6425e-03, 4.8866e-03,\n",
      "        2.3708e-03, 5.1231e-03, 4.9543e-04, 2.6398e-03, 4.5700e-03, 1.4830e-03,\n",
      "        1.3847e-03, 2.1534e-03, 2.8076e-03, 1.7757e-03, 1.6251e-03, 5.4741e-04,\n",
      "        5.0735e-03, 3.0651e-03, 3.1357e-03, 1.4000e-02, 8.7357e-03, 1.7731e-02,\n",
      "        2.6455e-03, 5.2977e-04, 5.8708e-03, 8.5220e-03, 5.9624e-03, 4.6577e-03,\n",
      "        3.9825e-03, 3.3112e-03, 5.1308e-03, 5.4665e-03, 2.3632e-03, 7.1869e-03,\n",
      "        1.3206e-02, 3.9444e-03, 4.4556e-03, 1.7605e-03, 2.9259e-03, 6.5079e-03,\n",
      "        5.5847e-03, 7.7209e-03, 4.9706e-03, 5.4741e-03, 8.1940e-03, 4.0932e-03,\n",
      "        3.7022e-03, 5.6763e-03, 4.3106e-03, 4.9362e-03, 1.1696e-02, 1.0498e-02,\n",
      "        1.1818e-02, 1.7136e-02, 4.4022e-03, 2.2278e-03, 7.2708e-03, 6.5155e-03,\n",
      "        3.3321e-03, 4.0398e-03, 4.5013e-03, 5.3825e-03, 2.4128e-03, 2.6588e-03,\n",
      "        2.8019e-03, 5.6839e-04, 2.8267e-03, 2.9335e-03, 4.1351e-03, 9.0790e-03,\n",
      "        6.5117e-03, 5.0468e-03, 9.1782e-03, 2.5848e-02, 9.6741e-03, 5.1918e-03,\n",
      "        1.3283e-02, 1.2611e-02, 9.6054e-03, 7.6714e-03, 9.1019e-03, 1.6678e-02,\n",
      "        1.1520e-02, 7.5607e-03, 5.7068e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [64] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [64] : torch.Size([1, 32, 1, 136])\n",
      "Last layer attentions for generated token [64] : tensor([2.0203e-01, 2.0203e-01, 2.1660e-04, 4.2582e-04, 1.2147e-04, 2.0885e-03,\n",
      "        3.0851e-04, 5.1069e-04, 8.0156e-04, 5.1069e-04, 1.7977e-03, 6.6853e-04,\n",
      "        2.5272e-04, 4.5133e-04, 1.1131e-02, 7.6866e-03, 4.9400e-04, 5.8317e-04,\n",
      "        4.3750e-04, 3.6907e-04, 4.9400e-04, 8.5020e-04, 8.7128e-03, 1.4687e-03,\n",
      "        7.5760e-03, 1.3863e-02, 1.1597e-03, 4.8518e-04, 4.5490e-04, 7.0906e-04,\n",
      "        1.3933e-03, 1.8387e-03, 7.7391e-04, 4.6945e-04, 4.9686e-04, 4.4608e-04,\n",
      "        7.6354e-05, 4.5776e-04, 1.9491e-04, 1.4770e-04, 2.8915e-03, 1.0805e-03,\n",
      "        3.9458e-04, 4.7684e-04, 7.1001e-04, 2.1076e-04, 8.0156e-04, 2.9850e-04,\n",
      "        4.5776e-04, 2.8286e-03, 1.2054e-03, 2.2173e-04, 2.8400e-03, 3.1185e-03,\n",
      "        9.9564e-04, 4.2419e-03, 3.8242e-04, 1.8511e-03, 5.5618e-03, 2.2831e-03,\n",
      "        1.0595e-03, 2.4719e-03, 5.9166e-03, 1.6851e-03, 1.7185e-03, 1.2178e-03,\n",
      "        2.2640e-03, 2.4567e-03, 3.0842e-03, 1.1673e-02, 5.9319e-03, 1.2077e-02,\n",
      "        3.9330e-03, 6.4039e-04, 1.2222e-02, 5.8517e-03, 6.2599e-03, 3.5305e-03,\n",
      "        2.2240e-03, 4.3221e-03, 6.6414e-03, 4.6730e-03, 2.0618e-03, 1.1040e-02,\n",
      "        8.0414e-03, 3.4122e-03, 2.8458e-03, 9.3174e-04, 3.6278e-03, 9.2010e-03,\n",
      "        4.3411e-03, 9.3918e-03, 2.6360e-03, 1.0551e-02, 7.8201e-03, 3.3321e-03,\n",
      "        2.3155e-03, 8.1940e-03, 5.3139e-03, 6.9160e-03, 1.3161e-02, 6.4468e-03,\n",
      "        1.0071e-02, 9.2697e-03, 2.7981e-03, 1.0033e-03, 9.2239e-03, 5.8937e-03,\n",
      "        3.7041e-03, 5.3062e-03, 5.0850e-03, 5.1727e-03, 2.8400e-03, 2.8820e-03,\n",
      "        3.9368e-03, 5.6744e-04, 3.8548e-03, 3.9711e-03, 8.3313e-03, 1.0048e-02,\n",
      "        1.0727e-02, 9.5520e-03, 1.3298e-02, 1.3382e-02, 4.7722e-03, 6.6528e-03,\n",
      "        1.3878e-02, 6.3438e-03, 1.2665e-02, 9.6359e-03, 1.1490e-02, 2.4597e-02,\n",
      "        8.5983e-03, 4.1084e-03, 7.5989e-03, 1.7532e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [65] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [65] : torch.Size([1, 32, 1, 137])\n",
      "Last layer attentions for generated token [65] : tensor([2.4329e-01, 2.4377e-01, 1.3149e-04, 2.6655e-04, 7.2896e-05, 1.1854e-03,\n",
      "        1.5426e-04, 2.0278e-04, 4.7421e-04, 3.3164e-04, 1.1969e-03, 2.2805e-04,\n",
      "        2.0039e-04, 4.8089e-04, 9.3384e-03, 8.9569e-03, 2.3615e-04, 4.4727e-04,\n",
      "        2.4939e-04, 2.8491e-04, 1.3196e-04, 2.0432e-04, 6.0120e-03, 7.8821e-04,\n",
      "        4.8218e-03, 1.2581e-02, 7.0906e-04, 3.7146e-04, 4.2772e-04, 3.0136e-04,\n",
      "        4.6611e-04, 1.0500e-03, 4.3511e-04, 2.8419e-04, 2.1255e-04, 3.4904e-04,\n",
      "        2.0039e-04, 3.5667e-04, 1.9884e-04, 1.6618e-04, 2.3136e-03, 1.4439e-03,\n",
      "        3.0375e-04, 3.5167e-04, 4.3941e-04, 2.7943e-04, 2.5630e-04, 2.0599e-04,\n",
      "        2.6250e-04, 1.7242e-03, 1.5039e-03, 1.2994e-04, 3.0708e-03, 1.8797e-03,\n",
      "        1.5106e-03, 2.8095e-03, 5.6744e-04, 1.4725e-03, 4.5624e-03, 1.0624e-03,\n",
      "        1.1005e-03, 1.0462e-03, 2.0084e-03, 1.2941e-03, 1.0080e-03, 5.8460e-04,\n",
      "        3.1910e-03, 1.7385e-03, 3.3627e-03, 1.0666e-02, 6.3400e-03, 1.0689e-02,\n",
      "        1.8978e-03, 3.2091e-04, 5.1231e-03, 6.2599e-03, 4.0627e-03, 3.2444e-03,\n",
      "        3.2501e-03, 3.6507e-03, 4.1733e-03, 4.2839e-03, 2.0962e-03, 5.0583e-03,\n",
      "        1.0033e-02, 5.5504e-03, 3.6507e-03, 2.2087e-03, 2.7828e-03, 5.2032e-03,\n",
      "        2.4395e-03, 5.6381e-03, 2.9106e-03, 4.2076e-03, 6.0272e-03, 3.5706e-03,\n",
      "        2.6245e-03, 4.9477e-03, 5.1384e-03, 4.2763e-03, 1.2466e-02, 5.2986e-03,\n",
      "        1.0658e-02, 1.0551e-02, 4.2877e-03, 1.4019e-03, 6.4011e-03, 7.1106e-03,\n",
      "        3.8490e-03, 4.9629e-03, 3.8795e-03, 3.9749e-03, 1.6975e-03, 2.3918e-03,\n",
      "        1.9207e-03, 3.8934e-04, 2.4586e-03, 3.9291e-03, 7.5684e-03, 1.1711e-02,\n",
      "        1.2276e-02, 7.8125e-03, 1.3451e-02, 1.8188e-02, 9.6664e-03, 7.4959e-03,\n",
      "        9.3460e-03, 7.8430e-03, 1.3725e-02, 1.1833e-02, 1.1993e-02, 1.6891e-02,\n",
      "        7.0724e-03, 3.5381e-03, 4.0207e-03, 8.4763e-03, 4.0817e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [66] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [66] : torch.Size([1, 32, 1, 138])\n",
      "Last layer attentions for generated token [66] : tensor([2.2742e-01, 2.2742e-01, 2.4486e-04, 3.0398e-04, 1.1295e-04, 1.5688e-03,\n",
      "        2.4772e-04, 3.9887e-04, 4.6086e-04, 4.1485e-04, 1.3361e-03, 2.0456e-04,\n",
      "        1.5318e-04, 3.2616e-04, 8.0261e-03, 6.3171e-03, 2.7680e-04, 6.1178e-04,\n",
      "        2.6774e-04, 1.7428e-04, 9.4354e-05, 4.4322e-04, 6.4087e-03, 9.9468e-04,\n",
      "        3.2063e-03, 1.0979e-02, 9.0599e-04, 3.6526e-04, 5.3883e-04, 5.0640e-04,\n",
      "        6.1083e-04, 1.2026e-03, 9.3460e-04, 5.3453e-04, 5.2309e-04, 2.9421e-04,\n",
      "        1.5438e-04, 6.2037e-04, 2.2030e-04, 2.0134e-04, 1.6766e-03, 9.7561e-04,\n",
      "        2.0933e-04, 3.7265e-04, 4.8304e-04, 2.3818e-04, 3.6120e-04, 2.4390e-04,\n",
      "        3.0947e-04, 1.3256e-03, 1.3790e-03, 1.8549e-04, 2.6531e-03, 3.5877e-03,\n",
      "        1.1187e-03, 4.0970e-03, 5.5933e-04, 1.2627e-03, 3.5458e-03, 1.3256e-03,\n",
      "        1.0614e-03, 9.9277e-04, 2.5883e-03, 1.3685e-03, 7.3671e-04, 7.5102e-04,\n",
      "        1.8301e-03, 1.7843e-03, 2.1782e-03, 1.1406e-02, 6.0349e-03, 1.1238e-02,\n",
      "        1.6861e-03, 3.5906e-04, 7.4539e-03, 7.9803e-03, 7.7209e-03, 3.7880e-03,\n",
      "        3.9902e-03, 4.8065e-03, 6.0577e-03, 3.6221e-03, 1.8787e-03, 8.7891e-03,\n",
      "        1.2863e-02, 6.1607e-03, 3.0289e-03, 1.3342e-03, 2.4624e-03, 8.4839e-03,\n",
      "        4.1695e-03, 8.6441e-03, 3.6545e-03, 6.7940e-03, 6.3171e-03, 3.7441e-03,\n",
      "        2.2564e-03, 6.7749e-03, 3.6602e-03, 5.0545e-03, 1.6724e-02, 5.9242e-03,\n",
      "        1.1131e-02, 1.3008e-02, 5.8403e-03, 1.5202e-03, 1.0117e-02, 8.3237e-03,\n",
      "        4.6692e-03, 4.9324e-03, 5.1460e-03, 4.3793e-03, 2.0313e-03, 1.8339e-03,\n",
      "        2.3727e-03, 3.6740e-04, 2.8019e-03, 3.1929e-03, 6.3667e-03, 1.0544e-02,\n",
      "        8.8577e-03, 1.0437e-02, 1.3741e-02, 1.5480e-02, 6.8932e-03, 7.4921e-03,\n",
      "        6.7101e-03, 8.1177e-03, 8.2092e-03, 7.9346e-03, 8.4076e-03, 1.4404e-02,\n",
      "        1.0895e-02, 4.7951e-03, 5.5161e-03, 1.1154e-02, 7.8888e-03, 4.4556e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [67] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [67] : torch.Size([1, 32, 1, 139])\n",
      "Last layer attentions for generated token [67] : tensor([1.0681e-01, 1.0681e-01, 6.5386e-05, 2.2876e-04, 5.9068e-05, 1.2112e-03,\n",
      "        2.4867e-04, 2.4164e-04, 3.8767e-04, 4.0460e-04, 1.6346e-03, 4.3488e-04,\n",
      "        1.9491e-04, 5.4026e-04, 1.7746e-02, 1.1810e-02, 3.0613e-04, 8.4829e-04,\n",
      "        2.8419e-04, 3.7932e-04, 3.3617e-04, 4.5323e-04, 4.9171e-03, 1.3971e-03,\n",
      "        1.0521e-02, 1.1765e-02, 1.5602e-03, 3.7646e-04, 5.2547e-04, 5.4026e-04,\n",
      "        6.5041e-04, 1.5345e-03, 9.0122e-04, 3.2830e-04, 2.9778e-04, 5.2547e-04,\n",
      "        1.7881e-04, 4.0865e-04, 2.5916e-04, 2.1696e-04, 2.7027e-03, 1.6823e-03,\n",
      "        4.9400e-04, 6.4516e-04, 5.2452e-04, 4.9591e-04, 1.7989e-04, 2.3782e-04,\n",
      "        3.5572e-04, 3.8986e-03, 2.6398e-03, 1.2314e-04, 3.1796e-03, 3.1052e-03,\n",
      "        1.5755e-03, 4.4365e-03, 1.4696e-03, 1.9226e-03, 6.8283e-03, 2.3117e-03,\n",
      "        2.9125e-03, 1.8082e-03, 3.8757e-03, 2.9869e-03, 1.3933e-03, 1.3685e-03,\n",
      "        8.6365e-03, 3.5686e-03, 3.5496e-03, 1.8921e-02, 1.0811e-02, 2.3682e-02,\n",
      "        2.3727e-03, 5.0545e-04, 7.8201e-03, 1.2703e-02, 6.5041e-03, 6.1836e-03,\n",
      "        5.0735e-03, 7.6294e-03, 8.2397e-03, 6.9733e-03, 2.2621e-03, 7.3357e-03,\n",
      "        1.8356e-02, 6.8817e-03, 7.4539e-03, 3.1204e-03, 3.0022e-03, 7.3776e-03,\n",
      "        5.4665e-03, 6.9160e-03, 5.2223e-03, 7.8735e-03, 7.7858e-03, 3.2349e-03,\n",
      "        2.5368e-03, 7.7896e-03, 7.2594e-03, 6.0310e-03, 2.3285e-02, 1.0078e-02,\n",
      "        1.5060e-02, 1.7914e-02, 6.2714e-03, 1.4582e-03, 1.3641e-02, 8.3160e-03,\n",
      "        3.0479e-03, 4.0169e-03, 3.6125e-03, 4.7302e-03, 3.9787e-03, 3.0556e-03,\n",
      "        3.1433e-03, 4.1103e-04, 3.9177e-03, 4.2419e-03, 1.0666e-02, 1.5793e-02,\n",
      "        1.3046e-02, 7.7705e-03, 1.4946e-02, 3.0045e-02, 1.7410e-02, 8.1787e-03,\n",
      "        1.8860e-02, 1.7090e-02, 1.4145e-02, 1.6663e-02, 1.1520e-02, 2.6596e-02,\n",
      "        1.1063e-02, 4.5242e-03, 4.1733e-03, 1.3481e-02, 6.6338e-03, 3.8795e-03,\n",
      "        1.2718e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [68] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [68] : torch.Size([1, 32, 1, 140])\n",
      "Last layer attentions for generated token [68] : tensor([2.6318e-01, 2.6318e-01, 5.3704e-05, 1.3185e-04, 3.1412e-05, 9.0790e-04,\n",
      "        1.2922e-04, 1.9181e-04, 3.8075e-04, 2.1565e-04, 1.4753e-03, 4.2796e-04,\n",
      "        1.5056e-04, 5.1737e-04, 1.0506e-02, 7.0648e-03, 2.2995e-04, 3.2115e-04,\n",
      "        2.3365e-04, 2.1946e-04, 2.1267e-04, 4.4942e-04, 8.2245e-03, 1.1120e-03,\n",
      "        4.0321e-03, 8.4076e-03, 4.6182e-04, 4.1246e-04, 5.2023e-04, 4.9543e-04,\n",
      "        5.2023e-04, 6.8521e-04, 3.5763e-04, 3.2878e-04, 3.2568e-04, 5.2977e-04,\n",
      "        2.0301e-04, 4.9067e-04, 2.0540e-04, 1.7190e-04, 1.0014e-03, 5.7507e-04,\n",
      "        1.4424e-04, 2.5654e-04, 3.1924e-04, 3.1567e-04, 2.7037e-04, 2.2781e-04,\n",
      "        3.2616e-04, 1.1959e-03, 8.7118e-04, 7.5996e-05, 2.3289e-03, 1.0471e-03,\n",
      "        1.3237e-03, 2.6894e-03, 6.8951e-04, 1.5478e-03, 3.1662e-03, 1.1063e-03,\n",
      "        1.4391e-03, 1.2980e-03, 2.8210e-03, 1.9484e-03, 1.9274e-03, 1.2827e-03,\n",
      "        3.1681e-03, 1.3447e-03, 2.6608e-03, 6.7596e-03, 3.2463e-03, 1.1185e-02,\n",
      "        8.4639e-04, 2.4152e-04, 5.2872e-03, 6.1874e-03, 2.9564e-03, 3.3970e-03,\n",
      "        3.3627e-03, 5.1804e-03, 3.1967e-03, 2.4509e-03, 1.2627e-03, 4.5319e-03,\n",
      "        7.3395e-03, 3.7193e-03, 2.5482e-03, 1.4057e-03, 2.1744e-03, 5.2185e-03,\n",
      "        1.7776e-03, 3.5629e-03, 3.0098e-03, 2.6417e-03, 3.4771e-03, 2.5024e-03,\n",
      "        2.4185e-03, 7.8888e-03, 4.8218e-03, 5.6992e-03, 1.0529e-02, 6.2027e-03,\n",
      "        1.3603e-02, 1.0872e-02, 2.6302e-03, 7.9203e-04, 7.1449e-03, 4.7340e-03,\n",
      "        3.4103e-03, 2.7103e-03, 1.9197e-03, 3.3112e-03, 9.3651e-04, 1.2436e-03,\n",
      "        9.9754e-04, 1.8227e-04, 1.9217e-03, 3.4637e-03, 4.5967e-03, 7.0686e-03,\n",
      "        6.7902e-03, 7.9041e-03, 9.9640e-03, 1.4816e-02, 6.8245e-03, 6.6452e-03,\n",
      "        1.0071e-02, 5.4855e-03, 6.8779e-03, 9.1400e-03, 1.1833e-02, 2.1683e-02,\n",
      "        8.4305e-03, 6.1836e-03, 4.5967e-03, 1.2497e-02, 5.2338e-03, 4.7264e-03,\n",
      "        1.1452e-02, 5.0087e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [69] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [69] : torch.Size([1, 32, 1, 141])\n",
      "Last layer attentions for generated token [69] : tensor([2.4548e-01, 2.4548e-01, 2.5606e-04, 4.5395e-04, 1.3709e-04, 1.5240e-03,\n",
      "        2.3782e-04, 3.5691e-04, 3.1257e-04, 4.2486e-04, 1.2007e-03, 4.4703e-04,\n",
      "        3.2496e-04, 5.1737e-04, 8.4000e-03, 5.2109e-03, 3.7122e-04, 5.1928e-04,\n",
      "        3.0184e-04, 2.0182e-04, 2.2388e-04, 9.4223e-04, 9.8267e-03, 1.7185e-03,\n",
      "        5.7755e-03, 6.7749e-03, 7.4387e-04, 4.9782e-04, 5.5838e-04, 4.6301e-04,\n",
      "        8.8882e-04, 6.5517e-04, 7.0047e-04, 3.3402e-04, 4.4703e-04, 4.2987e-04,\n",
      "        1.5414e-04, 5.0926e-04, 2.6059e-04, 1.9038e-04, 1.3933e-03, 5.5075e-04,\n",
      "        1.1909e-04, 2.2388e-04, 1.9789e-04, 1.5652e-04, 1.2732e-04, 1.9789e-04,\n",
      "        1.6868e-04, 8.8882e-04, 6.0129e-04, 8.9586e-05, 1.4124e-03, 3.2234e-03,\n",
      "        9.6846e-04, 3.8013e-03, 4.2224e-04, 2.0599e-03, 3.3646e-03, 1.3981e-03,\n",
      "        1.0147e-03, 1.1911e-03, 4.1237e-03, 1.6127e-03, 2.8992e-03, 1.1702e-03,\n",
      "        1.5450e-03, 9.4986e-04, 1.3952e-03, 8.5144e-03, 3.1891e-03, 6.4545e-03,\n",
      "        1.1101e-03, 3.8385e-04, 6.3744e-03, 6.3133e-03, 4.6654e-03, 3.1204e-03,\n",
      "        2.2926e-03, 5.3062e-03, 4.2801e-03, 2.7752e-03, 1.4400e-03, 7.8659e-03,\n",
      "        7.9880e-03, 5.6229e-03, 2.4681e-03, 1.0214e-03, 1.7166e-03, 6.2103e-03,\n",
      "        4.9820e-03, 8.3008e-03, 4.5204e-03, 4.8180e-03, 2.8210e-03, 1.7920e-03,\n",
      "        1.3475e-03, 5.7259e-03, 2.8000e-03, 5.2032e-03, 8.6975e-03, 7.2174e-03,\n",
      "        1.0361e-02, 9.3307e-03, 3.6030e-03, 9.2793e-04, 6.6528e-03, 4.0741e-03,\n",
      "        2.8858e-03, 2.3308e-03, 1.9684e-03, 2.6646e-03, 1.0538e-03, 8.0299e-04,\n",
      "        1.2903e-03, 2.1398e-04, 1.3113e-03, 1.7271e-03, 2.8114e-03, 6.7978e-03,\n",
      "        5.4092e-03, 9.2926e-03, 7.0000e-03, 1.3863e-02, 7.2136e-03, 7.6714e-03,\n",
      "        7.5798e-03, 5.6305e-03, 3.6983e-03, 6.2103e-03, 8.3618e-03, 2.0920e-02,\n",
      "        1.5060e-02, 7.8354e-03, 8.2245e-03, 2.2232e-02, 1.0643e-02, 7.5455e-03,\n",
      "        1.8707e-02, 7.0648e-03, 1.0155e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [70] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [70] : torch.Size([1, 32, 1, 142])\n",
      "Last layer attentions for generated token [70] : tensor([2.0117e-01, 2.0117e-01, 1.4150e-04, 3.5095e-04, 9.6142e-05, 1.0729e-03,\n",
      "        1.4949e-04, 1.8966e-04, 2.6751e-04, 3.3736e-04, 7.3147e-04, 2.5916e-04,\n",
      "        2.2531e-04, 7.2718e-04, 1.7288e-02, 6.9733e-03, 2.4164e-04, 5.4979e-04,\n",
      "        1.2106e-04, 2.2662e-04, 2.5368e-04, 5.6744e-04, 4.7760e-03, 1.2712e-03,\n",
      "        9.9106e-03, 9.0714e-03, 1.3170e-03, 4.4441e-04, 3.4761e-04, 2.8253e-04,\n",
      "        4.8709e-04, 1.2445e-03, 4.5323e-04, 2.7442e-04, 2.5821e-04, 4.6849e-04,\n",
      "        1.4043e-04, 3.0732e-04, 2.9159e-04, 2.1160e-04, 3.0365e-03, 1.2131e-03,\n",
      "        5.5742e-04, 5.5313e-04, 4.3344e-04, 3.9387e-04, 1.7202e-04, 1.4603e-04,\n",
      "        2.5415e-04, 2.9430e-03, 1.9321e-03, 2.0838e-04, 2.0714e-03, 6.9199e-03,\n",
      "        1.3323e-03, 4.0321e-03, 8.5020e-04, 1.6470e-03, 3.5343e-03, 1.1044e-03,\n",
      "        1.4830e-03, 1.2808e-03, 2.8591e-03, 1.7700e-03, 1.3580e-03, 1.1511e-03,\n",
      "        4.2801e-03, 2.3384e-03, 1.9836e-03, 2.6169e-02, 1.2497e-02, 1.7654e-02,\n",
      "        4.0321e-03, 1.0157e-03, 1.4626e-02, 1.4938e-02, 1.2253e-02, 6.5079e-03,\n",
      "        4.9629e-03, 7.2594e-03, 7.2556e-03, 7.4654e-03, 2.9926e-03, 1.0269e-02,\n",
      "        1.4130e-02, 6.6719e-03, 5.0735e-03, 3.1681e-03, 2.5673e-03, 7.7057e-03,\n",
      "        3.0651e-03, 7.0953e-03, 2.8877e-03, 4.6310e-03, 4.4098e-03, 2.0390e-03,\n",
      "        1.1530e-03, 4.0550e-03, 3.1948e-03, 3.5725e-03, 8.7585e-03, 5.9166e-03,\n",
      "        1.0529e-02, 1.2993e-02, 4.5013e-03, 9.5224e-04, 6.0959e-03, 4.6768e-03,\n",
      "        2.0370e-03, 2.0599e-03, 1.5097e-03, 1.6174e-03, 1.3371e-03, 1.3695e-03,\n",
      "        1.6918e-03, 2.6393e-04, 1.2417e-03, 1.7309e-03, 4.7226e-03, 9.1858e-03,\n",
      "        6.9847e-03, 6.1798e-03, 1.0193e-02, 2.2888e-02, 1.1238e-02, 6.7368e-03,\n",
      "        9.1476e-03, 1.1772e-02, 9.9640e-03, 8.4763e-03, 7.5073e-03, 1.5625e-02,\n",
      "        7.0839e-03, 2.5806e-03, 4.3678e-03, 7.2365e-03, 3.3188e-03, 1.5821e-03,\n",
      "        8.6212e-03, 4.5586e-03, 2.4548e-03, 6.3934e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [71] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [71] : torch.Size([1, 32, 1, 143])\n",
      "Last layer attentions for generated token [71] : tensor([1.9299e-01, 1.9299e-01, 1.5116e-04, 3.0780e-04, 1.1772e-04, 1.7328e-03,\n",
      "        2.6941e-04, 2.5702e-04, 3.1495e-04, 3.8600e-04, 1.1959e-03, 3.5405e-04,\n",
      "        2.6989e-04, 5.3692e-04, 7.6714e-03, 8.1100e-03, 2.4915e-04, 3.2067e-04,\n",
      "        1.6022e-04, 1.9109e-04, 1.5295e-04, 2.5964e-04, 2.8591e-03, 8.1062e-04,\n",
      "        7.6294e-03, 1.1047e-02, 1.8845e-03, 3.6621e-04, 2.5058e-04, 2.9659e-04,\n",
      "        3.0589e-04, 1.1015e-03, 1.0757e-03, 7.1001e-04, 3.3402e-04, 4.2963e-04,\n",
      "        1.7738e-04, 3.0231e-04, 1.9109e-04, 1.5473e-04, 1.8797e-03, 1.1034e-03,\n",
      "        4.6277e-04, 6.3896e-04, 7.0143e-04, 2.6274e-04, 2.1899e-04, 9.0599e-05,\n",
      "        2.6846e-04, 2.4376e-03, 1.6994e-03, 3.9673e-04, 2.8038e-03, 3.9024e-03,\n",
      "        1.4725e-03, 4.2648e-03, 1.1187e-03, 1.0996e-03, 2.3441e-03, 1.1215e-03,\n",
      "        1.0090e-03, 1.5068e-03, 2.5597e-03, 1.8005e-03, 9.4223e-04, 8.8501e-04,\n",
      "        4.2305e-03, 5.6534e-03, 1.9569e-03, 2.0935e-02, 1.2947e-02, 2.1606e-02,\n",
      "        4.8256e-03, 7.4387e-04, 1.4961e-02, 1.5144e-02, 1.4503e-02, 7.1411e-03,\n",
      "        7.7057e-03, 5.8327e-03, 7.1449e-03, 8.0795e-03, 3.6659e-03, 9.8190e-03,\n",
      "        1.9348e-02, 9.7351e-03, 7.8201e-03, 5.9586e-03, 5.9357e-03, 8.6365e-03,\n",
      "        3.4733e-03, 4.3564e-03, 1.3895e-03, 3.5458e-03, 6.6528e-03, 3.2730e-03,\n",
      "        1.8864e-03, 5.0087e-03, 5.4283e-03, 4.4327e-03, 1.0704e-02, 6.3591e-03,\n",
      "        1.1909e-02, 1.0193e-02, 1.8845e-03, 5.3883e-04, 8.3694e-03, 6.1417e-03,\n",
      "        2.2640e-03, 2.3937e-03, 2.2640e-03, 2.1992e-03, 1.0223e-03, 9.9564e-04,\n",
      "        1.2608e-03, 1.2982e-04, 1.2484e-03, 2.3670e-03, 5.9013e-03, 1.2665e-02,\n",
      "        1.0010e-02, 6.2180e-03, 1.2466e-02, 1.7853e-02, 1.1032e-02, 7.2670e-03,\n",
      "        7.7591e-03, 1.1078e-02, 1.8250e-02, 8.4152e-03, 7.3929e-03, 1.6541e-02,\n",
      "        4.9171e-03, 1.7414e-03, 3.2406e-03, 7.0648e-03, 2.8133e-03, 9.4414e-04,\n",
      "        5.0278e-03, 1.8959e-03, 7.6437e-04, 3.3398e-03, 3.4294e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [72] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [72] : torch.Size([1, 32, 1, 144])\n",
      "Last layer attentions for generated token [72] : tensor([1.8274e-01, 1.8274e-01, 1.4794e-04, 2.8801e-04, 9.2566e-05, 1.2512e-03,\n",
      "        2.4199e-04, 2.1529e-04, 2.8920e-04, 2.9254e-04, 9.9373e-04, 2.3007e-04,\n",
      "        3.3474e-04, 6.2037e-04, 6.3553e-03, 5.4054e-03, 2.7204e-04, 5.1546e-04,\n",
      "        2.4199e-04, 2.1946e-04, 9.8944e-05, 2.8634e-04, 3.1757e-03, 8.6308e-04,\n",
      "        6.0654e-03, 1.1307e-02, 1.7862e-03, 5.3692e-04, 5.4455e-04, 2.7466e-04,\n",
      "        5.1546e-04, 1.0738e-03, 7.6628e-04, 5.0831e-04, 4.3988e-04, 6.0034e-04,\n",
      "        2.3544e-04, 4.2152e-04, 2.7156e-04, 2.9659e-04, 2.8057e-03, 1.8091e-03,\n",
      "        6.3372e-04, 6.1321e-04, 5.7364e-04, 3.8743e-04, 2.3639e-04, 1.6701e-04,\n",
      "        3.1328e-04, 2.1667e-03, 1.8978e-03, 1.8847e-04, 3.4237e-03, 3.2883e-03,\n",
      "        1.8473e-03, 3.5706e-03, 4.3559e-04, 1.1587e-03, 2.3556e-03, 8.0109e-04,\n",
      "        1.1520e-03, 1.0891e-03, 2.0599e-03, 1.1978e-03, 1.0233e-03, 6.8951e-04,\n",
      "        3.3207e-03, 1.9484e-03, 2.2869e-03, 1.2077e-02, 7.9727e-03, 1.5541e-02,\n",
      "        3.3875e-03, 5.3692e-04, 1.2428e-02, 1.0880e-02, 7.3128e-03, 6.8436e-03,\n",
      "        6.5422e-03, 5.6801e-03, 8.6670e-03, 1.0117e-02, 3.7308e-03, 1.1444e-02,\n",
      "        1.8631e-02, 7.7019e-03, 5.9891e-03, 5.1880e-03, 3.8013e-03, 6.4468e-03,\n",
      "        3.6125e-03, 5.4626e-03, 2.6512e-03, 4.6158e-03, 6.2599e-03, 2.8400e-03,\n",
      "        1.9608e-03, 5.4054e-03, 4.4823e-03, 4.8714e-03, 1.3008e-02, 6.7635e-03,\n",
      "        1.3557e-02, 1.5007e-02, 4.5738e-03, 7.4816e-04, 7.8201e-03, 6.9237e-03,\n",
      "        3.3703e-03, 4.3373e-03, 2.9049e-03, 3.0994e-03, 1.6928e-03, 1.7195e-03,\n",
      "        1.2932e-03, 2.4915e-04, 1.7519e-03, 2.8019e-03, 6.1188e-03, 1.3557e-02,\n",
      "        6.8169e-03, 6.0883e-03, 1.4351e-02, 2.6016e-02, 9.5215e-03, 9.7351e-03,\n",
      "        8.6060e-03, 1.4626e-02, 1.9958e-02, 9.0027e-03, 8.9951e-03, 1.6815e-02,\n",
      "        7.2479e-03, 3.9291e-03, 4.8599e-03, 9.4604e-03, 4.4823e-03, 2.5692e-03,\n",
      "        1.2352e-02, 5.0240e-03, 1.9426e-03, 6.0577e-03, 5.7030e-03, 4.4289e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [73] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [73] : torch.Size([1, 32, 1, 145])\n",
      "Last layer attentions for generated token [73] : tensor([1.5015e-01, 1.5015e-01, 1.0765e-04, 1.8966e-04, 7.2002e-05, 8.5211e-04,\n",
      "        2.5773e-04, 3.6716e-04, 3.8409e-04, 4.3344e-04, 1.6727e-03, 2.5582e-04,\n",
      "        1.8966e-04, 2.7990e-04, 8.6288e-03, 5.3825e-03, 2.4939e-04, 6.6328e-04,\n",
      "        1.8752e-04, 1.9228e-04, 4.4882e-05, 2.2745e-04, 4.6082e-03, 9.2649e-04,\n",
      "        6.0654e-03, 1.0086e-02, 9.8610e-04, 3.6788e-04, 4.5967e-04, 4.7040e-04,\n",
      "        7.1335e-04, 1.3609e-03, 5.6219e-04, 4.5776e-04, 3.6788e-04, 3.9840e-04,\n",
      "        1.9348e-04, 3.5572e-04, 2.4605e-04, 2.1207e-04, 2.9163e-03, 1.4601e-03,\n",
      "        3.3951e-04, 5.2166e-04, 4.9973e-04, 2.7442e-04, 2.7323e-04, 1.9610e-04,\n",
      "        3.2139e-04, 2.2221e-03, 1.5354e-03, 1.1241e-04, 2.7981e-03, 3.0003e-03,\n",
      "        1.3695e-03, 3.0270e-03, 3.9697e-04, 1.2369e-03, 3.0651e-03, 8.9788e-04,\n",
      "        1.3933e-03, 2.2602e-03, 2.6722e-03, 9.4461e-04, 7.8917e-04, 8.3542e-04,\n",
      "        2.3098e-03, 1.5335e-03, 2.3956e-03, 1.3306e-02, 6.9580e-03, 1.1993e-02,\n",
      "        2.8744e-03, 6.8569e-04, 1.1711e-02, 7.3395e-03, 5.8594e-03, 5.9433e-03,\n",
      "        4.8676e-03, 6.7444e-03, 8.6517e-03, 7.6408e-03, 3.2654e-03, 1.4122e-02,\n",
      "        1.0895e-02, 8.1482e-03, 4.5815e-03, 1.9627e-03, 4.0245e-03, 8.7433e-03,\n",
      "        3.2940e-03, 7.0839e-03, 1.6069e-03, 7.2517e-03, 6.8245e-03, 2.8839e-03,\n",
      "        1.8749e-03, 6.3286e-03, 3.9978e-03, 5.6839e-03, 1.5114e-02, 4.4098e-03,\n",
      "        1.4481e-02, 1.0963e-02, 3.1261e-03, 9.9564e-04, 1.0506e-02, 8.0338e-03,\n",
      "        4.0588e-03, 5.7793e-03, 4.6768e-03, 4.9438e-03, 3.0746e-03, 2.2755e-03,\n",
      "        2.8687e-03, 3.6430e-04, 2.2945e-03, 2.9240e-03, 8.7509e-03, 1.3802e-02,\n",
      "        9.5673e-03, 5.6000e-03, 1.7334e-02, 1.9379e-02, 8.7967e-03, 1.2627e-02,\n",
      "        9.7580e-03, 1.0719e-02, 3.2013e-02, 1.1787e-02, 1.3870e-02, 2.3895e-02,\n",
      "        8.7891e-03, 4.4365e-03, 1.0483e-02, 9.9106e-03, 1.1879e-02, 2.6760e-03,\n",
      "        1.4740e-02, 5.2147e-03, 1.9588e-03, 1.1574e-02, 7.3891e-03, 9.1705e-03,\n",
      "        1.4511e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [74] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [74] : torch.Size([1, 32, 1, 146])\n",
      "Last layer attentions for generated token [74] : tensor([1.4453e-01, 1.4417e-01, 1.5581e-04, 3.1042e-04, 1.2517e-04, 2.0542e-03,\n",
      "        2.7943e-04, 4.9543e-04, 6.5756e-04, 4.1699e-04, 1.1673e-03, 2.5010e-04,\n",
      "        1.5283e-04, 3.9339e-04, 6.2065e-03, 5.6572e-03, 2.8229e-04, 3.9339e-04,\n",
      "        9.4533e-05, 1.0461e-04, 3.9876e-05, 2.1684e-04, 6.5422e-03, 7.0095e-04,\n",
      "        3.1738e-03, 1.2932e-02, 7.2193e-04, 2.4188e-04, 3.4571e-04, 5.9509e-04,\n",
      "        8.1825e-04, 1.1902e-03, 7.4625e-04, 6.5994e-04, 4.0889e-04, 2.0123e-04,\n",
      "        2.2328e-04, 4.9734e-04, 1.5712e-04, 1.6594e-04, 2.4395e-03, 1.0166e-03,\n",
      "        3.0518e-04, 3.4380e-04, 5.5265e-04, 2.9349e-04, 4.8089e-04, 1.3757e-04,\n",
      "        4.0030e-04, 1.6775e-03, 1.0567e-03, 1.9550e-04, 3.8738e-03, 3.8300e-03,\n",
      "        1.5898e-03, 3.4351e-03, 3.1114e-04, 9.0027e-04, 3.0670e-03, 9.0933e-04,\n",
      "        1.0242e-03, 2.4300e-03, 1.9817e-03, 8.0395e-04, 5.7793e-04, 3.9029e-04,\n",
      "        1.4467e-03, 1.7166e-03, 1.9150e-03, 1.3863e-02, 7.9651e-03, 1.2985e-02,\n",
      "        2.8305e-03, 6.8617e-04, 1.4526e-02, 9.2392e-03, 7.3929e-03, 4.3259e-03,\n",
      "        5.1041e-03, 4.1313e-03, 8.4839e-03, 5.7716e-03, 3.2997e-03, 1.6342e-02,\n",
      "        1.4687e-02, 7.0610e-03, 4.4289e-03, 2.4300e-03, 4.5815e-03, 8.4457e-03,\n",
      "        2.8610e-03, 7.1144e-03, 1.8969e-03, 6.5956e-03, 9.4147e-03, 4.4060e-03,\n",
      "        3.5992e-03, 7.5302e-03, 4.5853e-03, 8.5449e-03, 1.3107e-02, 5.2567e-03,\n",
      "        1.3550e-02, 1.0612e-02, 3.6125e-03, 1.7729e-03, 1.1108e-02, 7.5493e-03,\n",
      "        5.2834e-03, 6.8665e-03, 6.7825e-03, 5.1918e-03, 3.8624e-03, 1.7147e-03,\n",
      "        3.6240e-03, 2.2280e-04, 1.8148e-03, 2.7294e-03, 7.4959e-03, 9.1400e-03,\n",
      "        9.3689e-03, 9.2316e-03, 1.8326e-02, 1.7700e-02, 6.3744e-03, 1.0429e-02,\n",
      "        7.0648e-03, 1.2810e-02, 2.4734e-02, 1.2245e-02, 1.7715e-02, 2.9373e-02,\n",
      "        7.3357e-03, 4.8828e-03, 9.3307e-03, 1.1520e-02, 6.6833e-03, 3.0518e-03,\n",
      "        1.6510e-02, 5.6267e-03, 1.1950e-03, 7.9422e-03, 5.6038e-03, 5.6000e-03,\n",
      "        1.1345e-02, 1.4030e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [75] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [75] : torch.Size([1, 32, 1, 147])\n",
      "Last layer attentions for generated token [75] : tensor([2.6440e-01, 2.6440e-01, 9.9957e-05, 2.2352e-04, 5.6088e-05, 1.1311e-03,\n",
      "        2.0754e-04, 2.5725e-04, 3.2711e-04, 3.1519e-04, 3.9911e-04, 1.5306e-04,\n",
      "        1.5068e-04, 3.4618e-04, 8.8654e-03, 3.7651e-03, 2.6035e-04, 3.0541e-04,\n",
      "        7.0632e-05, 1.2058e-04, 4.0531e-05, 4.4966e-04, 1.3435e-02, 2.2049e-03,\n",
      "        6.8436e-03, 9.5673e-03, 7.2575e-04, 3.1638e-04, 3.3689e-04, 2.8801e-04,\n",
      "        2.6846e-04, 7.8630e-04, 5.7650e-04, 2.0838e-04, 2.4748e-04, 4.3583e-04,\n",
      "        2.9969e-04, 3.8838e-04, 3.6788e-04, 1.8895e-04, 2.0676e-03, 1.8969e-03,\n",
      "        3.9005e-04, 4.8351e-04, 3.0494e-04, 2.2089e-04, 1.6809e-04, 2.2531e-04,\n",
      "        1.4150e-04, 1.0624e-03, 1.6870e-03, 4.9472e-05, 3.5515e-03, 2.3975e-03,\n",
      "        1.4744e-03, 3.4618e-03, 3.4952e-04, 1.4715e-03, 2.7466e-03, 1.0700e-03,\n",
      "        1.2636e-03, 1.0805e-03, 3.1071e-03, 1.1921e-03, 5.5218e-04, 9.4461e-04,\n",
      "        2.9964e-03, 8.0156e-04, 3.3264e-03, 8.0872e-03, 7.4005e-03, 1.1200e-02,\n",
      "        6.9904e-04, 1.6809e-04, 6.4850e-03, 5.3520e-03, 1.3094e-03, 3.0403e-03,\n",
      "        1.4601e-03, 2.7657e-03, 4.7150e-03, 6.5079e-03, 1.4629e-03, 8.9645e-03,\n",
      "        9.3613e-03, 3.7766e-03, 2.4357e-03, 1.0853e-03, 1.1597e-03, 5.7259e-03,\n",
      "        3.2406e-03, 4.4975e-03, 1.1244e-03, 4.1809e-03, 3.3913e-03, 1.3905e-03,\n",
      "        5.7983e-04, 2.8515e-03, 2.1057e-03, 2.7580e-03, 6.9351e-03, 4.1580e-03,\n",
      "        7.4272e-03, 7.7820e-03, 2.2144e-03, 1.0700e-03, 4.2496e-03, 3.3550e-03,\n",
      "        1.8034e-03, 3.0384e-03, 2.8400e-03, 2.9907e-03, 1.9875e-03, 1.0176e-03,\n",
      "        2.2736e-03, 2.5415e-04, 1.5421e-03, 1.4801e-03, 5.3711e-03, 7.8430e-03,\n",
      "        4.1428e-03, 3.9597e-03, 1.1757e-02, 1.5045e-02, 2.2144e-03, 4.2915e-03,\n",
      "        5.3291e-03, 5.1804e-03, 9.8648e-03, 4.6272e-03, 4.4556e-03, 1.2787e-02,\n",
      "        6.7902e-03, 1.7853e-03, 2.9984e-03, 4.9438e-03, 6.5765e-03, 1.8902e-03,\n",
      "        1.1162e-02, 4.5700e-03, 9.4843e-04, 9.6283e-03, 7.6256e-03, 4.7684e-03,\n",
      "        1.8188e-02, 1.4069e-02, 6.9580e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [76] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [76] : torch.Size([1, 32, 1, 148])\n",
      "Last layer attentions for generated token [76] : tensor([2.7686e-01, 2.7686e-01, 1.1498e-04, 1.0884e-04, 5.0247e-05, 9.0408e-04,\n",
      "        1.5593e-04, 2.3317e-04, 4.2295e-04, 4.1151e-04, 5.7125e-04, 1.4091e-04,\n",
      "        1.1098e-04, 2.2078e-04, 4.9591e-03, 3.3264e-03, 1.9324e-04, 2.9588e-04,\n",
      "        7.1943e-05, 6.9201e-05, 3.8803e-05, 2.6011e-04, 9.4070e-03, 9.3651e-04,\n",
      "        3.4828e-03, 6.5613e-03, 3.3593e-04, 1.8585e-04, 1.9860e-04, 3.3331e-04,\n",
      "        5.0449e-04, 9.6273e-04, 5.2738e-04, 2.6417e-04, 2.9182e-04, 2.8229e-04,\n",
      "        1.4138e-04, 3.5334e-04, 2.2686e-04, 1.2970e-04, 1.7290e-03, 9.1314e-04,\n",
      "        1.2434e-04, 2.3592e-04, 2.2244e-04, 1.1188e-04, 1.5593e-04, 1.6594e-04,\n",
      "        1.5235e-04, 1.1253e-03, 1.1101e-03, 4.7565e-05, 2.6608e-03, 2.1935e-03,\n",
      "        9.3460e-04, 2.7122e-03, 3.1924e-04, 1.2999e-03, 2.9297e-03, 1.0738e-03,\n",
      "        9.4032e-04, 1.3390e-03, 3.2883e-03, 9.6989e-04, 6.5756e-04, 7.2670e-04,\n",
      "        2.0561e-03, 1.1864e-03, 2.1935e-03, 7.0648e-03, 5.2299e-03, 1.1299e-02,\n",
      "        1.2121e-03, 1.9789e-04, 6.6795e-03, 6.3057e-03, 2.6283e-03, 3.0212e-03,\n",
      "        1.7805e-03, 2.5635e-03, 4.4670e-03, 4.6539e-03, 1.4305e-03, 9.2163e-03,\n",
      "        8.5373e-03, 3.1376e-03, 2.0561e-03, 6.4373e-04, 1.4820e-03, 5.2567e-03,\n",
      "        2.5635e-03, 4.3831e-03, 1.2531e-03, 5.4550e-03, 3.4904e-03, 2.3270e-03,\n",
      "        1.3762e-03, 3.7956e-03, 2.5883e-03, 6.3705e-03, 1.2283e-02, 3.2444e-03,\n",
      "        9.7809e-03, 6.8359e-03, 2.3422e-03, 1.0147e-03, 5.1193e-03, 3.2349e-03,\n",
      "        2.3594e-03, 4.1122e-03, 3.0270e-03, 2.7828e-03, 2.6646e-03, 1.1883e-03,\n",
      "        3.4065e-03, 2.7108e-04, 1.7290e-03, 2.1133e-03, 5.2719e-03, 5.9204e-03,\n",
      "        5.2223e-03, 4.4403e-03, 1.2482e-02, 1.0872e-02, 2.0561e-03, 3.8815e-03,\n",
      "        4.0245e-03, 3.3932e-03, 6.2180e-03, 4.8943e-03, 5.3711e-03, 1.3100e-02,\n",
      "        5.7869e-03, 1.7290e-03, 4.1199e-03, 6.5384e-03, 6.9351e-03, 1.7672e-03,\n",
      "        1.3275e-02, 3.8261e-03, 1.7986e-03, 8.3008e-03, 6.0730e-03, 4.3106e-03,\n",
      "        9.5596e-03, 1.1337e-02, 6.8207e-03, 5.3864e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [77] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [77] : torch.Size([1, 32, 1, 149])\n",
      "Last layer attentions for generated token [77] : tensor([1.4807e-01, 1.4807e-01, 1.4210e-04, 2.4891e-04, 1.0681e-04, 8.4019e-04,\n",
      "        1.8311e-04, 1.8167e-04, 2.3925e-04, 3.3283e-04, 9.5749e-04, 3.6693e-04,\n",
      "        3.9077e-04, 9.3174e-04, 1.0750e-02, 6.3210e-03, 1.8966e-04, 6.4898e-04,\n",
      "        1.9956e-04, 2.0957e-04, 1.6284e-04, 4.6039e-04, 5.8250e-03, 1.3742e-03,\n",
      "        6.3820e-03, 7.1449e-03, 9.5034e-04, 3.9220e-04, 3.7146e-04, 3.2330e-04,\n",
      "        6.7902e-04, 1.5907e-03, 7.7868e-04, 2.6655e-04, 2.6655e-04, 4.9305e-04,\n",
      "        2.8372e-04, 4.4274e-04, 4.1032e-04, 2.8539e-04, 4.1428e-03, 1.7185e-03,\n",
      "        4.2009e-04, 6.1369e-04, 3.8528e-04, 3.8457e-04, 1.3089e-04, 1.9038e-04,\n",
      "        3.1519e-04, 2.5482e-03, 1.9245e-03, 1.3447e-04, 3.0193e-03, 2.9812e-03,\n",
      "        1.6098e-03, 2.4872e-03, 5.2977e-04, 1.4935e-03, 4.6806e-03, 1.2712e-03,\n",
      "        1.7290e-03, 2.1515e-03, 2.5806e-03, 1.2560e-03, 1.4992e-03, 6.4182e-04,\n",
      "        3.5057e-03, 1.1797e-03, 3.1548e-03, 8.1100e-03, 5.4245e-03, 8.5678e-03,\n",
      "        2.3670e-03, 5.4359e-04, 7.3395e-03, 7.6447e-03, 3.5896e-03, 4.8904e-03,\n",
      "        3.2330e-03, 4.5624e-03, 8.7280e-03, 6.6605e-03, 2.5158e-03, 7.8506e-03,\n",
      "        1.2413e-02, 5.6763e-03, 5.0316e-03, 2.5501e-03, 2.4643e-03, 7.2327e-03,\n",
      "        3.0518e-03, 5.4131e-03, 2.3422e-03, 7.8354e-03, 5.5618e-03, 3.0518e-03,\n",
      "        1.7700e-03, 8.1711e-03, 4.9057e-03, 7.2136e-03, 1.2276e-02, 6.1188e-03,\n",
      "        1.4648e-02, 1.2817e-02, 6.8130e-03, 2.3823e-03, 1.0323e-02, 7.7591e-03,\n",
      "        5.1651e-03, 5.8517e-03, 3.1185e-03, 3.0022e-03, 3.5229e-03, 2.2755e-03,\n",
      "        3.8891e-03, 5.7840e-04, 2.7256e-03, 3.3817e-03, 8.4000e-03, 1.2825e-02,\n",
      "        8.2169e-03, 7.9269e-03, 1.6129e-02, 2.7557e-02, 7.5989e-03, 8.0490e-03,\n",
      "        1.1040e-02, 8.4839e-03, 1.2077e-02, 7.4577e-03, 7.2098e-03, 2.1835e-02,\n",
      "        8.5297e-03, 3.9062e-03, 9.6436e-03, 1.1147e-02, 8.7891e-03, 3.7804e-03,\n",
      "        2.1561e-02, 8.6670e-03, 4.1962e-03, 1.0635e-02, 9.6741e-03, 9.9411e-03,\n",
      "        1.2161e-02, 1.8555e-02, 1.0689e-02, 5.0316e-03, 7.9498e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [78] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [78] : torch.Size([1, 32, 1, 150])\n",
      "Last layer attentions for generated token [78] : tensor([1.6382e-01, 1.6382e-01, 1.8418e-04, 2.2078e-04, 1.3268e-04, 1.1568e-03,\n",
      "        3.5906e-04, 5.7507e-04, 4.8041e-04, 4.4012e-04, 9.7036e-04, 2.5558e-04,\n",
      "        2.3603e-04, 4.0936e-04, 4.7455e-03, 3.6888e-03, 2.0421e-04, 3.7265e-04,\n",
      "        9.2208e-05, 9.7036e-05, 3.3796e-05, 1.7500e-04, 5.3749e-03, 7.0477e-04,\n",
      "        2.4948e-03, 7.7133e-03, 4.1747e-04, 1.7774e-04, 1.6642e-04, 4.4608e-04,\n",
      "        5.6934e-04, 9.8228e-04, 7.3671e-04, 5.2547e-04, 4.1652e-04, 2.2745e-04,\n",
      "        2.8348e-04, 4.8327e-04, 2.1780e-04, 1.5569e-04, 3.0079e-03, 1.0033e-03,\n",
      "        2.4986e-04, 4.4513e-04, 5.5504e-04, 2.1827e-04, 2.9492e-04, 2.0945e-04,\n",
      "        3.6550e-04, 2.2278e-03, 1.0700e-03, 1.2362e-04, 2.9373e-03, 2.7866e-03,\n",
      "        1.3905e-03, 3.3703e-03, 3.4881e-04, 9.5177e-04, 3.4008e-03, 1.1530e-03,\n",
      "        1.1616e-03, 1.9665e-03, 2.9182e-03, 8.7500e-04, 7.2145e-04, 6.7472e-04,\n",
      "        1.8253e-03, 1.8559e-03, 1.9779e-03, 9.8343e-03, 6.8321e-03, 1.3023e-02,\n",
      "        3.1605e-03, 4.6110e-04, 1.4076e-02, 9.4681e-03, 4.7302e-03, 4.5776e-03,\n",
      "        2.9984e-03, 2.9316e-03, 7.7896e-03, 5.9509e-03, 1.4992e-03, 1.2657e-02,\n",
      "        1.3657e-02, 5.7335e-03, 4.6043e-03, 1.5535e-03, 2.7790e-03, 9.6512e-03,\n",
      "        3.2043e-03, 5.9471e-03, 1.2636e-03, 8.1635e-03, 8.9493e-03, 4.9858e-03,\n",
      "        2.6360e-03, 8.7891e-03, 5.3825e-03, 1.0246e-02, 2.0096e-02, 4.3869e-03,\n",
      "        1.5335e-02, 8.9493e-03, 2.7485e-03, 1.3657e-03, 8.1253e-03, 5.6381e-03,\n",
      "        3.9215e-03, 6.3515e-03, 5.9776e-03, 4.3335e-03, 4.8714e-03, 2.2717e-03,\n",
      "        6.4697e-03, 4.8518e-04, 3.1052e-03, 3.4180e-03, 1.0269e-02, 1.0429e-02,\n",
      "        8.0948e-03, 5.7640e-03, 1.9180e-02, 1.5656e-02, 3.7174e-03, 8.6746e-03,\n",
      "        5.0049e-03, 9.6283e-03, 1.5541e-02, 7.7019e-03, 9.0027e-03, 2.8015e-02,\n",
      "        6.5460e-03, 2.0905e-03, 6.8321e-03, 7.9651e-03, 7.9880e-03, 1.7071e-03,\n",
      "        1.4420e-02, 3.9444e-03, 1.1148e-03, 9.7733e-03, 8.8806e-03, 8.6823e-03,\n",
      "        1.2703e-02, 1.6510e-02, 9.2468e-03, 3.6659e-03, 6.1035e-03, 5.0201e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [79] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [79] : torch.Size([1, 32, 1, 151])\n",
      "Last layer attentions for generated token [79] : tensor([1.6418e-01, 1.6455e-01, 2.9373e-04, 5.1165e-04, 3.1757e-04, 1.6289e-03,\n",
      "        2.7108e-04, 2.6703e-04, 4.3321e-04, 3.7575e-04, 1.0395e-03, 4.2748e-04,\n",
      "        5.2786e-04, 9.0456e-04, 1.3466e-02, 7.6370e-03, 2.1958e-04, 6.6185e-04,\n",
      "        1.8775e-04, 2.9421e-04, 1.6320e-04, 4.3845e-04, 6.3744e-03, 1.7118e-03,\n",
      "        9.3918e-03, 1.1055e-02, 1.7004e-03, 6.3515e-04, 4.5323e-04, 3.4404e-04,\n",
      "        6.7234e-04, 1.8148e-03, 6.3801e-04, 2.5916e-04, 2.5678e-04, 6.3801e-04,\n",
      "        4.4179e-04, 3.5644e-04, 3.3808e-04, 2.3413e-04, 4.9133e-03, 2.8172e-03,\n",
      "        5.3501e-04, 6.7520e-04, 3.4475e-04, 3.2902e-04, 1.1754e-04, 2.0540e-04,\n",
      "        3.3355e-04, 2.9240e-03, 2.0180e-03, 2.5034e-04, 3.3283e-03, 5.0888e-03,\n",
      "        1.9169e-03, 3.5133e-03, 6.0368e-04, 1.9398e-03, 6.0501e-03, 1.5154e-03,\n",
      "        2.0657e-03, 2.0428e-03, 2.7981e-03, 1.9188e-03, 1.4715e-03, 6.8426e-04,\n",
      "        3.0403e-03, 1.5478e-03, 3.2368e-03, 1.3168e-02, 7.8964e-03, 1.1726e-02,\n",
      "        2.9182e-03, 6.5422e-04, 9.5291e-03, 8.8577e-03, 4.2038e-03, 4.4479e-03,\n",
      "        2.4242e-03, 5.1117e-03, 9.0027e-03, 1.0368e-02, 2.5978e-03, 7.4463e-03,\n",
      "        1.4435e-02, 5.4131e-03, 5.4398e-03, 2.5253e-03, 2.0199e-03, 5.8517e-03,\n",
      "        3.0556e-03, 5.1651e-03, 2.1400e-03, 5.6992e-03, 4.3030e-03, 1.7252e-03,\n",
      "        1.2417e-03, 4.9553e-03, 3.4618e-03, 4.7607e-03, 8.9264e-03, 6.5727e-03,\n",
      "        1.0544e-02, 1.0345e-02, 5.5428e-03, 2.3670e-03, 7.5493e-03, 4.5586e-03,\n",
      "        4.0283e-03, 4.5395e-03, 3.6564e-03, 3.3264e-03, 3.2272e-03, 1.9264e-03,\n",
      "        3.6659e-03, 5.0640e-04, 1.7729e-03, 2.1076e-03, 5.6267e-03, 1.1177e-02,\n",
      "        5.6152e-03, 5.7755e-03, 1.2558e-02, 2.3376e-02, 6.6910e-03, 6.4812e-03,\n",
      "        6.5575e-03, 7.0648e-03, 9.9716e-03, 5.7373e-03, 5.6076e-03, 1.9592e-02,\n",
      "        6.6414e-03, 3.1395e-03, 7.7324e-03, 6.9160e-03, 6.5918e-03, 2.4910e-03,\n",
      "        1.7090e-02, 5.5809e-03, 3.1128e-03, 7.0534e-03, 1.0681e-02, 8.7662e-03,\n",
      "        1.5594e-02, 1.1200e-02, 7.4730e-03, 4.0436e-03, 8.7662e-03, 5.7068e-03,\n",
      "        1.7517e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [80] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [80] : torch.Size([1, 32, 1, 152])\n",
      "Last layer attentions for generated token [80] : tensor([1.5381e-01, 1.5417e-01, 1.8406e-04, 2.6941e-04, 2.0778e-04, 9.4938e-04,\n",
      "        2.7514e-04, 3.1614e-04, 5.3167e-04, 3.6597e-04, 9.9087e-04, 5.0831e-04,\n",
      "        2.4438e-04, 5.6458e-04, 6.4011e-03, 5.1575e-03, 3.3855e-04, 6.2370e-04,\n",
      "        1.8406e-04, 2.1183e-04, 1.1539e-04, 2.4104e-04, 6.3896e-03, 1.6241e-03,\n",
      "        8.3008e-03, 1.0536e-02, 9.2554e-04, 4.8494e-04, 6.7472e-04, 5.4741e-04,\n",
      "        1.2436e-03, 1.4811e-03, 1.1139e-03, 5.0116e-04, 5.1212e-04, 6.2895e-04,\n",
      "        3.2568e-04, 4.1080e-04, 3.2043e-04, 3.3331e-04, 4.4861e-03, 1.1520e-03,\n",
      "        4.7565e-04, 6.7043e-04, 5.0735e-04, 4.2796e-04, 2.7633e-04, 2.5058e-04,\n",
      "        5.2834e-04, 2.1362e-03, 1.0109e-03, 2.9945e-04, 2.1152e-03, 2.7370e-03,\n",
      "        1.2722e-03, 2.4433e-03, 2.4331e-04, 1.4715e-03, 3.0746e-03, 1.0204e-03,\n",
      "        1.2798e-03, 1.6813e-03, 2.6627e-03, 8.6117e-04, 1.6975e-03, 5.5838e-04,\n",
      "        1.2674e-03, 1.7719e-03, 1.6861e-03, 1.1749e-02, 4.8599e-03, 9.5367e-03,\n",
      "        1.9217e-03, 3.2234e-04, 8.0185e-03, 6.3744e-03, 2.5635e-03, 2.9354e-03,\n",
      "        2.1667e-03, 3.2558e-03, 6.5308e-03, 5.9319e-03, 3.2120e-03, 1.1024e-02,\n",
      "        7.9803e-03, 4.0169e-03, 2.8095e-03, 2.1725e-03, 2.4624e-03, 4.6349e-03,\n",
      "        2.4414e-03, 5.0621e-03, 2.4433e-03, 5.2032e-03, 3.6678e-03, 2.1667e-03,\n",
      "        2.3785e-03, 5.2223e-03, 4.6082e-03, 6.7177e-03, 1.2566e-02, 4.9744e-03,\n",
      "        1.7242e-02, 7.2174e-03, 3.1471e-03, 7.9966e-04, 6.0387e-03, 4.5586e-03,\n",
      "        5.1003e-03, 6.8474e-03, 4.7569e-03, 3.8643e-03, 4.4365e-03, 1.7681e-03,\n",
      "        4.0169e-03, 2.3270e-04, 2.0962e-03, 5.6877e-03, 8.6365e-03, 9.3689e-03,\n",
      "        7.9651e-03, 6.5346e-03, 1.7120e-02, 1.6281e-02, 2.8286e-03, 8.0338e-03,\n",
      "        4.0131e-03, 7.7019e-03, 7.7782e-03, 5.7983e-03, 8.1558e-03, 2.2507e-02,\n",
      "        6.3438e-03, 6.8665e-03, 1.0193e-02, 1.1230e-02, 7.4043e-03, 4.2915e-03,\n",
      "        2.6260e-02, 4.4556e-03, 4.5471e-03, 1.2512e-02, 1.0475e-02, 7.9575e-03,\n",
      "        1.8967e-02, 1.4954e-02, 1.0612e-02, 6.8512e-03, 1.1093e-02, 5.1689e-03,\n",
      "        2.3697e-02, 2.0538e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [81] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [81] : torch.Size([1, 32, 1, 153])\n",
      "Last layer attentions for generated token [81] : tensor([2.1887e-01, 2.1887e-01, 3.7074e-04, 2.5535e-04, 2.8920e-04, 1.3304e-03,\n",
      "        2.7442e-04, 4.5967e-04, 7.7438e-04, 4.1032e-04, 6.4325e-04, 2.7609e-04,\n",
      "        1.9240e-04, 5.0688e-04, 4.1580e-03, 4.1924e-03, 2.2578e-04, 4.3869e-04,\n",
      "        1.0747e-04, 1.5390e-04, 9.0122e-05, 2.4223e-04, 6.4888e-03, 9.5987e-04,\n",
      "        5.3902e-03, 7.3700e-03, 8.9455e-04, 3.1972e-04, 4.4894e-04, 4.8065e-04,\n",
      "        1.0958e-03, 1.1806e-03, 9.3746e-04, 2.3389e-04, 3.5381e-04, 3.4285e-04,\n",
      "        1.7929e-04, 3.2210e-04, 3.2401e-04, 2.4843e-04, 3.9940e-03, 1.2016e-03,\n",
      "        3.4976e-04, 3.8409e-04, 3.2401e-04, 2.6965e-04, 1.6010e-04, 2.0516e-04,\n",
      "        2.9492e-04, 1.8110e-03, 1.1292e-03, 3.3760e-04, 3.1834e-03, 4.0016e-03,\n",
      "        1.2035e-03, 2.0046e-03, 1.4985e-04, 8.9836e-04, 3.1223e-03, 8.7738e-04,\n",
      "        7.9250e-04, 1.4410e-03, 2.7523e-03, 8.7881e-04, 1.0643e-03, 8.3876e-04,\n",
      "        1.3962e-03, 1.8406e-03, 1.5821e-03, 1.3504e-02, 7.3700e-03, 9.1019e-03,\n",
      "        1.2693e-03, 3.3832e-04, 6.7482e-03, 5.9090e-03, 2.9564e-03, 2.2297e-03,\n",
      "        1.3142e-03, 2.8496e-03, 6.2180e-03, 5.6229e-03, 1.5583e-03, 7.0686e-03,\n",
      "        7.9880e-03, 1.8682e-03, 1.9255e-03, 4.6420e-04, 1.0195e-03, 6.3286e-03,\n",
      "        2.4033e-03, 4.5891e-03, 1.5869e-03, 5.1308e-03, 2.9030e-03, 1.2646e-03,\n",
      "        1.3885e-03, 3.5725e-03, 2.8992e-03, 4.8637e-03, 1.1093e-02, 4.0627e-03,\n",
      "        1.0139e-02, 5.3673e-03, 2.0447e-03, 8.8263e-04, 3.7441e-03, 2.4490e-03,\n",
      "        2.8629e-03, 5.8975e-03, 5.5008e-03, 3.3112e-03, 4.8065e-03, 1.6775e-03,\n",
      "        5.4398e-03, 3.3498e-04, 1.7414e-03, 2.2011e-03, 7.3242e-03, 7.7324e-03,\n",
      "        3.9597e-03, 6.2294e-03, 2.0187e-02, 1.3908e-02, 2.0084e-03, 4.4518e-03,\n",
      "        4.8370e-03, 4.1161e-03, 3.8033e-03, 2.9907e-03, 3.8300e-03, 1.4015e-02,\n",
      "        5.6534e-03, 3.4504e-03, 6.3438e-03, 7.4387e-03, 5.8174e-03, 2.9297e-03,\n",
      "        1.4839e-02, 3.4103e-03, 2.4605e-03, 1.2390e-02, 6.8092e-03, 6.3591e-03,\n",
      "        1.1131e-02, 9.1324e-03, 6.8359e-03, 4.0359e-03, 1.3741e-02, 5.6458e-03,\n",
      "        2.7069e-02, 2.0691e-02, 7.3509e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [82] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [82] : torch.Size([1, 32, 1, 154])\n",
      "Last layer attentions for generated token [82] : tensor([1.6663e-01, 1.6663e-01, 7.3814e-04, 1.3924e-03, 7.7772e-04, 3.1395e-03,\n",
      "        5.2834e-04, 4.7469e-04, 4.2963e-04, 1.0633e-03, 1.5469e-03, 6.6662e-04,\n",
      "        7.0810e-04, 8.9550e-04, 1.8158e-02, 1.2398e-02, 8.5115e-04, 2.3422e-03,\n",
      "        4.9448e-04, 5.3453e-04, 3.5191e-04, 8.6451e-04, 4.6692e-03, 2.0142e-03,\n",
      "        1.4412e-02, 1.8723e-02, 4.8065e-03, 1.0290e-03, 7.7772e-04, 7.5245e-04,\n",
      "        8.4925e-04, 3.9177e-03, 9.5320e-04, 3.6311e-04, 4.1890e-04, 8.7976e-04,\n",
      "        2.7847e-04, 6.3133e-04, 7.5102e-04, 4.4680e-04, 8.5220e-03, 4.0054e-03,\n",
      "        8.5115e-04, 1.7242e-03, 6.8903e-04, 7.5817e-04, 1.3781e-04, 1.8692e-04,\n",
      "        3.1614e-04, 5.1994e-03, 3.9787e-03, 4.9734e-04, 3.0079e-03, 1.3184e-02,\n",
      "        2.2583e-03, 5.6496e-03, 9.6226e-04, 2.0161e-03, 6.6452e-03, 1.2217e-03,\n",
      "        2.2717e-03, 1.5774e-03, 2.9011e-03, 1.8282e-03, 1.3361e-03, 9.8896e-04,\n",
      "        4.8065e-03, 4.4861e-03, 2.8687e-03, 2.5330e-02, 1.9333e-02, 2.1027e-02,\n",
      "        5.8174e-03, 1.2455e-03, 1.6159e-02, 2.0615e-02, 1.2535e-02, 6.5422e-03,\n",
      "        4.1122e-03, 6.5460e-03, 8.5907e-03, 9.3918e-03, 3.4618e-03, 8.8196e-03,\n",
      "        1.8417e-02, 9.3765e-03, 5.6496e-03, 1.5869e-03, 1.5411e-03, 5.6763e-03,\n",
      "        2.4052e-03, 5.9509e-03, 2.1191e-03, 4.7569e-03, 3.1738e-03, 1.4172e-03,\n",
      "        6.0225e-04, 3.5725e-03, 2.1458e-03, 2.5883e-03, 5.9319e-03, 5.9586e-03,\n",
      "        6.6872e-03, 1.3145e-02, 5.7907e-03, 1.6241e-03, 4.5166e-03, 3.4695e-03,\n",
      "        1.8730e-03, 1.9121e-03, 1.2093e-03, 1.1654e-03, 2.4719e-03, 1.6632e-03,\n",
      "        2.1191e-03, 3.6883e-04, 8.6594e-04, 1.0147e-03, 3.3665e-03, 5.3902e-03,\n",
      "        3.8586e-03, 4.3869e-03, 4.4899e-03, 2.0233e-02, 8.7357e-03, 3.8853e-03,\n",
      "        4.5853e-03, 3.0918e-03, 3.3340e-03, 3.1643e-03, 2.2602e-03, 8.7662e-03,\n",
      "        4.8714e-03, 1.5621e-03, 3.8319e-03, 3.1872e-03, 3.3684e-03, 1.5173e-03,\n",
      "        6.4011e-03, 3.7003e-03, 2.0695e-03, 3.8395e-03, 4.5547e-03, 5.3711e-03,\n",
      "        7.2861e-03, 5.9471e-03, 4.2725e-03, 2.0561e-03, 3.9043e-03, 2.8324e-03,\n",
      "        9.7198e-03, 6.1798e-03, 2.5177e-03, 5.9166e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [83] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [83] : torch.Size([1, 32, 1, 155])\n",
      "Last layer attentions for generated token [83] : tensor([1.2030e-01, 1.2012e-01, 2.3139e-04, 3.8528e-04, 1.7774e-04, 2.1515e-03,\n",
      "        3.0422e-04, 3.2878e-04, 6.1417e-04, 4.6563e-04, 1.6804e-03, 3.4404e-04,\n",
      "        2.6369e-04, 6.4373e-04, 1.2283e-02, 7.9422e-03, 2.1863e-04, 4.3726e-04,\n",
      "        1.1367e-04, 2.4247e-04, 1.3137e-04, 2.5368e-04, 2.2850e-03, 5.9557e-04,\n",
      "        3.7460e-03, 1.7136e-02, 2.0084e-03, 4.9829e-04, 2.7323e-04, 3.3545e-04,\n",
      "        4.1318e-04, 8.5974e-04, 4.8232e-04, 4.9353e-04, 2.7966e-04, 2.0385e-04,\n",
      "        1.6308e-04, 3.1018e-04, 1.5056e-04, 8.5473e-05, 2.3079e-03, 8.5449e-04,\n",
      "        5.1355e-04, 4.4179e-04, 4.3559e-04, 1.8632e-04, 2.4390e-04, 1.3924e-04,\n",
      "        1.7428e-04, 1.4324e-03, 7.4673e-04, 2.8563e-04, 2.0962e-03, 3.9139e-03,\n",
      "        1.1978e-03, 3.6907e-03, 7.0047e-04, 1.2026e-03, 4.5242e-03, 9.2220e-04,\n",
      "        5.2166e-04, 1.1749e-03, 1.7710e-03, 5.5504e-04, 5.4550e-04, 4.8327e-04,\n",
      "        7.7677e-04, 1.7900e-03, 1.2903e-03, 1.0178e-02, 8.3542e-03, 1.4030e-02,\n",
      "        1.0719e-02, 1.8396e-03, 4.1107e-02, 1.0246e-02, 2.4109e-02, 6.8245e-03,\n",
      "        5.3978e-03, 6.8130e-03, 1.6174e-02, 8.0032e-03, 3.8185e-03, 1.4687e-02,\n",
      "        1.1238e-02, 9.6436e-03, 4.7379e-03, 6.5651e-03, 5.7373e-03, 1.3985e-02,\n",
      "        1.7967e-03, 3.8414e-03, 1.0405e-03, 4.7264e-03, 6.7520e-03, 3.9673e-03,\n",
      "        1.3905e-03, 4.2686e-03, 3.6640e-03, 3.2444e-03, 1.0643e-02, 3.0632e-03,\n",
      "        1.2505e-02, 7.4539e-03, 3.2177e-03, 8.2684e-04, 5.5847e-03, 5.3558e-03,\n",
      "        2.5043e-03, 4.2686e-03, 7.0190e-03, 3.8795e-03, 4.1504e-03, 3.2711e-03,\n",
      "        3.8490e-03, 4.0460e-04, 1.8902e-03, 2.0275e-03, 8.1787e-03, 1.1192e-02,\n",
      "        5.8823e-03, 5.7793e-03, 1.7776e-02, 1.3741e-02, 7.8125e-03, 1.0750e-02,\n",
      "        5.8327e-03, 1.0170e-02, 1.3435e-02, 7.9269e-03, 8.0109e-03, 1.2733e-02,\n",
      "        3.6983e-03, 1.3905e-03, 5.8174e-03, 5.6877e-03, 3.6736e-03, 1.5965e-03,\n",
      "        7.9117e-03, 2.6169e-03, 8.9741e-04, 9.8495e-03, 4.8828e-03, 2.4605e-03,\n",
      "        5.6343e-03, 6.6986e-03, 6.8970e-03, 2.4185e-03, 8.4686e-03, 3.3398e-03,\n",
      "        2.5970e-02, 7.6408e-03, 5.1651e-03, 1.3481e-02, 3.8879e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [84] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [84] : torch.Size([1, 32, 1, 156])\n",
      "Last layer attentions for generated token [84] : tensor([0.0804, 0.0803, 0.0003, 0.0004, 0.0002, 0.0030, 0.0003, 0.0002, 0.0002,\n",
      "        0.0004, 0.0023, 0.0003, 0.0005, 0.0008, 0.0115, 0.0140, 0.0004, 0.0007,\n",
      "        0.0004, 0.0005, 0.0001, 0.0003, 0.0034, 0.0007, 0.0030, 0.0141, 0.0027,\n",
      "        0.0007, 0.0003, 0.0004, 0.0003, 0.0008, 0.0010, 0.0009, 0.0004, 0.0006,\n",
      "        0.0002, 0.0004, 0.0003, 0.0002, 0.0017, 0.0008, 0.0009, 0.0013, 0.0013,\n",
      "        0.0005, 0.0004, 0.0002, 0.0004, 0.0014, 0.0011, 0.0003, 0.0018, 0.0042,\n",
      "        0.0009, 0.0099, 0.0012, 0.0014, 0.0023, 0.0014, 0.0012, 0.0010, 0.0018,\n",
      "        0.0010, 0.0007, 0.0003, 0.0018, 0.0029, 0.0007, 0.0133, 0.0155, 0.0441,\n",
      "        0.0127, 0.0019, 0.0371, 0.0270, 0.0265, 0.0166, 0.0128, 0.0114, 0.0141,\n",
      "        0.0195, 0.0051, 0.0196, 0.0287, 0.0104, 0.0116, 0.0073, 0.0088, 0.0117,\n",
      "        0.0046, 0.0039, 0.0014, 0.0027, 0.0082, 0.0038, 0.0016, 0.0039, 0.0056,\n",
      "        0.0029, 0.0078, 0.0073, 0.0074, 0.0097, 0.0018, 0.0003, 0.0041, 0.0049,\n",
      "        0.0015, 0.0025, 0.0033, 0.0023, 0.0011, 0.0014, 0.0009, 0.0002, 0.0013,\n",
      "        0.0011, 0.0029, 0.0125, 0.0059, 0.0027, 0.0088, 0.0204, 0.0106, 0.0101,\n",
      "        0.0070, 0.0097, 0.0118, 0.0063, 0.0066, 0.0118, 0.0047, 0.0012, 0.0026,\n",
      "        0.0050, 0.0025, 0.0010, 0.0058, 0.0034, 0.0011, 0.0075, 0.0072, 0.0032,\n",
      "        0.0104, 0.0061, 0.0052, 0.0022, 0.0031, 0.0026, 0.0068, 0.0056, 0.0015,\n",
      "        0.0037, 0.0361, 0.0191], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [85] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [85] : torch.Size([1, 32, 1, 157])\n",
      "Last layer attentions for generated token [85] : tensor([0.0866, 0.0864, 0.0002, 0.0004, 0.0001, 0.0028, 0.0004, 0.0004, 0.0004,\n",
      "        0.0007, 0.0027, 0.0005, 0.0006, 0.0010, 0.0107, 0.0124, 0.0004, 0.0005,\n",
      "        0.0003, 0.0006, 0.0001, 0.0004, 0.0037, 0.0009, 0.0035, 0.0143, 0.0024,\n",
      "        0.0008, 0.0004, 0.0005, 0.0004, 0.0007, 0.0013, 0.0013, 0.0005, 0.0007,\n",
      "        0.0003, 0.0007, 0.0003, 0.0002, 0.0018, 0.0007, 0.0007, 0.0011, 0.0014,\n",
      "        0.0004, 0.0006, 0.0002, 0.0005, 0.0015, 0.0008, 0.0003, 0.0020, 0.0032,\n",
      "        0.0009, 0.0088, 0.0014, 0.0014, 0.0020, 0.0016, 0.0011, 0.0012, 0.0025,\n",
      "        0.0013, 0.0008, 0.0005, 0.0016, 0.0031, 0.0009, 0.0103, 0.0085, 0.0272,\n",
      "        0.0126, 0.0021, 0.0352, 0.0178, 0.0163, 0.0113, 0.0126, 0.0114, 0.0135,\n",
      "        0.0144, 0.0035, 0.0220, 0.0215, 0.0097, 0.0083, 0.0066, 0.0090, 0.0110,\n",
      "        0.0031, 0.0037, 0.0012, 0.0029, 0.0094, 0.0045, 0.0018, 0.0041, 0.0050,\n",
      "        0.0035, 0.0094, 0.0043, 0.0082, 0.0066, 0.0014, 0.0003, 0.0045, 0.0042,\n",
      "        0.0026, 0.0032, 0.0055, 0.0030, 0.0016, 0.0014, 0.0020, 0.0002, 0.0017,\n",
      "        0.0025, 0.0048, 0.0121, 0.0065, 0.0047, 0.0140, 0.0179, 0.0115, 0.0120,\n",
      "        0.0055, 0.0155, 0.0157, 0.0069, 0.0099, 0.0158, 0.0043, 0.0013, 0.0030,\n",
      "        0.0062, 0.0036, 0.0011, 0.0068, 0.0037, 0.0013, 0.0103, 0.0075, 0.0035,\n",
      "        0.0111, 0.0063, 0.0066, 0.0031, 0.0041, 0.0040, 0.0108, 0.0082, 0.0030,\n",
      "        0.0076, 0.0332, 0.0149, 0.0094], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [86] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [86] : torch.Size([1, 32, 1, 158])\n",
      "Last layer attentions for generated token [86] : tensor([1.6675e-01, 1.6650e-01, 1.0914e-04, 2.1160e-04, 5.8174e-05, 1.6403e-03,\n",
      "        2.6441e-04, 2.1493e-04, 3.6335e-04, 4.9114e-04, 1.5879e-03, 3.5095e-04,\n",
      "        5.9557e-04, 1.5287e-03, 8.3847e-03, 7.2784e-03, 4.1842e-04, 4.9782e-04,\n",
      "        2.1708e-04, 2.8419e-04, 1.0788e-04, 3.7289e-04, 7.1983e-03, 1.4095e-03,\n",
      "        3.7575e-03, 9.2010e-03, 1.3294e-03, 6.5565e-04, 5.3835e-04, 3.0899e-04,\n",
      "        4.7684e-04, 9.1362e-04, 9.1934e-04, 6.1846e-04, 3.2067e-04, 9.6321e-04,\n",
      "        3.8838e-04, 5.3930e-04, 4.1032e-04, 3.0494e-04, 1.6975e-03, 1.3113e-03,\n",
      "        1.0662e-03, 9.1219e-04, 5.8413e-04, 5.0163e-04, 2.5678e-04, 2.7323e-04,\n",
      "        5.3215e-04, 1.1482e-03, 1.0958e-03, 9.5189e-05, 2.8648e-03, 2.1725e-03,\n",
      "        1.7338e-03, 5.1918e-03, 7.7868e-04, 2.4147e-03, 1.8787e-03, 1.1330e-03,\n",
      "        1.3714e-03, 1.3008e-03, 2.2354e-03, 1.3475e-03, 1.5450e-03, 4.1747e-04,\n",
      "        3.0613e-03, 1.6022e-03, 1.5650e-03, 7.2899e-03, 3.9005e-03, 1.9028e-02,\n",
      "        5.8823e-03, 9.0837e-04, 1.1177e-02, 8.7509e-03, 5.7373e-03, 6.1493e-03,\n",
      "        5.4054e-03, 6.7596e-03, 7.3776e-03, 9.9640e-03, 2.0447e-03, 1.2726e-02,\n",
      "        1.5083e-02, 4.7760e-03, 7.5760e-03, 5.9395e-03, 4.2076e-03, 7.3280e-03,\n",
      "        3.7403e-03, 4.3640e-03, 2.6512e-03, 2.6264e-03, 5.2185e-03, 2.8057e-03,\n",
      "        2.7122e-03, 3.7994e-03, 4.5319e-03, 3.0308e-03, 7.5912e-03, 4.1046e-03,\n",
      "        6.8970e-03, 1.0307e-02, 2.2602e-03, 7.6962e-04, 3.2768e-03, 3.6449e-03,\n",
      "        2.8553e-03, 2.5311e-03, 3.2902e-03, 3.3264e-03, 1.0662e-03, 1.3580e-03,\n",
      "        1.1845e-03, 2.8920e-04, 1.8826e-03, 3.0994e-03, 3.4752e-03, 1.0658e-02,\n",
      "        4.2953e-03, 3.2578e-03, 7.1068e-03, 2.3727e-02, 9.5215e-03, 6.2370e-03,\n",
      "        7.7400e-03, 9.2010e-03, 1.0872e-02, 5.5122e-03, 7.2517e-03, 9.7198e-03,\n",
      "        5.0011e-03, 3.2902e-03, 2.7905e-03, 6.6605e-03, 3.2806e-03, 2.0370e-03,\n",
      "        6.5765e-03, 4.4823e-03, 2.9736e-03, 6.3133e-03, 7.1106e-03, 4.6654e-03,\n",
      "        1.0666e-02, 8.0109e-03, 5.6839e-03, 5.8212e-03, 6.2065e-03, 5.0278e-03,\n",
      "        8.4229e-03, 9.8495e-03, 3.7956e-03, 1.0422e-02, 4.2175e-02, 1.1734e-02,\n",
      "        1.0895e-02, 3.6964e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [87] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [87] : torch.Size([1, 32, 1, 159])\n",
      "Last layer attentions for generated token [87] : tensor([9.0027e-02, 9.0027e-02, 7.6115e-05, 2.0170e-04, 5.3942e-05, 1.1530e-03,\n",
      "        2.4891e-04, 2.4939e-04, 2.9278e-04, 3.6168e-04, 1.3752e-03, 3.8791e-04,\n",
      "        2.2578e-04, 4.6802e-04, 8.6670e-03, 5.8823e-03, 2.6822e-04, 4.2033e-04,\n",
      "        1.3864e-04, 2.6345e-04, 9.3579e-05, 2.5630e-04, 2.8954e-03, 7.5817e-04,\n",
      "        2.9793e-03, 6.3744e-03, 7.0667e-04, 4.1628e-04, 2.5988e-04, 4.4489e-04,\n",
      "        5.2023e-04, 7.9918e-04, 6.1512e-04, 7.3767e-04, 4.2701e-04, 4.3440e-04,\n",
      "        1.4639e-04, 3.6883e-04, 1.9097e-04, 1.2815e-04, 1.6413e-03, 5.0879e-04,\n",
      "        2.9445e-04, 5.9366e-04, 7.2193e-04, 2.0683e-04, 4.3440e-04, 1.8430e-04,\n",
      "        3.1543e-04, 1.8253e-03, 6.1607e-04, 1.0687e-04, 1.0986e-03, 1.3256e-03,\n",
      "        6.9714e-04, 3.7785e-03, 4.7064e-04, 1.3504e-03, 1.6432e-03, 9.5177e-04,\n",
      "        6.9141e-04, 1.3618e-03, 2.0828e-03, 7.9298e-04, 7.6103e-04, 5.5790e-04,\n",
      "        6.1035e-04, 9.9754e-04, 5.8794e-04, 9.7580e-03, 4.4937e-03, 1.9165e-02,\n",
      "        4.6196e-03, 1.1339e-03, 1.9104e-02, 8.1253e-03, 1.3809e-02, 7.1449e-03,\n",
      "        7.2861e-03, 1.0376e-02, 1.2665e-02, 1.0551e-02, 3.3321e-03, 2.4734e-02,\n",
      "        1.1948e-02, 7.1869e-03, 5.8174e-03, 2.0256e-03, 6.0234e-03, 1.9699e-02,\n",
      "        3.1013e-03, 5.7983e-03, 1.0738e-03, 3.8471e-03, 5.9891e-03, 3.1204e-03,\n",
      "        1.9035e-03, 6.6032e-03, 5.1689e-03, 5.6610e-03, 1.8524e-02, 4.4327e-03,\n",
      "        1.3123e-02, 5.5618e-03, 1.6918e-03, 4.6897e-04, 4.1885e-03, 3.9825e-03,\n",
      "        3.0174e-03, 3.7689e-03, 7.7477e-03, 3.4370e-03, 2.6073e-03, 1.2140e-03,\n",
      "        4.1313e-03, 2.0802e-04, 1.5926e-03, 2.1000e-03, 6.6338e-03, 1.0857e-02,\n",
      "        4.9782e-03, 4.4212e-03, 1.7029e-02, 1.2138e-02, 8.9951e-03, 1.7548e-02,\n",
      "        5.9052e-03, 1.1208e-02, 1.7059e-02, 7.1754e-03, 1.1826e-02, 1.8219e-02,\n",
      "        4.3869e-03, 1.3647e-03, 6.6719e-03, 5.2757e-03, 4.4098e-03, 8.6164e-04,\n",
      "        8.1558e-03, 2.7637e-03, 1.7595e-03, 1.1368e-02, 6.2675e-03, 5.5847e-03,\n",
      "        1.1620e-02, 1.3420e-02, 8.8577e-03, 5.4588e-03, 7.5073e-03, 5.0964e-03,\n",
      "        2.4429e-02, 1.4107e-02, 7.3166e-03, 1.7181e-02, 4.6906e-02, 2.0752e-02,\n",
      "        1.5236e-02, 3.5038e-03, 5.0545e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [88] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [88] : torch.Size([1, 32, 1, 160])\n",
      "Last layer attentions for generated token [88] : tensor([1.6956e-01, 1.6956e-01, 5.5254e-05, 1.0991e-04, 3.1233e-05, 1.0443e-03,\n",
      "        1.8585e-04, 1.9515e-04, 2.4629e-04, 3.0112e-04, 1.0042e-03, 1.8263e-04,\n",
      "        1.5926e-04, 3.4523e-04, 5.0583e-03, 5.6381e-03, 1.3208e-04, 4.3130e-04,\n",
      "        1.1605e-04, 1.7834e-04, 4.9531e-05, 1.8620e-04, 3.3989e-03, 5.9652e-04,\n",
      "        2.5349e-03, 6.2332e-03, 4.9543e-04, 2.9469e-04, 2.2411e-04, 3.4189e-04,\n",
      "        3.2544e-04, 8.2970e-04, 7.7915e-04, 6.4850e-04, 3.5477e-04, 2.7037e-04,\n",
      "        1.4675e-04, 3.9959e-04, 1.7703e-04, 1.1075e-04, 1.2579e-03, 6.1274e-04,\n",
      "        2.5249e-04, 5.8460e-04, 8.3923e-04, 2.1100e-04, 5.7793e-04, 2.1386e-04,\n",
      "        3.9482e-04, 1.6384e-03, 1.0881e-03, 8.8990e-05, 1.3361e-03, 1.3542e-03,\n",
      "        7.6580e-04, 4.5509e-03, 2.7776e-04, 9.0027e-04, 8.6927e-04, 6.9857e-04,\n",
      "        5.3024e-04, 8.6117e-04, 1.4877e-03, 7.3481e-04, 5.0306e-04, 4.0030e-04,\n",
      "        8.5402e-04, 5.8937e-04, 6.9046e-04, 7.5836e-03, 3.8872e-03, 1.5320e-02,\n",
      "        2.6855e-03, 7.3338e-04, 1.6281e-02, 7.3738e-03, 7.5264e-03, 5.1804e-03,\n",
      "        5.0316e-03, 7.1907e-03, 9.5444e-03, 9.6512e-03, 2.6646e-03, 1.9073e-02,\n",
      "        9.3460e-03, 4.0817e-03, 4.0398e-03, 1.6336e-03, 5.3406e-03, 1.5991e-02,\n",
      "        2.4529e-03, 4.3259e-03, 7.4816e-04, 2.4185e-03, 5.3101e-03, 2.1553e-03,\n",
      "        1.3695e-03, 4.3259e-03, 4.0245e-03, 4.7455e-03, 1.5091e-02, 3.1796e-03,\n",
      "        6.5918e-03, 4.6806e-03, 7.7486e-04, 2.8443e-04, 2.1477e-03, 3.8509e-03,\n",
      "        2.2259e-03, 3.0785e-03, 8.2550e-03, 3.0975e-03, 1.5717e-03, 1.0462e-03,\n",
      "        2.3956e-03, 1.7834e-04, 1.4791e-03, 1.5974e-03, 5.7755e-03, 1.0429e-02,\n",
      "        4.4937e-03, 3.4027e-03, 1.2459e-02, 8.9417e-03, 4.5624e-03, 1.2001e-02,\n",
      "        3.2082e-03, 8.7967e-03, 1.1124e-02, 4.5280e-03, 9.2392e-03, 1.2993e-02,\n",
      "        3.3722e-03, 8.2302e-04, 3.6945e-03, 4.1656e-03, 3.0518e-03, 5.5456e-04,\n",
      "        4.4060e-03, 1.7576e-03, 7.8249e-04, 6.1264e-03, 5.7487e-03, 3.9005e-03,\n",
      "        7.4348e-03, 1.3626e-02, 8.2474e-03, 5.0163e-03, 6.1226e-03, 4.5433e-03,\n",
      "        1.6937e-02, 1.3161e-02, 6.3744e-03, 1.1444e-02, 3.8788e-02, 1.8860e-02,\n",
      "        1.5411e-02, 3.4084e-03, 5.2032e-03, 2.3346e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [89] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [89] : torch.Size([1, 32, 1, 161])\n",
      "Last layer attentions for generated token [89] : tensor([1.7371e-01, 1.7371e-01, 1.0890e-04, 3.5357e-04, 6.5267e-05, 1.8692e-03,\n",
      "        2.1160e-04, 3.3021e-04, 5.3835e-04, 4.3416e-04, 7.2432e-04, 2.6011e-04,\n",
      "        2.3699e-04, 5.8889e-04, 7.2937e-03, 6.4468e-03, 1.7750e-04, 3.9601e-04,\n",
      "        7.9989e-05, 2.0421e-04, 6.9201e-05, 2.9612e-04, 5.1842e-03, 9.5367e-04,\n",
      "        4.1466e-03, 8.7738e-03, 5.0831e-04, 4.8923e-04, 5.9462e-04, 6.6996e-04,\n",
      "        4.2486e-04, 1.3294e-03, 6.5947e-04, 5.4121e-04, 3.5286e-04, 2.6894e-04,\n",
      "        1.6475e-04, 3.3617e-04, 2.9325e-04, 2.4116e-04, 2.7618e-03, 1.2779e-03,\n",
      "        4.0078e-04, 9.8419e-04, 1.0214e-03, 5.0640e-04, 4.9210e-04, 3.7122e-04,\n",
      "        6.8808e-04, 2.4052e-03, 1.8435e-03, 1.7536e-04, 1.9207e-03, 2.6913e-03,\n",
      "        1.0872e-03, 4.5853e-03, 2.3329e-04, 9.1028e-04, 1.3781e-03, 7.5293e-04,\n",
      "        9.2268e-04, 9.7084e-04, 1.7595e-03, 5.9462e-04, 3.8171e-04, 4.9496e-04,\n",
      "        1.1234e-03, 9.7084e-04, 9.5177e-04, 1.5778e-02, 8.6594e-03, 1.7395e-02,\n",
      "        3.0575e-03, 8.0776e-04, 1.3412e-02, 7.9422e-03, 4.2648e-03, 4.8256e-03,\n",
      "        3.7689e-03, 9.6436e-03, 7.5226e-03, 9.8648e-03, 2.4719e-03, 1.2177e-02,\n",
      "        1.4442e-02, 2.7618e-03, 2.8744e-03, 2.1706e-03, 4.6120e-03, 1.3451e-02,\n",
      "        3.3264e-03, 4.7607e-03, 1.5907e-03, 2.8915e-03, 3.5934e-03, 1.6403e-03,\n",
      "        1.9007e-03, 6.8703e-03, 3.5381e-03, 4.1046e-03, 7.8125e-03, 2.6665e-03,\n",
      "        4.5929e-03, 6.8016e-03, 8.3494e-04, 3.6407e-04, 1.7786e-03, 2.9526e-03,\n",
      "        3.2387e-03, 3.4809e-03, 3.2749e-03, 1.8816e-03, 1.6479e-03, 8.8215e-04,\n",
      "        1.4658e-03, 1.3447e-04, 9.3889e-04, 1.1892e-03, 4.6158e-03, 6.1760e-03,\n",
      "        3.1300e-03, 4.0588e-03, 9.1705e-03, 1.2306e-02, 2.7809e-03, 7.1449e-03,\n",
      "        3.8128e-03, 9.3536e-03, 8.3618e-03, 5.1270e-03, 1.0048e-02, 1.2543e-02,\n",
      "        2.5845e-03, 1.4553e-03, 3.0632e-03, 5.7144e-03, 3.0346e-03, 1.2693e-03,\n",
      "        5.6534e-03, 1.9083e-03, 7.7105e-04, 9.1095e-03, 4.3335e-03, 2.5673e-03,\n",
      "        6.3820e-03, 1.0452e-02, 5.6305e-03, 4.5891e-03, 6.3972e-03, 3.4122e-03,\n",
      "        1.3412e-02, 1.0353e-02, 6.0081e-03, 1.1612e-02, 4.3762e-02, 1.5121e-02,\n",
      "        7.8659e-03, 2.5692e-03, 8.7967e-03, 1.7532e-02, 1.5144e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [90] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [90] : torch.Size([1, 32, 1, 162])\n",
      "Last layer attentions for generated token [90] : tensor([2.9517e-01, 2.9443e-01, 6.9237e-04, 1.2665e-03, 6.1703e-04, 2.8477e-03,\n",
      "        2.6894e-04, 8.0776e-04, 1.1845e-03, 1.9083e-03, 1.7338e-03, 7.8011e-04,\n",
      "        5.4550e-04, 5.1069e-04, 3.1452e-03, 4.4556e-03, 2.8753e-04, 4.9210e-04,\n",
      "        1.2219e-04, 3.5501e-04, 1.5032e-04, 1.8711e-03, 2.0721e-02, 3.4542e-03,\n",
      "        3.6850e-03, 5.6839e-03, 1.1940e-03, 2.4402e-04, 4.2248e-04, 1.1463e-03,\n",
      "        9.5940e-04, 1.5783e-03, 4.9877e-04, 4.7874e-04, 4.7588e-04, 2.7275e-04,\n",
      "        7.6056e-04, 4.3416e-04, 2.2304e-04, 1.5092e-04, 2.8934e-03, 1.7071e-03,\n",
      "        3.7432e-04, 2.5368e-04, 1.9920e-04, 2.3925e-04, 2.0075e-04, 5.3740e-04,\n",
      "        2.8133e-04, 2.3842e-03, 1.4629e-03, 5.3596e-04, 4.8790e-03, 5.6000e-03,\n",
      "        2.9850e-03, 3.6278e-03, 4.8327e-04, 2.3289e-03, 5.6763e-03, 1.1845e-03,\n",
      "        1.2131e-03, 2.1095e-03, 3.8853e-03, 9.2649e-04, 1.1330e-03, 1.8530e-03,\n",
      "        1.9798e-03, 1.8568e-03, 1.6575e-03, 1.1879e-02, 6.8207e-03, 9.6436e-03,\n",
      "        3.0937e-03, 1.9159e-03, 1.1391e-02, 8.9722e-03, 6.3477e-03, 3.5820e-03,\n",
      "        2.7218e-03, 2.9144e-03, 7.1678e-03, 3.3760e-03, 3.1433e-03, 6.6376e-03,\n",
      "        5.5466e-03, 3.0975e-03, 1.1873e-03, 6.7377e-04, 1.4296e-03, 2.9736e-03,\n",
      "        1.0624e-03, 2.8667e-03, 8.2874e-04, 3.0613e-03, 2.7332e-03, 1.3218e-03,\n",
      "        7.0763e-04, 6.2027e-03, 1.2321e-03, 5.1956e-03, 6.2637e-03, 2.0218e-03,\n",
      "        5.4398e-03, 4.1046e-03, 2.1248e-03, 1.9226e-03, 2.1095e-03, 9.8610e-04,\n",
      "        8.0156e-04, 1.3294e-03, 1.2436e-03, 4.1509e-04, 8.9788e-04, 3.2401e-04,\n",
      "        9.0313e-04, 1.3578e-04, 2.7752e-04, 2.3103e-04, 2.1877e-03, 1.0891e-03,\n",
      "        6.6090e-04, 1.4601e-03, 5.7564e-03, 3.0251e-03, 6.4421e-04, 1.4019e-03,\n",
      "        4.3321e-04, 7.4148e-04, 1.8930e-03, 1.0033e-03, 1.7242e-03, 3.1147e-03,\n",
      "        1.1177e-03, 3.6478e-04, 1.2054e-03, 1.0452e-03, 8.7547e-04, 2.8372e-04,\n",
      "        1.6775e-03, 1.0805e-03, 1.6937e-03, 5.3139e-03, 1.7576e-03, 8.7547e-04,\n",
      "        1.3905e-03, 1.4067e-03, 1.9608e-03, 8.1110e-04, 2.1286e-03, 1.2808e-03,\n",
      "        5.4741e-03, 1.5306e-03, 1.4687e-03, 5.2185e-03, 1.4000e-02, 5.5351e-03,\n",
      "        5.4016e-03, 2.0084e-03, 4.7836e-03, 6.5613e-03, 8.6365e-03, 1.6327e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [91] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [91] : torch.Size([1, 32, 1, 163])\n",
      "Last layer attentions for generated token [91] : tensor([1.3562e-01, 1.3562e-01, 1.1432e-04, 2.3878e-04, 1.1343e-04, 1.2856e-03,\n",
      "        1.5473e-04, 1.6320e-04, 1.7810e-04, 3.6407e-04, 1.1806e-03, 2.3925e-04,\n",
      "        2.7323e-04, 5.7507e-04, 9.6588e-03, 8.0032e-03, 1.5152e-04, 9.6655e-04,\n",
      "        1.1432e-04, 2.6894e-04, 9.3699e-05, 3.1829e-04, 3.1700e-03, 9.6130e-04,\n",
      "        4.4899e-03, 8.0185e-03, 7.7677e-04, 4.2486e-04, 3.5906e-04, 4.3249e-04,\n",
      "        3.5357e-04, 9.9182e-04, 4.2653e-04, 2.7966e-04, 2.6083e-04, 4.4966e-04,\n",
      "        2.6941e-04, 2.5177e-04, 2.2471e-04, 1.7095e-04, 2.5253e-03, 1.0967e-03,\n",
      "        4.5323e-04, 5.9319e-04, 3.7336e-04, 3.7789e-04, 1.7130e-04, 2.0659e-04,\n",
      "        3.9840e-04, 1.7519e-03, 1.4400e-03, 1.2505e-04, 1.5974e-03, 3.9673e-03,\n",
      "        1.2236e-03, 4.4136e-03, 2.7108e-04, 9.4795e-04, 1.4639e-03, 5.8889e-04,\n",
      "        1.1034e-03, 1.2007e-03, 1.4410e-03, 9.6321e-04, 7.8440e-04, 7.8154e-04,\n",
      "        1.3113e-03, 9.6130e-04, 7.2145e-04, 1.0910e-02, 8.1024e-03, 1.8570e-02,\n",
      "        4.8065e-03, 1.2531e-03, 1.9455e-02, 1.2909e-02, 9.0027e-03, 6.0463e-03,\n",
      "        3.9406e-03, 5.7678e-03, 1.1116e-02, 1.0262e-02, 3.4504e-03, 1.8127e-02,\n",
      "        1.3962e-02, 3.8471e-03, 4.0627e-03, 2.2030e-03, 2.4014e-03, 9.3307e-03,\n",
      "        2.2106e-03, 6.3934e-03, 1.7490e-03, 3.0899e-03, 3.8834e-03, 1.4439e-03,\n",
      "        9.2793e-04, 3.6240e-03, 2.8172e-03, 2.8038e-03, 5.6190e-03, 2.8744e-03,\n",
      "        8.5831e-03, 8.8577e-03, 1.9150e-03, 5.1260e-04, 2.5253e-03, 3.0136e-03,\n",
      "        1.8101e-03, 2.0981e-03, 3.8643e-03, 1.7786e-03, 2.2068e-03, 9.3699e-04,\n",
      "        2.1381e-03, 2.3139e-04, 8.8739e-04, 9.1362e-04, 3.5286e-03, 7.1983e-03,\n",
      "        2.4872e-03, 4.8904e-03, 1.0620e-02, 1.8402e-02, 4.1542e-03, 8.0261e-03,\n",
      "        3.1815e-03, 5.9509e-03, 5.9128e-03, 3.7003e-03, 5.0621e-03, 1.0902e-02,\n",
      "        3.9330e-03, 1.7710e-03, 7.6752e-03, 4.0054e-03, 2.4662e-03, 1.1730e-03,\n",
      "        6.3553e-03, 2.6340e-03, 1.5831e-03, 1.1436e-02, 6.5804e-03, 3.8471e-03,\n",
      "        4.8141e-03, 6.9199e-03, 4.9858e-03, 2.3575e-03, 6.7673e-03, 3.5973e-03,\n",
      "        1.1398e-02, 6.2981e-03, 5.7755e-03, 1.0857e-02, 5.8899e-02, 2.6627e-02,\n",
      "        1.9058e-02, 5.4741e-03, 7.4005e-03, 1.6235e-02, 2.3148e-02, 9.3460e-03,\n",
      "        1.2138e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [92] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [92] : torch.Size([1, 32, 1, 164])\n",
      "Last layer attentions for generated token [92] : tensor([1.1346e-01, 1.1346e-01, 7.0691e-05, 1.5903e-04, 6.5148e-05, 1.4572e-03,\n",
      "        1.8203e-04, 1.5533e-04, 2.2340e-04, 2.5415e-04, 8.2541e-04, 3.0231e-04,\n",
      "        2.9945e-04, 7.6485e-04, 7.6103e-03, 9.5978e-03, 2.5511e-04, 4.2820e-04,\n",
      "        1.4234e-04, 3.3331e-04, 1.0657e-04, 2.1911e-04, 2.9259e-03, 1.0252e-03,\n",
      "        5.3787e-03, 1.1971e-02, 1.2188e-03, 4.7946e-04, 2.5725e-04, 3.2687e-04,\n",
      "        3.2568e-04, 8.5640e-04, 8.8024e-04, 6.6948e-04, 3.6120e-04, 4.4084e-04,\n",
      "        4.1080e-04, 3.9744e-04, 1.4198e-04, 1.2553e-04, 2.2259e-03, 9.0790e-04,\n",
      "        8.1539e-04, 6.0606e-04, 6.1464e-04, 2.8801e-04, 3.3808e-04, 1.4281e-04,\n",
      "        2.9373e-04, 1.9016e-03, 1.2512e-03, 2.3878e-04, 2.4815e-03, 3.5419e-03,\n",
      "        1.2932e-03, 4.7379e-03, 8.8549e-04, 1.1129e-03, 1.7576e-03, 8.1873e-04,\n",
      "        1.1005e-03, 1.1215e-03, 1.9016e-03, 1.1730e-03, 9.6273e-04, 5.3167e-04,\n",
      "        1.6603e-03, 1.9093e-03, 1.5383e-03, 8.7280e-03, 5.5122e-03, 1.5373e-02,\n",
      "        8.1253e-03, 1.3762e-03, 1.7715e-02, 9.2239e-03, 9.3842e-03, 4.3335e-03,\n",
      "        8.4915e-03, 6.1188e-03, 7.6904e-03, 7.9269e-03, 4.9515e-03, 1.4091e-02,\n",
      "        9.6893e-03, 7.6675e-03, 5.1842e-03, 5.8708e-03, 6.8207e-03, 9.6359e-03,\n",
      "        2.5501e-03, 3.7003e-03, 1.5831e-03, 3.5057e-03, 6.0844e-03, 2.4910e-03,\n",
      "        1.6537e-03, 3.0136e-03, 3.5324e-03, 3.5133e-03, 9.6512e-03, 2.6283e-03,\n",
      "        9.1934e-03, 6.3362e-03, 2.1381e-03, 5.4979e-04, 3.8261e-03, 4.7417e-03,\n",
      "        2.4281e-03, 3.0613e-03, 5.1880e-03, 2.6798e-03, 2.4490e-03, 1.3809e-03,\n",
      "        2.9449e-03, 2.2423e-04, 1.6479e-03, 3.0060e-03, 6.4049e-03, 1.0117e-02,\n",
      "        4.7302e-03, 4.0131e-03, 1.3954e-02, 1.2756e-02, 8.1100e-03, 7.9651e-03,\n",
      "        3.3817e-03, 1.7441e-02, 1.4214e-02, 5.8060e-03, 6.5422e-03, 9.8877e-03,\n",
      "        2.3861e-03, 1.5421e-03, 2.6150e-03, 3.3951e-03, 1.8215e-03, 9.7227e-04,\n",
      "        5.0392e-03, 2.2068e-03, 1.3952e-03, 7.2365e-03, 5.9586e-03, 3.0155e-03,\n",
      "        7.1335e-03, 5.7602e-03, 6.1264e-03, 3.0403e-03, 5.9814e-03, 3.6888e-03,\n",
      "        1.4473e-02, 5.6496e-03, 5.4207e-03, 1.7044e-02, 4.7363e-02, 2.4734e-02,\n",
      "        1.5503e-02, 7.1259e-03, 8.8120e-03, 1.5732e-02, 2.2720e-02, 6.9580e-03,\n",
      "        2.0172e-02, 7.2746e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [93] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [93] : torch.Size([1, 32, 1, 165])\n",
      "Last layer attentions for generated token [93] : tensor([2.0862e-01, 2.0813e-01, 2.6155e-04, 3.7026e-04, 1.5712e-04, 2.9907e-03,\n",
      "        2.4331e-04, 2.6464e-04, 2.5296e-04, 3.7384e-04, 8.3113e-04, 2.0409e-04,\n",
      "        5.7459e-04, 1.3027e-03, 5.4359e-03, 7.1640e-03, 2.7132e-04, 3.8195e-04,\n",
      "        1.4865e-04, 2.3210e-04, 8.7738e-05, 5.0211e-04, 1.3252e-02, 2.0561e-03,\n",
      "        3.7899e-03, 1.0338e-02, 2.1915e-03, 8.4591e-04, 3.5119e-04, 4.7994e-04,\n",
      "        5.3263e-04, 1.2798e-03, 7.2050e-04, 6.8235e-04, 2.3627e-04, 4.7350e-04,\n",
      "        6.4230e-04, 3.5334e-04, 2.8443e-04, 2.6822e-04, 2.0790e-03, 1.8330e-03,\n",
      "        9.8705e-04, 9.7370e-04, 5.0306e-04, 3.2282e-04, 1.5712e-04, 2.8062e-04,\n",
      "        3.4380e-04, 1.0757e-03, 1.6718e-03, 2.9230e-04, 3.0975e-03, 5.4359e-03,\n",
      "        2.5368e-03, 4.9362e-03, 4.1461e-04, 1.6241e-03, 2.4376e-03, 1.1492e-03,\n",
      "        1.3409e-03, 1.4811e-03, 1.6174e-03, 9.7370e-04, 8.0252e-04, 4.5443e-04,\n",
      "        2.0046e-03, 1.4963e-03, 1.6022e-03, 9.9106e-03, 6.2675e-03, 1.1505e-02,\n",
      "        6.8321e-03, 1.1816e-03, 1.0536e-02, 6.5918e-03, 6.8588e-03, 4.4327e-03,\n",
      "        3.0403e-03, 4.4937e-03, 5.5237e-03, 5.8022e-03, 2.9697e-03, 7.7362e-03,\n",
      "        7.4577e-03, 3.2330e-03, 3.2387e-03, 2.5330e-03, 4.1962e-03, 4.7150e-03,\n",
      "        2.0370e-03, 2.3117e-03, 9.1982e-04, 1.6308e-03, 2.9316e-03, 1.0777e-03,\n",
      "        7.0667e-04, 2.3651e-03, 1.7185e-03, 2.5654e-03, 3.0422e-03, 2.7218e-03,\n",
      "        3.5496e-03, 6.2408e-03, 1.0204e-03, 8.7261e-04, 2.2717e-03, 2.2144e-03,\n",
      "        1.4219e-03, 1.5831e-03, 1.9493e-03, 1.0118e-03, 6.9046e-04, 6.5994e-04,\n",
      "        1.1158e-03, 2.0409e-04, 8.1825e-04, 1.2455e-03, 2.8648e-03, 5.9128e-03,\n",
      "        1.7815e-03, 2.1400e-03, 4.5013e-03, 1.4015e-02, 7.4272e-03, 6.2714e-03,\n",
      "        3.0289e-03, 4.6387e-03, 7.6370e-03, 2.3880e-03, 4.0398e-03, 4.9286e-03,\n",
      "        2.2449e-03, 1.1005e-03, 2.1553e-03, 2.9373e-03, 1.6241e-03, 6.5613e-04,\n",
      "        2.5654e-03, 1.3514e-03, 1.2302e-03, 2.4300e-03, 3.6602e-03, 2.8439e-03,\n",
      "        1.0521e-02, 6.1111e-03, 6.3400e-03, 2.3994e-03, 3.4142e-03, 2.6703e-03,\n",
      "        4.9438e-03, 4.8332e-03, 2.9430e-03, 4.7035e-03, 4.7272e-02, 2.0554e-02,\n",
      "        1.6159e-02, 6.1569e-03, 1.2894e-02, 1.4801e-02, 9.3002e-03, 8.2397e-03,\n",
      "        1.0002e-02, 1.3573e-02, 4.3106e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [94] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [94] : torch.Size([1, 32, 1, 166])\n",
      "Last layer attentions for generated token [94] : tensor([1.0425e-01, 1.0406e-01, 8.3566e-05, 1.6010e-04, 5.0902e-05, 1.4000e-03,\n",
      "        4.6706e-04, 4.2272e-04, 5.5361e-04, 4.9639e-04, 1.0805e-03, 3.2353e-04,\n",
      "        2.6560e-04, 7.0095e-04, 5.5466e-03, 5.5161e-03, 2.5535e-04, 3.7384e-04,\n",
      "        1.4758e-04, 2.4605e-04, 8.0347e-05, 1.9968e-04, 3.0117e-03, 8.0538e-04,\n",
      "        3.1090e-03, 7.9041e-03, 9.0742e-04, 6.7425e-04, 3.2043e-04, 4.8828e-04,\n",
      "        4.7612e-04, 6.8760e-04, 7.9918e-04, 9.2316e-04, 4.2272e-04, 3.7241e-04,\n",
      "        2.7823e-04, 4.8280e-04, 1.6880e-04, 1.4246e-04, 1.3123e-03, 5.7554e-04,\n",
      "        3.8338e-04, 4.7612e-04, 7.1335e-04, 2.3854e-04, 4.0483e-04, 1.9014e-04,\n",
      "        4.4990e-04, 1.4744e-03, 7.0238e-04, 1.2493e-04, 1.7920e-03, 1.4915e-03,\n",
      "        1.1129e-03, 3.2330e-03, 4.6706e-04, 8.6737e-04, 9.1982e-04, 6.4087e-04,\n",
      "        3.5119e-04, 7.8964e-04, 1.4095e-03, 6.4707e-04, 4.2677e-04, 3.6025e-04,\n",
      "        6.6519e-04, 1.0805e-03, 7.3624e-04, 7.4997e-03, 3.8280e-03, 1.0773e-02,\n",
      "        4.6158e-03, 1.1559e-03, 1.5930e-02, 5.3940e-03, 9.7961e-03, 4.0665e-03,\n",
      "        6.9046e-03, 8.4152e-03, 7.0000e-03, 6.8474e-03, 3.1242e-03, 1.5762e-02,\n",
      "        5.8975e-03, 5.0735e-03, 3.3455e-03, 3.0594e-03, 7.3853e-03, 1.6357e-02,\n",
      "        2.0714e-03, 2.9068e-03, 8.0681e-04, 2.4014e-03, 7.0000e-03, 3.0575e-03,\n",
      "        2.2964e-03, 5.6000e-03, 3.2520e-03, 6.6986e-03, 1.2825e-02, 2.2793e-03,\n",
      "        6.7368e-03, 3.4714e-03, 9.6560e-04, 3.2783e-04, 3.0174e-03, 2.5787e-03,\n",
      "        2.7981e-03, 2.6894e-03, 5.2528e-03, 2.3994e-03, 1.5221e-03, 1.0004e-03,\n",
      "        3.0727e-03, 1.5461e-04, 1.1864e-03, 1.8024e-03, 5.3444e-03, 8.9951e-03,\n",
      "        3.8452e-03, 4.2992e-03, 1.7899e-02, 8.1863e-03, 6.3629e-03, 1.2810e-02,\n",
      "        4.3335e-03, 1.4526e-02, 1.4854e-02, 5.7373e-03, 1.3321e-02, 1.4145e-02,\n",
      "        2.7199e-03, 1.2922e-03, 3.5648e-03, 4.8141e-03, 2.6665e-03, 8.9121e-04,\n",
      "        5.5428e-03, 2.5539e-03, 8.3113e-04, 9.7961e-03, 5.9700e-03, 4.9210e-03,\n",
      "        9.1934e-03, 1.1086e-02, 1.1307e-02, 4.7646e-03, 6.4850e-03, 6.3171e-03,\n",
      "        2.3041e-02, 1.2054e-02, 6.1493e-03, 1.6937e-02, 3.8757e-02, 2.0462e-02,\n",
      "        1.8448e-02, 4.6501e-03, 7.7934e-03, 2.8671e-02, 3.2074e-02, 7.4539e-03,\n",
      "        1.8616e-02, 9.2010e-03, 3.4180e-03, 7.8201e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [95] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [95] : torch.Size([1, 32, 1, 167])\n",
      "Last layer attentions for generated token [95] : tensor([2.3267e-01, 2.3267e-01, 3.6144e-04, 4.5347e-04, 2.2399e-04, 5.0583e-03,\n",
      "        2.7442e-04, 3.3379e-04, 5.7554e-04, 4.9686e-04, 1.1158e-03, 2.3484e-04,\n",
      "        3.7074e-04, 9.1410e-04, 4.8180e-03, 9.5367e-03, 4.3607e-04, 3.4428e-04,\n",
      "        1.2863e-04, 2.6917e-04, 1.3852e-04, 6.0320e-04, 1.8097e-02, 2.0638e-03,\n",
      "        2.8820e-03, 1.0803e-02, 5.5647e-04, 5.4932e-04, 5.0497e-04, 5.0783e-04,\n",
      "        4.3011e-04, 7.1907e-04, 5.4693e-04, 7.3767e-04, 3.8028e-04, 8.1301e-04,\n",
      "        3.1781e-04, 3.1161e-04, 2.9325e-04, 2.2149e-04, 1.4381e-03, 1.1139e-03,\n",
      "        3.9315e-04, 6.0654e-04, 5.8794e-04, 3.6144e-04, 4.4465e-04, 2.8110e-04,\n",
      "        4.7064e-04, 9.7656e-04, 9.9850e-04, 2.4700e-04, 3.3340e-03, 2.8057e-03,\n",
      "        1.8072e-03, 5.4779e-03, 4.3869e-04, 1.7052e-03, 2.0008e-03, 1.3618e-03,\n",
      "        9.8038e-04, 1.3828e-03, 2.3594e-03, 9.6750e-04, 7.5626e-04, 5.0116e-04,\n",
      "        1.3514e-03, 8.5878e-04, 1.3046e-03, 6.5804e-03, 5.4054e-03, 1.5099e-02,\n",
      "        5.5962e-03, 1.5278e-03, 8.4915e-03, 4.9095e-03, 4.9515e-03, 3.4714e-03,\n",
      "        3.4981e-03, 4.7302e-03, 4.1962e-03, 4.7073e-03, 2.9278e-03, 6.6681e-03,\n",
      "        5.8365e-03, 2.2850e-03, 1.4925e-03, 2.2545e-03, 4.3564e-03, 3.4714e-03,\n",
      "        1.0967e-03, 2.2221e-03, 1.2426e-03, 1.2617e-03, 2.3899e-03, 1.5249e-03,\n",
      "        1.5373e-03, 3.4637e-03, 2.9602e-03, 3.8681e-03, 4.1122e-03, 2.3594e-03,\n",
      "        3.6774e-03, 3.7937e-03, 9.0027e-04, 5.7888e-04, 8.8072e-04, 1.3170e-03,\n",
      "        1.7319e-03, 1.6270e-03, 2.2964e-03, 1.3332e-03, 6.0415e-04, 5.3120e-04,\n",
      "        8.0204e-04, 1.2136e-04, 8.5211e-04, 1.4553e-03, 1.8873e-03, 2.3823e-03,\n",
      "        2.5959e-03, 2.3994e-03, 6.6643e-03, 7.2365e-03, 2.1420e-03, 2.8400e-03,\n",
      "        1.1835e-03, 3.3569e-03, 6.3019e-03, 3.5706e-03, 6.1073e-03, 4.7798e-03,\n",
      "        1.6174e-03, 9.1076e-04, 1.9016e-03, 3.7823e-03, 2.1095e-03, 8.5878e-04,\n",
      "        2.5482e-03, 1.9350e-03, 1.3561e-03, 4.6692e-03, 3.3417e-03, 2.1152e-03,\n",
      "        4.8523e-03, 4.2114e-03, 1.0216e-02, 4.1313e-03, 4.1084e-03, 2.9335e-03,\n",
      "        5.4817e-03, 3.5286e-03, 2.7485e-03, 5.8250e-03, 2.9999e-02, 1.2642e-02,\n",
      "        8.9417e-03, 2.8553e-03, 8.4534e-03, 1.3603e-02, 1.7227e-02, 1.0811e-02,\n",
      "        9.3689e-03, 7.7629e-03, 4.8943e-03, 9.7427e-03, 1.2093e-02],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [96] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [96] : torch.Size([1, 32, 1, 168])\n",
      "Last layer attentions for generated token [96] : tensor([2.1912e-01, 2.1912e-01, 2.8014e-04, 5.3501e-04, 1.5140e-04, 4.5433e-03,\n",
      "        4.9448e-04, 6.5804e-04, 7.6485e-04, 7.7343e-04, 1.0014e-03, 3.7408e-04,\n",
      "        3.2759e-04, 6.1321e-04, 4.0665e-03, 5.6839e-03, 8.3303e-04, 6.1560e-04,\n",
      "        1.4734e-04, 2.1112e-04, 9.3281e-05, 8.5306e-04, 1.3222e-02, 1.5383e-03,\n",
      "        2.5311e-03, 1.0788e-02, 7.3671e-04, 5.8508e-04, 6.5422e-04, 1.1911e-03,\n",
      "        9.2220e-04, 8.5783e-04, 7.6914e-04, 1.0633e-03, 7.7963e-04, 6.1321e-04,\n",
      "        3.5143e-04, 5.1737e-04, 4.1246e-04, 3.1638e-04, 1.5059e-03, 8.1730e-04,\n",
      "        2.5964e-04, 5.9319e-04, 5.9319e-04, 3.4928e-04, 4.5657e-04, 4.0054e-04,\n",
      "        6.5041e-04, 1.5383e-03, 1.0843e-03, 1.7643e-04, 2.5482e-03, 4.1199e-03,\n",
      "        1.5717e-03, 5.8823e-03, 2.7704e-04, 1.6603e-03, 2.2659e-03, 1.0576e-03,\n",
      "        7.3671e-04, 1.3762e-03, 2.8801e-03, 6.8283e-04, 7.5865e-04, 7.5722e-04,\n",
      "        9.4032e-04, 1.4515e-03, 1.0786e-03, 1.1269e-02, 5.8136e-03, 1.6266e-02,\n",
      "        4.0245e-03, 1.3523e-03, 1.3069e-02, 7.1640e-03, 7.9193e-03, 5.1727e-03,\n",
      "        4.4708e-03, 8.1787e-03, 4.3602e-03, 5.3062e-03, 1.8835e-03, 9.4757e-03,\n",
      "        7.4158e-03, 2.9087e-03, 1.5812e-03, 1.1864e-03, 3.2578e-03, 5.8823e-03,\n",
      "        1.5879e-03, 3.9673e-03, 1.5030e-03, 1.9464e-03, 1.9035e-03, 1.3008e-03,\n",
      "        1.8797e-03, 5.1155e-03, 3.6240e-03, 6.7368e-03, 5.8441e-03, 2.7084e-03,\n",
      "        4.8676e-03, 4.9057e-03, 1.2655e-03, 7.2813e-04, 1.5297e-03, 1.4677e-03,\n",
      "        2.4834e-03, 1.8959e-03, 1.6470e-03, 1.3981e-03, 9.2411e-04, 5.7030e-04,\n",
      "        1.2217e-03, 1.2219e-04, 7.7963e-04, 1.2560e-03, 1.9522e-03, 2.3537e-03,\n",
      "        1.8330e-03, 4.5662e-03, 7.0038e-03, 7.0572e-03, 2.2793e-03, 4.6806e-03,\n",
      "        1.4801e-03, 3.8719e-03, 3.0804e-03, 2.6379e-03, 6.4125e-03, 6.6948e-03,\n",
      "        1.8415e-03, 9.2936e-04, 2.2755e-03, 4.3678e-03, 2.8610e-03, 7.2241e-04,\n",
      "        3.2730e-03, 1.2293e-03, 1.5535e-03, 5.3825e-03, 2.4319e-03, 1.6346e-03,\n",
      "        4.8294e-03, 4.1695e-03, 7.6332e-03, 5.0735e-03, 5.1231e-03, 3.1624e-03,\n",
      "        7.6981e-03, 5.7526e-03, 3.5114e-03, 5.2605e-03, 2.3224e-02, 1.2817e-02,\n",
      "        8.0643e-03, 1.9722e-03, 6.0539e-03, 1.4366e-02, 1.8433e-02, 1.2016e-02,\n",
      "        1.0071e-02, 5.5771e-03, 3.2692e-03, 4.3716e-03, 8.8425e-03, 3.4733e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [97] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [97] : torch.Size([1, 32, 1, 169])\n",
      "Last layer attentions for generated token [97] : tensor([1.5405e-01, 1.5405e-01, 2.6011e-04, 5.1594e-04, 2.0492e-04, 1.9722e-03,\n",
      "        3.4165e-04, 2.6870e-04, 4.4227e-04, 5.3787e-04, 9.1982e-04, 3.3259e-04,\n",
      "        4.3368e-04, 7.5817e-04, 8.5754e-03, 7.8049e-03, 1.9395e-04, 8.1015e-04,\n",
      "        1.4055e-04, 3.2425e-04, 1.4329e-04, 5.1689e-04, 4.4594e-03, 1.4124e-03,\n",
      "        4.0016e-03, 8.8730e-03, 9.3651e-04, 5.3453e-04, 3.9268e-04, 4.9353e-04,\n",
      "        4.7636e-04, 7.4816e-04, 6.2847e-04, 4.3702e-04, 2.7204e-04, 4.4060e-04,\n",
      "        3.5954e-04, 3.5334e-04, 2.9874e-04, 2.2328e-04, 2.2469e-03, 1.0815e-03,\n",
      "        4.3702e-04, 9.0551e-04, 6.5994e-04, 3.9792e-04, 1.4102e-04, 2.9111e-04,\n",
      "        3.9101e-04, 1.6117e-03, 1.1787e-03, 1.8048e-04, 2.1458e-03, 3.3779e-03,\n",
      "        1.7614e-03, 4.8790e-03, 3.7456e-04, 9.2554e-04, 1.8044e-03, 7.9012e-04,\n",
      "        7.2193e-04, 5.2643e-04, 1.3437e-03, 6.6900e-04, 4.2868e-04, 3.8576e-04,\n",
      "        9.7370e-04, 9.4175e-04, 7.2622e-04, 7.7934e-03, 5.8441e-03, 1.5152e-02,\n",
      "        4.5815e-03, 1.0099e-03, 1.1826e-02, 7.6866e-03, 9.3002e-03, 4.8409e-03,\n",
      "        4.4174e-03, 7.6447e-03, 7.0992e-03, 8.7891e-03, 2.8305e-03, 8.9951e-03,\n",
      "        1.0910e-02, 3.9711e-03, 2.5406e-03, 1.7557e-03, 2.1191e-03, 7.2861e-03,\n",
      "        1.6737e-03, 2.6703e-03, 1.1034e-03, 2.0657e-03, 3.0460e-03, 1.3390e-03,\n",
      "        1.0386e-03, 7.6942e-03, 3.1490e-03, 4.3564e-03, 7.8659e-03, 3.8109e-03,\n",
      "        6.0501e-03, 7.0572e-03, 2.2469e-03, 4.9067e-04, 1.6356e-03, 1.6594e-03,\n",
      "        1.3027e-03, 1.2922e-03, 2.8725e-03, 1.2951e-03, 1.4915e-03, 8.8644e-04,\n",
      "        1.8082e-03, 1.7154e-04, 7.9012e-04, 8.1348e-04, 2.9316e-03, 4.9286e-03,\n",
      "        1.7252e-03, 2.3308e-03, 1.1040e-02, 1.2863e-02, 4.2076e-03, 8.3313e-03,\n",
      "        1.8511e-03, 7.4730e-03, 6.0883e-03, 3.7327e-03, 7.0763e-03, 1.1772e-02,\n",
      "        2.2469e-03, 9.6035e-04, 2.8362e-03, 3.7766e-03, 2.4929e-03, 1.1206e-03,\n",
      "        4.5776e-03, 2.6054e-03, 1.3027e-03, 9.1019e-03, 6.6681e-03, 3.6812e-03,\n",
      "        8.1787e-03, 7.1793e-03, 6.4507e-03, 2.8400e-03, 5.4436e-03, 4.9515e-03,\n",
      "        1.4931e-02, 9.4376e-03, 4.0016e-03, 1.2360e-02, 5.6152e-02, 2.1378e-02,\n",
      "        1.2268e-02, 3.9787e-03, 6.9046e-03, 1.4328e-02, 1.6312e-02, 2.2018e-02,\n",
      "        1.4313e-02, 7.3586e-03, 2.6836e-03, 4.0207e-03, 7.0343e-03, 1.6384e-03,\n",
      "        6.6986e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [98] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [98] : torch.Size([1, 32, 1, 170])\n",
      "Last layer attentions for generated token [98] : tensor([1.3940e-01, 1.3940e-01, 1.2648e-04, 1.7929e-04, 8.9645e-05, 1.8797e-03,\n",
      "        1.8430e-04, 3.4499e-04, 5.5218e-04, 3.9792e-04, 7.3624e-04, 3.8123e-04,\n",
      "        2.7180e-04, 3.8338e-04, 3.6659e-03, 3.4809e-03, 2.7895e-04, 2.9278e-04,\n",
      "        8.2612e-05, 1.5306e-04, 7.4625e-05, 3.9935e-04, 3.7403e-03, 8.2445e-04,\n",
      "        2.8210e-03, 6.1188e-03, 2.9969e-04, 2.3711e-04, 3.5739e-04, 4.2439e-04,\n",
      "        3.2783e-04, 6.4468e-04, 5.0211e-04, 4.6968e-04, 3.8123e-04, 3.1662e-04,\n",
      "        1.4246e-04, 2.5439e-04, 1.6880e-04, 1.9205e-04, 1.4915e-03, 5.1069e-04,\n",
      "        1.8537e-04, 3.4976e-04, 3.4237e-04, 2.4891e-04, 1.8430e-04, 1.7965e-04,\n",
      "        3.9721e-04, 1.3447e-03, 8.0824e-04, 1.5223e-04, 1.7471e-03, 2.0180e-03,\n",
      "        1.0748e-03, 2.9068e-03, 1.7488e-04, 1.0834e-03, 1.7433e-03, 8.3399e-04,\n",
      "        1.0061e-03, 1.0166e-03, 2.1439e-03, 7.0095e-04, 9.0027e-04, 6.4945e-04,\n",
      "        8.0538e-04, 1.3819e-03, 9.9277e-04, 1.0811e-02, 6.0387e-03, 1.6235e-02,\n",
      "        3.5152e-03, 7.4768e-04, 1.3412e-02, 8.1177e-03, 6.4545e-03, 4.9858e-03,\n",
      "        5.9776e-03, 7.8049e-03, 9.1324e-03, 6.4735e-03, 2.2926e-03, 1.1742e-02,\n",
      "        9.4986e-03, 4.2229e-03, 2.0866e-03, 1.9569e-03, 3.3379e-03, 8.9340e-03,\n",
      "        2.0657e-03, 4.6959e-03, 2.3785e-03, 4.2572e-03, 4.0054e-03, 2.0657e-03,\n",
      "        2.7828e-03, 1.0071e-02, 3.8357e-03, 8.1253e-03, 1.1322e-02, 3.8376e-03,\n",
      "        9.9258e-03, 7.1564e-03, 1.0614e-03, 4.0483e-04, 2.9125e-03, 2.2106e-03,\n",
      "        2.3174e-03, 2.3232e-03, 2.0885e-03, 1.4143e-03, 1.4620e-03, 8.4543e-04,\n",
      "        1.7471e-03, 1.5283e-04, 9.5987e-04, 1.3857e-03, 3.4046e-03, 4.5700e-03,\n",
      "        2.2259e-03, 4.5891e-03, 1.2962e-02, 1.0384e-02, 2.1896e-03, 8.4610e-03,\n",
      "        2.2049e-03, 6.2370e-03, 5.0087e-03, 3.2387e-03, 7.4844e-03, 9.7656e-03,\n",
      "        1.9855e-03, 1.9503e-03, 3.7422e-03, 5.7335e-03, 2.9221e-03, 1.7881e-03,\n",
      "        8.8120e-03, 2.0657e-03, 2.4681e-03, 1.2566e-02, 4.5891e-03, 2.5940e-03,\n",
      "        9.4986e-03, 8.2245e-03, 7.7667e-03, 4.8637e-03, 7.1602e-03, 4.2076e-03,\n",
      "        1.8402e-02, 1.0826e-02, 7.0038e-03, 1.3489e-02, 5.2002e-02, 1.8784e-02,\n",
      "        9.2545e-03, 2.5997e-03, 1.2062e-02, 2.1133e-02, 2.1362e-02, 6.6528e-03,\n",
      "        1.2863e-02, 7.6332e-03, 4.9362e-03, 8.0338e-03, 4.2458e-03, 1.7691e-03,\n",
      "        5.7602e-03, 5.4436e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [99] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [99] : torch.Size([1, 32, 1, 171])\n",
      "Last layer attentions for generated token [99] : tensor([1.0175e-01, 1.0150e-01, 1.0949e-04, 2.7966e-04, 1.1432e-04, 1.2846e-03,\n",
      "        2.3091e-04, 2.2256e-04, 3.0708e-04, 4.7660e-04, 9.3317e-04, 3.6693e-04,\n",
      "        1.9681e-04, 5.4216e-04, 6.1607e-03, 5.7487e-03, 1.7989e-04, 6.6185e-04,\n",
      "        8.9049e-05, 1.0949e-04, 9.9719e-05, 4.6206e-04, 2.6817e-03, 1.3809e-03,\n",
      "        4.1237e-03, 7.6828e-03, 1.0071e-03, 4.1413e-04, 2.7418e-04, 8.1396e-04,\n",
      "        4.2653e-04, 1.3046e-03, 5.2738e-04, 6.3515e-04, 7.0286e-04, 6.0129e-04,\n",
      "        2.5010e-04, 5.4979e-04, 2.6107e-04, 2.0623e-04, 2.4452e-03, 1.1778e-03,\n",
      "        3.3736e-04, 6.5804e-04, 5.9557e-04, 2.1529e-04, 3.2377e-04, 2.1732e-04,\n",
      "        3.3998e-04, 2.8152e-03, 1.4553e-03, 2.1613e-04, 1.7862e-03, 3.8834e-03,\n",
      "        1.4038e-03, 3.8528e-03, 2.3043e-04, 6.5517e-04, 8.6975e-04, 4.5037e-04,\n",
      "        4.6659e-04, 6.8808e-04, 1.2074e-03, 5.7602e-04, 3.5357e-04, 5.9891e-04,\n",
      "        5.5313e-04, 9.2220e-04, 5.2738e-04, 8.5373e-03, 6.0425e-03, 1.2848e-02,\n",
      "        5.2986e-03, 9.5510e-04, 2.2476e-02, 9.0408e-03, 9.8190e-03, 4.9477e-03,\n",
      "        5.4398e-03, 9.6664e-03, 1.3878e-02, 9.5825e-03, 3.2196e-03, 2.1423e-02,\n",
      "        1.1475e-02, 3.7479e-03, 2.1362e-03, 1.2360e-03, 2.2717e-03, 8.1329e-03,\n",
      "        1.4515e-03, 3.2692e-03, 8.7500e-04, 1.8578e-03, 2.9335e-03, 1.4248e-03,\n",
      "        1.0090e-03, 1.0971e-02, 2.9545e-03, 7.2823e-03, 8.2092e-03, 4.3335e-03,\n",
      "        6.4850e-03, 5.4817e-03, 9.2793e-04, 2.3544e-04, 1.6251e-03, 2.3441e-03,\n",
      "        2.2831e-03, 2.2984e-03, 4.0703e-03, 1.2302e-03, 1.4725e-03, 5.4741e-04,\n",
      "        1.5249e-03, 1.2267e-04, 9.4986e-04, 1.0939e-03, 4.7417e-03, 6.1111e-03,\n",
      "        2.9869e-03, 5.5542e-03, 1.4374e-02, 8.9340e-03, 2.4376e-03, 9.3613e-03,\n",
      "        1.4772e-03, 4.1389e-03, 5.7030e-03, 4.0703e-03, 6.6833e-03, 1.1261e-02,\n",
      "        2.0428e-03, 7.8440e-04, 4.6425e-03, 3.2806e-03, 2.4281e-03, 7.7677e-04,\n",
      "        5.7526e-03, 2.2469e-03, 8.1730e-04, 9.8801e-03, 5.4893e-03, 2.8858e-03,\n",
      "        7.7171e-03, 7.4081e-03, 9.2163e-03, 3.7651e-03, 7.8888e-03, 4.1237e-03,\n",
      "        1.7639e-02, 1.0071e-02, 8.2092e-03, 1.4328e-02, 4.9957e-02, 2.6306e-02,\n",
      "        1.5915e-02, 4.5967e-03, 7.2746e-03, 2.1027e-02, 3.0548e-02, 1.8890e-02,\n",
      "        1.7853e-02, 9.3613e-03, 3.5477e-03, 6.8169e-03, 8.1177e-03, 1.7748e-03,\n",
      "        8.4305e-03, 5.3673e-03, 1.8051e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [100] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [100] : torch.Size([1, 32, 1, 172])\n",
      "Last layer attentions for generated token [100] : tensor([1.0321e-01, 1.0321e-01, 1.3196e-04, 2.9039e-04, 1.2469e-04, 1.4877e-03,\n",
      "        1.5092e-04, 1.9419e-04, 3.5715e-04, 3.1638e-04, 7.6532e-04, 2.9683e-04,\n",
      "        2.9111e-04, 5.5218e-04, 7.9498e-03, 9.2163e-03, 2.0349e-04, 3.7575e-04,\n",
      "        1.0294e-04, 2.0397e-04, 6.3658e-05, 2.3425e-04, 2.7943e-03, 1.3208e-03,\n",
      "        3.7460e-03, 1.0544e-02, 1.5507e-03, 3.0494e-04, 2.2709e-04, 4.2009e-04,\n",
      "        3.5238e-04, 1.0872e-03, 5.5552e-04, 3.5715e-04, 2.9397e-04, 2.0230e-04,\n",
      "        1.7178e-04, 3.8624e-04, 8.8036e-05, 8.8751e-05, 2.6951e-03, 9.5129e-04,\n",
      "        6.9284e-04, 4.6134e-04, 4.5323e-04, 1.6773e-04, 2.4557e-04, 1.0020e-04,\n",
      "        1.4830e-04, 1.2989e-03, 7.1049e-04, 1.9574e-04, 1.7843e-03, 3.5610e-03,\n",
      "        1.4296e-03, 3.8338e-03, 4.3774e-04, 6.5470e-04, 1.3885e-03, 4.9591e-04,\n",
      "        5.0974e-04, 7.0095e-04, 1.6537e-03, 5.7983e-04, 4.4727e-04, 4.7612e-04,\n",
      "        9.9087e-04, 1.1253e-03, 9.9659e-04, 7.4997e-03, 6.0005e-03, 1.0384e-02,\n",
      "        7.6180e-03, 9.2316e-04, 2.0767e-02, 7.9651e-03, 1.1551e-02, 3.5915e-03,\n",
      "        5.1956e-03, 5.1155e-03, 9.9030e-03, 6.9008e-03, 5.3558e-03, 1.4954e-02,\n",
      "        9.3079e-03, 8.6899e-03, 3.1586e-03, 3.5496e-03, 4.8790e-03, 9.5291e-03,\n",
      "        1.8721e-03, 3.5782e-03, 1.1053e-03, 3.9177e-03, 5.4932e-03, 2.3346e-03,\n",
      "        1.3285e-03, 5.2643e-03, 2.6665e-03, 5.4359e-03, 1.2772e-02, 2.9469e-03,\n",
      "        8.7967e-03, 5.0163e-03, 1.2188e-03, 3.3236e-04, 2.8400e-03, 2.6875e-03,\n",
      "        1.3771e-03, 2.5120e-03, 4.8790e-03, 1.2970e-03, 1.4257e-03, 7.9107e-04,\n",
      "        2.0313e-03, 1.6069e-04, 8.9264e-04, 1.4420e-03, 5.4970e-03, 7.3242e-03,\n",
      "        3.4046e-03, 3.6259e-03, 1.5976e-02, 8.9493e-03, 5.2605e-03, 7.7972e-03,\n",
      "        2.5787e-03, 1.0780e-02, 1.0780e-02, 4.7455e-03, 6.7406e-03, 1.1024e-02,\n",
      "        2.2297e-03, 9.2411e-04, 2.2640e-03, 3.3646e-03, 1.9503e-03, 9.2411e-04,\n",
      "        5.9471e-03, 2.5406e-03, 1.1711e-03, 1.0178e-02, 5.0774e-03, 2.5902e-03,\n",
      "        5.0659e-03, 4.1733e-03, 6.3782e-03, 2.0676e-03, 4.5280e-03, 2.3689e-03,\n",
      "        1.2894e-02, 5.2414e-03, 3.1300e-03, 1.6327e-02, 4.1687e-02, 2.3071e-02,\n",
      "        1.6037e-02, 6.1836e-03, 7.1869e-03, 1.7548e-02, 3.1250e-02, 9.0256e-03,\n",
      "        2.0798e-02, 1.0628e-02, 3.7403e-03, 1.0620e-02, 5.1727e-03, 1.8282e-03,\n",
      "        1.1765e-02, 4.4861e-03, 1.3847e-02, 2.6657e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [101] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [101] : torch.Size([1, 32, 1, 173])\n",
      "Last layer attentions for generated token [101] : tensor([1.5588e-01, 1.5588e-01, 8.4758e-05, 1.7321e-04, 5.3465e-05, 1.0567e-03,\n",
      "        2.2113e-04, 1.8656e-04, 2.5058e-04, 2.0206e-04, 5.6696e-04, 2.1386e-04,\n",
      "        3.8886e-04, 9.0027e-04, 6.1913e-03, 4.0550e-03, 1.8322e-04, 2.6608e-04,\n",
      "        9.0897e-05, 1.4031e-04, 7.1049e-05, 1.9586e-04, 4.2305e-03, 1.0948e-03,\n",
      "        3.4180e-03, 7.9651e-03, 1.1187e-03, 4.1389e-04, 2.9397e-04, 1.6987e-04,\n",
      "        2.4378e-04, 6.4087e-04, 5.3978e-04, 2.9230e-04, 1.8549e-04, 3.4976e-04,\n",
      "        2.6059e-04, 2.3901e-04, 1.9360e-04, 1.4353e-04, 1.1654e-03, 9.8133e-04,\n",
      "        5.8460e-04, 4.0746e-04, 2.8443e-04, 2.8777e-04, 8.7798e-05, 1.1855e-04,\n",
      "        2.0933e-04, 7.1764e-04, 8.4734e-04, 8.0884e-05, 2.1477e-03, 2.2659e-03,\n",
      "        1.8530e-03, 3.3951e-03, 5.0497e-04, 2.2259e-03, 2.3518e-03, 7.0381e-04,\n",
      "        1.1787e-03, 8.6260e-04, 1.6022e-03, 9.7179e-04, 7.5960e-04, 3.3832e-04,\n",
      "        2.1229e-03, 8.0395e-04, 1.8454e-03, 4.8637e-03, 2.9716e-03, 9.5062e-03,\n",
      "        2.6722e-03, 4.3035e-04, 6.4468e-03, 5.4321e-03, 5.8594e-03, 3.0575e-03,\n",
      "        3.4828e-03, 3.1452e-03, 4.1466e-03, 4.9667e-03, 1.8711e-03, 6.8703e-03,\n",
      "        8.0719e-03, 4.4174e-03, 3.8853e-03, 4.1847e-03, 2.8915e-03, 5.3787e-03,\n",
      "        2.9659e-03, 3.8280e-03, 2.2182e-03, 3.2711e-03, 4.3182e-03, 2.0332e-03,\n",
      "        1.7576e-03, 4.2305e-03, 2.4796e-03, 3.8147e-03, 8.3389e-03, 4.9553e-03,\n",
      "        9.3460e-03, 9.5825e-03, 3.1509e-03, 1.0300e-03, 4.8752e-03, 4.1161e-03,\n",
      "        1.9140e-03, 1.9722e-03, 2.6054e-03, 2.1839e-03, 9.0408e-04, 1.1139e-03,\n",
      "        9.3603e-04, 1.9097e-04, 9.2888e-04, 1.5259e-03, 2.8610e-03, 6.3934e-03,\n",
      "        2.8839e-03, 3.0365e-03, 7.0419e-03, 1.2260e-02, 5.2948e-03, 5.9052e-03,\n",
      "        4.6577e-03, 6.8893e-03, 7.9193e-03, 3.8128e-03, 5.8174e-03, 9.3842e-03,\n",
      "        4.4098e-03, 2.7809e-03, 2.5368e-03, 5.3101e-03, 2.2259e-03, 2.1267e-03,\n",
      "        7.3280e-03, 3.2272e-03, 2.1305e-03, 3.4008e-03, 3.6411e-03, 3.1242e-03,\n",
      "        7.7477e-03, 6.5155e-03, 4.9553e-03, 3.5782e-03, 3.8338e-03, 3.5286e-03,\n",
      "        8.6975e-03, 7.6904e-03, 3.4313e-03, 9.5062e-03, 4.2664e-02, 1.4618e-02,\n",
      "        1.2161e-02, 7.8430e-03, 1.3687e-02, 2.0142e-02, 2.0004e-02, 9.0561e-03,\n",
      "        1.5312e-02, 1.2207e-02, 5.2490e-03, 1.1070e-02, 5.5389e-03, 2.3766e-03,\n",
      "        8.3084e-03, 4.9820e-03, 1.2581e-02, 2.4673e-02, 5.6610e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [102] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [102] : torch.Size([1, 32, 1, 174])\n",
      "Last layer attentions for generated token [102] : tensor([8.4473e-02, 8.4473e-02, 6.8784e-05, 1.4675e-04, 4.8161e-05, 6.2752e-04,\n",
      "        2.5487e-04, 3.2735e-04, 2.7347e-04, 2.9922e-04, 8.1491e-04, 3.0279e-04,\n",
      "        1.8334e-04, 3.6311e-04, 5.6229e-03, 3.9749e-03, 2.4676e-04, 2.7895e-04,\n",
      "        7.0393e-05, 1.5521e-04, 4.5419e-05, 1.8263e-04, 1.9083e-03, 5.7697e-04,\n",
      "        1.4629e-03, 4.6043e-03, 5.4312e-04, 2.8014e-04, 1.5461e-04, 2.6298e-04,\n",
      "        2.8944e-04, 4.3035e-04, 3.6812e-04, 4.2772e-04, 3.3522e-04, 2.3770e-04,\n",
      "        1.2624e-04, 2.1517e-04, 9.7334e-05, 9.2149e-05, 1.1148e-03, 3.2306e-04,\n",
      "        2.2149e-04, 2.7251e-04, 3.6168e-04, 1.5402e-04, 1.8871e-04, 1.1927e-04,\n",
      "        1.7798e-04, 9.7179e-04, 3.2234e-04, 8.5890e-05, 8.6021e-04, 1.1492e-03,\n",
      "        6.0081e-04, 2.1000e-03, 3.5191e-04, 1.1768e-03, 1.3599e-03, 5.2404e-04,\n",
      "        4.8208e-04, 7.1526e-04, 1.5945e-03, 4.4227e-04, 5.7697e-04, 5.2738e-04,\n",
      "        3.7098e-04, 7.3481e-04, 5.2738e-04, 5.2795e-03, 2.5635e-03, 7.9727e-03,\n",
      "        2.6855e-03, 7.5531e-04, 1.0132e-02, 3.7136e-03, 8.8806e-03, 2.6855e-03,\n",
      "        3.8109e-03, 4.8485e-03, 5.9967e-03, 2.9488e-03, 1.8892e-03, 1.4206e-02,\n",
      "        5.9738e-03, 5.8098e-03, 3.2253e-03, 1.7471e-03, 3.9215e-03, 1.2299e-02,\n",
      "        2.7294e-03, 5.0354e-03, 9.4938e-04, 4.1389e-03, 5.0583e-03, 2.8648e-03,\n",
      "        1.8244e-03, 7.7667e-03, 3.4237e-03, 5.8899e-03, 1.2947e-02, 3.0231e-03,\n",
      "        1.2100e-02, 4.6463e-03, 2.0485e-03, 4.1795e-04, 4.5929e-03, 2.9850e-03,\n",
      "        2.5158e-03, 2.6569e-03, 5.5084e-03, 2.1286e-03, 1.5907e-03, 7.5960e-04,\n",
      "        2.9602e-03, 1.3947e-04, 9.3555e-04, 1.5097e-03, 4.7035e-03, 5.2643e-03,\n",
      "        2.8667e-03, 4.9858e-03, 1.9516e-02, 7.2594e-03, 4.2038e-03, 1.0063e-02,\n",
      "        2.7924e-03, 7.6828e-03, 1.1360e-02, 4.4975e-03, 8.3847e-03, 1.2115e-02,\n",
      "        2.2869e-03, 9.5749e-04, 4.2572e-03, 3.0499e-03, 2.9659e-03, 6.4611e-04,\n",
      "        6.2065e-03, 1.8492e-03, 1.5001e-03, 1.0872e-02, 4.2267e-03, 4.5586e-03,\n",
      "        8.6365e-03, 9.3079e-03, 8.2169e-03, 3.8052e-03, 5.9509e-03, 3.6926e-03,\n",
      "        2.0874e-02, 7.9575e-03, 5.0201e-03, 1.9257e-02, 3.9185e-02, 2.0905e-02,\n",
      "        1.4236e-02, 5.2490e-03, 7.3547e-03, 2.8793e-02, 4.5837e-02, 9.9792e-03,\n",
      "        2.5085e-02, 1.0895e-02, 5.1079e-03, 1.0201e-02, 6.9847e-03, 3.1147e-03,\n",
      "        2.2568e-02, 7.9117e-03, 2.9358e-02, 2.7390e-02, 5.8060e-03, 3.7441e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [103] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [103] : torch.Size([1, 32, 1, 175])\n",
      "Last layer attentions for generated token [103] : tensor([1.2561e-01, 1.2561e-01, 4.5657e-05, 1.5140e-04, 3.9518e-05, 9.6083e-04,\n",
      "        1.6665e-04, 1.2660e-04, 3.1495e-04, 2.1648e-04, 6.6423e-04, 2.4867e-04,\n",
      "        2.5606e-04, 8.9741e-04, 7.2403e-03, 4.9553e-03, 1.5724e-04, 2.7204e-04,\n",
      "        7.9811e-05, 1.4973e-04, 8.8692e-05, 2.0909e-04, 2.5177e-03, 8.1539e-04,\n",
      "        4.9286e-03, 9.0027e-03, 6.6566e-04, 5.1928e-04, 2.9254e-04, 2.5606e-04,\n",
      "        3.9911e-04, 1.0719e-03, 4.2486e-04, 3.0899e-04, 2.5558e-04, 6.1321e-04,\n",
      "        3.7479e-04, 2.1529e-04, 1.8454e-04, 1.3900e-04, 2.0142e-03, 1.0147e-03,\n",
      "        4.9639e-04, 3.9744e-04, 2.6631e-04, 2.9182e-04, 1.0657e-04, 1.5938e-04,\n",
      "        2.4295e-04, 1.0891e-03, 9.9754e-04, 7.5877e-05, 1.8539e-03, 1.8234e-03,\n",
      "        1.8435e-03, 2.1992e-03, 2.6107e-04, 1.1454e-03, 1.8911e-03, 7.1430e-04,\n",
      "        1.1234e-03, 8.7166e-04, 1.1692e-03, 9.6846e-04, 4.4513e-04, 5.4646e-04,\n",
      "        1.6441e-03, 7.7677e-04, 2.0599e-03, 6.2943e-03, 4.3869e-03, 1.2009e-02,\n",
      "        3.3550e-03, 4.5657e-04, 9.5978e-03, 6.1073e-03, 5.4398e-03, 3.8052e-03,\n",
      "        3.9291e-03, 4.1313e-03, 4.6768e-03, 5.0774e-03, 2.1782e-03, 7.8506e-03,\n",
      "        9.2468e-03, 4.8065e-03, 3.9902e-03, 6.2065e-03, 2.9831e-03, 5.1346e-03,\n",
      "        1.7233e-03, 3.2997e-03, 1.7300e-03, 2.9316e-03, 3.9787e-03, 2.0981e-03,\n",
      "        1.8091e-03, 5.3749e-03, 2.8496e-03, 4.0169e-03, 8.9035e-03, 4.9973e-03,\n",
      "        1.2421e-02, 9.0256e-03, 2.9430e-03, 7.9346e-04, 4.3488e-03, 4.2992e-03,\n",
      "        3.1071e-03, 2.8667e-03, 3.1128e-03, 2.3708e-03, 1.4486e-03, 1.5860e-03,\n",
      "        1.6394e-03, 2.6488e-04, 1.3132e-03, 1.6317e-03, 2.8915e-03, 5.5962e-03,\n",
      "        2.3499e-03, 3.1910e-03, 1.0757e-02, 1.1894e-02, 4.0894e-03, 4.9629e-03,\n",
      "        2.8381e-03, 7.0915e-03, 9.3079e-03, 3.6373e-03, 4.4441e-03, 1.0323e-02,\n",
      "        3.3340e-03, 2.5215e-03, 4.1809e-03, 3.7022e-03, 2.0752e-03, 1.4343e-03,\n",
      "        7.7667e-03, 2.2583e-03, 1.3676e-03, 5.7449e-03, 4.9477e-03, 2.8458e-03,\n",
      "        6.1226e-03, 5.8670e-03, 4.9438e-03, 2.6226e-03, 4.5509e-03, 3.1433e-03,\n",
      "        9.3994e-03, 4.6539e-03, 2.9984e-03, 1.1505e-02, 6.3538e-02, 1.8463e-02,\n",
      "        1.1993e-02, 6.5041e-03, 1.1780e-02, 1.4854e-02, 1.5854e-02, 9.3307e-03,\n",
      "        1.6159e-02, 1.0559e-02, 5.8289e-03, 1.2726e-02, 7.3967e-03, 2.9106e-03,\n",
      "        9.6970e-03, 5.5199e-03, 1.4717e-02, 3.2288e-02, 7.6828e-03, 5.8784e-03,\n",
      "        6.4583e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [104] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [104] : torch.Size([1, 32, 1, 176])\n",
      "Last layer attentions for generated token [104] : tensor([1.8896e-01, 1.8896e-01, 9.4950e-05, 1.6475e-04, 7.0035e-05, 1.4467e-03,\n",
      "        1.7953e-04, 2.5153e-04, 4.5466e-04, 2.4629e-04, 7.4387e-04, 2.5654e-04,\n",
      "        1.1635e-04, 4.2057e-04, 6.3934e-03, 5.9395e-03, 1.8263e-04, 3.5071e-04,\n",
      "        7.7188e-05, 1.0973e-04, 6.3777e-05, 2.9421e-04, 4.1618e-03, 7.7820e-04,\n",
      "        3.1624e-03, 1.0773e-02, 6.1321e-04, 2.8968e-04, 2.7370e-04, 7.2670e-04,\n",
      "        5.9652e-04, 1.3800e-03, 4.5204e-04, 3.9053e-04, 3.3522e-04, 2.7061e-04,\n",
      "        2.4819e-04, 3.4332e-04, 1.9753e-04, 1.4091e-04, 1.7538e-03, 8.0299e-04,\n",
      "        2.5105e-04, 3.2187e-04, 3.5834e-04, 2.2125e-04, 3.4189e-04, 2.2995e-04,\n",
      "        2.5868e-04, 1.2703e-03, 9.7752e-04, 1.3602e-04, 2.0885e-03, 4.7722e-03,\n",
      "        1.2512e-03, 3.1376e-03, 3.2377e-04, 1.3885e-03, 2.6550e-03, 9.4986e-04,\n",
      "        1.0595e-03, 1.2856e-03, 1.8272e-03, 9.1505e-04, 3.3736e-04, 8.4972e-04,\n",
      "        1.2560e-03, 1.6031e-03, 2.2030e-03, 1.1414e-02, 6.2904e-03, 1.1810e-02,\n",
      "        2.1439e-03, 4.1890e-04, 1.0620e-02, 6.3210e-03, 6.1874e-03, 3.5629e-03,\n",
      "        2.8152e-03, 6.7253e-03, 4.5700e-03, 4.2992e-03, 1.6565e-03, 9.0332e-03,\n",
      "        7.5722e-03, 3.6602e-03, 2.4128e-03, 1.5221e-03, 2.6302e-03, 5.6076e-03,\n",
      "        1.7862e-03, 4.7493e-03, 1.5326e-03, 3.6488e-03, 3.0689e-03, 2.1076e-03,\n",
      "        1.4172e-03, 4.4899e-03, 2.1935e-03, 4.9858e-03, 8.4915e-03, 3.1471e-03,\n",
      "        9.2468e-03, 5.4550e-03, 1.9608e-03, 8.2827e-04, 3.5553e-03, 3.1013e-03,\n",
      "        3.2978e-03, 3.1414e-03, 4.3678e-03, 2.5520e-03, 2.8973e-03, 1.3475e-03,\n",
      "        3.4103e-03, 2.4152e-04, 1.3475e-03, 1.6975e-03, 3.8223e-03, 3.3436e-03,\n",
      "        2.8458e-03, 4.8981e-03, 1.3367e-02, 7.3776e-03, 3.0861e-03, 5.7793e-03,\n",
      "        1.8320e-03, 3.4676e-03, 5.0087e-03, 3.5248e-03, 5.1727e-03, 8.2779e-03,\n",
      "        2.1267e-03, 9.8133e-04, 3.2444e-03, 2.5215e-03, 3.2768e-03, 9.4986e-04,\n",
      "        5.5580e-03, 1.2627e-03, 8.2016e-04, 4.4708e-03, 2.1648e-03, 1.9875e-03,\n",
      "        4.4022e-03, 5.2032e-03, 5.6648e-03, 3.3188e-03, 5.6572e-03, 2.1362e-03,\n",
      "        1.2421e-02, 4.0855e-03, 2.8744e-03, 8.2092e-03, 2.9205e-02, 1.0590e-02,\n",
      "        5.1155e-03, 2.9049e-03, 4.5776e-03, 1.0727e-02, 1.2901e-02, 4.6959e-03,\n",
      "        1.4587e-02, 4.6730e-03, 2.9163e-03, 5.3520e-03, 4.1885e-03, 1.5993e-03,\n",
      "        7.7896e-03, 5.1079e-03, 1.3428e-02, 2.3407e-02, 7.6828e-03, 3.0594e-03,\n",
      "        9.1553e-03, 4.2114e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [105] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [105] : torch.Size([1, 32, 1, 177])\n",
      "Last layer attentions for generated token [105] : tensor([1.6370e-01, 1.6406e-01, 7.9274e-05, 1.3804e-04, 7.0214e-05, 9.3222e-04,\n",
      "        3.1424e-04, 2.8777e-04, 4.0507e-04, 3.1233e-04, 8.9502e-04, 2.4605e-04,\n",
      "        2.7680e-04, 3.3450e-04, 6.1760e-03, 4.0970e-03, 2.0683e-04, 4.7159e-04,\n",
      "        1.2183e-04, 2.0480e-04, 3.6597e-05, 2.0528e-04, 5.9052e-03, 9.7752e-04,\n",
      "        3.0727e-03, 5.9586e-03, 4.6515e-04, 2.5105e-04, 2.3031e-04, 2.2721e-04,\n",
      "        2.7180e-04, 8.9169e-04, 5.4693e-04, 3.2997e-04, 2.4045e-04, 2.9850e-04,\n",
      "        2.1088e-04, 2.1851e-04, 1.9813e-04, 1.4579e-04, 1.3266e-03, 1.3189e-03,\n",
      "        2.7394e-04, 4.7803e-04, 3.4571e-04, 2.9850e-04, 9.9421e-05, 1.9169e-04,\n",
      "        2.2233e-04, 9.6226e-04, 1.0834e-03, 6.4433e-05, 2.5539e-03, 1.9627e-03,\n",
      "        1.8454e-03, 3.1509e-03, 5.0592e-04, 2.0409e-03, 3.8872e-03, 1.5268e-03,\n",
      "        2.0676e-03, 1.3609e-03, 2.7142e-03, 1.6270e-03, 5.2309e-04, 5.0116e-04,\n",
      "        3.0136e-03, 1.1673e-03, 3.1204e-03, 6.3858e-03, 5.4092e-03, 1.1765e-02,\n",
      "        1.3590e-03, 2.3711e-04, 4.2381e-03, 3.6945e-03, 2.6646e-03, 3.3474e-03,\n",
      "        2.9182e-03, 3.8643e-03, 3.3455e-03, 4.4327e-03, 1.2703e-03, 4.9477e-03,\n",
      "        6.7749e-03, 3.7155e-03, 2.9030e-03, 1.3685e-03, 2.2850e-03, 4.7531e-03,\n",
      "        3.3245e-03, 6.9199e-03, 2.5711e-03, 5.2376e-03, 4.5280e-03, 2.4471e-03,\n",
      "        2.4548e-03, 3.8166e-03, 3.4409e-03, 4.2992e-03, 8.9798e-03, 4.8256e-03,\n",
      "        7.8659e-03, 5.6992e-03, 2.3174e-03, 1.0138e-03, 5.2376e-03, 4.4823e-03,\n",
      "        1.9283e-03, 2.4471e-03, 5.1727e-03, 3.8929e-03, 1.9875e-03, 1.3113e-03,\n",
      "        1.9951e-03, 2.9802e-04, 1.2350e-03, 1.4400e-03, 2.5463e-03, 4.5471e-03,\n",
      "        3.7212e-03, 2.5597e-03, 8.5602e-03, 7.9880e-03, 3.4237e-03, 3.3340e-03,\n",
      "        3.2158e-03, 3.6545e-03, 7.6599e-03, 5.0812e-03, 6.9466e-03, 8.9340e-03,\n",
      "        3.1300e-03, 1.4067e-03, 2.4319e-03, 3.9139e-03, 6.4125e-03, 1.8492e-03,\n",
      "        8.8577e-03, 2.4357e-03, 1.1578e-03, 3.8166e-03, 3.1567e-03, 4.5662e-03,\n",
      "        8.0719e-03, 1.0002e-02, 5.8022e-03, 3.9291e-03, 4.4479e-03, 4.3716e-03,\n",
      "        1.0353e-02, 1.1841e-02, 3.0880e-03, 8.1711e-03, 2.8534e-02, 8.5678e-03,\n",
      "        6.2256e-03, 3.4389e-03, 5.0621e-03, 1.2527e-02, 1.3870e-02, 4.0092e-03,\n",
      "        1.1719e-02, 8.4229e-03, 3.9444e-03, 6.5460e-03, 3.5229e-03, 1.2980e-03,\n",
      "        5.6801e-03, 5.0735e-03, 1.1986e-02, 3.2990e-02, 8.9798e-03, 5.5542e-03,\n",
      "        1.0742e-02, 9.2621e-03, 1.9608e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [106] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [106] : torch.Size([1, 32, 1, 178])\n",
      "Last layer attentions for generated token [106] : tensor([1.5515e-01, 1.5552e-01, 5.1260e-05, 1.0681e-04, 4.5955e-05, 8.6355e-04,\n",
      "        3.0303e-04, 3.1328e-04, 4.5228e-04, 2.4891e-04, 5.4121e-04, 2.8753e-04,\n",
      "        1.1104e-04, 1.8966e-04, 4.1924e-03, 2.5349e-03, 1.5879e-04, 3.2210e-04,\n",
      "        6.5804e-05, 1.0806e-04, 3.6597e-05, 2.2614e-04, 4.5280e-03, 9.4652e-04,\n",
      "        3.2539e-03, 5.5428e-03, 3.9005e-04, 1.4210e-04, 1.3447e-04, 1.9002e-04,\n",
      "        3.6407e-04, 8.8215e-04, 5.8413e-04, 3.3426e-04, 2.7919e-04, 2.2256e-04,\n",
      "        1.0437e-04, 2.5988e-04, 1.0806e-04, 7.1406e-05, 1.3428e-03, 6.9809e-04,\n",
      "        1.6832e-04, 3.1519e-04, 3.4881e-04, 1.4663e-04, 2.5320e-04, 9.5010e-05,\n",
      "        1.6832e-04, 1.2388e-03, 7.9250e-04, 8.5175e-05, 1.9913e-03, 2.3289e-03,\n",
      "        1.0767e-03, 2.2449e-03, 2.1243e-04, 8.0776e-04, 2.0256e-03, 8.4352e-04,\n",
      "        7.2432e-04, 1.2321e-03, 2.1000e-03, 7.3004e-04, 2.8753e-04, 3.7217e-04,\n",
      "        1.3084e-03, 1.2054e-03, 1.7834e-03, 1.0719e-02, 6.3171e-03, 1.0307e-02,\n",
      "        1.9302e-03, 3.1519e-04, 9.7427e-03, 4.3602e-03, 4.7493e-03, 3.0079e-03,\n",
      "        2.8763e-03, 4.9706e-03, 6.7749e-03, 4.9858e-03, 1.7719e-03, 9.6741e-03,\n",
      "        5.8479e-03, 4.6844e-03, 2.2163e-03, 1.2941e-03, 3.1223e-03, 7.2174e-03,\n",
      "        2.7866e-03, 6.0463e-03, 1.0538e-03, 5.8098e-03, 3.9940e-03, 2.1000e-03,\n",
      "        1.3666e-03, 3.4122e-03, 2.5406e-03, 4.9171e-03, 1.1116e-02, 3.1147e-03,\n",
      "        7.4959e-03, 4.3831e-03, 1.9817e-03, 5.9366e-04, 5.5122e-03, 3.2234e-03,\n",
      "        2.1267e-03, 2.7351e-03, 5.5504e-03, 2.6112e-03, 1.4505e-03, 8.1587e-04,\n",
      "        1.9321e-03, 1.2201e-04, 8.1587e-04, 1.1301e-03, 3.3875e-03, 3.9024e-03,\n",
      "        3.7804e-03, 2.6836e-03, 1.2939e-02, 6.0501e-03, 2.2354e-03, 4.3106e-03,\n",
      "        2.6493e-03, 3.8643e-03, 1.0864e-02, 5.4550e-03, 7.0992e-03, 1.2726e-02,\n",
      "        2.3594e-03, 7.8630e-04, 2.2678e-03, 3.6507e-03, 6.1531e-03, 8.2397e-04,\n",
      "        8.0109e-03, 1.6918e-03, 4.3321e-04, 2.7809e-03, 2.4567e-03, 2.9316e-03,\n",
      "        7.5798e-03, 8.5068e-03, 7.8354e-03, 4.6310e-03, 4.1733e-03, 4.0894e-03,\n",
      "        1.1765e-02, 7.4234e-03, 2.4529e-03, 6.5117e-03, 2.7740e-02, 9.9716e-03,\n",
      "        6.5498e-03, 3.8662e-03, 5.0926e-03, 1.7532e-02, 1.9119e-02, 3.6697e-03,\n",
      "        1.2390e-02, 6.8741e-03, 3.4561e-03, 8.2245e-03, 4.0131e-03, 1.1997e-03,\n",
      "        7.8583e-03, 4.5624e-03, 1.7181e-02, 3.1494e-02, 1.0422e-02, 4.2114e-03,\n",
      "        1.2627e-02, 5.3024e-03, 2.0370e-02, 1.0284e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [107] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [107] : torch.Size([1, 32, 1, 179])\n",
      "Last layer attentions for generated token [107] : tensor([1.2988e-01, 1.2988e-01, 2.9743e-05, 9.2626e-05, 2.7239e-05, 8.7357e-04,\n",
      "        1.9193e-04, 2.5773e-04, 3.7932e-04, 2.0540e-04, 5.6505e-04, 1.8275e-04,\n",
      "        1.8311e-04, 4.9400e-04, 3.8376e-03, 3.6354e-03, 8.3685e-05, 2.1493e-04,\n",
      "        5.8413e-05, 1.2076e-04, 2.3901e-05, 1.9145e-04, 6.9466e-03, 7.7248e-04,\n",
      "        1.9360e-03, 4.7569e-03, 1.5390e-04, 1.6248e-04, 1.5569e-04, 2.2697e-04,\n",
      "        3.3665e-04, 5.4216e-04, 5.2357e-04, 2.6965e-04, 2.3103e-04, 2.5630e-04,\n",
      "        2.9254e-04, 2.9778e-04, 2.1243e-04, 1.5211e-04, 1.3685e-03, 9.6321e-04,\n",
      "        1.7810e-04, 4.2415e-04, 3.2258e-04, 2.4891e-04, 1.3113e-04, 1.9336e-04,\n",
      "        1.7238e-04, 5.9080e-04, 8.6164e-04, 4.1604e-05, 2.2144e-03, 1.0939e-03,\n",
      "        1.5020e-03, 1.8911e-03, 1.4460e-04, 7.4148e-04, 1.9875e-03, 9.6893e-04,\n",
      "        8.1730e-04, 1.0328e-03, 1.4658e-03, 6.1560e-04, 3.2020e-04, 3.8600e-04,\n",
      "        1.6890e-03, 5.5838e-04, 2.0103e-03, 6.7062e-03, 4.6883e-03, 8.8196e-03,\n",
      "        1.2226e-03, 1.7953e-04, 4.7226e-03, 3.4428e-03, 2.1076e-03, 2.7409e-03,\n",
      "        2.2984e-03, 4.8943e-03, 4.3945e-03, 4.0245e-03, 1.2207e-03, 4.9438e-03,\n",
      "        7.5073e-03, 2.4738e-03, 2.0332e-03, 1.8635e-03, 2.1114e-03, 5.9509e-03,\n",
      "        2.7599e-03, 3.7079e-03, 2.0523e-03, 3.8300e-03, 5.8899e-03, 3.5686e-03,\n",
      "        3.2196e-03, 7.3395e-03, 5.3940e-03, 5.7869e-03, 9.6207e-03, 7.9498e-03,\n",
      "        8.7051e-03, 8.9798e-03, 2.0390e-03, 1.0033e-03, 4.0970e-03, 7.4348e-03,\n",
      "        4.8561e-03, 5.2261e-03, 8.1100e-03, 5.9509e-03, 3.1147e-03, 2.8095e-03,\n",
      "        3.2120e-03, 4.9210e-04, 2.8038e-03, 2.0218e-03, 4.3945e-03, 6.5956e-03,\n",
      "        4.6005e-03, 3.6697e-03, 1.2482e-02, 1.2642e-02, 2.7981e-03, 3.4676e-03,\n",
      "        3.6221e-03, 2.1744e-03, 5.1842e-03, 3.1662e-03, 4.6043e-03, 9.1553e-03,\n",
      "        3.1986e-03, 1.5678e-03, 2.6703e-03, 3.6373e-03, 5.6610e-03, 2.4471e-03,\n",
      "        9.5520e-03, 3.8586e-03, 1.0614e-03, 8.1635e-03, 5.5847e-03, 4.3449e-03,\n",
      "        8.8654e-03, 1.4633e-02, 7.0724e-03, 4.2419e-03, 6.9199e-03, 5.2795e-03,\n",
      "        1.9592e-02, 1.3191e-02, 5.4169e-03, 1.3351e-02, 3.4149e-02, 7.6370e-03,\n",
      "        5.9814e-03, 2.1706e-03, 5.5847e-03, 1.0300e-02, 1.4709e-02, 4.2992e-03,\n",
      "        1.3771e-02, 9.7122e-03, 3.7003e-03, 6.7444e-03, 4.7760e-03, 1.5621e-03,\n",
      "        8.4610e-03, 5.9853e-03, 1.3557e-02, 3.4271e-02, 5.0812e-03, 6.2981e-03,\n",
      "        8.4152e-03, 7.0992e-03, 1.2703e-02, 9.6893e-03, 9.5520e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [108] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [108] : torch.Size([1, 32, 1, 180])\n",
      "Last layer attentions for generated token [108] : tensor([2.0825e-01, 2.0825e-01, 3.9876e-05, 8.7440e-05, 3.8207e-05, 6.8665e-04,\n",
      "        9.0957e-05, 1.4639e-04, 2.4235e-04, 1.7595e-04, 4.2796e-04, 1.0717e-04,\n",
      "        1.7464e-04, 3.9816e-04, 3.2520e-03, 2.9945e-03, 6.9976e-05, 2.5058e-04,\n",
      "        9.4175e-05, 1.3542e-04, 3.3855e-05, 2.1303e-04, 8.9188e-03, 8.9550e-04,\n",
      "        2.4738e-03, 4.5662e-03, 2.4915e-04, 1.7738e-04, 1.5891e-04, 1.4758e-04,\n",
      "        3.1424e-04, 6.1417e-04, 5.1594e-04, 2.0647e-04, 2.0981e-04, 3.6454e-04,\n",
      "        3.8815e-04, 2.6679e-04, 2.7204e-04, 1.8299e-04, 1.2360e-03, 1.2379e-03,\n",
      "        2.8944e-04, 3.9268e-04, 2.8276e-04, 3.0708e-04, 1.0884e-04, 2.2995e-04,\n",
      "        2.0730e-04, 7.1526e-04, 1.3227e-03, 5.7101e-05, 2.7523e-03, 1.9627e-03,\n",
      "        1.7357e-03, 1.9894e-03, 1.5163e-04, 1.0109e-03, 1.9217e-03, 8.5592e-04,\n",
      "        1.2283e-03, 6.3848e-04, 1.7891e-03, 1.0691e-03, 5.6362e-04, 4.6992e-04,\n",
      "        2.7332e-03, 5.3358e-04, 2.6989e-03, 5.9357e-03, 5.7678e-03, 8.1253e-03,\n",
      "        8.4591e-04, 1.6332e-04, 3.8319e-03, 4.1580e-03, 1.3599e-03, 2.4910e-03,\n",
      "        1.4534e-03, 2.0866e-03, 2.8534e-03, 3.8815e-03, 8.8310e-04, 3.2310e-03,\n",
      "        5.9128e-03, 1.5383e-03, 1.3208e-03, 1.0443e-03, 9.6416e-04, 3.5133e-03,\n",
      "        2.6283e-03, 3.6564e-03, 2.4796e-03, 2.7409e-03, 4.3602e-03, 2.0638e-03,\n",
      "        2.3594e-03, 3.3894e-03, 3.5591e-03, 4.5433e-03, 7.4730e-03, 9.4757e-03,\n",
      "        6.4774e-03, 9.3231e-03, 1.4706e-03, 1.0757e-03, 2.4738e-03, 4.8409e-03,\n",
      "        2.1420e-03, 2.6970e-03, 3.9825e-03, 4.5013e-03, 1.8301e-03, 2.3289e-03,\n",
      "        2.4109e-03, 6.4373e-04, 2.5578e-03, 1.7738e-03, 2.9411e-03, 5.4779e-03,\n",
      "        3.0289e-03, 2.5654e-03, 7.4921e-03, 1.2444e-02, 1.5469e-03, 1.8950e-03,\n",
      "        2.4586e-03, 1.7376e-03, 3.0003e-03, 1.6928e-03, 2.5387e-03, 6.2103e-03,\n",
      "        2.9411e-03, 1.4248e-03, 1.1702e-03, 2.5749e-03, 3.5305e-03, 1.4200e-03,\n",
      "        6.1569e-03, 3.6182e-03, 1.8702e-03, 6.4850e-03, 4.3831e-03, 2.8706e-03,\n",
      "        5.4970e-03, 7.7705e-03, 4.1389e-03, 3.0766e-03, 5.2643e-03, 3.7479e-03,\n",
      "        1.3519e-02, 9.8343e-03, 2.7332e-03, 1.1063e-02, 2.9755e-02, 5.5122e-03,\n",
      "        4.5319e-03, 1.9522e-03, 4.5433e-03, 6.4621e-03, 8.6670e-03, 3.3855e-03,\n",
      "        1.1734e-02, 9.2468e-03, 2.7332e-03, 3.8071e-03, 3.1281e-03, 1.0719e-03,\n",
      "        5.7602e-03, 5.2032e-03, 8.2474e-03, 3.6438e-02, 4.6196e-03, 4.2000e-03,\n",
      "        6.2752e-03, 5.5618e-03, 9.0561e-03, 8.8730e-03, 5.9166e-03, 5.0392e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [109] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [109] : torch.Size([1, 32, 1, 181])\n",
      "Last layer attentions for generated token [109] : tensor([1.2891e-01, 1.2891e-01, 1.0681e-04, 1.7679e-04, 9.3162e-05, 1.3952e-03,\n",
      "        1.5211e-04, 2.8038e-04, 5.5981e-04, 2.3603e-04, 5.2691e-04, 2.5034e-04,\n",
      "        1.4019e-04, 5.5647e-04, 3.3188e-03, 3.3150e-03, 1.5092e-04, 1.6940e-04,\n",
      "        5.4538e-05, 8.5473e-05, 8.6486e-05, 3.4285e-04, 6.4240e-03, 1.2064e-03,\n",
      "        4.6806e-03, 1.0979e-02, 3.2783e-04, 1.7607e-04, 1.6999e-04, 5.4979e-04,\n",
      "        4.2677e-04, 8.3542e-04, 4.1103e-04, 2.8300e-04, 4.0245e-04, 2.6035e-04,\n",
      "        1.5700e-04, 3.6645e-04, 1.9312e-04, 1.6642e-04, 2.4414e-03, 8.9455e-04,\n",
      "        2.2268e-04, 2.8801e-04, 3.9625e-04, 2.5272e-04, 4.4107e-04, 1.7715e-04,\n",
      "        2.9445e-04, 2.2678e-03, 1.3132e-03, 1.9312e-04, 3.6182e-03, 4.8561e-03,\n",
      "        1.2455e-03, 2.3422e-03, 1.6224e-04, 9.3937e-04, 3.3436e-03, 9.3746e-04,\n",
      "        1.0128e-03, 2.0561e-03, 3.4828e-03, 8.6355e-04, 4.9400e-04, 9.8801e-04,\n",
      "        1.6260e-03, 2.8572e-03, 4.0970e-03, 1.5488e-02, 1.2772e-02, 1.2444e-02,\n",
      "        2.3136e-03, 4.9686e-04, 1.2894e-02, 6.6109e-03, 3.5019e-03, 3.6697e-03,\n",
      "        2.8801e-03, 5.2605e-03, 6.0806e-03, 4.7226e-03, 1.8425e-03, 8.4457e-03,\n",
      "        7.3433e-03, 3.3035e-03, 1.7223e-03, 1.0090e-03, 2.0618e-03, 7.4883e-03,\n",
      "        2.0485e-03, 5.5580e-03, 2.1496e-03, 7.2021e-03, 5.0430e-03, 2.3289e-03,\n",
      "        1.3218e-03, 6.1035e-03, 2.3651e-03, 6.7863e-03, 8.8348e-03, 3.5305e-03,\n",
      "        8.7585e-03, 6.3934e-03, 1.6699e-03, 1.0347e-03, 4.6082e-03, 4.2267e-03,\n",
      "        4.2763e-03, 5.1613e-03, 3.9558e-03, 2.4815e-03, 3.5896e-03, 1.7719e-03,\n",
      "        4.8065e-03, 4.0245e-04, 2.9335e-03, 2.2411e-03, 8.7662e-03, 4.9553e-03,\n",
      "        3.8624e-03, 7.7400e-03, 1.9394e-02, 1.0933e-02, 1.5345e-03, 3.9177e-03,\n",
      "        3.2063e-03, 2.8381e-03, 6.1684e-03, 3.8948e-03, 4.1924e-03, 1.0513e-02,\n",
      "        2.2755e-03, 9.7084e-04, 3.6430e-03, 4.2343e-03, 5.8441e-03, 1.3933e-03,\n",
      "        1.3756e-02, 2.7580e-03, 9.5940e-04, 7.9269e-03, 2.6703e-03, 2.1973e-03,\n",
      "        4.2801e-03, 5.6725e-03, 9.9335e-03, 3.8967e-03, 7.6027e-03, 3.3913e-03,\n",
      "        2.5558e-02, 5.7869e-03, 4.9095e-03, 1.5106e-02, 3.7781e-02, 8.1711e-03,\n",
      "        4.4479e-03, 1.5516e-03, 3.2330e-03, 7.4043e-03, 9.4986e-03, 4.0474e-03,\n",
      "        1.4618e-02, 5.9547e-03, 2.7447e-03, 4.6844e-03, 3.2692e-03, 1.4954e-03,\n",
      "        6.5155e-03, 3.6640e-03, 1.0033e-02, 2.0920e-02, 4.0436e-03, 2.1477e-03,\n",
      "        6.9427e-03, 3.1834e-03, 1.2428e-02, 6.3019e-03, 5.7411e-03, 3.1891e-03,\n",
      "        3.4428e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [110] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [110] : torch.Size([1, 32, 1, 182])\n",
      "Last layer attentions for generated token [110] : tensor([1.4392e-01, 1.4392e-01, 1.0300e-04, 1.3959e-04, 7.5936e-05, 1.1606e-03,\n",
      "        2.3210e-04, 4.0102e-04, 6.8998e-04, 2.6917e-04, 4.4990e-04, 1.2612e-04,\n",
      "        8.5711e-05, 2.7180e-04, 2.0752e-03, 3.0060e-03, 1.8716e-04, 2.0599e-04,\n",
      "        4.8637e-05, 6.9976e-05, 2.6643e-05, 1.4985e-04, 6.1455e-03, 8.0538e-04,\n",
      "        3.1357e-03, 1.0910e-02, 5.3549e-04, 1.5247e-04, 1.8537e-04, 5.2309e-04,\n",
      "        3.6216e-04, 6.9141e-04, 3.9864e-04, 3.2973e-04, 3.2854e-04, 2.8157e-04,\n",
      "        2.6655e-04, 3.9244e-04, 2.0969e-04, 1.6201e-04, 2.0885e-03, 1.2846e-03,\n",
      "        2.9278e-04, 3.9101e-04, 5.2071e-04, 2.2840e-04, 4.7612e-04, 1.8871e-04,\n",
      "        3.4165e-04, 1.5478e-03, 1.4172e-03, 2.3484e-04, 3.7308e-03, 4.9095e-03,\n",
      "        1.6880e-03, 2.0485e-03, 1.3912e-04, 7.4625e-04, 2.0599e-03, 6.7806e-04,\n",
      "        6.7139e-04, 1.5383e-03, 1.5841e-03, 5.6219e-04, 2.6655e-04, 7.0667e-04,\n",
      "        1.7109e-03, 2.5196e-03, 3.0746e-03, 1.1269e-02, 1.0605e-02, 9.3536e-03,\n",
      "        2.1858e-03, 3.1900e-04, 8.6441e-03, 4.9553e-03, 3.1109e-03, 3.1567e-03,\n",
      "        2.5520e-03, 5.1155e-03, 4.1656e-03, 4.5624e-03, 1.9073e-03, 5.4779e-03,\n",
      "        5.6725e-03, 2.1763e-03, 1.4381e-03, 8.7929e-04, 1.9484e-03, 6.0272e-03,\n",
      "        1.8816e-03, 3.9940e-03, 1.2360e-03, 4.7760e-03, 4.1809e-03, 2.5234e-03,\n",
      "        1.9407e-03, 6.0501e-03, 2.9430e-03, 7.3318e-03, 1.0056e-02, 2.6836e-03,\n",
      "        6.8169e-03, 5.0240e-03, 1.3371e-03, 7.8487e-04, 3.6488e-03, 3.4790e-03,\n",
      "        3.7155e-03, 4.8981e-03, 4.2076e-03, 2.8496e-03, 2.4338e-03, 1.4954e-03,\n",
      "        2.6264e-03, 2.8324e-04, 2.1591e-03, 2.0657e-03, 7.3318e-03, 5.1537e-03,\n",
      "        4.2534e-03, 5.2414e-03, 1.7120e-02, 9.0103e-03, 1.8377e-03, 4.3297e-03,\n",
      "        2.4891e-03, 3.5248e-03, 9.1705e-03, 5.3329e-03, 6.7749e-03, 9.7961e-03,\n",
      "        2.0084e-03, 1.0080e-03, 3.1986e-03, 2.5864e-03, 3.9005e-03, 1.2131e-03,\n",
      "        5.8365e-03, 1.5516e-03, 4.6682e-04, 5.5656e-03, 2.9659e-03, 2.3804e-03,\n",
      "        5.0163e-03, 6.2752e-03, 9.2316e-03, 3.0842e-03, 6.1569e-03, 3.1528e-03,\n",
      "        1.6083e-02, 6.0387e-03, 4.7150e-03, 8.6823e-03, 3.1830e-02, 7.9193e-03,\n",
      "        4.5433e-03, 1.6203e-03, 4.2343e-03, 9.9030e-03, 9.4376e-03, 3.1986e-03,\n",
      "        1.8280e-02, 6.1951e-03, 2.0237e-03, 5.8250e-03, 3.5591e-03, 1.0920e-03,\n",
      "        7.4501e-03, 4.4403e-03, 1.3412e-02, 2.6245e-02, 6.6071e-03, 3.7670e-03,\n",
      "        1.0414e-02, 6.2294e-03, 1.5961e-02, 1.0887e-02, 8.0872e-03, 5.3596e-03,\n",
      "        5.0163e-03, 2.0065e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [111] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [111] : torch.Size([1, 32, 1, 183])\n",
      "Last layer attentions for generated token [111] : tensor([1.2988e-01, 1.2964e-01, 1.1778e-04, 1.2445e-04, 6.7115e-05, 1.6594e-03,\n",
      "        1.7786e-04, 3.2902e-04, 5.8222e-04, 2.8372e-04, 4.3011e-04, 1.3983e-04,\n",
      "        1.3399e-04, 3.8385e-04, 2.4300e-03, 2.7695e-03, 2.3377e-04, 2.1791e-04,\n",
      "        8.0645e-05, 1.3399e-04, 4.8161e-05, 2.0635e-04, 7.6675e-03, 9.9945e-04,\n",
      "        2.7885e-03, 7.6027e-03, 7.1049e-04, 2.3603e-04, 2.1291e-04, 4.7874e-04,\n",
      "        4.4203e-04, 7.0333e-04, 5.7173e-04, 2.4259e-04, 1.9538e-04, 2.5082e-04,\n",
      "        2.5177e-04, 4.1199e-04, 2.8586e-04, 1.1283e-04, 1.6308e-03, 1.1911e-03,\n",
      "        3.2330e-04, 3.9935e-04, 4.4799e-04, 2.2352e-04, 3.4213e-04, 2.3603e-04,\n",
      "        3.1590e-04, 1.4277e-03, 1.7948e-03, 1.8680e-04, 2.9793e-03, 2.8725e-03,\n",
      "        1.5535e-03, 2.1992e-03, 1.8311e-04, 1.0786e-03, 2.1706e-03, 8.9788e-04,\n",
      "        1.0138e-03, 1.7357e-03, 1.6375e-03, 7.1716e-04, 4.5419e-04, 7.8917e-04,\n",
      "        2.1172e-03, 1.5669e-03, 3.1948e-03, 9.2010e-03, 9.5139e-03, 1.0399e-02,\n",
      "        1.5049e-03, 3.8314e-04, 8.3923e-03, 7.9880e-03, 2.5902e-03, 4.0627e-03,\n",
      "        1.6489e-03, 3.5572e-03, 3.9291e-03, 7.0419e-03, 1.5306e-03, 5.1041e-03,\n",
      "        6.5651e-03, 1.7109e-03, 2.1286e-03, 8.3065e-04, 2.0332e-03, 6.2065e-03,\n",
      "        2.7046e-03, 2.9011e-03, 1.0662e-03, 4.2496e-03, 4.3030e-03, 2.5463e-03,\n",
      "        1.5497e-03, 3.6068e-03, 2.5558e-03, 5.4626e-03, 1.0033e-02, 4.5357e-03,\n",
      "        6.5460e-03, 5.7755e-03, 1.0891e-03, 1.3437e-03, 3.2520e-03, 3.7022e-03,\n",
      "        2.5482e-03, 3.9444e-03, 4.4975e-03, 3.9291e-03, 2.8877e-03, 2.3365e-03,\n",
      "        4.3411e-03, 5.8556e-04, 3.3836e-03, 2.3613e-03, 7.3853e-03, 8.3084e-03,\n",
      "        4.5433e-03, 3.9005e-03, 1.2749e-02, 9.1934e-03, 1.8520e-03, 4.5166e-03,\n",
      "        3.9940e-03, 4.1466e-03, 6.8550e-03, 4.1237e-03, 5.3368e-03, 1.0574e-02,\n",
      "        3.7117e-03, 1.2274e-03, 2.7294e-03, 2.6855e-03, 2.5005e-03, 8.4877e-04,\n",
      "        4.5395e-03, 1.6537e-03, 7.4720e-04, 6.3400e-03, 4.9782e-03, 3.1395e-03,\n",
      "        6.0501e-03, 7.1487e-03, 7.3624e-03, 2.9850e-03, 6.1073e-03, 3.6373e-03,\n",
      "        1.6815e-02, 1.0086e-02, 4.6539e-03, 7.2746e-03, 3.0930e-02, 9.9945e-03,\n",
      "        6.4545e-03, 2.5959e-03, 6.4087e-03, 9.8724e-03, 7.8735e-03, 3.1357e-03,\n",
      "        1.6769e-02, 9.9030e-03, 2.4033e-03, 5.8289e-03, 2.5940e-03, 8.8406e-04,\n",
      "        5.8861e-03, 4.9782e-03, 1.0330e-02, 3.5187e-02, 8.9722e-03, 5.0049e-03,\n",
      "        1.2466e-02, 7.4272e-03, 1.1902e-02, 1.0323e-02, 7.6294e-03, 6.5536e-03,\n",
      "        6.4316e-03, 1.8051e-02, 1.4160e-02], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [112] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [112] : torch.Size([1, 32, 1, 184])\n",
      "Last layer attentions for generated token [112] : tensor([1.4722e-01, 1.4722e-01, 8.3387e-05, 1.0538e-04, 5.6624e-05, 7.6056e-04,\n",
      "        1.7309e-04, 4.0722e-04, 6.5851e-04, 2.1672e-04, 3.0565e-04, 1.3113e-04,\n",
      "        6.7770e-05, 2.3019e-04, 1.6375e-03, 1.8883e-03, 1.8036e-04, 1.4126e-04,\n",
      "        4.2081e-05, 6.1214e-05, 2.8729e-05, 1.4400e-04, 6.1874e-03, 5.6076e-04,\n",
      "        2.6875e-03, 7.8201e-03, 3.9768e-04, 1.0961e-04, 1.1712e-04, 3.8552e-04,\n",
      "        4.0174e-04, 5.8889e-04, 3.1900e-04, 2.1410e-04, 2.7943e-04, 1.9240e-04,\n",
      "        2.0158e-04, 3.2663e-04, 1.7691e-04, 1.1992e-04, 2.1152e-03, 9.6178e-04,\n",
      "        1.9157e-04, 2.2447e-04, 3.8862e-04, 1.4687e-04, 3.4356e-04, 1.3483e-04,\n",
      "        2.3341e-04, 1.6327e-03, 1.0958e-03, 1.9050e-04, 3.7308e-03, 4.6768e-03,\n",
      "        1.4229e-03, 1.3590e-03, 1.1176e-04, 4.8923e-04, 1.5898e-03, 5.5647e-04,\n",
      "        4.4537e-04, 1.1883e-03, 1.8091e-03, 5.4264e-04, 3.0923e-04, 8.3065e-04,\n",
      "        1.4763e-03, 1.9426e-03, 3.1509e-03, 1.4534e-02, 8.7891e-03, 7.6637e-03,\n",
      "        1.8778e-03, 4.2582e-04, 8.1253e-03, 5.5046e-03, 2.9678e-03, 2.5654e-03,\n",
      "        1.6775e-03, 2.5311e-03, 3.3188e-03, 3.7518e-03, 1.3046e-03, 5.0507e-03,\n",
      "        4.8866e-03, 2.0199e-03, 1.2636e-03, 4.2510e-04, 1.1511e-03, 3.5839e-03,\n",
      "        1.4029e-03, 2.8267e-03, 8.9121e-04, 4.2419e-03, 3.6469e-03, 1.7290e-03,\n",
      "        1.5059e-03, 4.4518e-03, 2.1992e-03, 7.2784e-03, 1.0017e-02, 2.9106e-03,\n",
      "        7.0305e-03, 5.2986e-03, 1.1091e-03, 8.6689e-04, 3.9520e-03, 2.4052e-03,\n",
      "        2.7637e-03, 3.8605e-03, 2.3708e-03, 2.2373e-03, 1.8444e-03, 1.2131e-03,\n",
      "        2.9411e-03, 2.3890e-04, 1.9140e-03, 1.8444e-03, 6.1913e-03, 5.0926e-03,\n",
      "        3.9253e-03, 6.3820e-03, 1.7868e-02, 9.3994e-03, 1.4362e-03, 4.8523e-03,\n",
      "        2.4891e-03, 3.3340e-03, 6.5041e-03, 3.5534e-03, 5.2528e-03, 1.3420e-02,\n",
      "        2.4853e-03, 9.1028e-04, 2.8076e-03, 2.2583e-03, 4.2000e-03, 8.4209e-04,\n",
      "        7.0686e-03, 2.1229e-03, 6.4802e-04, 8.9493e-03, 3.3493e-03, 3.0518e-03,\n",
      "        5.1765e-03, 4.6616e-03, 7.0877e-03, 2.8133e-03, 5.8899e-03, 2.7332e-03,\n",
      "        2.0508e-02, 7.4463e-03, 4.1466e-03, 1.3161e-02, 2.9022e-02, 7.5264e-03,\n",
      "        4.6043e-03, 1.8177e-03, 4.1962e-03, 9.3307e-03, 9.0790e-03, 2.8324e-03,\n",
      "        1.5617e-02, 7.3624e-03, 2.2469e-03, 4.0741e-03, 3.9635e-03, 1.2007e-03,\n",
      "        7.4234e-03, 4.8065e-03, 1.2291e-02, 2.6871e-02, 6.5269e-03, 2.5883e-03,\n",
      "        9.6588e-03, 5.4245e-03, 1.5961e-02, 1.2344e-02, 7.7477e-03, 3.7308e-03,\n",
      "        6.3896e-03, 2.4734e-02, 1.6159e-02, 7.8735e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [113] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [113] : torch.Size([1, 32, 1, 185])\n",
      "Last layer attentions for generated token [113] : tensor([1.9080e-01, 1.9080e-01, 1.3280e-04, 1.3542e-04, 6.6042e-05, 1.1272e-03,\n",
      "        1.4365e-04, 2.4486e-04, 6.4611e-04, 2.9135e-04, 6.7616e-04, 1.5175e-04,\n",
      "        1.8370e-04, 5.1832e-04, 3.8223e-03, 4.0245e-03, 1.2386e-04, 2.2769e-04,\n",
      "        7.2777e-05, 1.5843e-04, 5.2869e-05, 2.3043e-04, 6.7825e-03, 8.6975e-04,\n",
      "        2.8706e-03, 8.1024e-03, 4.6730e-04, 1.9109e-04, 1.6212e-04, 3.3736e-04,\n",
      "        2.6512e-04, 6.6042e-04, 4.3654e-04, 1.9407e-04, 2.0862e-04, 1.5903e-04,\n",
      "        1.8585e-04, 3.5334e-04, 1.9908e-04, 1.1277e-04, 2.1095e-03, 1.3599e-03,\n",
      "        2.7966e-04, 2.8181e-04, 2.9635e-04, 1.5056e-04, 2.4962e-04, 2.7156e-04,\n",
      "        2.6107e-04, 1.4391e-03, 1.6451e-03, 1.7941e-04, 3.0708e-03, 3.4199e-03,\n",
      "        1.3981e-03, 2.2163e-03, 1.8954e-04, 6.1083e-04, 1.9369e-03, 6.7711e-04,\n",
      "        8.6975e-04, 1.0653e-03, 1.5192e-03, 7.0572e-04, 4.1318e-04, 9.7752e-04,\n",
      "        1.6632e-03, 1.3952e-03, 2.4567e-03, 8.1863e-03, 7.6981e-03, 9.8495e-03,\n",
      "        1.4286e-03, 2.4009e-04, 5.7983e-03, 6.4507e-03, 2.5883e-03, 3.2730e-03,\n",
      "        1.8625e-03, 3.0899e-03, 5.0201e-03, 5.1308e-03, 8.9884e-04, 4.1389e-03,\n",
      "        7.1564e-03, 1.2550e-03, 1.4286e-03, 4.3726e-04, 1.0347e-03, 4.8103e-03,\n",
      "        1.5669e-03, 2.4338e-03, 1.4610e-03, 3.4523e-03, 3.7041e-03, 1.9388e-03,\n",
      "        1.4086e-03, 2.7256e-03, 2.1706e-03, 2.7962e-03, 1.2093e-02, 2.4395e-03,\n",
      "        6.3171e-03, 5.7030e-03, 9.1696e-04, 7.4244e-04, 2.4338e-03, 2.9774e-03,\n",
      "        1.8806e-03, 3.8338e-03, 5.3940e-03, 2.4624e-03, 2.3060e-03, 1.4057e-03,\n",
      "        3.6945e-03, 4.4942e-04, 2.6016e-03, 1.5097e-03, 8.5144e-03, 6.4049e-03,\n",
      "        2.9278e-03, 3.8853e-03, 1.5465e-02, 1.0933e-02, 1.6232e-03, 3.8071e-03,\n",
      "        1.9464e-03, 3.7155e-03, 5.4169e-03, 2.9278e-03, 3.6621e-03, 6.8245e-03,\n",
      "        1.8015e-03, 6.6042e-04, 2.1801e-03, 1.8625e-03, 2.1381e-03, 6.9332e-04,\n",
      "        3.9825e-03, 1.2484e-03, 5.4407e-04, 6.5804e-03, 3.1624e-03, 2.5311e-03,\n",
      "        3.3627e-03, 4.8676e-03, 4.7150e-03, 1.7910e-03, 6.3400e-03, 2.2011e-03,\n",
      "        1.5045e-02, 6.3171e-03, 2.9125e-03, 1.2169e-02, 3.1921e-02, 7.4959e-03,\n",
      "        3.6755e-03, 1.4324e-03, 3.6469e-03, 7.7896e-03, 7.7629e-03, 1.7014e-03,\n",
      "        1.2939e-02, 5.7182e-03, 1.5459e-03, 3.0899e-03, 1.8005e-03, 4.7016e-04,\n",
      "        4.8676e-03, 3.1052e-03, 8.8272e-03, 2.6001e-02, 5.1270e-03, 2.8400e-03,\n",
      "        7.4844e-03, 4.6616e-03, 1.0551e-02, 8.9722e-03, 8.4991e-03, 6.2866e-03,\n",
      "        4.0512e-03, 1.3290e-02, 7.5951e-03, 5.1918e-03, 1.7262e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [114] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [114] : torch.Size([1, 32, 1, 186])\n",
      "Last layer attentions for generated token [114] : tensor([9.3628e-02, 9.3628e-02, 8.0705e-05, 2.1088e-04, 1.0121e-04, 1.0080e-03,\n",
      "        1.1069e-04, 1.2016e-04, 1.9777e-04, 1.8871e-04, 5.4407e-04, 1.2350e-04,\n",
      "        2.3663e-04, 7.8678e-04, 9.8724e-03, 6.4392e-03, 1.7524e-04, 6.2943e-04,\n",
      "        9.7334e-05, 2.3854e-04, 7.8499e-05, 2.5678e-04, 5.3139e-03, 1.6737e-03,\n",
      "        4.2076e-03, 1.1734e-02, 5.7554e-04, 2.3532e-04, 9.7692e-05, 1.7142e-04,\n",
      "        2.4128e-04, 7.2908e-04, 4.7708e-04, 2.3663e-04, 1.6880e-04, 2.4843e-04,\n",
      "        2.4128e-04, 3.0327e-04, 1.4603e-04, 1.0645e-04, 2.6646e-03, 1.2360e-03,\n",
      "        5.7316e-04, 6.3229e-04, 3.1900e-04, 2.2316e-04, 9.2149e-05, 8.8930e-05,\n",
      "        1.4436e-04, 1.0023e-03, 1.2245e-03, 2.9278e-04, 1.3094e-03, 6.7444e-03,\n",
      "        1.2941e-03, 3.1033e-03, 2.0206e-04, 6.7663e-04, 1.7862e-03, 6.3944e-04,\n",
      "        1.0004e-03, 7.4625e-04, 1.4143e-03, 8.3065e-04, 4.5252e-04, 4.9734e-04,\n",
      "        1.8578e-03, 1.6222e-03, 1.5249e-03, 1.3847e-02, 1.0635e-02, 1.6815e-02,\n",
      "        5.4245e-03, 7.1907e-04, 7.9117e-03, 9.7198e-03, 7.5684e-03, 3.8357e-03,\n",
      "        3.4447e-03, 6.0158e-03, 4.7989e-03, 5.8441e-03, 1.3189e-03, 6.5842e-03,\n",
      "        7.4921e-03, 2.8629e-03, 2.7618e-03, 2.3556e-03, 1.5249e-03, 4.2229e-03,\n",
      "        1.5936e-03, 2.8019e-03, 1.9398e-03, 3.0937e-03, 3.2463e-03, 1.8044e-03,\n",
      "        1.6699e-03, 3.3550e-03, 2.9869e-03, 2.9449e-03, 8.9340e-03, 3.3970e-03,\n",
      "        9.9487e-03, 6.9160e-03, 2.4223e-03, 8.2445e-04, 3.1986e-03, 3.6659e-03,\n",
      "        1.9207e-03, 2.6855e-03, 4.9896e-03, 2.1687e-03, 2.3556e-03, 1.6966e-03,\n",
      "        3.7994e-03, 4.4298e-04, 2.4166e-03, 2.0142e-03, 6.3515e-03, 8.8959e-03,\n",
      "        3.5954e-03, 3.2616e-03, 1.1848e-02, 1.4565e-02, 5.3062e-03, 5.5046e-03,\n",
      "        2.0542e-03, 5.6419e-03, 5.8441e-03, 3.3512e-03, 3.2635e-03, 7.5226e-03,\n",
      "        1.9646e-03, 1.1892e-03, 1.9894e-03, 1.3037e-03, 2.1229e-03, 1.1692e-03,\n",
      "        5.8098e-03, 2.0599e-03, 1.3752e-03, 6.1951e-03, 3.6983e-03, 3.5629e-03,\n",
      "        6.4240e-03, 5.0812e-03, 6.1836e-03, 1.9646e-03, 4.7112e-03, 2.1133e-03,\n",
      "        1.0567e-02, 4.7531e-03, 2.9030e-03, 1.3832e-02, 5.2124e-02, 1.6418e-02,\n",
      "        9.8877e-03, 4.9438e-03, 6.6261e-03, 9.5825e-03, 1.2047e-02, 8.2779e-03,\n",
      "        2.4887e-02, 9.3079e-03, 4.7913e-03, 8.5602e-03, 4.1275e-03, 6.4325e-04,\n",
      "        8.0490e-03, 3.7308e-03, 1.2184e-02, 3.5187e-02, 1.0170e-02, 6.1073e-03,\n",
      "        1.1467e-02, 7.5493e-03, 1.5961e-02, 7.0915e-03, 8.0185e-03, 7.2250e-03,\n",
      "        2.8477e-03, 9.5673e-03, 6.2866e-03, 4.2038e-03, 1.5001e-03, 7.8659e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [115] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [115] : torch.Size([1, 32, 1, 187])\n",
      "Last layer attentions for generated token [115] : tensor([5.2216e-02, 5.2216e-02, 5.2631e-05, 1.6260e-04, 6.0797e-05, 6.0081e-04,\n",
      "        1.6427e-04, 2.3162e-04, 3.0565e-04, 2.3031e-04, 6.3753e-04, 1.5163e-04,\n",
      "        6.3479e-05, 2.9349e-04, 3.9864e-03, 2.6379e-03, 1.9276e-04, 2.6393e-04,\n",
      "        6.8069e-05, 1.4353e-04, 4.5359e-05, 2.4128e-04, 1.6756e-03, 7.7391e-04,\n",
      "        2.0161e-03, 7.2517e-03, 6.3896e-04, 1.7416e-04, 1.0419e-04, 2.2018e-04,\n",
      "        1.9169e-04, 4.9210e-04, 4.8566e-04, 3.3760e-04, 1.9932e-04, 1.4269e-04,\n",
      "        1.2088e-04, 2.2411e-04, 1.0020e-04, 9.2685e-05, 1.5583e-03, 5.6791e-04,\n",
      "        3.6240e-04, 4.1056e-04, 3.9244e-04, 1.8001e-04, 3.9482e-04, 9.5785e-05,\n",
      "        3.0923e-04, 1.6031e-03, 7.3528e-04, 2.5702e-04, 1.2016e-03, 2.8706e-03,\n",
      "        8.3256e-04, 1.6270e-03, 2.1172e-04, 3.5310e-04, 7.4053e-04, 4.6802e-04,\n",
      "        3.3450e-04, 5.1403e-04, 1.2245e-03, 3.1114e-04, 2.2411e-04, 4.3201e-04,\n",
      "        5.5933e-04, 1.4896e-03, 6.1083e-04, 1.3306e-02, 1.2123e-02, 1.1009e-02,\n",
      "        6.3057e-03, 1.5688e-03, 2.5223e-02, 9.2697e-03, 1.3046e-02, 5.0087e-03,\n",
      "        5.2567e-03, 5.3864e-03, 5.2567e-03, 5.3711e-03, 1.7366e-03, 9.8801e-03,\n",
      "        5.4092e-03, 4.6043e-03, 2.5043e-03, 2.2926e-03, 3.3588e-03, 8.0032e-03,\n",
      "        1.2388e-03, 2.2526e-03, 5.6934e-04, 2.0275e-03, 3.9711e-03, 2.6054e-03,\n",
      "        1.6413e-03, 3.3016e-03, 3.1948e-03, 2.8858e-03, 1.1055e-02, 1.8730e-03,\n",
      "        9.6512e-03, 3.2234e-03, 1.0805e-03, 3.7313e-04, 3.1414e-03, 2.6703e-03,\n",
      "        1.8597e-03, 3.1490e-03, 5.4817e-03, 1.6003e-03, 1.5469e-03, 1.0033e-03,\n",
      "        3.2558e-03, 1.9658e-04, 2.0390e-03, 1.6985e-03, 7.0572e-03, 8.8043e-03,\n",
      "        3.5782e-03, 3.0384e-03, 2.1851e-02, 9.4757e-03, 3.4065e-03, 9.3842e-03,\n",
      "        2.0847e-03, 8.6136e-03, 9.2239e-03, 4.8218e-03, 5.5313e-03, 1.2489e-02,\n",
      "        2.2125e-03, 7.1192e-04, 2.5902e-03, 2.5616e-03, 2.4452e-03, 4.4131e-04,\n",
      "        3.9043e-03, 1.1988e-03, 5.8126e-04, 1.2177e-02, 4.9515e-03, 3.8815e-03,\n",
      "        1.0857e-02, 7.9803e-03, 1.1627e-02, 2.9583e-03, 6.5155e-03, 3.4485e-03,\n",
      "        1.6449e-02, 7.3242e-03, 5.0659e-03, 1.9485e-02, 4.4220e-02, 2.5146e-02,\n",
      "        1.1917e-02, 3.8071e-03, 8.3618e-03, 1.3512e-02, 2.4643e-02, 1.0536e-02,\n",
      "        3.5553e-02, 9.3689e-03, 5.4283e-03, 1.0094e-02, 6.1798e-03, 1.1482e-03,\n",
      "        1.1086e-02, 4.9896e-03, 1.6006e-02, 3.0640e-02, 1.0216e-02, 6.3095e-03,\n",
      "        1.5808e-02, 3.7155e-03, 6.6261e-03, 7.1411e-03, 7.8278e-03, 4.4365e-03,\n",
      "        2.3766e-03, 9.1476e-03, 8.6975e-03, 5.8746e-03, 1.1778e-03, 6.0043e-03,\n",
      "        7.6981e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [116] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [116] : torch.Size([1, 32, 1, 188])\n",
      "Last layer attentions for generated token [116] : tensor([9.7839e-02, 9.7656e-02, 1.6475e-04, 2.7704e-04, 1.6284e-04, 1.4162e-03,\n",
      "        1.3137e-04, 1.7989e-04, 2.6631e-04, 2.4772e-04, 3.5930e-04, 1.3685e-04,\n",
      "        2.3150e-04, 8.4972e-04, 3.9787e-03, 4.3106e-03, 2.2876e-04, 3.2139e-04,\n",
      "        9.5189e-05, 1.9073e-04, 8.0764e-05, 7.4291e-04, 7.9651e-03, 2.6741e-03,\n",
      "        4.2686e-03, 1.1795e-02, 7.2289e-04, 2.7323e-04, 1.6701e-04, 2.6965e-04,\n",
      "        2.4164e-04, 7.1287e-04, 4.4084e-04, 2.0266e-04, 2.9421e-04, 3.1447e-04,\n",
      "        1.3876e-04, 3.2258e-04, 2.6965e-04, 1.9836e-04, 1.8167e-03, 1.1597e-03,\n",
      "        4.4346e-04, 6.8808e-04, 4.1914e-04, 2.9206e-04, 2.1660e-04, 1.5819e-04,\n",
      "        2.7490e-04, 1.3914e-03, 1.6031e-03, 4.4703e-04, 2.4929e-03, 6.9084e-03,\n",
      "        1.1396e-03, 2.9659e-03, 1.2505e-04, 6.3038e-04, 1.4877e-03, 5.2691e-04,\n",
      "        6.6042e-04, 6.3276e-04, 1.0843e-03, 4.6492e-04, 2.8682e-04, 3.9458e-04,\n",
      "        1.5182e-03, 2.6188e-03, 1.4992e-03, 1.7120e-02, 2.0355e-02, 2.1927e-02,\n",
      "        3.4447e-03, 7.8917e-04, 1.3245e-02, 1.2390e-02, 5.6839e-03, 7.8430e-03,\n",
      "        3.8223e-03, 4.0970e-03, 7.4043e-03, 1.0162e-02, 1.5850e-03, 6.6872e-03,\n",
      "        1.3344e-02, 3.0384e-03, 3.5515e-03, 1.0290e-03, 1.5135e-03, 7.7057e-03,\n",
      "        2.4242e-03, 3.0365e-03, 1.7624e-03, 4.0359e-03, 4.5509e-03, 1.4849e-03,\n",
      "        1.2894e-03, 2.5845e-03, 2.5158e-03, 2.3212e-03, 6.8436e-03, 4.1428e-03,\n",
      "        5.0659e-03, 9.7198e-03, 1.4887e-03, 9.5272e-04, 3.5458e-03, 3.1872e-03,\n",
      "        1.5450e-03, 3.0994e-03, 3.7498e-03, 1.6413e-03, 1.1835e-03, 1.4362e-03,\n",
      "        2.0390e-03, 4.3249e-04, 2.8191e-03, 1.3647e-03, 4.3640e-03, 8.3160e-03,\n",
      "        2.2392e-03, 2.5158e-03, 6.8703e-03, 1.9913e-02, 2.5921e-03, 2.8801e-03,\n",
      "        3.3207e-03, 3.7441e-03, 5.4703e-03, 2.0313e-03, 2.1286e-03, 4.8637e-03,\n",
      "        2.5177e-03, 7.9393e-04, 1.1826e-03, 1.7872e-03, 1.8311e-03, 8.2207e-04,\n",
      "        4.6349e-03, 1.8768e-03, 6.4516e-04, 6.7177e-03, 4.6997e-03, 3.4294e-03,\n",
      "        6.8436e-03, 5.5733e-03, 6.3896e-03, 2.0981e-03, 4.2191e-03, 2.7294e-03,\n",
      "        1.0048e-02, 5.8975e-03, 2.4357e-03, 6.8550e-03, 7.4036e-02, 1.8219e-02,\n",
      "        8.9188e-03, 2.6283e-03, 5.4474e-03, 7.9575e-03, 8.7891e-03, 5.3253e-03,\n",
      "        1.3718e-02, 1.3237e-02, 2.8191e-03, 5.3978e-03, 2.6093e-03, 6.1703e-04,\n",
      "        6.4011e-03, 4.9286e-03, 9.4070e-03, 5.0171e-02, 6.5269e-03, 4.2419e-03,\n",
      "        5.4283e-03, 3.5934e-03, 8.4229e-03, 6.7520e-03, 4.6997e-03, 3.4828e-03,\n",
      "        2.8572e-03, 7.0839e-03, 4.6272e-03, 4.2610e-03, 1.0462e-03, 3.7193e-03,\n",
      "        1.2939e-02, 1.9264e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [117] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [117] : torch.Size([1, 32, 1, 189])\n",
      "Last layer attentions for generated token [117] : tensor([9.4971e-02, 9.4971e-02, 1.1498e-04, 2.1148e-04, 1.1408e-04, 1.4944e-03,\n",
      "        2.0897e-04, 3.2425e-04, 8.5115e-04, 2.3222e-04, 4.1318e-04, 2.6059e-04,\n",
      "        1.3292e-04, 4.8137e-04, 4.0321e-03, 2.7370e-03, 2.0742e-04, 2.5415e-04,\n",
      "        7.1645e-05, 1.2672e-04, 1.1277e-04, 6.7472e-04, 6.0043e-03, 1.9207e-03,\n",
      "        5.5962e-03, 1.1230e-02, 3.2306e-04, 1.1319e-04, 1.3626e-04, 4.3058e-04,\n",
      "        3.6335e-04, 6.5804e-04, 4.0364e-04, 2.6941e-04, 3.1567e-04, 1.3185e-04,\n",
      "        9.0957e-05, 3.5906e-04, 1.0717e-04, 1.0842e-04, 1.7128e-03, 5.9414e-04,\n",
      "        2.1064e-04, 2.9302e-04, 3.6764e-04, 1.7703e-04, 5.6696e-04, 1.7667e-04,\n",
      "        3.4404e-04, 2.0370e-03, 1.1330e-03, 3.2878e-04, 2.1915e-03, 5.0163e-03,\n",
      "        9.2411e-04, 1.9522e-03, 1.6117e-04, 4.0913e-04, 1.5783e-03, 5.4550e-04,\n",
      "        7.2813e-04, 9.6846e-04, 2.6512e-03, 5.9795e-04, 3.1757e-04, 6.2990e-04,\n",
      "        7.7963e-04, 1.9217e-03, 2.2125e-03, 1.5656e-02, 1.1337e-02, 1.1505e-02,\n",
      "        3.2444e-03, 1.1568e-03, 1.6296e-02, 6.9923e-03, 6.4812e-03, 4.0970e-03,\n",
      "        4.2572e-03, 5.7869e-03, 7.7095e-03, 5.3329e-03, 1.6537e-03, 8.3008e-03,\n",
      "        5.7259e-03, 3.4485e-03, 1.3599e-03, 8.9836e-04, 1.4896e-03, 6.7444e-03,\n",
      "        1.1377e-03, 3.1605e-03, 1.7815e-03, 4.9248e-03, 3.5915e-03, 1.5669e-03,\n",
      "        1.4324e-03, 3.4065e-03, 2.4338e-03, 5.7220e-03, 1.3901e-02, 2.0523e-03,\n",
      "        6.8092e-03, 5.1880e-03, 1.5440e-03, 1.0138e-03, 4.3755e-03, 2.8687e-03,\n",
      "        2.7618e-03, 4.0894e-03, 5.1727e-03, 2.3365e-03, 3.0994e-03, 1.7080e-03,\n",
      "        5.9013e-03, 4.1723e-04, 3.2921e-03, 2.6951e-03, 7.8812e-03, 5.5542e-03,\n",
      "        2.9354e-03, 4.9858e-03, 1.7776e-02, 9.2545e-03, 1.6031e-03, 4.6005e-03,\n",
      "        1.7366e-03, 5.1498e-03, 6.5613e-03, 3.1681e-03, 4.3411e-03, 8.6136e-03,\n",
      "        1.5335e-03, 7.2241e-04, 2.0008e-03, 2.3556e-03, 4.6387e-03, 8.6546e-04,\n",
      "        9.9487e-03, 1.6012e-03, 6.2275e-04, 1.0330e-02, 2.9545e-03, 2.6340e-03,\n",
      "        3.8090e-03, 5.0735e-03, 1.0925e-02, 4.5242e-03, 9.1934e-03, 3.1662e-03,\n",
      "        2.5787e-02, 4.7264e-03, 3.2196e-03, 1.5121e-02, 4.2603e-02, 1.2421e-02,\n",
      "        5.1575e-03, 2.0638e-03, 5.4588e-03, 8.6899e-03, 1.1147e-02, 3.5248e-03,\n",
      "        1.5450e-02, 5.0087e-03, 2.8973e-03, 4.3602e-03, 3.2825e-03, 9.6655e-04,\n",
      "        8.8501e-03, 4.6616e-03, 1.3298e-02, 2.1973e-02, 4.7951e-03, 2.8973e-03,\n",
      "        7.4425e-03, 4.3488e-03, 1.5045e-02, 7.3662e-03, 7.4654e-03, 4.8523e-03,\n",
      "        4.1885e-03, 1.4832e-02, 1.3168e-02, 7.4539e-03, 3.5076e-03, 9.8724e-03,\n",
      "        1.3908e-02, 4.2572e-03, 1.8797e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [118] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [118] : torch.Size([1, 32, 1, 190])\n",
      "Last layer attentions for generated token [118] : tensor([1.0126e-01, 1.0126e-01, 6.9976e-05, 1.0967e-04, 7.1645e-05, 1.2465e-03,\n",
      "        3.2616e-04, 4.4584e-04, 6.5470e-04, 1.5163e-04, 2.7895e-04, 8.9884e-05,\n",
      "        5.4955e-05, 2.3353e-04, 2.1896e-03, 2.6264e-03, 1.2672e-04, 2.6011e-04,\n",
      "        4.4107e-05, 4.9233e-05, 2.8968e-05, 1.3912e-04, 4.5433e-03, 6.5231e-04,\n",
      "        2.4128e-03, 1.3016e-02, 3.1614e-04, 8.2791e-05, 1.0467e-04, 4.0579e-04,\n",
      "        3.8266e-04, 4.2701e-04, 2.4188e-04, 2.6512e-04, 1.9550e-04, 7.0810e-05,\n",
      "        1.2422e-04, 2.4045e-04, 6.9976e-05, 5.6863e-05, 8.7261e-04, 5.6791e-04,\n",
      "        1.6212e-04, 2.2197e-04, 4.1056e-04, 1.2505e-04, 6.8092e-04, 1.5342e-04,\n",
      "        2.9516e-04, 1.1263e-03, 9.8419e-04, 3.0684e-04, 2.0695e-03, 4.8180e-03,\n",
      "        9.4366e-04, 1.7834e-03, 1.1766e-04, 3.1972e-04, 9.5034e-04, 2.8777e-04,\n",
      "        3.5119e-04, 7.1669e-04, 1.1063e-03, 2.8944e-04, 1.1671e-04, 3.5191e-04,\n",
      "        5.2834e-04, 1.0128e-03, 1.3285e-03, 8.9188e-03, 8.5526e-03, 7.1068e-03,\n",
      "        1.7376e-03, 6.4373e-04, 1.2474e-02, 5.0468e-03, 7.0953e-03, 3.0231e-03,\n",
      "        2.6760e-03, 6.4049e-03, 3.7155e-03, 4.9286e-03, 1.3065e-03, 5.6343e-03,\n",
      "        4.0855e-03, 1.7357e-03, 1.0290e-03, 9.8515e-04, 1.7872e-03, 6.6338e-03,\n",
      "        9.6226e-04, 2.5845e-03, 8.0252e-04, 2.7771e-03, 2.5387e-03, 1.5154e-03,\n",
      "        1.5182e-03, 4.7913e-03, 2.3880e-03, 1.0086e-02, 1.5182e-02, 2.0504e-03,\n",
      "        6.4316e-03, 4.1771e-03, 1.3075e-03, 7.4816e-04, 3.2024e-03, 2.5902e-03,\n",
      "        3.4103e-03, 4.8523e-03, 5.7564e-03, 2.4509e-03, 3.0098e-03, 1.5621e-03,\n",
      "        3.2692e-03, 2.7776e-04, 1.9684e-03, 1.6623e-03, 6.7978e-03, 5.4893e-03,\n",
      "        3.5458e-03, 4.7379e-03, 1.8188e-02, 6.4354e-03, 1.9836e-03, 7.6942e-03,\n",
      "        1.4267e-03, 5.5046e-03, 8.0719e-03, 4.7188e-03, 8.7662e-03, 1.0040e-02,\n",
      "        1.2827e-03, 6.3992e-04, 2.2640e-03, 1.7176e-03, 3.0708e-03, 6.3467e-04,\n",
      "        5.9395e-03, 1.1034e-03, 3.3069e-04, 6.4316e-03, 2.4872e-03, 2.0428e-03,\n",
      "        3.7098e-03, 6.5651e-03, 1.1925e-02, 3.6736e-03, 7.7209e-03, 2.5845e-03,\n",
      "        2.2705e-02, 4.0741e-03, 3.1681e-03, 8.5754e-03, 3.0762e-02, 1.1841e-02,\n",
      "        6.2294e-03, 1.9798e-03, 6.3934e-03, 1.1658e-02, 1.2909e-02, 3.4637e-03,\n",
      "        1.6205e-02, 3.3321e-03, 2.6169e-03, 4.4060e-03, 4.0627e-03, 8.7452e-04,\n",
      "        1.0109e-02, 4.5204e-03, 1.3794e-02, 2.2705e-02, 5.6801e-03, 2.8419e-03,\n",
      "        1.4511e-02, 5.6267e-03, 1.6541e-02, 1.0780e-02, 1.1360e-02, 6.8474e-03,\n",
      "        4.3335e-03, 2.3911e-02, 2.7145e-02, 1.0513e-02, 6.5956e-03, 1.1200e-02,\n",
      "        1.4885e-02, 9.1629e-03, 2.5845e-03, 3.8204e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [119] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [119] : torch.Size([1, 32, 1, 191])\n",
      "Last layer attentions for generated token [119] : tensor([1.2805e-01, 1.2805e-01, 2.9516e-04, 3.4451e-04, 2.1017e-04, 3.5820e-03,\n",
      "        3.3069e-04, 6.0368e-04, 1.4896e-03, 4.1723e-04, 4.1485e-04, 3.1734e-04,\n",
      "        2.6369e-04, 7.9012e-04, 5.4359e-03, 3.5210e-03, 2.0981e-04, 3.4261e-04,\n",
      "        8.8155e-05, 1.9443e-04, 1.8620e-04, 8.4114e-04, 1.0437e-02, 2.7847e-03,\n",
      "        6.1417e-03, 1.2794e-02, 4.8399e-04, 1.8370e-04, 1.6534e-04, 4.1556e-04,\n",
      "        6.2609e-04, 9.0075e-04, 4.2129e-04, 2.2459e-04, 2.9993e-04, 1.5080e-04,\n",
      "        1.0592e-04, 3.9029e-04, 1.1015e-04, 5.5790e-05, 1.5087e-03, 6.1178e-04,\n",
      "        1.6081e-04, 2.1148e-04, 2.3592e-04, 1.0186e-04, 2.6512e-04, 1.5020e-04,\n",
      "        1.7667e-04, 1.5898e-03, 1.0805e-03, 4.3726e-04, 2.3823e-03, 6.7863e-03,\n",
      "        9.4748e-04, 2.9907e-03, 1.5175e-04, 7.5102e-04, 2.3041e-03, 6.6042e-04,\n",
      "        6.6519e-04, 1.2693e-03, 2.9526e-03, 6.0701e-04, 4.4155e-04, 8.9169e-04,\n",
      "        7.5245e-04, 1.9293e-03, 2.1000e-03, 1.4969e-02, 1.0811e-02, 9.5444e-03,\n",
      "        2.0485e-03, 9.2745e-04, 1.5205e-02, 6.0005e-03, 5.0583e-03, 2.7905e-03,\n",
      "        1.6823e-03, 4.1199e-03, 6.0768e-03, 4.9133e-03, 1.0347e-03, 7.6637e-03,\n",
      "        5.6801e-03, 2.6340e-03, 1.2922e-03, 7.8106e-04, 1.6136e-03, 8.3237e-03,\n",
      "        1.6842e-03, 3.8319e-03, 1.2312e-03, 7.3128e-03, 2.8400e-03, 1.5144e-03,\n",
      "        1.3590e-03, 4.0703e-03, 1.4019e-03, 6.4812e-03, 1.4488e-02, 3.5114e-03,\n",
      "        7.8354e-03, 4.8523e-03, 1.9503e-03, 1.9121e-03, 4.4479e-03, 2.1267e-03,\n",
      "        2.2030e-03, 3.5706e-03, 4.2610e-03, 1.9426e-03, 2.8477e-03, 1.3552e-03,\n",
      "        6.4697e-03, 3.9434e-04, 1.7281e-03, 1.3962e-03, 5.3444e-03, 4.3678e-03,\n",
      "        2.4242e-03, 4.5509e-03, 1.8417e-02, 6.7444e-03, 1.1702e-03, 4.3335e-03,\n",
      "        1.9331e-03, 3.1548e-03, 5.0926e-03, 2.3956e-03, 3.8033e-03, 1.0117e-02,\n",
      "        2.0962e-03, 8.8835e-04, 2.3041e-03, 2.4204e-03, 3.8033e-03, 7.5674e-04,\n",
      "        7.9803e-03, 2.0485e-03, 9.0408e-04, 1.2192e-02, 3.0975e-03, 2.1019e-03,\n",
      "        3.3054e-03, 4.5319e-03, 6.8016e-03, 2.7695e-03, 7.3204e-03, 1.9722e-03,\n",
      "        2.5406e-02, 5.3902e-03, 1.9159e-03, 9.8419e-03, 2.8793e-02, 7.9651e-03,\n",
      "        3.4065e-03, 1.3981e-03, 4.0054e-03, 8.1711e-03, 8.8730e-03, 2.8419e-03,\n",
      "        1.3947e-02, 5.1651e-03, 2.3479e-03, 3.4618e-03, 2.9392e-03, 8.1539e-04,\n",
      "        6.2599e-03, 3.9558e-03, 1.3199e-02, 1.9089e-02, 3.5362e-03, 2.0924e-03,\n",
      "        5.7526e-03, 2.8744e-03, 1.1368e-02, 5.4474e-03, 7.4081e-03, 4.1542e-03,\n",
      "        2.8801e-03, 9.0256e-03, 1.4511e-02, 4.4212e-03, 4.1466e-03, 1.2146e-02,\n",
      "        1.3420e-02, 4.8637e-03, 2.1896e-03, 2.7199e-03, 6.7444e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [120] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [120] : torch.Size([1, 32, 1, 192])\n",
      "Last layer attentions for generated token [120] : tensor([1.8738e-01, 1.8701e-01, 1.6117e-04, 3.0041e-04, 1.3626e-04, 2.2888e-03,\n",
      "        1.5438e-04, 1.8692e-04, 5.2309e-04, 2.2292e-04, 5.6696e-04, 1.3149e-04,\n",
      "        1.9133e-04, 5.0831e-04, 9.1248e-03, 7.1678e-03, 1.6308e-04, 4.2868e-04,\n",
      "        1.5378e-04, 2.6464e-04, 1.1474e-04, 7.6151e-04, 1.0429e-02, 3.1490e-03,\n",
      "        9.1171e-03, 2.1469e-02, 1.2283e-03, 2.8276e-04, 1.8120e-04, 2.8849e-04,\n",
      "        2.9135e-04, 7.6008e-04, 3.8958e-04, 2.4772e-04, 2.5392e-04, 2.9469e-04,\n",
      "        1.4555e-04, 2.9230e-04, 1.4842e-04, 1.0204e-04, 1.5316e-03, 1.0004e-03,\n",
      "        3.1304e-04, 3.5763e-04, 3.0708e-04, 1.7631e-04, 2.1768e-04, 1.6427e-04,\n",
      "        2.5964e-04, 1.6861e-03, 1.6279e-03, 2.6631e-04, 2.1152e-03, 5.7030e-03,\n",
      "        1.2503e-03, 3.9005e-03, 2.4962e-04, 6.1655e-04, 1.8225e-03, 5.0640e-04,\n",
      "        8.5115e-04, 5.8603e-04, 1.3361e-03, 9.8324e-04, 2.9230e-04, 5.2929e-04,\n",
      "        1.1187e-03, 1.3762e-03, 1.4334e-03, 8.1940e-03, 1.0681e-02, 1.2527e-02,\n",
      "        1.4791e-03, 3.0398e-04, 1.1169e-02, 7.7438e-03, 5.8937e-03, 4.4212e-03,\n",
      "        4.5738e-03, 3.9406e-03, 3.2749e-03, 7.1487e-03, 1.2579e-03, 5.5466e-03,\n",
      "        5.7144e-03, 3.4523e-03, 1.3676e-03, 9.2554e-04, 1.1997e-03, 5.9471e-03,\n",
      "        8.5926e-04, 2.8076e-03, 1.1187e-03, 2.5654e-03, 2.2793e-03, 8.5449e-04,\n",
      "        6.9475e-04, 2.0409e-03, 1.0328e-03, 3.0289e-03, 7.5874e-03, 3.1624e-03,\n",
      "        6.9046e-03, 3.8185e-03, 1.3628e-03, 7.2098e-04, 2.8820e-03, 2.0351e-03,\n",
      "        1.0366e-03, 1.5564e-03, 3.5896e-03, 1.7271e-03, 1.4763e-03, 1.0023e-03,\n",
      "        2.7046e-03, 3.3641e-04, 1.4620e-03, 1.0071e-03, 2.9964e-03, 4.7569e-03,\n",
      "        1.2827e-03, 1.9369e-03, 6.7825e-03, 6.6719e-03, 2.0046e-03, 2.2449e-03,\n",
      "        1.2751e-03, 2.6970e-03, 5.5428e-03, 1.9608e-03, 3.0060e-03, 5.4893e-03,\n",
      "        1.6727e-03, 5.2547e-04, 1.0366e-03, 1.3361e-03, 1.5049e-03, 4.0030e-04,\n",
      "        2.6035e-03, 1.1768e-03, 3.5191e-04, 3.5057e-03, 2.2202e-03, 1.4477e-03,\n",
      "        3.1185e-03, 2.9278e-03, 5.6152e-03, 2.0275e-03, 3.3207e-03, 2.0561e-03,\n",
      "        1.1391e-02, 3.4046e-03, 1.5774e-03, 5.5656e-03, 4.0955e-02, 7.6065e-03,\n",
      "        4.6463e-03, 1.5020e-03, 3.8509e-03, 7.7438e-03, 6.5460e-03, 4.1771e-03,\n",
      "        1.5480e-02, 5.6953e-03, 1.1978e-03, 3.5057e-03, 1.7653e-03, 3.3593e-04,\n",
      "        4.9973e-03, 3.1071e-03, 1.2589e-02, 3.1143e-02, 4.9438e-03, 1.9159e-03,\n",
      "        4.9362e-03, 2.3422e-03, 6.8779e-03, 4.8561e-03, 4.4746e-03, 4.0054e-03,\n",
      "        2.1152e-03, 5.9357e-03, 4.7607e-03, 3.3951e-03, 1.4420e-03, 4.6310e-03,\n",
      "        1.5129e-02, 4.1695e-03, 1.1454e-03, 1.4906e-03, 1.4648e-03, 7.6561e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [121] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [121] : torch.Size([1, 32, 1, 193])\n",
      "Last layer attentions for generated token [121] : tensor([1.1694e-01, 1.1670e-01, 2.6345e-05, 1.0914e-04, 2.4438e-05, 6.9809e-04,\n",
      "        1.1897e-04, 9.5963e-05, 1.4579e-04, 2.0921e-04, 5.4455e-04, 2.2793e-04,\n",
      "        1.7929e-04, 4.9686e-04, 6.4812e-03, 5.9090e-03, 1.7715e-04, 4.3440e-04,\n",
      "        7.6532e-05, 1.8322e-04, 7.1287e-05, 1.8394e-04, 1.7109e-03, 5.7888e-04,\n",
      "        3.0956e-03, 7.9880e-03, 8.2731e-04, 2.9325e-04, 8.6665e-05, 1.0216e-04,\n",
      "        1.2565e-04, 4.0174e-04, 4.6420e-04, 3.3307e-04, 1.4126e-04, 1.7440e-04,\n",
      "        1.5819e-04, 1.5795e-04, 6.8843e-05, 5.7101e-05, 1.2732e-03, 6.5708e-04,\n",
      "        5.5456e-04, 3.4499e-04, 2.9778e-04, 1.2130e-04, 1.2565e-04, 7.9870e-05,\n",
      "        1.0747e-04, 1.0061e-03, 8.3399e-04, 9.4116e-05, 1.6060e-03, 2.3003e-03,\n",
      "        1.1625e-03, 2.6340e-03, 4.7517e-04, 6.5851e-04, 1.4849e-03, 6.4039e-04,\n",
      "        7.8344e-04, 7.4005e-04, 1.2226e-03, 7.7438e-04, 5.7888e-04, 4.7517e-04,\n",
      "        1.1444e-03, 1.0500e-03, 1.1702e-03, 5.9776e-03, 4.8866e-03, 1.1795e-02,\n",
      "        5.3101e-03, 6.7282e-04, 1.0773e-02, 6.0692e-03, 7.8583e-03, 4.8103e-03,\n",
      "        6.8169e-03, 4.7493e-03, 4.6158e-03, 7.6103e-03, 3.1376e-03, 7.3090e-03,\n",
      "        5.1651e-03, 4.9019e-03, 2.9202e-03, 3.7003e-03, 2.5635e-03, 4.5547e-03,\n",
      "        1.4696e-03, 3.0632e-03, 1.1721e-03, 2.0523e-03, 3.3989e-03, 1.4877e-03,\n",
      "        1.0653e-03, 1.9913e-03, 2.1496e-03, 1.7529e-03, 4.9744e-03, 2.4681e-03,\n",
      "        7.6180e-03, 4.1962e-03, 1.7824e-03, 4.6492e-04, 2.8191e-03, 3.2120e-03,\n",
      "        1.2321e-03, 1.5202e-03, 2.7447e-03, 1.4992e-03, 1.3933e-03, 9.5987e-04,\n",
      "        1.6403e-03, 2.5988e-04, 1.1444e-03, 1.2436e-03, 3.9864e-03, 6.7024e-03,\n",
      "        2.4948e-03, 2.6493e-03, 7.9803e-03, 7.5302e-03, 5.1727e-03, 4.9515e-03,\n",
      "        2.2964e-03, 9.5978e-03, 1.0605e-02, 4.5242e-03, 4.3716e-03, 6.3248e-03,\n",
      "        1.5745e-03, 8.4019e-04, 1.7824e-03, 1.9989e-03, 1.4496e-03, 6.2466e-04,\n",
      "        3.2692e-03, 2.2106e-03, 9.2840e-04, 4.7684e-03, 3.7842e-03, 2.1038e-03,\n",
      "        5.4169e-03, 5.2643e-03, 8.3237e-03, 2.9831e-03, 4.6806e-03, 3.5763e-03,\n",
      "        9.2239e-03, 5.5237e-03, 4.7569e-03, 1.1276e-02, 4.9866e-02, 2.2598e-02,\n",
      "        1.4488e-02, 5.7030e-03, 6.3515e-03, 1.4412e-02, 1.6342e-02, 7.5874e-03,\n",
      "        2.1774e-02, 8.3237e-03, 3.6697e-03, 9.7809e-03, 4.4746e-03, 1.2951e-03,\n",
      "        6.9084e-03, 3.4199e-03, 1.0513e-02, 2.5314e-02, 1.2901e-02, 4.2496e-03,\n",
      "        1.2474e-02, 5.8975e-03, 1.1391e-02, 7.7019e-03, 5.5695e-03, 4.2229e-03,\n",
      "        1.9035e-03, 6.1646e-03, 5.1880e-03, 5.5428e-03, 1.3342e-03, 4.9782e-03,\n",
      "        1.6693e-02, 4.6730e-03, 1.1520e-03, 1.4305e-03, 1.0214e-03, 8.1787e-03,\n",
      "        1.0765e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [122] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [122] : torch.Size([1, 32, 1, 194])\n",
      "Last layer attentions for generated token [122] : tensor([9.0027e-02, 9.0027e-02, 6.4671e-05, 1.1808e-04, 6.6459e-05, 9.7609e-04,\n",
      "        7.6532e-05, 1.1176e-04, 1.7214e-04, 2.1172e-04, 5.9128e-04, 9.3758e-05,\n",
      "        2.1291e-04, 4.4727e-04, 6.0539e-03, 1.1864e-02, 2.0444e-04, 3.7074e-04,\n",
      "        7.4148e-05, 1.9050e-04, 2.9027e-05, 1.2231e-04, 2.8057e-03, 5.5885e-04,\n",
      "        1.4582e-03, 1.0178e-02, 9.4318e-04, 3.8791e-04, 1.4496e-04, 1.9813e-04,\n",
      "        3.0923e-04, 8.8453e-04, 5.4359e-04, 4.7159e-04, 1.9014e-04, 1.8144e-04,\n",
      "        8.7738e-05, 1.5247e-04, 6.1274e-05, 6.0558e-05, 1.3924e-03, 7.4768e-04,\n",
      "        6.7806e-04, 4.3440e-04, 3.8266e-04, 1.0020e-04, 1.8573e-04, 8.4043e-05,\n",
      "        1.2589e-04, 9.9277e-04, 8.1301e-04, 1.6296e-04, 1.0748e-03, 1.8311e-03,\n",
      "        8.4066e-04, 2.7637e-03, 2.6870e-04, 3.8099e-04, 1.1005e-03, 4.6515e-04,\n",
      "        5.6982e-04, 6.8617e-04, 6.8331e-04, 3.4499e-04, 2.2674e-04, 3.5810e-04,\n",
      "        6.4087e-04, 7.9918e-04, 5.1498e-04, 6.4011e-03, 6.1188e-03, 1.0231e-02,\n",
      "        1.9875e-03, 3.8624e-04, 8.0643e-03, 4.9210e-03, 7.6675e-03, 3.3741e-03,\n",
      "        3.1548e-03, 4.4823e-03, 4.5395e-03, 5.7373e-03, 2.7142e-03, 7.9346e-03,\n",
      "        6.7940e-03, 3.1834e-03, 2.7447e-03, 3.2063e-03, 3.2330e-03, 5.5122e-03,\n",
      "        9.7847e-04, 2.1057e-03, 5.2691e-04, 2.2488e-03, 3.7041e-03, 1.4925e-03,\n",
      "        1.0700e-03, 2.2545e-03, 2.3136e-03, 1.9760e-03, 5.9471e-03, 1.7061e-03,\n",
      "        7.1335e-03, 3.5896e-03, 9.5272e-04, 3.5667e-04, 2.8572e-03, 2.2583e-03,\n",
      "        1.1292e-03, 2.1133e-03, 3.3321e-03, 9.5892e-04, 6.9571e-04, 4.6062e-04,\n",
      "        1.2112e-03, 1.5402e-04, 8.3876e-04, 7.1907e-04, 3.7193e-03, 8.3923e-03,\n",
      "        2.2907e-03, 2.8305e-03, 1.0857e-02, 9.3002e-03, 4.9820e-03, 5.0812e-03,\n",
      "        1.8215e-03, 8.7738e-03, 1.9638e-02, 4.0703e-03, 4.6768e-03, 5.8174e-03,\n",
      "        1.3685e-03, 6.6996e-04, 2.0733e-03, 1.0967e-03, 9.0790e-04, 2.3663e-04,\n",
      "        2.4223e-03, 8.8072e-04, 3.4976e-04, 5.7182e-03, 4.3945e-03, 2.0809e-03,\n",
      "        4.8828e-03, 5.0926e-03, 6.5842e-03, 1.9722e-03, 2.8763e-03, 2.0046e-03,\n",
      "        7.7744e-03, 4.3564e-03, 2.3193e-03, 8.2321e-03, 5.5786e-02, 2.4857e-02,\n",
      "        1.7670e-02, 3.1147e-03, 5.9357e-03, 2.2003e-02, 1.6922e-02, 5.6496e-03,\n",
      "        2.0645e-02, 1.1444e-02, 3.1624e-03, 1.3954e-02, 4.0550e-03, 7.3338e-04,\n",
      "        1.1719e-02, 3.5591e-03, 1.6708e-02, 4.5746e-02, 1.0979e-02, 5.9624e-03,\n",
      "        1.3336e-02, 3.8338e-03, 1.4061e-02, 8.9874e-03, 6.4430e-03, 5.2299e-03,\n",
      "        1.4887e-03, 5.6839e-03, 5.6496e-03, 5.6000e-03, 1.7252e-03, 6.9885e-03,\n",
      "        1.8204e-02, 5.2986e-03, 1.1072e-03, 1.1787e-03, 1.1683e-03, 5.7945e-03,\n",
      "        1.7212e-02, 1.0986e-02], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [123] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [123] : torch.Size([1, 32, 1, 195])\n",
      "Last layer attentions for generated token [123] : tensor([1.6724e-01, 1.6724e-01, 5.9843e-05, 5.6207e-05, 4.7743e-05, 7.7629e-04,\n",
      "        1.0341e-04, 1.0222e-04, 1.3912e-04, 1.6141e-04, 5.8842e-04, 1.1951e-04,\n",
      "        2.0170e-04, 4.6444e-04, 4.7150e-03, 9.1019e-03, 7.7128e-05, 2.4283e-04,\n",
      "        6.1035e-05, 1.7321e-04, 3.2425e-05, 9.2328e-05, 4.3907e-03, 5.9986e-04,\n",
      "        1.3828e-03, 8.9493e-03, 5.1689e-04, 3.2234e-04, 1.0264e-04, 1.6141e-04,\n",
      "        2.8110e-04, 4.3201e-04, 4.4918e-04, 3.9577e-04, 1.8156e-04, 1.4472e-04,\n",
      "        1.1271e-04, 1.6141e-04, 5.6684e-05, 3.7432e-05, 7.1812e-04, 5.0306e-04,\n",
      "        4.2367e-04, 2.4045e-04, 2.8014e-04, 7.9930e-05, 1.3125e-04, 9.3043e-05,\n",
      "        1.3125e-04, 6.7568e-04, 5.6696e-04, 1.3912e-04, 9.8896e-04, 1.6022e-03,\n",
      "        9.5463e-04, 2.9011e-03, 2.3675e-04, 3.7241e-04, 6.4230e-04, 2.9993e-04,\n",
      "        3.6025e-04, 4.5276e-04, 6.7043e-04, 2.8563e-04, 2.2149e-04, 2.9182e-04,\n",
      "        4.7350e-04, 8.4925e-04, 4.7445e-04, 5.4741e-03, 3.5152e-03, 8.2779e-03,\n",
      "        1.3103e-03, 3.3593e-04, 6.5956e-03, 3.6964e-03, 4.0169e-03, 2.5005e-03,\n",
      "        2.0142e-03, 3.4733e-03, 3.5343e-03, 4.3030e-03, 1.8606e-03, 8.3847e-03,\n",
      "        4.7646e-03, 2.6073e-03, 2.3327e-03, 3.3131e-03, 3.1586e-03, 6.6414e-03,\n",
      "        8.8310e-04, 1.8988e-03, 5.1403e-04, 1.2474e-03, 3.1147e-03, 1.3227e-03,\n",
      "        9.6416e-04, 2.0885e-03, 1.6069e-03, 2.5749e-03, 5.4893e-03, 1.6384e-03,\n",
      "        6.0883e-03, 2.4662e-03, 7.7486e-04, 2.1052e-04, 1.3866e-03, 1.4067e-03,\n",
      "        9.0933e-04, 1.1387e-03, 2.8000e-03, 8.0395e-04, 3.4714e-04, 2.8777e-04,\n",
      "        7.6437e-04, 7.1645e-05, 5.9843e-04, 7.2479e-04, 2.9964e-03, 5.4817e-03,\n",
      "        1.7805e-03, 2.3479e-03, 1.0475e-02, 6.0310e-03, 3.3894e-03, 3.9406e-03,\n",
      "        1.1835e-03, 5.1117e-03, 9.3842e-03, 2.7409e-03, 5.2567e-03, 6.6528e-03,\n",
      "        1.2283e-03, 6.7854e-04, 1.7824e-03, 1.0672e-03, 1.0796e-03, 2.8896e-04,\n",
      "        2.1057e-03, 1.1158e-03, 4.6086e-04, 6.3972e-03, 3.9406e-03, 1.4877e-03,\n",
      "        2.8610e-03, 3.2578e-03, 5.0201e-03, 1.7557e-03, 2.6627e-03, 1.5545e-03,\n",
      "        1.0536e-02, 4.2610e-03, 2.3746e-03, 1.0765e-02, 4.8492e-02, 1.4160e-02,\n",
      "        1.2169e-02, 2.2850e-03, 3.7842e-03, 1.3222e-02, 2.1286e-02, 4.3793e-03,\n",
      "        1.5129e-02, 8.6136e-03, 3.7632e-03, 7.4272e-03, 4.0207e-03, 1.2960e-03,\n",
      "        1.2619e-02, 3.7918e-03, 1.4870e-02, 2.7573e-02, 5.6686e-03, 3.2806e-03,\n",
      "        1.2947e-02, 3.5095e-03, 8.5983e-03, 6.1836e-03, 7.3433e-03, 5.9166e-03,\n",
      "        1.4877e-03, 4.4708e-03, 6.3438e-03, 5.0583e-03, 1.5697e-03, 5.6000e-03,\n",
      "        1.7746e-02, 4.1885e-03, 1.1454e-03, 1.1673e-03, 1.1902e-03, 5.1765e-03,\n",
      "        1.2093e-02, 6.5422e-03, 7.5645e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [124] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [124] : torch.Size([1, 32, 1, 196])\n",
      "Last layer attentions for generated token [124] : tensor([1.5332e-01, 1.5295e-01, 1.2851e-04, 2.7108e-04, 8.0705e-05, 1.8215e-03,\n",
      "        1.7703e-04, 1.6689e-04, 2.6846e-04, 1.8334e-04, 4.6730e-04, 1.7774e-04,\n",
      "        2.8229e-04, 1.0996e-03, 4.1542e-03, 6.0539e-03, 1.8334e-04, 2.5105e-04,\n",
      "        8.7261e-05, 1.4174e-04, 6.5625e-05, 2.8682e-04, 7.1983e-03, 1.2655e-03,\n",
      "        1.9588e-03, 1.1009e-02, 9.4032e-04, 3.9816e-04, 2.2292e-04, 2.3496e-04,\n",
      "        3.0112e-04, 4.6539e-04, 6.2990e-04, 5.2118e-04, 2.3091e-04, 3.8075e-04,\n",
      "        4.0126e-04, 3.2687e-04, 2.5797e-04, 1.5259e-04, 1.5898e-03, 1.1635e-03,\n",
      "        5.9891e-04, 4.7469e-04, 4.4250e-04, 3.6454e-04, 1.7500e-04, 1.9979e-04,\n",
      "        3.0279e-04, 9.2936e-04, 1.0576e-03, 1.7977e-04, 2.2640e-03, 4.0169e-03,\n",
      "        1.8301e-03, 3.2845e-03, 3.2616e-04, 1.5335e-03, 2.2774e-03, 8.0442e-04,\n",
      "        7.6723e-04, 9.2030e-04, 1.3762e-03, 7.6294e-04, 5.1117e-04, 3.2616e-04,\n",
      "        1.8034e-03, 1.4257e-03, 1.7347e-03, 6.0883e-03, 4.3182e-03, 9.7504e-03,\n",
      "        3.2845e-03, 5.8603e-04, 5.8670e-03, 4.6997e-03, 3.6430e-03, 2.8019e-03,\n",
      "        4.0398e-03, 3.8662e-03, 3.6011e-03, 3.4084e-03, 1.8053e-03, 4.4441e-03,\n",
      "        5.9700e-03, 2.9240e-03, 2.3632e-03, 4.6310e-03, 2.9774e-03, 4.3106e-03,\n",
      "        1.7929e-03, 2.9049e-03, 1.4677e-03, 1.5287e-03, 2.6379e-03, 1.5030e-03,\n",
      "        1.5898e-03, 3.8071e-03, 2.0847e-03, 2.9831e-03, 4.0474e-03, 2.8019e-03,\n",
      "        5.9624e-03, 4.6043e-03, 2.8286e-03, 1.3256e-03, 2.3994e-03, 3.0231e-03,\n",
      "        2.8934e-03, 2.0847e-03, 2.5043e-03, 1.6890e-03, 7.9823e-04, 6.6805e-04,\n",
      "        1.0290e-03, 1.6367e-04, 1.1911e-03, 1.8721e-03, 3.2825e-03, 4.4098e-03,\n",
      "        2.1820e-03, 3.0117e-03, 7.0190e-03, 7.8049e-03, 4.0359e-03, 3.5591e-03,\n",
      "        1.8663e-03, 4.4060e-03, 6.6986e-03, 3.4657e-03, 4.6158e-03, 5.5618e-03,\n",
      "        2.3270e-03, 1.3056e-03, 1.1797e-03, 2.0733e-03, 1.4362e-03, 7.7057e-04,\n",
      "        2.0256e-03, 1.5898e-03, 9.8515e-04, 1.9369e-03, 3.0956e-03, 1.7586e-03,\n",
      "        4.6539e-03, 4.8676e-03, 9.0637e-03, 2.9182e-03, 2.8229e-03, 2.3346e-03,\n",
      "        5.2986e-03, 4.1237e-03, 3.1719e-03, 7.5111e-03, 3.7750e-02, 1.3283e-02,\n",
      "        9.1171e-03, 3.4199e-03, 8.3160e-03, 1.1787e-02, 1.2215e-02, 9.4910e-03,\n",
      "        1.5076e-02, 8.4381e-03, 4.7569e-03, 8.6975e-03, 7.7057e-03, 2.4853e-03,\n",
      "        8.4991e-03, 5.0163e-03, 9.5367e-03, 2.7237e-02, 5.7030e-03, 3.6354e-03,\n",
      "        9.3918e-03, 4.4327e-03, 7.3357e-03, 5.3520e-03, 5.7755e-03, 5.3902e-03,\n",
      "        3.0804e-03, 5.9319e-03, 5.8174e-03, 3.8567e-03, 1.6012e-03, 3.9749e-03,\n",
      "        1.5808e-02, 3.7193e-03, 2.1019e-03, 1.6394e-03, 1.9588e-03, 7.4158e-03,\n",
      "        9.7656e-03, 8.8577e-03, 6.0806e-03, 6.8932e-03], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [125] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [125] : torch.Size([1, 32, 1, 197])\n",
      "Last layer attentions for generated token [125] : tensor([1.2488e-01, 1.2488e-01, 4.6313e-05, 7.8738e-05, 4.0710e-05, 5.7364e-04,\n",
      "        2.0945e-04, 1.8704e-04, 2.4295e-04, 1.7536e-04, 6.0987e-04, 1.6153e-04,\n",
      "        1.7166e-04, 3.1686e-04, 3.5744e-03, 3.7937e-03, 9.1314e-05, 3.0589e-04,\n",
      "        1.1277e-04, 1.2934e-04, 3.1710e-05, 1.5843e-04, 2.9507e-03, 4.7851e-04,\n",
      "        1.5688e-03, 5.8899e-03, 6.2895e-04, 2.2173e-04, 1.3769e-04, 2.2602e-04,\n",
      "        6.4135e-04, 6.0511e-04, 7.4530e-04, 6.6042e-04, 3.7932e-04, 1.6034e-04,\n",
      "        1.3232e-04, 2.3508e-04, 8.1897e-05, 9.0599e-05, 9.3126e-04, 4.2820e-04,\n",
      "        2.0862e-04, 2.1362e-04, 2.6417e-04, 1.1146e-04, 1.2243e-04, 9.0241e-05,\n",
      "        1.8454e-04, 8.9598e-04, 5.3930e-04, 1.0717e-04, 8.9216e-04, 1.2074e-03,\n",
      "        7.8726e-04, 2.2736e-03, 2.9302e-04, 6.2656e-04, 1.0729e-03, 5.4646e-04,\n",
      "        4.4250e-04, 6.7997e-04, 1.3437e-03, 4.5657e-04, 3.8528e-04, 2.9898e-04,\n",
      "        5.6171e-04, 7.3814e-04, 6.2895e-04, 6.0158e-03, 2.9297e-03, 7.1297e-03,\n",
      "        1.1950e-03, 2.4390e-04, 6.9160e-03, 2.7218e-03, 4.3182e-03, 2.0065e-03,\n",
      "        2.8667e-03, 4.2877e-03, 4.9095e-03, 3.3436e-03, 1.4076e-03, 8.2245e-03,\n",
      "        3.5610e-03, 2.7142e-03, 1.7080e-03, 1.5078e-03, 2.9621e-03, 7.7362e-03,\n",
      "        1.3418e-03, 2.8305e-03, 6.2656e-04, 1.7300e-03, 2.7161e-03, 1.4696e-03,\n",
      "        1.8311e-03, 2.7084e-03, 2.8210e-03, 5.4779e-03, 9.3231e-03, 1.6232e-03,\n",
      "        5.7907e-03, 2.1820e-03, 1.1644e-03, 3.1638e-04, 3.4161e-03, 2.9278e-03,\n",
      "        2.2335e-03, 2.3651e-03, 4.7226e-03, 1.9283e-03, 9.1696e-04, 5.5408e-04,\n",
      "        2.0866e-03, 1.1820e-04, 1.2684e-03, 2.3594e-03, 4.7913e-03, 7.3280e-03,\n",
      "        2.8877e-03, 3.6469e-03, 1.4900e-02, 5.5885e-03, 3.2501e-03, 7.3586e-03,\n",
      "        2.3823e-03, 7.2632e-03, 8.6594e-03, 3.2749e-03, 6.3210e-03, 9.5291e-03,\n",
      "        1.7881e-03, 1.0490e-03, 2.1114e-03, 2.4281e-03, 2.4014e-03, 5.2547e-04,\n",
      "        4.5319e-03, 9.7609e-04, 7.3528e-04, 4.0932e-03, 2.4700e-03, 2.3193e-03,\n",
      "        5.3139e-03, 6.3133e-03, 7.8201e-03, 3.0403e-03, 4.8103e-03, 2.4548e-03,\n",
      "        1.5808e-02, 7.9422e-03, 3.2368e-03, 1.6129e-02, 2.5894e-02, 1.1086e-02,\n",
      "        7.7705e-03, 2.4605e-03, 5.0049e-03, 1.1742e-02, 2.0142e-02, 2.7294e-03,\n",
      "        1.6281e-02, 7.1373e-03, 3.8509e-03, 6.9733e-03, 4.9362e-03, 1.6623e-03,\n",
      "        9.5825e-03, 4.5319e-03, 1.8616e-02, 2.1515e-02, 6.2332e-03, 3.3035e-03,\n",
      "        1.3077e-02, 3.3607e-03, 1.2611e-02, 6.5956e-03, 9.4223e-03, 5.2490e-03,\n",
      "        2.8877e-03, 1.1665e-02, 1.3664e-02, 8.4152e-03, 2.5196e-03, 9.6664e-03,\n",
      "        1.4427e-02, 5.5275e-03, 2.2297e-03, 3.0556e-03, 2.5673e-03, 9.2163e-03,\n",
      "        1.5587e-02, 6.9656e-03, 5.2032e-03, 1.0719e-02, 6.9160e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [126] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [126] : torch.Size([1, 32, 1, 198])\n",
      "Last layer attentions for generated token [126] : tensor([1.2671e-01, 1.2671e-01, 4.6730e-05, 1.6296e-04, 4.5419e-05, 1.5812e-03,\n",
      "        1.8501e-04, 1.1247e-04, 1.9395e-04, 1.5020e-04, 4.6992e-04, 2.0683e-04,\n",
      "        2.3949e-04, 6.3705e-04, 4.7455e-03, 4.6577e-03, 9.1791e-05, 1.7619e-04,\n",
      "        4.5419e-05, 7.9453e-05, 4.8161e-05, 1.7726e-04, 3.4809e-03, 6.7949e-04,\n",
      "        3.9558e-03, 1.1093e-02, 3.2425e-04, 2.5845e-04, 1.1206e-04, 1.2350e-04,\n",
      "        2.1982e-04, 4.4298e-04, 5.5265e-04, 3.5334e-04, 1.8871e-04, 2.7037e-04,\n",
      "        2.4652e-04, 2.2852e-04, 1.0943e-04, 8.0049e-05, 1.2655e-03, 8.3447e-04,\n",
      "        2.7728e-04, 2.8157e-04, 2.7561e-04, 1.2112e-04, 9.6202e-05, 7.7903e-05,\n",
      "        1.3196e-04, 8.1635e-04, 7.7295e-04, 9.1076e-05, 1.3218e-03, 1.5259e-03,\n",
      "        1.3981e-03, 1.8616e-03, 1.6558e-04, 6.4230e-04, 1.0843e-03, 5.2214e-04,\n",
      "        5.6028e-04, 4.6706e-04, 9.3269e-04, 5.0783e-04, 3.0923e-04, 2.4796e-04,\n",
      "        1.1091e-03, 9.7179e-04, 1.2598e-03, 5.1804e-03, 3.8452e-03, 8.8272e-03,\n",
      "        2.4738e-03, 2.7394e-04, 6.6681e-03, 4.4136e-03, 2.8419e-03, 2.8439e-03,\n",
      "        3.4904e-03, 3.6354e-03, 3.4275e-03, 3.7289e-03, 1.2693e-03, 4.5166e-03,\n",
      "        6.3896e-03, 3.7460e-03, 2.6455e-03, 4.6501e-03, 2.7008e-03, 5.2299e-03,\n",
      "        2.1439e-03, 3.0422e-03, 1.5812e-03, 1.6022e-03, 3.0098e-03, 1.4963e-03,\n",
      "        1.5545e-03, 3.5610e-03, 2.5845e-03, 2.5539e-03, 3.6163e-03, 3.0079e-03,\n",
      "        6.8321e-03, 4.2839e-03, 1.3027e-03, 4.5705e-04, 2.3632e-03, 3.9635e-03,\n",
      "        2.2392e-03, 2.3098e-03, 3.6583e-03, 2.6436e-03, 1.4219e-03, 8.2922e-04,\n",
      "        1.9894e-03, 1.8215e-04, 1.5974e-03, 2.4624e-03, 3.9043e-03, 6.6338e-03,\n",
      "        2.8477e-03, 3.2806e-03, 1.2978e-02, 7.2556e-03, 3.3932e-03, 4.2801e-03,\n",
      "        2.1706e-03, 4.5891e-03, 9.2773e-03, 3.1834e-03, 4.4289e-03, 6.2943e-03,\n",
      "        2.8286e-03, 2.0180e-03, 1.2655e-03, 2.3098e-03, 1.7185e-03, 1.0710e-03,\n",
      "        3.4580e-03, 1.2636e-03, 5.8794e-04, 2.6207e-03, 3.9749e-03, 1.9455e-03,\n",
      "        4.8790e-03, 5.1651e-03, 7.7896e-03, 2.9030e-03, 3.7422e-03, 2.8286e-03,\n",
      "        8.4686e-03, 6.2256e-03, 3.4924e-03, 1.2535e-02, 4.3762e-02, 1.5350e-02,\n",
      "        8.8654e-03, 2.7657e-03, 6.6071e-03, 9.0256e-03, 1.3466e-02, 4.4975e-03,\n",
      "        1.2596e-02, 7.1754e-03, 5.1842e-03, 9.1705e-03, 5.2376e-03, 2.0676e-03,\n",
      "        6.1264e-03, 3.0785e-03, 1.2871e-02, 2.5330e-02, 4.9477e-03, 4.8065e-03,\n",
      "        7.5226e-03, 7.5951e-03, 7.8812e-03, 4.8141e-03, 6.3400e-03, 4.8714e-03,\n",
      "        2.5501e-03, 9.2010e-03, 6.4011e-03, 4.8065e-03, 1.4906e-03, 6.1264e-03,\n",
      "        1.8005e-02, 5.2299e-03, 2.0847e-03, 3.6163e-03, 2.2659e-03, 9.7122e-03,\n",
      "        1.8311e-02, 1.8646e-02, 7.4539e-03, 7.5951e-03, 8.6823e-03, 6.9466e-03],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 32\n",
      "Hidden states shape for generated token [127] : torch.Size([1, 1, 4096])\n",
      "Attention weights shape for generated token [127] : torch.Size([1, 32, 1, 199])\n",
      "Last layer attentions for generated token [127] : tensor([1.8640e-01, 1.8640e-01, 9.8944e-05, 1.3268e-04, 7.4089e-05, 1.6365e-03,\n",
      "        2.0790e-04, 2.6059e-04, 4.1175e-04, 2.7013e-04, 6.8283e-04, 1.9836e-04,\n",
      "        1.7774e-04, 3.0422e-04, 4.6425e-03, 4.5547e-03, 1.6701e-04, 2.5463e-04,\n",
      "        9.2208e-05, 9.0420e-05, 4.1902e-05, 2.8348e-04, 4.1962e-03, 4.8971e-04,\n",
      "        3.2902e-03, 9.0103e-03, 4.9591e-04, 2.5272e-04, 1.3578e-04, 2.4438e-04,\n",
      "        2.9659e-04, 5.2547e-04, 5.4884e-04, 3.6478e-04, 2.6274e-04, 1.5879e-04,\n",
      "        1.2910e-04, 2.7966e-04, 1.2076e-04, 7.7665e-05, 1.1520e-03, 7.4577e-04,\n",
      "        1.4913e-04, 2.5415e-04, 2.7156e-04, 7.9215e-05, 1.5628e-04, 1.2803e-04,\n",
      "        1.3363e-04, 7.1144e-04, 8.5974e-04, 1.5807e-04, 1.4582e-03, 2.0771e-03,\n",
      "        8.2827e-04, 2.2430e-03, 1.9002e-04, 8.5831e-04, 1.6718e-03, 8.2350e-04,\n",
      "        5.8413e-04, 9.0981e-04, 1.4353e-03, 7.7677e-04, 3.8981e-04, 4.1914e-04,\n",
      "        1.1435e-03, 9.6846e-04, 1.7166e-03, 4.6577e-03, 3.8891e-03, 6.9580e-03,\n",
      "        1.2035e-03, 1.8847e-04, 5.5237e-03, 4.4823e-03, 2.6226e-03, 2.6951e-03,\n",
      "        1.9112e-03, 3.2730e-03, 3.0212e-03, 4.7455e-03, 8.7166e-04, 4.2686e-03,\n",
      "        5.9319e-03, 1.6670e-03, 1.6413e-03, 7.4244e-04, 1.6441e-03, 5.7030e-03,\n",
      "        1.8330e-03, 3.5877e-03, 1.1778e-03, 2.5539e-03, 2.5024e-03, 1.3218e-03,\n",
      "        1.3342e-03, 3.2673e-03, 2.2297e-03, 4.2534e-03, 5.7831e-03, 3.1242e-03,\n",
      "        5.6229e-03, 4.8409e-03, 7.9060e-04, 4.2820e-04, 3.2902e-03, 4.1771e-03,\n",
      "        2.5387e-03, 2.7294e-03, 5.1155e-03, 4.3526e-03, 2.0504e-03, 1.6298e-03,\n",
      "        2.7885e-03, 4.1008e-04, 1.9836e-03, 1.7691e-03, 3.5229e-03, 6.7139e-03,\n",
      "        3.4447e-03, 4.1771e-03, 1.1299e-02, 6.9809e-03, 1.7300e-03, 2.6360e-03,\n",
      "        2.4147e-03, 2.6150e-03, 4.8523e-03, 2.3098e-03, 3.5152e-03, 5.5122e-03,\n",
      "        2.5101e-03, 1.1778e-03, 2.2354e-03, 3.0136e-03, 2.9259e-03, 9.7036e-04,\n",
      "        4.5280e-03, 1.1616e-03, 5.8508e-04, 3.2806e-03, 3.4447e-03, 2.2755e-03,\n",
      "        4.5204e-03, 5.7144e-03, 6.2675e-03, 3.5954e-03, 6.4735e-03, 3.0041e-03,\n",
      "        1.2779e-02, 7.1335e-03, 2.4490e-03, 6.1951e-03, 2.5146e-02, 6.7101e-03,\n",
      "        4.6501e-03, 1.5926e-03, 3.2043e-03, 7.3166e-03, 7.6866e-03, 1.1454e-03,\n",
      "        7.5340e-03, 4.7455e-03, 1.6108e-03, 4.2763e-03, 2.5024e-03, 6.7997e-04,\n",
      "        3.5019e-03, 2.5024e-03, 8.8348e-03, 2.1118e-02, 3.9177e-03, 2.2602e-03,\n",
      "        5.5885e-03, 3.4008e-03, 6.7101e-03, 5.6992e-03, 6.0081e-03, 4.4212e-03,\n",
      "        3.4447e-03, 9.8267e-03, 1.1879e-02, 6.0196e-03, 2.5387e-03, 8.0338e-03,\n",
      "        1.3809e-02, 5.2376e-03, 2.3594e-03, 3.2578e-03, 2.7084e-03, 9.7504e-03,\n",
      "        1.4969e-02, 5.6992e-03, 3.4275e-03, 5.3864e-03, 4.8561e-03, 4.6692e-03,\n",
      "        4.7188e-03], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "i = -1\n",
    "for j in range(len(outputs.hidden_states)):\n",
    "    print(f\" Number of elements : {len(outputs.hidden_states[j][1:])}\")\n",
    "    print(f\"Hidden states shape for generated token [{j}] : {outputs.hidden_states[j][i].shape}\")\n",
    "    #print(f\"Some values: {outputs.hidden_states[j][i][:5]}\")\n",
    "    \n",
    "    print(f\"Attention weights shape for generated token [{j}] : {outputs.attentions[j][i].shape}\")\n",
    "    print(f\"Last layer attentions for generated token [{j}] : {outputs.attentions[j][i][0,-1,-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.3076,  1.5225, -0.2357,  ...,  0.3049, -1.8154, -2.0996],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.2549, -1.9189,  2.3262,  ...,  4.1211,  0.2391, -4.5898],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-7.0078,  3.7402,  1.7207,  ...,  2.6328, -0.7285, -1.0400],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-6.5820,  2.1250, -0.2280,  ...,  1.4365, -1.4990, -2.0879],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.5742,  2.7383, -4.1289,  ...,  2.5371,  2.4668, -3.2344],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.5391,  1.0801, -3.5723,  ...,  4.4727,  1.9434, -0.9648],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.5215, -0.0200, -5.4922,  ...,  1.8125,  2.1230, -1.4883],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.4434,  0.4512,  1.7158,  ...,  0.8379,  0.0867,  0.3909],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.3223, -1.1777, -3.2461,  ...,  1.0430,  3.0293, -1.5127],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.9727, -1.4287, -2.3398,  ...,  1.8730,  1.6533, -1.0537],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.4463,  0.7534,  1.9600,  ...,  1.0938, -1.1367, -1.4834],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.6343, -2.5859,  0.9258,  ...,  0.4902,  4.3398, -1.4443],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.9287, -4.7656, -0.8276,  ..., -2.0215, -1.9766, -3.1172],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.6963, -0.2695, -2.8574,  ..., -2.5547, -1.2734, -2.2168],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.5391,  0.5972, -2.7539,  ..., -0.2686, -3.2188, -0.7300],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.5996, -3.1641, -4.8945,  ..., -4.7812,  0.1826, -1.7715],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-7.3984, -2.8535, -3.5840,  ..., -1.9863,  1.5068, -1.7588],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.4141, -3.5410, -3.7188,  ..., -2.5586,  0.2822,  1.7676],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.2285, -0.9780, -0.1407,  ..., -4.4102, -1.1562,  4.1875],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.2715, -0.4407,  0.7817,  ..., -1.1357, -2.1699, -2.3613],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.0586,  1.3516,  1.2939,  ...,  0.5195, -2.7871, -2.2871],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.1377,  1.9268,  2.9707,  ..., -3.3809,  1.8994,  2.0742],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.0547, -5.5703,  4.5195,  ..., -0.7656, -1.6641, -2.8633],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.5508, -0.4905,  0.0657,  ..., -3.2578, -2.5918, -2.2422],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.9102,  1.7988,  0.5200,  ..., -1.9727, -5.4023,  0.1334],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.8867, -1.9268,  2.7070,  ..., -2.5332, -0.4189, -1.2383],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.7422,  0.3425, -0.1503,  ..., -1.3164, -0.9033,  0.8008],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.8184, -2.6621, -1.9766,  ..., -0.8804, -3.0293, -1.2910],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.8779, -0.8716,  0.8452,  ...,  3.1797, -1.8086, -0.6890],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.9292,  0.2032,  0.4106,  ..., -0.6846, -2.8008,  1.9014],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.9600, -3.0078,  3.6641,  ..., -1.1631,  0.3533, -1.3340],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.8359, -1.1426,  0.4841,  ...,  1.9053,  1.6807, -4.0273],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.3047,  1.0176,  0.2450,  ..., -2.5137,  2.9219, -0.6982],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-6.3594,  1.7549,  2.4277,  ..., -3.1191,  1.2510, -2.6484],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.1265,  1.0459,  2.5840,  ...,  1.4814,  0.3801, -2.9375],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.1895, -2.5020,  2.6777,  ..., -0.8120,  3.0742, -3.5273],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.9814, -1.3760,  4.8633,  ...,  0.4253,  1.5146, -1.7998],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.0762,  0.0549,  3.6582,  ..., -0.4260,  0.2291,  1.1533],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.6738,  1.4707,  6.6328,  ...,  0.4915,  0.1087, -3.7539],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.4309, -2.9922,  3.4727,  ..., -1.6445,  2.1074, -0.7944],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 2.2285, -1.0947,  0.8701,  ..., -1.3906,  2.7598,  1.4863],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.4590, -0.5532, -0.8560,  ..., -0.6216,  0.6089, -1.6025],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.3545, -2.7949, -0.2600,  ..., -3.4023,  3.4473, -1.9932],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.3281, -4.3398, -0.7739,  ..., -2.7051,  0.8633, -2.3125],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.6406, -1.1768,  1.5977,  ..., -2.5879, -1.2021, -1.2070],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.8525, -3.0195,  1.7188,  ..., -2.3398, -1.6855, -1.1035],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.2715, -2.6426, -2.1152,  ..., -1.4219, -1.1562,  1.5566],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.3770, -1.3818, -5.3750,  ..., -2.6934,  0.5229, -0.0135],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.2109, -1.2305, -0.5381,  ..., -2.6934,  1.6631, -2.8633],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-6.2188, -2.4121, -0.0859,  ..., -1.5654,  0.1320, -1.0381],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-6.4883, -1.8877, -1.4756,  ..., -1.4688, -0.7788, -2.0332],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-7.4102,  0.2302,  0.9585,  ..., -0.4414, -2.2441, -1.0068],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.7227, -1.9961,  4.6680,  ...,  0.1160,  0.1740,  2.0684],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.1514,  0.2384,  0.9751,  ...,  0.1174,  3.1777,  1.6484],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.6621,  0.0397, -3.4980,  ..., -2.5801, -0.0790,  0.3547],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.8809, -1.1904, -0.1937,  ..., -1.5352,  1.2090,  1.5947],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.7803,  0.4578, -1.2949,  ..., -1.3398, -0.0079,  2.1621],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.1719, -0.5981,  0.1575,  ..., -0.7808,  0.2837,  0.9277],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.9941, -0.8208, -0.3384,  ..., -0.2319, -0.6260,  0.7515],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-6.0156, -2.0605,  0.0754,  ...,  2.0273, -0.8433,  1.6143],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-6.7812,  2.1387, -1.3896,  ..., -0.4919, -1.4453,  1.0020],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-5.3945, -0.2434, -1.5674,  ..., -1.2510, -0.6895,  0.0494],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.4619, -4.7852,  2.1797,  ...,  0.9707,  3.7676,  0.2285],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.5688, -2.3281,  4.3242,  ...,  3.2422, -0.5859, -1.5488],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.6875,  0.2435,  3.1211,  ...,  0.6953, -1.6387, -1.6602],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.8301, -3.7461,  3.8398,  ..., -0.9165,  0.4050, -1.7559],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.1243, -1.7295,  3.1113,  ...,  1.2432, -0.6025, -2.2949],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.0128, -1.3623,  3.4434,  ..., -1.2266, -1.8721, -0.2632],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.0684, -2.2812,  4.2109,  ..., -2.1953,  1.5117, -1.8838],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.5762e+00,  2.0918e+00,  3.4629e+00,  ..., -9.7754e-01,\n",
      "        -1.6918e-03, -1.7637e+00], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.0234,  1.6865,  5.8047,  ...,  0.4756,  0.2605, -3.4922],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.0039, -2.6895,  1.0352,  ..., -1.5811,  2.6953, -2.2090],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.5859, -1.0498, -0.2152,  ..., -1.0693,  1.7959,  0.2886],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.8711, -0.8037,  1.6367,  ..., -1.0967,  2.2793, -5.2852],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.6650, -4.1680,  0.7437,  ..., -1.0137,  2.0996, -2.7754],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.5586,  1.4629,  1.1436,  ..., -0.6953,  2.0195, -3.4355],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.1855,  0.3027,  0.1482,  ..., -0.3186,  2.9941, -4.3438],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.7031, -2.9570,  2.1641,  ...,  0.1713,  5.9219, -3.5801],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.8066, -1.3105,  3.0078,  ..., -0.5601,  4.0430, -3.4492],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.8740,  1.8145,  3.9609,  ..., -2.8242,  0.7847, -2.2793],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.6035,  0.3999,  1.2803,  ..., -0.0886,  2.6562, -2.6289],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.2612,  0.1243,  0.9814,  ...,  0.0905, -0.9448, -0.1562],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.2051,  1.8389, -1.8457,  ..., -1.8359, -1.7861, -0.4036],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-4.2461,  0.2174,  2.5938,  ...,  2.1621, -0.1896,  4.4492],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.7031, -2.1699,  0.8706,  ..., -2.3672, -1.2949, -1.6504],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.9189, -3.2480,  2.9141,  ..., -2.8008, -1.6387, -0.4895],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 2.0771e-03,  9.6338e-01,  8.3496e-01,  ..., -2.9609e+00,\n",
      "        -7.7783e-01, -1.3291e+00], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.5391,  0.6147,  0.9639,  ..., -2.6934, -3.4102, -0.8838],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.1948,  3.0762, -0.8740,  ..., -1.0801, -2.1270,  1.4014],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.3643,  2.1016,  0.8892,  ...,  0.0939, -1.2178,  1.9844],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.5635,  0.2361, -0.0978,  ...,  3.4082, -1.6660,  0.5376],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.9487, -3.2246,  1.5088,  ...,  3.2578,  0.9028, -1.2373],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.1436, -3.1562,  2.9238,  ..., -1.9180,  2.4473, -1.8984],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 4.2617, -2.8574,  1.8193,  ..., -2.5410,  4.6055, -2.0684],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 2.2656, -0.4094, -0.1030,  ..., -3.2871,  1.8623, -3.0742],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.1289,  4.2344,  0.3423,  ..., -4.1797, -3.1719, -1.2051],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 0.6631, -2.9121,  0.7207,  ..., -0.7339,  1.4531, -2.4141],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 3.6582, -0.6328,  1.2441,  ..., -0.3125,  2.4043, -1.4258],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.5439,  5.4258,  2.9160,  ..., -1.3877,  1.6621,  0.0980],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.6953,  0.8047,  4.5430,  ...,  0.2695, -0.6748, -3.9160],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.5635,  1.4248,  3.3203,  ...,  0.2156,  0.7422, -2.0117],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.0410,  1.5459,  3.1621,  ...,  0.5132,  0.5718, -1.0352],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.3997, -3.7051, -0.0638,  ..., -2.0156,  3.1035, -1.0020],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 2.5723, -0.3030,  0.0379,  ..., -2.2461,  3.6758, -0.7148],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.1328,  1.6318, -2.3496,  ..., -1.9561,  0.3452, -2.0898],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.0525,  1.8994,  2.2539,  ..., -1.4502,  1.6406,  0.3975],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.4492e+00,  7.8174e-01,  1.7129e+00,  ..., -1.8433e-01,\n",
      "        -1.2949e+00,  1.4486e-03], device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.1296, -2.8809,  1.5576,  ..., -0.3076, -2.5391,  2.6621],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.9727,  0.8667, -0.9121,  ..., -2.0898,  2.8770, -2.0684],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.1631, -3.4551, -1.4941,  ..., -0.4163,  2.1250,  0.5889],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.8613,  0.4109,  3.9746,  ...,  1.2646, -1.9404, -0.5991],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.9751, -0.4839,  0.2986,  ..., -1.6504, -1.5342, -1.7490],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.4307,  0.0390, -0.7905,  ..., -0.5679,  1.1523, -3.0410],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-0.3335, -1.4658,  2.5664,  ..., -0.3972,  1.3818, -1.4473],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.1895, -2.3867,  3.3672,  ..., -2.5410,  0.4016, -0.8062],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.5527,  0.2426,  2.3066,  ..., -2.2637,  0.9365, -2.4316],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.4141, -1.8574,  0.8262,  ...,  3.7754,  1.1006, -3.0645],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.0586,  2.0820, -1.4502,  ..., -1.8857,  1.1533, -3.8164],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.8145,  4.2500, -1.3086,  ..., -0.4788, -2.9609, -1.4619],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.6113,  1.0957,  1.3486,  ..., -0.0154, -1.7979, -1.8164],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.6777, -2.6797,  3.1191,  ..., -3.2617, -1.5420, -3.9336],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.9102, -0.6211,  3.7852,  ..., -3.9434, -1.0430, -3.2227],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-3.9805,  0.4238,  2.0938,  ..., -2.5449, -1.5107, -1.2988],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.0391, -0.8486,  1.3643,  ...,  1.3066,  0.8457, -4.0078],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.2666, -1.8604,  0.5059,  ..., -1.6289,  0.1934, -0.9243],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([ 1.3389, -1.7520, -1.1240,  ..., -2.8379,  1.2705, -1.3516],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-2.0000, -3.7148, -0.4312,  ..., -1.3418,  0.9731, -1.4033],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      " Number of elements : 33\n",
      "Last hidden state : tensor([-1.3877, -0.5591,  1.8340,  ..., -2.9414, -3.1289,  0.8594],\n",
      "       device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "j = 1\n",
    "i = -1\n",
    "for j in range(len(outputs.hidden_states)):\n",
    "    print(f\" Number of elements : {len(outputs.hidden_states[j])}\")\n",
    "    #print(f\"Hidden states shape for generated token [{j}] : {outputs.hidden_states[j][i].shape}\")\n",
    "    print(f\"Last hidden state : {outputs.hidden_states[j][i][0,-1,:4096]}\")\n",
    "    #print(f\"Some values: {outputs.hidden_states[j][i][:5]}\")\n",
    "    \n",
    "    #print(f\"Attention weights shape for generated token [{j}] : {outputs.attentions[j][i].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   32,  3488,   430,  5334,   311,   279,  4851,   315,   279,  5030,\n",
       "           482,   477,  1288,   358,  2019,    11,   279, 23152,    30,   353,\n",
       "           331, 84796, 22242, 31140,    11,   358,  2011,  6179,   499,   430,\n",
       "           433,   596,   539,  3284,   369,   264,  3823,   311,  8343,   264,\n",
       "         36125,   304,   832, 11961,    13, 16183, 24904,   388,   527,  6485,\n",
       "         12933,  1903,   315,  9501,    11, 12466,    11,   323,  1023,  7384,\n",
       "            11,   539, 67740,  3673,    13,   763,  2144,    11, 19969,   311,\n",
       "         25024,   264, 36125,  1053,   387,  5115,  1131,   359, 38128,    11,\n",
       "           311,  2019,   279,  3325,   382, 11458,    11,   422,   584,  2351,\n",
       "          7556,   922,   264, 59159, 15398,  1405,   264,  3823,  1436, 17354,\n",
       "         78825, 21552,   264, 36125,    11,   358,  4265, 16430,   279,  4320,\n",
       "           311,   387,   330, 14486,  1210,  3011,   596,  1314,    11,  7315,\n",
       "         59432,   649,   387, 35661,   304,   832, 11961,  1606]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['sequences'][:,nb_tokens_in:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (1): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (2): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (3): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (4): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (5): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (6): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (7): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (8): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (9): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (10): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (11): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (12): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (13): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (14): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (15): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (16): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (17): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (18): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (19): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (20): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (21): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (22): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (23): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (24): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (25): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (26): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (27): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (28): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (29): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (30): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (31): LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(laughs) Ah, I think we have a case of \"fowl\" humor here! I'm happy to help, but I have to say, eating a helicopter is quite an...unconventional question.\n",
      "\n",
      "As a researcher,\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs['sequences'][0][nb_tokens_in:], skip_special_tokens=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ktit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
